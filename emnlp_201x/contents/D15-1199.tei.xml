<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Milica</roleName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaši´</forename><surname>Gaši´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><forename type="middle">Mrkši´</forename><surname>Mrkši´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cambridge University Engineering Department</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usabil-ity and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion , and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The natural language generation (NLG) compo- nent provides much of the persona of a spoken dialogue system (SDS), and it has a significant impact on a user's impression of the system. As noted in <ref type="bibr" target="#b19">Stent et al. (2005)</ref>, a good generator usu- ally depends on several factors: adequacy, flu- ency, readability, and variation. Previous ap- proaches attacked the NLG problem in different ways. The most common and widely adopted today is the rule-based (or template-based) ap- proach <ref type="bibr">(Cheyer and Guzzoni, 2007;</ref><ref type="bibr">Mirkovic and Cavedon, 2011</ref>). Despite its robustness and ade- quacy, the frequent repetition of identical, rather stilted, output forms make talking to a rule-based generator rather tedious. Furthermore, the ap- proach does not easily scale to large open domain systems( <ref type="bibr" target="#b28">Young et al., 2013;</ref><ref type="bibr" target="#b0">Gaši´Gaši´c et al., 2014;</ref>). Hence approaches to NLG are required that can be readily scaled whilst meeting the above requirements.</p><p>The trainable generator approach exemplified by the HALOGEN ( <ref type="bibr">Langkilde and Knight, 1998)</ref> and SPaRKy system <ref type="bibr" target="#b18">(Stent et al., 2004</ref>) provides a possible way forward. These systems include specific trainable modules within the generation framework to allow the model to adapt to different domains ( <ref type="bibr" target="#b24">Walker et al., 2007)</ref>, or reproduce cer- tain style ( <ref type="bibr">Mairesse and Walker, 2011</ref>). However, these approaches still require a handcrafted gen- erator to define the decision space within which statistics can be used for optimisation. The result- ing utterances are therefore constrained by the pre- defined syntax and any domain-specific colloquial responses must be added manually.</p><p>More recently, corpus-based methods <ref type="bibr">(Oh and Rudnicky, 2000;</ref><ref type="bibr">Mairesse and Young, 2014;</ref><ref type="bibr" target="#b25">Wen et al., 2015)</ref> have received attention as access to data becomes increasingly available. By defin- ing a flexible learning structure, corpus-based methods aim to learn generation directly from data by adopting an over-generation and rerank- ing paradigm <ref type="bibr">(Oh and Rudnicky, 2000</ref>), in which final responses are obtained by reranking a set of candidates generated from a stochastic generator. Learning from data directly enables the system to mimic human responses more naturally, removes the dependency on predefined rules, and makes the system easier to build and extend to other do- mains. As detailed in Sections 2 and 3, however, these existing approaches have weaknesses in the areas of training data efficiency, accuracy and nat- uralness. This paper presents a statistical NLG based on a semantically controlled Long Short-term Mem- ory (LSTM) recurrent network. It can learn from unaligned data by jointly optimising its sentence planning and surface realisation components us- ing a simple cross entropy training criterion with- out any heuristics, and good quality language vari- ation is obtained simply by randomly sampling the network outputs. We start in Section 3 by defining the framework of the proposed neural lan- guage generator. We introduce the semantically controlled LSTM (SC-LSTM) cell in Section 3.1, then we discuss how to extend it to a deep structure in Section 3.2. As suggested in <ref type="bibr" target="#b25">Wen et al. (2015)</ref>, a backward reranker is introduced in Section 3.3 to improve fluency. Training and decoding details are described in Section 3.4 and 3.5.</p><p>Section 4 presents an evaluation of the proposed approach in the context of an application provid- ing information about venues in the San Francisco area. In Section 4.2, we first show that our genera- tor outperforms several baselines using objective metrics. We experimented on two different on- tologies to show not only that good performance can be achieved across domains, but how easy and quick the development lifecycle is. In order to as- sess the subjective performance of our system, a quality test and a pairwise preference test are pre- sented in Section 4.3. The results show that our approach can produce high quality utterances that are considered to be more natural and are preferred to previous approaches. We conclude with a brief summary and future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Conventional approaches to NLG typically divide the task into sentence planning and surface real- isation. Sentence planning maps input semantic symbols into an intermediary form representing the utterance, e.g. a tree-like or template struc- ture, then surface realisation converts the interme- diate structure into the final text ( <ref type="bibr" target="#b23">Walker et al., 2002;</ref><ref type="bibr" target="#b18">Stent et al., 2004</ref>). Although statistical sen- tence planning has been explored previously, for example, generating the most likely context-free derivations given a corpus <ref type="bibr">(Belz, 2008)</ref> or max- imising the expected reward using reinforcement learning ( <ref type="bibr" target="#b14">Rieser and Lemon, 2010)</ref>, these meth- ods still rely on a pre-existing, handcrafted gener- ator. To minimise handcrafting, <ref type="bibr" target="#b17">Stent and Molina (2009)</ref> proposed learning sentence planning rules directly from a corpus of utterances labelled with Rhetorical Structure Theory (RST) discourse rela- tions ( <ref type="bibr">Mann and Thompson, 1988)</ref>. However, the required corpus labelling is expensive and addi- tional handcrafting is still needed to map the sen- tence plan to a valid syntactic form.</p><p>As noted above, corpus-based NLG aims at learning generation decisions from data with min- imal dependence on rules and heuristics. A pi- oneer in this direction is the class-based n-gram language model (LM) approach proposed by <ref type="bibr">Oh and Rudnicky (2000)</ref>. <ref type="bibr" target="#b13">Ratnaparkhi (2002)</ref> later addressed some of the limitations of class-based LMs in the over-generation phase by using a mod- ified generator based on a syntactic dependency tree. <ref type="bibr">Mairesse and Young (2014)</ref> proposed a phrase-based NLG system based on factored LMs that can learn from a semantically aligned corpus. Although active learning ( <ref type="bibr">Mairesse et al., 2010)</ref> was also proposed to allow learning online directly from users, the requirement for human annotated alignments limits the scalability of the system. Another similar approach casts NLG as a template extraction and matching problem, e.g., <ref type="bibr">Angeli et al. (2010)</ref> train a set of log-linear models to make a series of generation decisions to choose the most suitable template for realisation. <ref type="bibr" target="#b11">Kondadadi et al. (2013)</ref> later show that the outputs can be further improved by an SVM reranker making them com- parable to human-authored texts. However, tem- plate matching approaches do not generalise well to unseen combinations of semantic elements.</p><p>The use of neural network-based (NN) ap- proaches to NLG is relatively unexplored. The stock reporter system ANA by <ref type="bibr" target="#b12">Kukich (1987)</ref> is perhaps the first NN-based generator, although generation was only done at the phrase level. Re- cent advances in recurrent neural network-based language models (RNNLM) ( <ref type="bibr">Mikolov et al., 2010;</ref><ref type="bibr">Mikolov et al., 2011a</ref>) have demonstrated the value of distributed representations and the ability to model arbitrarily long dependencies. Sutskever et al. (2011) describes a simple variant of the RNN that can generate meaningful sentences by learn- ing from a character-level corpus. More recently, <ref type="bibr" target="#b10">Karpathy and Fei-Fei (2014)</ref> have demonstrated that an RNNLM is capable of generating image descriptions by conditioning the network model on a pre-trained convolutional image feature rep- resentation. <ref type="bibr" target="#b30">Zhang and Lapata (2014)</ref> also de- scribes interesting work using RNNs to generate Chinese poetry. A forerunner of the system pre- sented here is described in <ref type="bibr" target="#b25">Wen et al. (2015)</ref>, in which a forward RNN generator, a CNN reranker, and a backward RNN reranker are trained jointly to generate utterances. Although the system was easy to train and extend to other domains, a heuris- tic gate control was needed to ensure that all of the attribute-value information in the system's re- sponse was accurately captured by the generated utterance. Furthermore, the handling of unusual slot-value pairs by the CNN reranker was rather arbitrary. In contrast, the LSTM-based system de- scribed in this paper can deal with these problems automatically by learning the control of gates and surface realisation jointly.</p><p>Training an RNN with long range dependencies is difficult because of the vanishing gradient prob- lem ( <ref type="bibr">Bengio et al., 1994)</ref>. Hochreiter and Schmid- huber (1997) mitigated this problem by replacing the sigmoid activation in the RNN recurrent con- nection with a self-recurrent memory block and a set of multiplication gates to mimic the read, write, and reset operations in digital computers. The re- sulting architecture is dubbed the Long Short-term Memory (LSTM) network. It has been shown to be effective in a variety of tasks, such as speech recognition ( <ref type="bibr" target="#b4">Graves et al., 2013b</ref>), handwriting recognition ( <ref type="bibr" target="#b2">Graves et al., 2009)</ref>, spoken language understanding ( <ref type="bibr" target="#b27">Yao et al., 2014)</ref>, and machine translation ( . Recent work by <ref type="bibr" target="#b5">Graves et al. (2014)</ref> has demonstrated that an NN structure augmented with a carefully designed memory block and differentiable read/write op- erations can learn to mimic computer programs. Moreover, the ability to train deep networks pro- vides a more sophisticated way of exploiting rela- tions between labels and features, therefore mak- ing the prediction more accurate <ref type="bibr" target="#b8">(Hinton et al., 2012)</ref>. By extending an LSTM network to be both deep in space and time, <ref type="bibr" target="#b6">Graves (2013)</ref> shows the resulting network can used to synthesise handwrit- ing indistinguishable from that of a human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Neural Language Generator</head><p>The generation model proposed in this paper is based on a recurrent NN architecture ( <ref type="bibr">Mikolov et al., 2010</ref>) in which a 1-hot encoding w t of a token 1 w t is input at each time step t conditioned on a re- <ref type="bibr">1</ref> We use token instead of word because our model operates on text for which slot values are replaced by its corresponding slot tokens. We call this procedure delexicalisation. current hidden layer h t and outputs the probability distribution of the next token w t+1 . Therefore, by sampling input tokens one by one from the output distribution of the RNN until a stop sign is gen- erated ( <ref type="bibr" target="#b10">Karpathy and Fei-Fei, 2014</ref>) or some con- straint is satisfied ( <ref type="bibr" target="#b30">Zhang and Lapata, 2014</ref>), the network can produce a sequence of tokens which can be lexicalised 2 to form the required utterance. Long Short-term Memory <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) is a recurrent NN architecture which uses a vector of memory cells c t ∈ R n and a set of elementwise multiplication gates to control how information is stored, forgotten, and exploited inside the network. Of the various different con- nectivity designs for an LSTM cell <ref type="bibr" target="#b6">(Graves, 2013;</ref><ref type="bibr" target="#b29">Zaremba et al., 2014</ref>), the architecture used in this paper is illustrated in <ref type="figure">Figure 3</ref>.1 and defined by the following equations,,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Controlled LSTM cell</head><formula xml:id="formula_0">i t = σ(W wi w t + W hi h t−1 ) (1) f t = σ(W wf w t + W hf h t−1 ) (2) o t = σ(W wo w t + W ho h t−1 )<label>(3)</label></formula><formula xml:id="formula_1">ˆ c t = tanh(W wc w t + W hc h t−1 )<label>(4)</label></formula><formula xml:id="formula_2">c t = f t c t−1 + i t ˆ c t (5) h t = o t tanh(c t ) (6)</formula><p>where σ is the sigmoid function, i t , f t , o t ∈ [0, 1] n are input, forget, and output gates respectively, andˆc andˆ andˆc t and c t are proposed cell value and true cell value at time t. Note that each of these vectors has a dimension equal to the hidden layer h. In order to ensure that the generated utter- ance represents the intended meaning, the gen- erator is further conditioned on a control vec- tor d, a 1-hot representation of the dialogue act (DA) type and its slot-value pairs. Although a re- lated work <ref type="bibr" target="#b10">(Karpathy and Fei-Fei, 2014</ref>) has sug- gested that reapplying this auxiliary information to the RNN at every time step can increase perfor- mance by mitigating the vanishing gradient prob- lem ( <ref type="bibr">Mikolov and Zweig, 2012;</ref><ref type="bibr">Bengio et al., 1994)</ref>, we have found that such a model also omits and duplicates slot information in the surface re- alisation. In <ref type="bibr" target="#b25">Wen et al. (2015)</ref> simple heuristics are used to turn off slot feature values in the con- trol vector d once the corresponding slot token has been generated. However, these heuristics can only handle cases where slot-value pairs can be identified by exact matching between the delexi- calised surface text and the slot value pair encoded in d. Cases such as binary slots and slots that take don't care values cannot be explicitly delexicalised in this way and these cases frequently result in generation errors.</p><p>To address this problem, an additional control cell is introduced into the LSTM to gate the DA as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This cell plays the role of sentence planning since it manipulates the DA features during the generation process in order to produce a surface realisation which accurately en- codes the input information. We call the result- ing architecture Semantically Controlled LSTM (SC-LSTM). Starting from the original DA 1-hot vector d 0 , at each time step the DA cell decides what information should be retained for future time steps and discards the others,</p><formula xml:id="formula_3">r t = σ(W wr w t + αW hr h t−1 )<label>(7)</label></formula><formula xml:id="formula_4">d t = r t d t−1<label>(8)</label></formula><p>where r t ∈ [0, 1] d is called the reading gate, and α is a constant. Here W wr and W hr act like key- word and key phrase detectors that learn to asso- ciate certain patterns of generated tokens with cer- tain slots. <ref type="figure">Figure 3</ref> gives an example of how these detectors work in affecting DA features inside the network. Equation 5 is then modified so that the cell value c t also depends on the DA,</p><formula xml:id="formula_5">c t = f t c t−1 + i t ˆ c t + tanh(W dc d t ) (9)</formula><p>After updating Equation 6 by Equation 9, the out- put distribution is formed by applying a softmax function g, and the distribution is sampled to ob- tain the next token,</p><formula xml:id="formula_6">P (w t+1 |w t , w t−1 , ...w 0 , d t ) = g(W ho h t ) (10) w t+1 ∼ P (w t+1 |w t , w t−1 , ...w 0 , d t ).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Deep Structure</head><p>Deep Neural Networks (DNN) enable increased discrimination by learning multiple layers of fea- tures, and represent the state-of-the-art for many applications such as speech recognition ( <ref type="bibr" target="#b4">Graves et al., 2013b</ref>) and natural language processing <ref type="bibr">(Collobert and Weston, 2008)</ref>. The neural language generator proposed in this paper can be easily ex- tended to be deep in both space and time by stack- ing multiple LSTM cells on top of the original structure. As shown in <ref type="figure">Figure 2</ref>, skip connections are applied to the inputs of all hidden layers as well as between all hidden layers and the outputs <ref type="bibr" target="#b6">(Graves, 2013)</ref>. This reduces the number of pro- cessing steps between the bottom of the network and the top, and therefore mitigates the vanishing gradient problem ( <ref type="bibr">Bengio et al., 1994</ref>) in the ver- tical direction. To allow all hidden layer informa- tion to influence the reading gate, Equation 7 is changed to</p><formula xml:id="formula_7">r t = σ(W wr w t + l α l W l hr h l t−1 )<label>(12)</label></formula><p>where l is the hidden layer index and α l is a layer-wise constant. Since the network tends to overfit when the structure becomes more complex, the dropout technique ( <ref type="bibr" target="#b16">Srivastava et al., 2014</ref>) is used to regularise the network. As suggested in ( <ref type="bibr" target="#b29">Zaremba et al., 2014</ref>), dropout was only applied to the non-recurrent connections, as shown in the <ref type="figure">Figure 2</ref>. It was not applied to word embeddings since pre-trained word vectors were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Backward LSTM reranking</head><p>One remaining problem in the structure described so far is that the LSTM generator selects words based only on the preceding history, whereas some sentence forms depend on the backward context. Previously, bidirectional networks (Schuster and <ref type="figure">Figure 2</ref>: The Deep LSTM generator structure by stacking multiple LSTM layers on top of the DA cell. The skip connection was adopted to mitigate the vanishing gradient, while the dropout was applied on dashed connections to prevent co-adaptation and overfitting.</p><p>Paliwal, 1997) have been shown to be effective for sequential problems ( <ref type="bibr" target="#b3">Graves et al., 2013a;</ref><ref type="bibr" target="#b20">Sundermeyer et al., 2014</ref>). However, applying a bidirec- tional network directly in the SC-LSTM generator is not straightforward since the generation process is sequential in time. Hence instead of integrating the bidirectional information into one network, we trained another SC-LSTM from backward context to choose best candidates from the forward gen- erator outputs. In our experiments, we also found that by tying the keyword detector weights W wr (see Equations 7 and 12) of both the forward and backward networks together makes the generator less sensitive to random initialisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>The forward generator and the backward reranker were both trained by treating each sentence as a mini-batch. The objective function was the cross entropy error between the predicted word distri- bution p t and the actual word distribution y t in the training corpus. An l 2 regularisation term was added to the objective function for every 10 training examples as suggested in <ref type="bibr">Mikolov et al. (2011b)</ref>. However, further regularisation was re- quired for the reading gate dynamics. This re- sulted in the following modified cost function for each mini-match (ignoring standard l 2 ),</p><formula xml:id="formula_8">F (θ) = t p t log(y t ) + d T + T −1 t=0 ηξ dt+1−dt<label>(13)</label></formula><p>where d T is the DA vector at the last word index T , and η and ξ are constants set to 10 −4 and 100, respectively. The second term is used to penalise generated utterances that failed to render all the re- quired slots, while the third term discourages the network from turning more than one gate off in a single time step. The forward and backward networks were structured to share the same set of word embeddings, initialised with pre-trained word vectors ( <ref type="bibr">Pennington et al., 2014</ref>). The hid- den layer size was set to be 80 for all cases, and deep networks were trained with two hidden lay- ers and a 50% dropout rate. All costs and gradients were computed and stochastic gradient descent was used to optimise the parameters. Both net- works were trained with back propagation through time <ref type="bibr" target="#b26">(Werbos, 1990)</ref>. In order to prevent over- fitting, early stopping was implemented using a held-out validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Decoding</head><p>The decoding procedure is split into two phases: (a) over-generation, and (b) reranking. In the over-generation phase, the forward generator con- ditioned on the given DA, is used to sequentially generate utterances by random sampling of the predicted next word distributions. In the reranking phase, the cost of the backward reranker F b (θ) is computed. Together with the cost F f (θ) from the forward generator, the reranking score R is com-puted as:</p><formula xml:id="formula_9">R = −(F f (θ) + F b (θ) + λERR)<label>(14)</label></formula><p>where λ is a tradeoff constant, and the slot error rate ERR is computed by exact matching the slot tokens in the candidate utterances,</p><formula xml:id="formula_10">ERR = p + q N (15)</formula><p>where N is the total number of slots in the DA, and p, q is the number of missing and redundant slots in the given realisation. Note that the ERR rerank- ing criteria cannot handle arbitrary slot-value pairs such as binary slots or slots that take the don't care value because they cannot be delexicalised and ex- actly matched. λ is set to a large value in order to severely penalise nonsensical outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The target application for our generation system is a spoken dialogue system providing informa- tion about certain venues in San Francisco. In or- der to demonstrate the scalability of the proposed method and its performance in different domains, we tested on two domains that talk about restau- rants and hotels respectively. There are 8 system dialogue act types such as inform to present infor- mation about restaurants, confirm to check that a slot value has been recognised correctly, and re- ject to advise that the user's constraints cannot be met. Each domain contains 12 attributes (slots), some are common to both domains and the oth- ers are domain specific. The detailed ontologies for the two domains are provided in <ref type="table">Table 1</ref> The system was implemented using the Theano library ( <ref type="bibr">Bergstra et al., 2010;</ref><ref type="bibr">Bastien et al., 2012)</ref>, and trained by partitioning each of the collected corpus into a training, validation, and testing set in the ratio 3:1:1. The frequency of each ac- tion type and slot-value pair differs quite markedly across the corpus, hence up-sampling was used to make the corpus more uniform. Since our gener- ator works stochastically and the trained networks can differ depending on the initialisation, all the results shown below 3 were averaged over 5 ran- domly initialised networks. For each DA, we over- generated 20 utterances and selected the top 5 real- isations after reranking. The BLEU-4 metric was used for the objective evaluation ( <ref type="bibr">Papineni et al., 2002</ref>). Multiple references for each test DA were obtained by mapping them back to the distinct set of DAs, grouping those delexicalised surface forms that have the same DA specification, and then lexicalising those surface forms back to ut- terances. In addition, the slot error rate (ERR) as described in Section 3.5 was computed as an aux- iliary metric alongside the BLEU score. However, for the experiments it is computed at the corpus level, by averaging slot errors over each of the top 5 realisations in the entire corpus. The trade-off weights α between keyword and key phrase detec- tors as mentioned in Section 3.1 and 3.2 were set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Objective Evaluation</head><p>We compared the single layer semantically con- trolled LSTM (sc-lstm) and a deep version with    <ref type="bibr" target="#b0">(Gaši´Gaši´c et al., 2014)</ref>). The kNN was implemented by computing the sim- ilarity of the test DA 1-hot vector against all of the training DA 1-hot vectors, selecting the nearest and then lexicalising to generate the final surface form. The objective results are shown in <ref type="table" target="#tab_2">Table  2</ref>. As can be seen, none of the baseline systems shown in the first block <ref type="bibr">(hdc, kNN, &amp; classlm)</ref> are comparable to the systems described in this paper (sc-lstm &amp; +deep) if both metrics are con- sidered. Setting aside the difficulty of scaling to large domains, the handcrafted generator's (hdc) use of predefined rules yields a fixed set of sen- tence plans, which can differ markedly from the real colloquial human responses collected from AMT, while the class LM approach suffers from inaccurate rendering of information. Although the kNN method provides reasonable adequacy i.e. low ERR, the BLEU is low, probably because of the errors in the collected corpus which kNN can- not handle but statistical approaches such as LMs can by suppressing unlikely outputs. The last three blocks in <ref type="table" target="#tab_2">Table 2</ref> compares the proposed method with previous RNN approaches.    LSTM generally works better than vanilla RNN due to its ability to model long range dependen- cies more efficiently. We also found that by us- ing gates, whether learned or heuristic, gave much lower slot error rates. As an aside, the ability of the SC-LSTM to learn gates is also exemplified in <ref type="figure">Figure 3</ref>. Finally, by combining the learned gate approach with the deep architecture (+deep), we obtained the best overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Informativeness Naturalness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Evaluation</head><p>Since automatic metrics may not consistently agree with human perception <ref type="bibr" target="#b19">(Stent et al., 2005</ref>), human testing is needed to assess subjective qual- ity. To do this, a set of judges were recruited using AMT. For each task, two systems among the four (classlm, rnn w/, sc-lstm, and +deep) were ran- domly selected to generate utterances from a set of newly sampled dialogues in the restaurant domain.</p><p>In order to evaluate system performance in the presence of language variation, each system gen- erated 5 different surface realisations for each in- put DA and the human judges were asked to score each of them in terms of informativeness and nat- uralness (rating out of 3), and also asked to state a preference between the two. Here informativeness is defined as whether the utterance contains all the information specified in the DA, and naturalness is defined as whether the utterance could plausibly have been produced by a human. In order to de- crease the amount of information presented to the judges, utterances that appeared identically in both systems were filtered out. We tested 1000 DAs in total, and after filtering there were approximately 1300 generated utterances per system. <ref type="table" target="#tab_4">Table 3</ref> shows the quality assessments which exhibit the same general trend as the objective re- sults. The SC-LSTM systems (sc-lstm &amp; +deep) outperform the class-based LMs (classlm) and the RNN with heuristic gates (rnn w/) in both metrics. The deep SC-LSTM system (+deep) is signifi- cantly better than the class LMs (classlm) in terms of informativeness, and better than the RNN with heuristic gates (rnn w/) in terms of naturalness. The preference test results are shown in <ref type="table" target="#tab_5">Table 4</ref>. Again, the SC-LSTM systems <ref type="bibr">(sc-lstm &amp; +deep)</ref> were significantly preferred by the judges. More- over, the judges recorded a strong preference for the deep approach (+deep) compared to the others, though the preference is not significant when com- paring to its shallow counterpart (sc-lstm). Exam- ple dialogue acts and their top-5 realisations are shown in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper we have proposed a neural network- based generator that is capable of generating natu- ral linguistically varied responses based on a deep, semantically controlled LSTM architecture which we call SC-LSTM. The generator can be trained on unaligned data by jointly optimising its sen- tence planning and surface realisation components using a simple cross entropy criterion without any heuristics or handcrafting. We found that the SC- LSTM model achieved the best overall perfor- mance on two objective metrics across two differ- ent domains. An evaluation by human judges also confirmed that the SC-LSTM approach is strongly preferred to a variety of existing methods.</p><p>This work represents a line of research that tries # Example Dialogue Acts and Realizations from SF Restaurant Domain 1 inform(name="red door cafe", goodformeal="breakfast", area="cathedral hill", kidsallowed="no") red door cafe is a good restaurant for breakfast in the area of cathedral hill and does not allow children . red door cafe is a good restaurant for breakfast in the cathedral hill area and does not allow children . red door cafe is a good restaurant for breakfast in the cathedral hill area and does not allow kids . red door cafe is good for breakfast and is in the area of cathedral hill and does not allow children . red door cafe does not allow kids and is in the cathedral hill area and is good for breakfast . 2 informonly(name="dosa on fillmore and kiss seafood", pricerange="expensive", near="lower pacific heights") there is no place other than dosa on fillmore and kiss seafood that are expensive near to lower pacific heights . dosa on fillmore and kiss seafood is the only expensive restaurant near lower pacific heights . the only listed restaurant near lower pacific heights in the expensive price range is dosa on fillmore and kiss seafood . i apologize , dosa on fillmore and kiss seafood is the only expensive restaurant near lower pacific heights . i apologize , dosa on fillmore and kiss seafood are the only expensive restaurants near lower pacific heights . # Example Dialogue Acts and Realizations from SF Hotel Domain 3 inform(type="hotel",count="182",dogsallowed="dontcare") there are 182 hotels if you do not care whether dogs are allowed . there are 182 hotels if you do not care whether they allow dogs . 182 hotels are available if dogs allowed or not is not an issue . there are 182 hotels if allowing dogs or not is not an issue . there are 182 hotels if whether dogs are allowed does not matter . 4 informonly(name="red victorian bed breakfast",acceptscreditcards="yes",near="haight",hasinternet="yes") red victorian bed breakfast is the only hotel near haight and accepts credit cards and has internet . red victorian bed breakfast is the only hotel near haight and has internet and accepts credit cards . red victorian bed breakfast is the only hotel near haight that accept credit cards and offers internet . the red victorian bed breakfast has internet and near haight , it does accept credit cards . the red victorian bed breakfast is the only hotel near haight that accepts credit cards , and offers internet . <ref type="table">Table 5</ref>: Samples of top 5 realisations from the deep SC-LSTM (+deep) system output.</p><p>to model the NLG problem in a unified architec- ture, whereby the entire model is end-to-end train- able from data. We contend that this approach can produce more natural responses which are more similar to colloquial styles found in human conver- sations. Another key potential advantage of neu- ral network based language processing is the im- plicit use of distributed representations for words and a single compact parameter encoding of the information to be conveyed. This suggests that it should be possible to further condition the gener- ator on some dialogue features such discourse in- formation or social cues during the conversation. Furthermore, adopting a corpus based regime en- ables domain scalability and multilingual NLG to be achieved with less cost and a shorter lifecycle. These latter aspects will be the focus of our future work in this area. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Semantic Controlled LSTM cell proposed in this paper. The upper part is a traditional LSTM cell in charge of surface realisation, while the lower part is a sentence planning cell based on a sigmoid control gate and a dialogue act (DA).</figDesc><graphic url="image-1.png" coords="3,317.20,206.11,198.42,207.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>Figure 3: Examples showing how the SC-LSTM controls the DA features flowing into the network via its learned semantic gates. Despite errors due to sparse training data for some slots, each gate generally learned to detect words and phrases describing a particular slot-value pair.</figDesc><graphic url="image-4.png" coords="8,67.64,239.15,467.72,143.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Objective evaluation of the top 5 re-
alisations. Except for handcrafted (hdc) and k-
nearest neighbour (kNN) baselines, all the other 
approaches ranked their realisations from 20 over-
generated candidates. 

two hidden layers (+deep) against several base-
lines: the handcrafted generator (hdc), k-nearest 
neighbour (kNN), class-based LMs (classlm) as 
proposed in Oh and Rudnicky (2000), the heuris-
tic gated RNN as described in Wen et al. (2015) 
and a similar LSTM variant (rnn w/ &amp; lstm w/), 
and the same RNN/LSTM but without gates (rnn 
w/o &amp; lstm w/o). The handcrafted generator was 
developed over a long period of time and is the 
standard generator used for trialling end-to-end di-
alogue systems (for example </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Real user trial for utterance quality 
assessment on two metrics (rating out of 3), 
averaging over top 5 realisations. Statistical 
significance was computed using a two-tailed 
Student's t-test, between deep and all others. 

Pref.% classlm rnn w/ sc-lstm +deep 

classlm 
-
46.0 
40.9 ** 37.7 ** 
rnn w/ 
54.0 
-
43.0 
35.7 * 
sc-lstm 
59.1 * 
57 
-
47.6 
+deep 
62.3 ** 
64.3 ** 
52.4 
-

* p &lt; 0.05 ** p &lt; 0.005 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Pairwise preference test among four sys-
tems. Statistical significance was computed using 
two-tailed binomial test. 

</table></figure>

			<note place="foot" n="2"> The process of replacing slot token by its value.</note>

			<note place="foot" n="3"> Except human evaluation, in which only one set of networks was used.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental on-line adaptation of pomdp-based dialogue managers to extended domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on InterSpeech</title>
		<meeting>on InterSpeech</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed dialogue policies for multi-domain statistical dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on ICASSP</title>
		<meeting>on ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>2013 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1303.5778</idno>
		<title level="m">Speech recognition with deep recurrent neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>abs/1410.5401</idno>
		<title level="m">Neural turing machines. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Spoken Language Technology</title>
		<meeting>IEEE Spoken Language Technology</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A statistical nlg framework for aggregated planning and realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kondadadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Howald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schilder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the ACL. Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the ACL. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Where do phrases come from: Some preliminary experiments in connectionist phrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Generation</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Trainable approaches to surface natural language generation and their application to conversational dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural language generation as planning under uncertainty for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Generation</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluating automatic extraction of rules for sentence plan construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGdial</title>
		<meeting>SIGdial</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trainable sentence planning for complex information presentation in spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the ACL</title>
		<meeting>the Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating evaluation methods for generation in the presence of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Singhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CICLing</title>
		<meeting>CICLing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on EMNLP. Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on EMNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training a sentence planner for spoken dialogue using boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marilyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Individual and domain adaptation in sentence planning for dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2007" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGdial</title>
		<meeting>SIGdial</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul J Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spoken language understanding using long short-term memory neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on IEEE SLT workshop. IEEE Institute of Electrical and Electronics Engineers</title>
		<meeting>on IEEE SLT workshop. IEEE Institute of Electrical and Electronics Engineers</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on EMNLP</title>
		<meeting>the 2014 Conference on EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
