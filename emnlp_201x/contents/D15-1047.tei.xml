<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Graph-based Readability Assessment Method using Word Coupling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Jiang</surname></persName>
							<email>jiangzhiwei@outlook.com, sungangnju@163.com, guq@nju.edu.cn, bt@xjau.edu.cn, cdx@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoxu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Graph-based Readability Assessment Method using Word Coupling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a graph-based read-ability assessment method using word coupling. Compared to the state-of-the-art methods such as the readability for-mulae, the word-based and feature-based methods, our method develops a coupled bag-of-words model which combines the merits of word frequencies and text features. Unlike the general bag-of-words model which assumes words are independent , our model correlates the words based on their similarities on readability. By applying TF-IDF (Term Frequency and Inverse Document Frequency), the coupled TF-IDF matrix is built, and used in the graph-based classification framework, which involves graph building, merging and label propagation. Experiments are conducted on both English and Chinese datasets. The results demonstrate both effectiveness and potential of the method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Readability assessment is a task that aims to eval- uate the reading difficulty or comprehending easi- ness of text documents. It is helpful for education- ists to select texts appropriate to the reading/grade levels of the students, and for web designers to or- ganize texts on web pages for the users doing per- sonalized searches for information retrieval.</p><p>Research on readability assessment starts from the early 20th century ( <ref type="bibr" target="#b4">Dale and Chall, 1948</ref>). Many useful readability formulae have been devel- oped since then ( <ref type="bibr" target="#b4">Dale and Chall, 1948;</ref><ref type="bibr" target="#b15">McLaughlin, 1969;</ref><ref type="bibr" target="#b13">Kincaid et al., 1975</ref>). Currently, due to the development of natural language processing, the methods on readability assessment have made a great progress <ref type="bibr">(Zakaluk and Samuels, 1988</ref>; * Corresponding author. <ref type="bibr" target="#b0">Benjamin, 2012;</ref><ref type="bibr" target="#b7">Gonzalez-Dios et al., 2014</ref>). The word-based methods compute word frequencies in documents to estimate their readability <ref type="bibr">(CollinsThompson and Callan, 2004;</ref><ref type="bibr" target="#b11">Kidwell et al., 2009</ref>). The feature-based methods extract text features from documents and train classification models to classify the readability ( <ref type="bibr">Schwarm and Ostendorf, 2005;</ref><ref type="bibr" target="#b5">Feng et al., 2010;</ref><ref type="bibr" target="#b6">François and Fairon, 2012;</ref><ref type="bibr" target="#b8">Hancke et al., 2012)</ref>.</p><p>In this paper, we propose a graph-based method using word coupling, which combines the mer- its of both word frequencies and text features for readability assessment. We design a cou- pled bag-of-words model, which correlates words based on their similarities on sentence-level read- ability computed using text features. The model is used in a graph-based classification frame- work, which involves graph building, graph merg- ing/combination, and label propagation. We per- form experiments on datasets of both English and Chinese. The results demonstrate both effective- ness and potential of our method.</p><p>The rest of this paper is organized as follows: Section 2 introduces backgrounds of our work. Section 3 presents the details of the method. Sec- tion 4 designs the experiments and explains the re- sults. Finally, Section 5 concludes the paper with planned future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we introduce briefly three research topics relevant to our work: readability assess- ment, the bag-of-words model and the graph- based label propagation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Readability Assessment</head><p>Research on readability assessment has devel- oped three types of methods: the readability for- mula, the word-based methods and the feature- based methods <ref type="bibr" target="#b13">(Kincaid et al., 1975;</ref><ref type="bibr">CollinsThompson and Callan, 2004;</ref><ref type="bibr">Schwarm and Os-tendorf, 2005</ref>). During the early time, many well-known readability formulae have been devel- oped to assess the readability of text documents <ref type="bibr" target="#b4">(Dale and Chall, 1948;</ref><ref type="bibr" target="#b15">McLaughlin, 1969;</ref><ref type="bibr" target="#b13">Kincaid et al., 1975)</ref>. Surface text features are de- fined in these formulae to measure both lexical and grammatical complexities of a document. The word-based methods focus on words and their fre- quencies in a document to assess its readability, which mainly include the unigram/bigram/n-gram models <ref type="bibr" target="#b2">(Collins-Thompson and Callan, 2004;</ref><ref type="bibr">Schwarm and Ostendorf, 2005)</ref> and the word acquisition model ( <ref type="bibr" target="#b11">Kidwell et al., 2009</ref>). The feature-based methods focus on extracting text features from a document and training a classifi- cation model to classify its readability <ref type="bibr" target="#b5">(Feng et al., 2010;</ref><ref type="bibr" target="#b6">François and Fairon, 2012;</ref><ref type="bibr" target="#b8">Hancke et al., 2012)</ref>. Suitable text features are usually essen- tial to the success of these methods. The Support vector machine and logistic regression model are two classification models commonly used in these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Bag-of-Words Model</head><p>The bag-of-words model is mostly used for doc- ument classification. It constructs a feature space that contains all the distinct words in a language (or the document set). A document is repre- sented by a vector, whose components reflect the weight of every distinct word contained in the doc- ument. Normally, it assumes the words are inde- pendent. Now the capturing of the relationship among words has attracted considerable attention ( <ref type="bibr">Wong et al., 1985;</ref><ref type="bibr" target="#b1">Cheng et al., 2013)</ref>. Inspired by these works, this paper adopts the bag-of-words model in readability assessment, and refines the model by computing similarity among words on reading difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Graph-based Label Propagation Method</head><p>Graph-based label propagation is applied on a graph to propagate class labels from labeled nodes to unlabeled ones ( <ref type="bibr" target="#b12">Kim et al., 2013)</ref>. It has been successfully applied in various applications, such as dictionary construction ( <ref type="bibr" target="#b12">Kim et al., 2013)</ref>, word segmentation and tagging ( <ref type="bibr">Zeng et al., 2013)</ref>, and sentiment classification <ref type="bibr" target="#b16">(Ponomareva and Thelwall, 2012</ref>). Typically, a graph-based label propa- gation method consists of two main steps: graph construction and label propagation ( <ref type="bibr">Zeng et al., 2013)</ref>. During the first step, a similarity function is required to build edges and compute weights between pairs of the nodes ( <ref type="bibr" target="#b3">Daitch et al., 2009)</ref>. Some form of edge pruning is required to refine the graph ( <ref type="bibr" target="#b9">Jebara et al., 2009)</ref>. After that, effective algorithms have been developed to propagate the label distributions to all the nodes ( <ref type="bibr">Subramanya et al., 2010;</ref><ref type="bibr" target="#b12">Kim et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>In this section, we present GRAW (Graph-based Readability Assessment method using Word cou- pling), which constructs a coupled bag-of-words model by exploiting the correlation of readabil- ity among the words. Unlike the general bag-of- words model which models document relationship on topic, the coupled bag-of-words model extends it to model the relationship among documents on readability. In the following sections, we describe in detail how to build the coupled bag-of-words model. The model is then used in the graph- based classification framework for readability as- sessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The General Bag-of-Words Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF-IDF (Term Frequency and Inverse Document</head><p>Frequency) is the most popular scheme of the bag- of-words model. Given the set of documents D, the TF-IDF matrix M can be calculated based on the logarithmically scaled term (i.e. word) fre- quency ( <ref type="bibr">Salton and Buckley, 1988)</ref> as follows.</p><formula xml:id="formula_0">M t,d = tf t,d · idf t,d = (1 + log f (t, d)) · log |D| |{d|t ∈ d}| (1)</formula><p>where f (t, d) is the number of times that a term (word) t occurs in a document d ∈ D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Coupled Bag-of-Words Model</head><p>As shown in <ref type="figure">Figure 1</ref>, three main stages are required to construct the coupled bag-of-words model: per-sentence readability estimation, word coupling matrix construction and coupled TF-IDF matrix calculation. The following sections de- scribe the details of these stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Per-Sentence Readability Estimation</head><p>Two steps are required for the per-sentence read- ability estimation. The first is to compute a read- ing score of a sentence by heuristic functions. The second is to determine the difficulty level of the sentence by discretizing the score.   <ref type="figure">Figure 1</ref>: The Framework of GRAW</p><p>Step 1. Given a sentence s, its reading diffi- culty can be quantified as a reading score which is a continuous variable denoted by r(s). The more difficult s is, the greater r(s) will be. Based on text features of s, r(s) can be computed by one of the eight heuristic functions listed in <ref type="table" target="#tab_1">Table 1</ref> which are grouped into three aspects.    Step 2. Let η denote the pre-determined number of difficulty levels, r max and r min denote the max- imum and minimum reading score respectively of all the sentences in D. To determine the difficulty level l * (s) (l * (s) ∈ <ref type="bibr">[1, η]</ref>) of a sentence s, the range [r min , r max ] is divided into η intervals, so that each interval contains the reading scores of 1 η of all the sentences. The assumption is that all the sentences are equally distributed among the diffi- culty levels. l * (s) will be i, if the reading score r(s) resides in the i-th interval.</p><p>For each of the three aspects, we compute one l * (s) for a sentence s by combining the heuristic functions using the following equations. The as- sumption is that the reading difficulty of a sentence may be determined by the maximum measure on the text features.</p><formula xml:id="formula_1">l sur (s) = max [l len (s), l ans (s), l anc (s)] l lex (s) = max [l lv (s), l atr (s), l ntr (s)] l syn (s) = max [l pth (s), l anp (s)] (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Word Coupling Matrix Construction</head><p>Let V denote the set of all the words, a word cou- pling matrix is defined as C * ∈ R |V|×|V| , the ele- ment of which reflects the correlation between two words (i.e. terms). Two steps are required to con- struct this matrix. The first is to count the difficulty distributions of words, and the second is to com- pute the correlation between each pair of words according to the similarity of their difficulty dis- tributions.</p><p>Step 1. Let S denote the set of all the sen- tences, p t denote the difficulty distribution of a word (term) t. p t is a vector containing η (i.e. the number of difficulty levels) values, the i-th part of which can be calculated by the following formula.</p><formula xml:id="formula_2">p t (i) = 1 n t · s∈S δ(t ∈ s) · δ(l * (s) = i)<label>(3)</label></formula><p>where n t refers to the number of sentences in which t appears. The indicator function δ(x) re- turns 1 if x is true and 0 otherwise. l * (s) refers to one of the functions l sur (s), l lex (s) or l syn (s).</p><p>Step 2. Given two words (terms) t 1 and t 2 , whose level distributions are p t 1 and p t 2 re- spectively, we measure the distribution difference c KL (t 1 , t 2 ) using the Kullback-Leibler divergence <ref type="bibr" target="#b14">(Kullback and Leibler, 1951)</ref>, computed by the following formula.</p><formula xml:id="formula_3">c KL (t 1 , t 2 ) = 1 2 (KL(p t 1 ||p t 2 ) + KL(p t 2 ||p t 1 )) (4) where KL(p||q) = i p(i) log p(i) q(i) .</formula><p>After that, the logistic function is applied on the computed difference to get the normalized distribution simi- larity, i.e.</p><formula xml:id="formula_4">sim(t 1 , t 2 ) = 2 1 + e c KL (t 1 ,t 2 )<label>(5)</label></formula><p>Given a word t i , only λ other words with high- est correlation (similarity) are selected to build the neighbor set of t i , denoted as N (t i ). If a word t j is not selected (i.e. t j / ∈ N (t i )), the correspond- ing sim(t i , t j ) will be assigned 0. After that, the word coupling matrix (i.e. C * ) with sim(t i , t j ) as elements is normalized along the rows so that the sum of each row is 1. Based on three different l * (s), we construct three word coupling matrices C sur , C lex and C syn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Coupled TF-IDF Matrix Calculation</head><p>In the general bag-of-words model, the words are treated as independent of each other. However, for readability assessment, words may be correlated according to the similarity of their difficulty dis- tributions. To improve the TF-IDF matrix M de- scribed in Section 3.1, we multiply it by the word coupling matrix C * , so that the term frequencies are shared among the highly correlated (coupled) words. We denote the coupled TF-IDF matrix as M * , obtained by the following formula.</p><formula xml:id="formula_5">M * = C * · M (6)</formula><p>Specifically, three homogenous coupled TF- IDF matrices M sur , M lex and M syn can be built according to the three word coupling matrices C * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph-based Readability Assessment</head><p>We employ the coupled bag-of-words model for readability assessment under the graph-based clas- sification framework as described in the previ- ous work ( <ref type="bibr">Zhu and Ghahramani, 2002</ref>). Firstly, we construct a graph representing the readabil- ity relationship among documents by using the coupled bag-of-words model to compute the rela- tions among these documents. Secondly, we esti- mate reading levels of documents by applying la- bel propagation on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Graph Construction</head><p>We build a directed graph G * to represent the read- ability relation among documents, where nodes represent documents, and edges are weighted by the similarities between pairs of documents. Given a similarity function, we link documents d i to d j with an edge of weight G * ij , defined as:</p><formula xml:id="formula_6">G * i,j = sim(d i , d j ) if d j ∈ N (d i ) 0 otherwise (7)</formula><p>where N (d i ) is the set of k-nearest neighbors of d i determined by the similarity function. Given the coupled matrix M * ∈ R m×|D| which maps each document into a m-dimension feature space, the similarity function sim(d i , d j ) can be defined by the Euclidean distance as follows.</p><formula xml:id="formula_7">sim(d i , d j ) = 1 m k=1 (M k,i − M k,j ) 2 + (8)</formula><p>where is a small constant to avoid zero denomi- nators.</p><p>Merge the three graphs Refer to Section 3.2, the three coupled TF-IDF matrices will lead to three different document graphs, denoted as G sur , G lex and G syn respectively. To take advantage of the three aspects at one time, we need to merge the three graphs into one, denoted as G c .</p><p>In G c , each node also keeps k neighbors, and some edges shall be filtered out from the three graphs. The basic idea is to remove edges con- taining redundant information, as shown in Fig- ure 2. For each node v, we firstly select the neigh- bors which are common in all the three graphs (i.e. N sur (v) ∩ N lex (v) ∩ N syn (v)). Secondly, for the rest candidate nodes, which are the neighbors of v in at least one graph, we select one by one the node which possesses the least number of com- mon neighbors (from all the three graphs) with the nodes that are already selected in N c (v). The ob- jective is to keep the number of triangles in G c to a minimum. The edge weights of G c are averaged on the corresponding edges appeared in the three graphs.</p><p>Combine with the feature-based graph Previ- ous studies usually extract text features from doc- uments to assess the readability using classifica- tion models. Here, we also take into consideration the feature-based graph, where similarities among documents are computed on text features. We use the features defined in ( <ref type="bibr" target="#b10">Jiang et al., 2014</ref>), where the model based features are eliminated since the computation depends on pre-assigned class labels, and represent a document as a vector of the feature values. We compute the similarity between any pair of documents using the Euclidean distance, and built the feature-based graph (denoted as G f ) in the same way as above.</p><p>Additionally, to take advantage of both graphs, we combine them into one (denoted as G cf ) using the following formula.</p><formula xml:id="formula_8">G cf i,j = max [G c i,j , G f i,j ]<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Graph Propagation</head><p>Given a graph G * constructed in previous sections, its nodes are divided into two sets: the labeled set V l and the unlabeled set V u . The goal of label propagation is to propagate class labels from the labeled nodes (i.e. documents) to the entire graph.</p><p>Here, we use a simplified version of the label prop- agation method presented in ( <ref type="bibr">Subramanya et al., 2010)</ref>, which has been proved effective <ref type="bibr" target="#b12">(Kim et al., 2013</ref>). The method iteratively updates the la- bel distribution on a document node using the fol- lowing equation.</p><formula xml:id="formula_9">p (i) d (l) = 1 κ d   p 0 d (l)δ(d ∈ V l ) + v∈N (d) G d,v p (i−1) v (l)  <label>(10)</label></formula><p>At the left side of Eq.10, p </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Studies</head><p>In this section, we conduct experiments on datasets of both English and Chinese, to investi- gate the following three research questions:</p><p>RQ1: Whether the proposed method (i.e. GRAW) outperforms the state-of-the-art methods for readability assessment? RQ2: What are the effects of adding the word coupling matrix to the general bag-of-words model?</p><p>RQ3: Whether the graph merging strategy is effective, and whether the performance can be further improved by combining the feature-based graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus and Metrics</head><p>To evaluate our proposed method, we collected two datasets. The first is CPT (Chinese primary textbook) ( <ref type="bibr" target="#b10">Jiang et al., 2014</ref>  We conduct experiments on both datasets us- ing the cross-validation which randomly divides a dataset into labeled (training) and unlabeled (test) sets. The labeling proportion is varied to inves- tigate the performance of GRAW under differ- ent circumstances. To reduce variability, given certain labeling proportion, 100 rounds of cross- validation are performed, and the validation re- sults are averaged over all the rounds. We choose the precision (P), recall (R) and F1-measure (F1) as the performance metrics.</p><note type="other">), which contains Chi- nese documents of six reading levels. The second is ENCT (English New Concept textbook) which contains English documents of four reading levels. Both datasets are built from well-known textbooks where documents are labeled as grade levels by credible educationists. The details of the datasets are listed in</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to the State-of-the-Art Methods</head><p>To address RQ1, we implement the follow- ing readability assessment methods and compare GRAW to them: (1) SMOG <ref type="bibr" target="#b15">(McLaughlin, 1969)</ref> and FK ( <ref type="bibr" target="#b13">Kincaid et al., 1975</ref>  For GRAW, we implement label propagation on both the merged graph G c and the final graph G cf (Section 3.3), denoted as GRAW c and GRAW cf respectively. <ref type="table" target="#tab_5">Table 3</ref> gives the average perfor- mance measure per reading level resulted by the implemented methods on both datasets. Unless otherwise specified, we fixed η to 3, and λ to 2800 for CPT and 2000 for ENCT. The proportion of the labeled (training) set is set to 0.7.</p><p>In <ref type="table" target="#tab_5">Table 3</ref>, the precision, recall and F1-measure of all the seven methods are calculated per read- ing (grade) level on both English and Chinese datasets. The values marked in bold in each row refer to the maximum (best) measure gained by the methods.</p><p>From <ref type="table" target="#tab_5">Table 3</ref>, the readability formulae (SMOG and FK) perform poorly on either the precision or recall measure, and their F1-measure values are generally the poorest. Both SMOG and FK are designed for English, and have acceptable per- formance on the English dataset ENCT. The un- igram model (SUM) performs a little better than the readability formulae. On ENCT, It has rel- atively good performance on grade levels 1 and 4, while on the Chinese dataset CPT, the perfor- mance is not satisfactory. The feature-based meth- ods (LR and SVM) perform well on both ENCT and CPT, which means both the text features de- veloped and the classifiers trained are useful. In general, GRAW c performs better than both LR and SVM, which demonstrates the effectiveness of our method. In addition, by combining the feature- based graph (GRAW cf ), GRAW can be improved, and performs the best on all the three metrics over the majority of reading levels on both datasets. The only exception is on level 5 in CPT, which suggests the requirement of further improvements.</p><p>We study the effect of labeling proportion on the performance of these methods on both datasets. The F1-measure averaged over the reading levels is used, since it is a good representative of the three metrics according to <ref type="table" target="#tab_5">Table 3</ref>. <ref type="figure" target="#fig_6">Figure 3</ref> depicts the performance trends of all the methods.</p><p>From <ref type="figure" target="#fig_6">Figure 3</ref>, neither SMOG nor FK benefits   from the increasing size of the labeled set. This suggests that the performance of the readability formulae can hardly be improved by accumulat- ing training data. The other 5 methods achieve better performance on larger labeled set, and out- perform the two formulae even if the labeling pro- portion is small. Both LR and SVM perform bet- ter than SUM, but the performance is not good when the labeling proportion is less than 0.3, es- pecially on the Chinese dataset. On the Chinese dataset, SVM performs better than LR, while on the English dataset, the situation is reversed. Both versions of GRAW outperform the other methods over the labeling ranges on both datasets. In ad- dition, GRAW performs well when the labeling proportion is still small. Again, by combining the feature-based graph, the performance of GRAW is consistently improved.</p><p>In summary, GRAW can outperform the state- of-the-art methods for readability assessment on both English and Chinese datasets. By combin- ing the feature-based graph, the performance of GRAW can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effects of the Word Coupling Matrix</head><p>For RQ2, we firstly compare the coupled bag-of- words model to the general model in the process of graph construction. Four graphs are built by us- ing each of the three word coupling matrices (i.e. M sur , M lex and M syn ) and the TF-IDF matrix respectively. Label propagation is applied on each graph to predict reading levels of unlabeled docu- ments. The labeling proportion is varied from 0.1 to 0.9 on both the English and Chinese datasets. <ref type="figure">Figure 4</ref>(a) depicts the average F1-measure re- sulted from the four graphs.</p><p>From <ref type="figure">Figure 4</ref>(a), the three word coupling ma- trices greatly outperform the TF-IDF matrix, espe- cially on the Chinese dataset. This demonstrates that the word coupling matrices are very effective in improving the performance of the general bag- of-words model for readability assessment.</p><p>Secondly, we investigate the performance of the four matrices per reading level. <ref type="figure">Figure 4</ref>(b) de- picts the recall rate per reading level of the four corresponding graphs in bar charts. The labeling proportion is set to 0.7. The recall rate is used because it makes the reason evident that the TF- IDF matrix performs poorly. From <ref type="figure">Figure 4(b)</ref>, on the Chinese dataset, nearly all the unlabeled docu- ments are classified as level 1 by the TF-IDF ma- trix, in which the word frequencies are too few to make meaningful discrimination among the read- ing levels. On the English dataset, the TF-IDF matrix performs better, but still prefers to classify documents into lower levels.</p><p>As described in Section 3.2.2, η (the number of difficulty levels of sentences) and λ (the number of neighbors pertained for each document node) are two parameters in building the word coupling matrices. To investigate their effects on the per- formance of the built matrices, we vary the val- ues of both η and λ, and compute the average F1- measure on the two datasets. <ref type="figure">Figure 4</ref>(c) depicts the results in line charts, where η varies from 2 to 9 step by 1, while λ varies from 400 to 4000 step by 400 on Chinese and from 200 to 2000 step by 200 on English (the difference is due to the dissimilar number of documents between the two datasets). The three word coupling matrices exhibit similar behavior during experiments, hence, only M syn is depicted.</p><p>From <ref type="figure">Figure 4</ref>(c), a small η (e.g. 2 or 3) is good on the Chinese dataset. However, on the English dataset, η = 2 leads to the poorest performance. It seems the increasing of η causes vibrated perfor- mance, and the trend is further complicated when involving λ. Above all, η = 3 gives a prefer- able option on both datasets. For λ, most of the lines exhibit a similar trend that rises first and then keeps stable on both datasets, although some may drop when λ is too large. This suggests that mak- ing a relatively large number of the other words as the neighbors of one (i.e. λ = 2800 on the Chi- nese dataset and λ = 2000 on the English dataset) will make an effective word coupling matrix.</p><p>The word coupling matrix constructed in GRAW uses the whole corpus on either English or Chinese. To investigate if the corpus size takes effects on the performance of GRAW, we vary the proportion of the corpus used by randomly re- moving documents from each reading level. From <ref type="figure">Figure 4(d)</ref>, on the Chinese dataset, the performance of GRAW suffers little from remov- ing documents, even if only 20% documents are left for building the word coupling matrix. How- ever, on the English dataset, the mean perfor- mance drops sharply and the deviation increases evidently. This suggests that cumulating sufficient corpus is required for building a suitable word coupling matrix in GRAW, and factors other than number of documents may influence the corpus quality, which deserves further study.</p><p>In summary, the word coupling matrix plays an essential role in GRAW. For building a suitable word coupling matrix, the number of difficulty levels of sentences (η) can be set to 3, and a rel- atively large number of the other words should be selected as the neighbors of a word. A sufficient corpus is required for refining the matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of Graph Combination</head><p>For RQ3, we compare graphs built on each sin- gular word coupling matrix (i.e. M sur , M lex and M syn ) to the merged graph (i.e. GRAW c ) and the combined graph (i.e. GRAW cf ). <ref type="figure" target="#fig_10">Figure 5</ref> depicts the average F1-measure resulted after applying la- bel propagation on these graphs with labeling pro- portion varied from 0.1 to 0.9. The feature-based graph (i.e. G f ) is also depicted for comparison.  <ref type="figure" target="#fig_10">Figure 5</ref>, the merged graph GRAW c out- performs the three basic graphs on both datasets in most cases. Within the three, M syn performs best, especially on the English dataset, where it can outperform GRAW c slightly when the label- ing proportion is small (0.2 − 0.4). By combining the feature-based graph, GRAW cf performs even better on both datasets, although G f performs poorest among all the graphs. In summary, the graph merging strategy is effective, and by com- bining the feature-based graph, the performance of GRAW can be improved. This demonstrates the potential of GRAW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a graph-based readabil- ity assessment method using word coupling. The coupled bag-of-words model is designed, which exploits the correlation of readability among the words, and by applying TF-IDF, models the rela- tionship among documents on reading levels. The model is employed in the graph-based classifica- tion framework for readability assessment, which involves graph building, merging, and label prop- agation. Experiments are conducted on both Chi- nese and English datasets. The results show that our method can outperform the commonly used methods for readability assessment. In addition, the evaluation demonstrates the potential of the coupled bag-of-words model and the graph com- bination/merging strategies.</p><p>In our future work, we plan to verify the sound- ness of the results by applying our method on large volume corpus of both English and Chinese. In ad- dition, we will investigate other ways of comput- ing the word coupling matrices, such as incorpo- rating word coherency or semantics, and develop efficient merging strategies which can be used for training classification models, as well as for build- ing graphs.</p><p>Empirical Methods in Natural Language Process- ing and Computational Natural Language Learning, pages 655-665. Association for Computational Lin- guistics.</p><p>Gerard <ref type="bibr">Salton and Christopher Buckley. 1988. Term</ref> weighting approaches in automatic text retrieval. In- formation processing and management, 24 <ref type="formula" target="#formula_4">(5)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Documents</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Aspect</head><label></label><figDesc>Function Description Surface len(s) the length of the sentence s. ans(s) the average number of syllables (or strokes for Chinese) per word (or character for Chinese) in s. anc(s) the average number of characters per word in s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the number of distinct types of POS, i.e. part of speech, in s. atr(s) the ratio of adjectives in s. ntr(s) the ratio of nouns in s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Syntatic pth(s) the height of the syntax parser tree of s. anp(s) the average number of (noun, verb, and preposi- tion) phrases in s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the graph merging strategy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>d</head><label></label><figDesc>(l) is the afterward probability of l (i.e. the class label) on a node d at the i-th iteration. At the right side, κ d is the nor- malizing constant to make sure the sum of all the probabilities is 1, and p 0 d (l) is the initial probabil- ity of l on d if d is initially labeled (i.e. belonging to the labeled set V l ). δ(x) is the indicator func- tion. N (d) denotes the set of neighbors of d. The iteration stops when the changes in p (i) d (l) for all the nodes and label values are small enough (e.g. less than e −3 ), or i exceeds a predefined number (e.g. greater than 30).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The average F1-measure of the seven methods on both datasets with the labeling proportion varied from 0.1 to 0.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 4: Four perspectives on the effectiveness of the word coupling matrices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig- ure 4(d) depicts the average F1-measure resulted by M syn . The removing ratio is selected from {0, 0.05, 0.1, 0.2, 0.4, 0.8}. Both the mean values and deviations are shown on the line chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The average F1-measure of different types of graphs on the English and Chinese datasets From Figure 5, the merged graph GRAW c outperforms the three basic graphs on both datasets in most cases. Within the three, M syn performs best, especially on the English dataset, where it can outperform GRAW c slightly when the labeling proportion is small (0.2 − 0.4). By combining the feature-based graph, GRAW cf performs even better on both datasets, although G f performs poorest among all the graphs. In summary, the graph merging strategy is effective, and by combining the feature-based graph, the performance of GRAW can be improved. This demonstrates the potential of GRAW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Three aspects of estimating reading diffi-
culty of sentences using heuristic functions 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Dataset Language #Grade #Doc #Sent #Word 
CPT 
Chinese 
6 
637 16145 234372 
ENCT 
English 
4 
279 
4671 
62921 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets on both English and Chinese</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The average Precision, Recall and F1-measure (%) per reading level of the seven methods for 
readability assessment on both datasets when the labeling proportion is 0.7 

support vector machine are used as the classifiers 
respectively. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>9</head><label>9</label><figDesc></figDesc><table>Proportion of the labeled set 
F1−measure (English) 
TF−IDF 

M sur 

M lex 

M syn 

(a) Comparison of the average F1-measure between the cou-
pling and general TF-IDF matrices 

G1 
G2 
G3 
G4 
G5 
G6 

0 

0.2 

0.4 

0.6 

0.8 

1 

M syn 

Graph 

M lex 
M sur 
TF−IDF 

Recall (Chinese) 

Reading level 

G1 
G2 
G3 

G4 

0 

0.2 

0.4 

0.6 

0.8 

1 

M syn 

Graph 

M lex 
M sur 
TF−IDF 

Recall (English) 

Reading level 

(b) Comparison of the Recall rate per reading level between 
the coupling and general TF-IDF matrices 

400 
800 
1200 
1600 
2000 
2400 
2800 
3200 
3600 
4000 
0.35 

0.375 

0.4 

0.425 

0.45 

0.475 

0.5 

Number of neighbors (λ) 
F1−measure (Chinese) 

η=2 
η=3 
η=4 
η=5 
η=6 
η=7 
η=8 
η=9 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National NSFC projects under Grant Nos. 61373012, 61321491, and 91218302.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reconstructing readability: Recent developments and recommendations in the analysis of text difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebekah</forename><forename type="middle">George</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="88" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coupled term-term relation analysis for document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duoqian</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Joint Conference on Neural Networks</title>
		<meeting>the 2013 International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A language modeling approach to predicting reading difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fitting a graph to vector data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">A</forename><surname>Samuel I Daitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A formula for predicting readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanne</forename><forename type="middle">S</forename><surname>Chall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Research Bulletin</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="28" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparison of features for automatic readability assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An ai readability formula for french as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>François</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédrick</forename><surname>Fairon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="466" to="477" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple or complex? assessing the readability of basque texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itziar</forename><surname>Gonzalez-Dios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marıa</forename><forename type="middle">Jesús</forename><surname>Aranzabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="334" to="344" />
		</imprint>
	</monogr>
	<note>Arantza Dıaz de Ilarraza, and Haritz Salaberri</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Readability classification for german using lexical, syntactic, and morphological features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hancke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1063" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph construction and b-matching for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An ordinal multi-class classification method for readability assessment of chinese documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoxu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Knowledge Science, Engineering and Management</title>
		<imprint>
			<biblScope unit="page" from="61" to="72" />
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical estimation of word acquisition with application to readability prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kidwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevyn</forename><surname>Collinsthompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="900" to="909" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint extraction and labeling via graph propagation for dictionary construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Doo Soon Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter Z</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 27th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Peter</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert P Fishburne</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Richard L Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Air Station</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Smog grading: A new readability formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Harry Mclaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of reading</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="639" to="646" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do neighbours help?: an exploration of graph-based algorithms for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on</title>
		<meeting>the 2012 Joint Conference on</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
