<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Neural Networks with Recurrent Neural Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yyang464@bloomberg.net</email>
							<affiliation key="aff0">
								<orgName type="institution">Bloomberg New York</orgName>
								<address>
									<postCode>10022</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Neural Networks with Recurrent Neural Filters</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="912" to="917"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>912</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a class of convolutional neural networks (CNNs) that utilize recurrent neu-ral networks (RNNs) as convolution filters. A convolution filter is typically implemented as a linear affine transformation followed by a non-linear function, which fails to account for language compositionality. As a result, it limits the use of high-order filters that are often warranted for natural language processing tasks. In this work, we model convolution filters with RNNs that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) have been shown to achieve state-of-the-art results on various natural language processing (NLP) tasks, such as sentence classification <ref type="bibr">(Kim, 2014)</ref>, question an- swering ( <ref type="bibr" target="#b2">Dong et al., 2015)</ref>, and machine transla- tion ( <ref type="bibr" target="#b3">Gehring et al., 2017)</ref>. In an NLP system, a convolution operation is typically a sliding win- dow function that applies a convolution filter to every possible window of words in a sentence. Hence, the key components of CNNs are a set of convolution filters that compose low-level word features into higher-level representations.</p><p>Convolution filters are usually realized as linear systems, as their outputs are affine transformations of the inputs followed by some non-linear activa- tion functions. Prior work directly adopts a lin- ear convolution filter to NLP problems by utiliz- ing a concatenation of embeddings of a window of words as the input, which leverages word order information in a shallow additive way. As early CNN architectures have mainly drawn inspiration from models of the primate visual system, the ne- cessity of coping with language compositionality is largely overlooked in these systems. Due to the linear nature of the convolution filters, they lack the ability to capture complex language phenom- ena, such as compositionality and long-term de- pendencies.</p><p>To overcome this, we propose to employ re- current neural networks (RNNs) as convolution filters of CNN systems for various NLP tasks. Our recurrent neural filters (RNFs) can naturally deal with language compositionality with a recur- rent function that models word relations, and they are also able to implicitly model long-term de- pendencies. RNFs are typically applied to word sequences of moderate lengths, which alleviates some well-known drawbacks of RNNs, including their vulnerability to the gradient vanishing and exploding problems ( <ref type="bibr" target="#b7">Pascanu et al., 2013)</ref>. As in conventional CNNs, the computation of the con- volution operation with RNFs can be easily paral- lelized. As a result, RNF-based CNN models can be 3-8x faster than their RNN counterparts.</p><p>We present two RNF-based CNN architectures for sentence classification and answer sentence selection problems. Experimental results on the Stanford Sentiment Treebank and the QASent and WikiQA datasets demonstrate that RNFs signifi- cantly improve CNN performance over linear fil- ters by 4-5% accuracies and 3-6% MAP scores respectively. Analysis results suggest that RNFs perform much better than linear filters in detecting longer key phrases, which provide stronger cues for categorizing the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>The aim of a convolution filter is to produce a lo- cal feature for a window of words. We describe a novel approach to learning filters using RNNs, which is especially suitable for NLP problems.</p><p>We then present two CNN architectures equipped with RNFs for sentence classification and sentence matching tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recurrent neural filters</head><p>We briefly review the linear convolution filter implementation by <ref type="bibr">Kim (2014)</ref>, which has been widely adopted in later works. Consider an m-gram word sequence [x i , · · · , x i+m−1 ], where x i ∈ R k is a k-dimensional word vector. A lin- ear convolution filter is a function applied to the m-gram to produce a feature c i,j :</p><formula xml:id="formula_0">c i,j =f (w j x i:i+m−1 + b j ), x i:i+m−1 =x i ⊕ x i+1 ⊕ · · · ⊕ x i+m−1 ,<label>(1)</label></formula><p>where ⊕ is the concatenation operator, b j is a bias term, and f is a non-linear activation function. We typically use multiple independent linear fil- ters to yield a feature vector c i for the word se- quence. Linear convolution filters make strong as- sumptions about language that could harm the per- formance of NLP systems. First, linear filters as- sume local compositionality and ignore long-term dependencies in language. Second, they use sep- arate parameters for each value of the time in- dex, which hinders parameter sharing for the same word type ( <ref type="bibr" target="#b4">Goodfellow et al., 2016</ref>). The assump- tions become more problematic if we increase the window size m. We propose to address the limitations by em- ploying RNNs to realize convolution filters, which we term recurrent neural filters (RNFs). RNFs compose the words of the m-gram from left to right using the same recurrent unit:</p><formula xml:id="formula_1">h t = RNN(h t−1 , x t ),<label>(2)</label></formula><p>where h t is a hidden state vector that encoded in- formation about previously processed words, and the function RNN is a recurrent unit such as Long Short-Term Memory (LSTM) unit (Hochre- iter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b0">Cho et al., 2014</ref>). We use the last hidden state h i+m−1 as the RNF output feature vector c i . Features learned by RNFs are interde- pendent of each other, which permits the learning of complementary information about the word se- quence. The left-to-right word composing proce- dure in RNFs preserves word order information and implicitly models long-term dependencies in language. RNFs can be treated as simple drop- in replacements of linear filters and potentially adopted in numerous CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CNN architectures</head><p>Sentence encoder We use a CNN architec- ture with one convolution layer followed by one max pooling layer to encode a sentence. In particular, the RNFs are applied to every possible window of m words in the sentence {x 1:m , x 2:m+1 , · · · , x n−m+1:n } to generate fea-</p><formula xml:id="formula_2">ture maps C = [c 1 , c 2 , · · · , c n−m+1 ]</formula><p>. Then a max-over-time pooling operation <ref type="bibr" target="#b1">(Collobert et al., 2011</ref>) is used to produce a fixed size sentence rep- resentation: v = max{C}.</p><p>Sentence classification We use a CNN architec- ture that is similar to the CNN-static model de- scribed in <ref type="bibr">Kim (2014)</ref> for sentence classification.</p><p>After obtaining the sentence representation v, a fully connected softmax layer is used to map v to an output probability distribution. The network is optimized against a cross-entropy loss function.</p><p>Sentence matching We exploit a CNN architec- ture that is nearly identical to the CNN-Cnt model introduced by <ref type="bibr" target="#b14">Yang et al. (2015)</ref>. Let v 1 and v 2 be the vector representations of the two sentences. A bilinear function is applied to v 1 and v 2 to pro- duce a sentence matching score. The score is com- bined with two word matching count features and fed into a sigmoid layer. The output of the sig- moid layer is used by binary cross-entropy loss to optimize the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate RNFs on some of the most popular datasets for the sentence classification and sen- tence matching tasks. After describing the experi- mental setup, we compare RNFs against both lin- ear filters and conventional RNN models, and re- port our findings in § 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental settings</head><p>Data We use the Stanford Sentiment Tree- bank ( <ref type="bibr" target="#b10">Socher et al., 2013</ref>) in our sentence classifi- cation experiments. We report accuracy results for both binary classification and fine-grained classi- fication settings. Two answer sentence selection datasets, QASent ( <ref type="bibr" target="#b12">Wang et al., 2007)</ref> and Wik- iQA ( <ref type="bibr" target="#b14">Yang et al., 2015)</ref>, are adopted in our sen- tence matching experiments. We use MAP and MRR to evaluate the performance of answer sen- tence selection models.</p><p>Competitive systems We consider CNN vari- ants with linear filters and RNFs. For RNFs, we adopt two implementations based on GRUs and LSTMs respectively. We also compare against the following RNN variants: GRU, LSTM, GRU with max pooling, and LSTM with max pooling. <ref type="bibr">2</ref> We use the publicly available 300-dimensional GloVe vectors ( <ref type="bibr" target="#b8">Pennington et al., 2014</ref>) pre-trained with 840 billion tokens to initialize the word embed- dings for all the models. The word vectors are fixed during downstream training. Finally, we re- port best published results for each dataset. 3</p><p>Parameter tuning For all the experiments, we tune hyperparameters on the development sets and report results obtained with the selected hyperpa- rameters on the test sets. After the preliminary search, we choose the number of hidden units (fea- ture maps) from {200, 300, 400}, and the filter window width from {2, 3, 4, 5} for linear filters and from {5, 6, 7, 8} for RNFs. Linear filters tend to perform well with small window widths, while RNFs work better with larger window widths. We apply dropout to embedding layers, pooling lay- ers, RNN input layers, and RNN recurrent layers. The dropout rates are selected from {0, 0.2, 0.4}, where 0 indicates that dropout is not used for the specific layer. Optimization is performed by Adam ( <ref type="bibr">Kingma and Ba, 2015</ref>) with early stopping. We conduct random search with a budget of 100 runs to seek the best hyperparameter configuration for each system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The evaluation results on sentiment classification and answer sentence selection are shown in <ref type="table" target="#tab_1">Ta- ble 1 and Table 2</ref>  MAP score on WikiQA. In particular, CNN-RNF- LSTM achieves 53.4% and 90.0% accuracies on the fine-grained and binary sentiment classifica- tion tasks respectively, which match the state-of- the-art results on the Stanford Sentiment Tree- bank. CNN-RNF-LSTM also obtains competitive results on answer sentence selection datasets, de- spite the simple model architecture compared to state-of-the-art systems.</p><p>Conventional RNN models clearly benefit from max pooling, especially on the task of answer sen- tence selection. Like RNF-based CNN models, max-pooled RNNs also consist of two essential layers. The recurrent layer learns a set of hid- den states corresponding to different time steps, and the max pooling layer extracts the most salient information from the hidden states. However, a hidden state in RNNs encodes information about all the previous time steps, while RNFs focus on detecting local features from a window of words that can be more relevant to specific tasks. As a result, RNF-based CNN models perform consis- tently better than max-pooled RNN models.</p><p>CNN-RNF models are much faster to train than their corresponding RNN counterparts, despite they have the same numbers of parameters, as RNFs are applied to word sequences of shorter lengths and the computation is parallelizable. The training of CNN-RNF-LSTM models takes 10-20 mins on the Stanford Sentiment Treebank, which is 3-8x faster than the training of LSTM models, on an NVIDIA Tesla P100 GPU accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>QASent WikiQA  Evaluation results on answer sentence selection datasets. The best results obtained from our implementa- tions are in bold. The best published results are underlined. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>We now investigate why RNFs are more effective than linear convolution filters on the binary senti- ment classification task. We perform quantitative analysis on the development set of the Stanford Sentiment Treebank (SST), in which sentiment la- bels for some phrases are also available.</p><p>Local label consistency (LLC) ratio We first inspect whether longer phrases have a higher ten- dency to express the same sentiment as the entire sentence. We define the local label consistency (LLC) ratio as the ratio of m-grams that share the same sentiment labels as the original sentences. The LLC ratios with respect to different phrase lengths are shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Longer phrases are more likely to convey the same sentiments as the original sentences. Therefore, the ability to model long phrases is crucial to convolution filters.</p><p>Key phrase hit rate We examine linear filters and RNFs on the ability to detect a key phrase in a sentence. Specifically, we define the key phrases for a sentence to be the set of phrases that are la- beled with the same sentiment as the original sen- tence. The key phrase hit rate is then defined as the ratio of filter-detected m-grams that fall into the corresponding key phrase sets. The filter-detected m-gram of a sentence is the one whose convo- lution feature vector has the shortest Euclidean distance to the max-pooled vector. The hit rate results computed on SST dev set are presented in <ref type="figure" target="#fig_2">Figure 2</ref>. As shown, RNFs consistently per-form better than linear filters on identifying key phrases across different phrase lengths, especially on phrases of moderate lengths (4-7). The results suggest that RNFs are superior to linear filters, as they can better model longer phrases whose labels are more consistent with the sentences. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Linear convolution filters are dominated in CNN- based systems for both computer vision and nat- ural language processing tasks. One exception is the work of <ref type="bibr" target="#b15">Zoumpourlis et al. (2017)</ref>, which proposes a convolution filter that is a function with quadratic forms through the Volterra kernels. However, this non-linear convolution filter is de- veloped in the context of a computational model of the visual cortex, which is not suitable for NLP problems. In contrast, RNFs are specifically de- rived for NLP tasks, in which a different form of non-linearity, language compositionality, often plays a critical role. Several works have employed neural network architectures that contain both CNN and RNN components to tackle NLP problems. <ref type="bibr" target="#b11">Tan et al. (2016)</ref> present a deep neural network for answer sentence selection, in which a convolution layer is applied to the output of a BiLSTM layer for ex- tracting sentence representations. <ref type="bibr" target="#b6">Ma and Hovy (2016)</ref> propose to compose character representa- tions of a word using a CNN, whose output is then fed into a BiLSTM for sequence tagging. <ref type="bibr">Kalchbrenner and Blunsom (2013)</ref> introduce a neural architecture that uses a sentence model based on CNNs and a discourse model based on RNNs. Their system achieves state-of-the-art results on the task of dialogue act classification. Instead of treating an RNN and a CNN as isolated compo- nents, our work directly marries RNNs with the convolution operation, which illustrates a new di- rection in mixing RNNs with CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We present RNFs, a new class of convolution fil- ters based on recurrent neural networks. RNFs se- quentially apply the same recurrent unit to words of a phrase, which naturally capture language compositionality and implicitly model long-term dependencies. Experiments on sentiment classifi- cation and answer sentence selection tasks demon- strate that RNFs give a significant boost in per- formance compared to linear convolution filters. RNF-based CNNs also outperform a variety of RNN-based models, as they focus on extracting lo- cal information that could be more relevant to par- ticular problems. The quantitive analysis reveals that RNFs can handle long phrases much better than linear filters, which explains their superior- ity over the linear counterparts. In the future, we would like to investigate the effectiveness of RNFs on a wider range of NLP tasks, such as natural lan- guage inference and machine translation. <ref type="bibr">Nal Kalchbrenner and Phil Blunsom. 2013</ref>. Recurrent convolutional neural networks for discourse compo- sitionality. In Proceedings of the Workshop on Con- tinuous Vector Space Models and their Composition- ality. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Local label consistency ratio results computed on SST dev set.</figDesc><graphic url="image-1.png" coords="4,81.85,335.15,195.84,146.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Key phrase hit rate results computed on SST dev set. The RNFs are implemented with LSTMs.</figDesc><graphic url="image-2.png" coords="4,322.89,335.15,184.32,138.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The code is available at https://github.com/ bloomberg/cnn-rnf.</note>

			<note place="foot" n="2"> Max pooling performed better than mean pooling in our preliminary experiments. 3 We exclude results obtained from systems using external resources beyond word embeddings.</note>

			<note place="foot" n="4"> Phrases of very long lengths (8-10) are rarely annotated in SST dev data, which could explain why linear filters and RNFs achieve similar hit rates, as small data sample often leads to high variance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank Chunyang Xiao for helping us to run the analysis experiments. We thank Kazi Shefaet Rahman, Ozan Irsoy, Chen-Tse Tsai, and Lingjia Deng for their valuable comments on earlier ver-sions of this paper. We also thank the EMNLP reviewers for their helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Question answering over freebase with multicolumn convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation for answer selection with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International on Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the ACM International on Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing</title>
		<meeting>Empirical Methods for Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What is the jeopardy model? a quasisynchronous grammar for qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear convolution filters for cnn-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Zoumpourlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Doumanoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Nicholas Vretos, and Petros Daras</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
