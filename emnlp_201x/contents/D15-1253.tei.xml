<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Hyper-parameter Optimization for NLP Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson, T. J. Watson Research Center</orgName>
								<address>
									<settlement>NY</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson, T. J. Watson Research Center</orgName>
								<address>
									<settlement>NY</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson, T. J. Watson Research Center</orgName>
								<address>
									<settlement>NY</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson, T. J. Watson Research Center</orgName>
								<address>
									<settlement>NY</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
							<email>mahadeva@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson, T. J. Watson Research Center</orgName>
								<address>
									<settlement>NY</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">U. of Massachusetts Amherst</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Hyper-parameter Optimization for NLP Applications</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Hyper-parameter optimization is an important problem in natural language processing (NLP) and machine learning. Recently , a group of studies has focused on using sequential Bayesian Optimization to solve this problem, which aims to reduce the number of iterations and trials required during the optimization process. In this paper, we explore this problem from a different angle, and propose a multi-stage hyper-parameter optimization that breaks the problem into multiple stages with increasingly amounts of data. Early stage provides fast estimates of good candidates which are used to initialize later stages for better performance and speed. We demonstrate the utility of this new algorithm by evaluating its speed and accuracy against state-of-the-art Bayesian Optimization algorithms on classification and prediction tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hyper-parameter optimization has been receiv- ing an increasingly amount of attention in the NLP and machine learning communities <ref type="bibr" target="#b16">(Thornton et al., 2013;</ref><ref type="bibr" target="#b12">Komer et al., 2014;</ref><ref type="bibr" target="#b2">Bergstra et al., 2011;</ref><ref type="bibr" target="#b0">Bardenet et al., 2013;</ref><ref type="bibr" target="#b20">Zheng et al., 2013)</ref>. The performance of learning algorithms depend on the correct instantiations of their hyper- parameters, ranging from algorithms such as lo- gistic regression and support vector machines, to more complex model families such as boosted regression trees and neural networks. While hyper-parameter settings often make the differ- ence between mediocre and state-of-the-art per- formance ( ), it is typically very time-consuming to find an optimal setting due to the complexity of model classes, and the amount of training data available for tuning. The issue is particularly important in large-scale problems where the size of the data can be so large that even a quadratic running time is prohibitively large.</p><p>Recently several sequential Bayesian Op- timization methods have been proposed for hyper-parameter search <ref type="bibr" target="#b14">(Snoek et al., 2012;</ref><ref type="bibr" target="#b6">Eggensperger et al., 2015;</ref><ref type="bibr" target="#b4">Brochu et al., 2010;</ref><ref type="bibr" target="#b10">Hutter et al., 2011;</ref><ref type="bibr" target="#b5">Eggensperger et al., 2014</ref>). The common theme is to perform a set of it- erative hyper-parameter optimizations, where in each round, these methods fit a hyper-parameter response surface using a probabilistic regression function such as Gaussian Process <ref type="bibr" target="#b14">(Snoek et al., 2012</ref>) or tree-based models <ref type="bibr" target="#b10">(Hutter et al., 2011)</ref>, where the response surface maps each hyper- parameter setting to an approximated accuracy. The learned regression model is then used as a cheap surrogate of the response surface to quickly explore the search space and identify promising hyper-parameter candidates to evaluate next in or- der to enhance validation accuracy.</p><p>While these methods have enjoyed great success compared to conventional random search ( <ref type="bibr" target="#b3">Bergstra et al., 2012;</ref><ref type="bibr" target="#b1">Bengio et al., 2013</ref>) and grid search algorithms by significantly reduc- ing the number of iterations and trials required during the process, the focus and starting point of these work have largely been on dealing with many dimensions of hyper-parameters, rather than scaling to large amount of data as typical in many NLP tasks, where the efficiency bottleneck stems from the size of the training data in addition to hyper-parameter dimensions. For example, as dataset size grows, even simple models (with few hyper-parameters) such as logistic regression can require more training time per iteration in these algorithms, leading to increased overall time complexity.</p><p>In this work, we introduce a multi-stage Bayesian Optimization framework for efficient hyper-parameter optimization, and empirically study the impact of the multi-stage algorithm on hyper-parameter tuning. Unlike the previous approaches, the multi-stage approach considers hyper-parameter optimization in successive stages with increasingly amounts of training data. The first stage uses a small subset of training data, applies sequential optimization to quickly iden- tify an initial set of promising hyper-parameter settings, and these promising candidates are then used to initialize Bayesian Optimization on later stages with full training dataset to enable the ex- pensive stages operate with better prior knowledge and converge to optimal solution faster.</p><p>The key intuition behind the proposed approach is that both dataset size and search space of hyper- parameter can be large, and applying the Bayesian Optimization algorithm on the data can be both expensive and unnecessary, since many evaluated candidates may not even be within range of best final settings. We note our approach is orthogo- nal and complementary to parallel Bayesian Opti- mization ( <ref type="bibr" target="#b14">Snoek et al., 2012)</ref> and multi-task learn- ing ( <ref type="bibr" target="#b19">Yogatama et al., 2014;</ref><ref type="bibr">Swersky et al., 2012)</ref>, because the improved efficiency per iteration, as achieved by our algorithm, is a basic building block of the other algorithms, thus can directly help the efficiency of multiple parallel runs <ref type="bibr" target="#b14">(Snoek et al., 2012)</ref>, as well as runs across different datasets ( <ref type="bibr" target="#b19">Yogatama et al., 2014;</ref><ref type="bibr">Swersky et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The new multi-stage Bayesian Optimization is a generalization of the standard Bayesian Optimiza- tion for hyper-parameter learning <ref type="bibr" target="#b14">(Snoek et al., 2012;</ref><ref type="bibr" target="#b8">Feurer et al., 2015)</ref>. It is designed to scale standard Bayesian Optimization to large amounts of training data. Before delving into the de- tails, we first describe hyper-parameter optimiza- tion and give a quick overview on the standard Bayesian Optimization solution for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hyper-parameter Optimization</head><p>Let λ = {λ 1 , . . . , λ m } denote the hyper- parameters of a machine learning algorithm, and let {Λ 1 , . . . , Λ m } denote their respective domains. When trained with λ on training data T train , the validation accuracy on T valid is denoted as L(λ, T train , T valid ). The goal of hyper-parameter optimization is to find a hyper-parameter setting λ * such that the validation accuracy L is maxi- mized. Current state-of-the-art methods have fo- cused on using model-based Bayesian Optimiza- tion ( <ref type="bibr" target="#b14">Snoek et al., 2012;</ref><ref type="bibr" target="#b10">Hutter et al., 2011)</ref> to solve this problem due to its ability to identify good solutions within a small number of iterations as compared to conventional methods such as grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bayesian Optimization for</head><p>Hyper-parameter Learning </p><formula xml:id="formula_0">a(λ, V ) = ∞ −∞ max(L − L * , 0)p V (L|λ)d L</formula><p>where p V (L|λ) denotes the probability of accu- racy L given configuration λ, which is encoded by the probabilistic regression model V . The acquisi- tion function is used to identify the next candidate (the one with the highest expected improvement over current best L * ). More details of acquisition functions can be found in ( <ref type="bibr" target="#b4">Brochu et al., 2010</ref>).</p><p>The most common probabilistic regression model V is the Gaussian Process prior <ref type="bibr" target="#b14">(Snoek et al., 2012)</ref>, which is a convenient and powerful prior distribution on functions. For the purpose of our experiments, we also use Gaussian Process prior as the regression model. However, we would like to note the fact that the proposed multi-stage Bayesian Optimization is agnostic of the regres- sion model used, and can easily handle other in- stantiations of the regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Multi-stage Bayesian Optimiza- tion for Hyper-parameter Tuning</head><p>Input: Loss function L, number of stages S, iterations per stage</p><formula xml:id="formula_1">Y = Y 1 , . . . , Y S , training data per stage T train = T 1 train , . . . , T S train , validation data T valid , initialization λ 1:k Output: hyper-parameter λ * for stage s=1 to S do for i=1 to k do L i = Evaluate L(λ i , T s train , T valid ) end for j=k+1 to Y s do V : regression model on λ i , L i j−1 i=1 λ j = arg max λ∈Λ a(λ, V ) L j = Evaluate L(λ j , T s train , T valid ) end reset λ 1:k =best k configs ∈ λ 1 , . . . λ Ys based on validation accuracy L end return λ * = arg max λ j ∈{λ Y 1 ,...,λ Y S } L j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-stage Bayesian Optimization for</head><p>Hyper-parameter Tuning</p><p>The multi-stage algorithm as shown in Algo- rithm 1 is an extension of the standard Bayesian Optimization (Section 2.2) to enable speed on large-scale datasets. It proceeds in multiple stages of Bayesian Optimization with increas- ingly amounts of training data |T 1 train | ≤ . . . , ≤ |T S train |. During each stage s, the k best configu- rations (based on validation accuracy) passed from the previous stage 1 are first evaluated on the cur- rent stage's training data T s train , and then the stan- dard Bayesian Optimization algorithm are initial- ized with these k settings and applied for Y s − k iterations on T s train (discounting the k evaluations done earlier in the stage), where Y s is the total number of iterations for stage s. Then the top k configurations based on validation accuracy are used to initialize the next stage's run.</p><p>We note after the initial stage, rather than only considering candidates passed from the previous stage, the algorithm expands from these points on larger data. Continued exploration using larger <ref type="bibr">1</ref> A special case is the initial stage. We adopt the con- vention that a Sobol sequence is used to initialize the first stage ( <ref type="bibr" target="#b14">Snoek et al., 2012</ref>). The value k for the first stage is the number of points in the Sobol sequence.</p><p>Hyper-parameters SVM bias, cost parameter, and regularization parameter Boosted regression trees feature sampling rate, data sampling rate, learn- ing rate, # trees, # leaves, and minimum # instance per leaf <ref type="table">Table 1</ref>: Hyper-parameters used in SVM and boosted regression trees. data allows the algorithm to eliminate any po- tential sensitivity the hyper-parameters may have with respect to dataset size. After running all S stages the algorithm terminates, and outputs the configuration with the highest validation accuracy from all hyper-parameters explored by all stages (including the initialization points explored by the first stage).</p><p>This multi-stage algorithm subsumes the stan- dard Bayesian optimization algorithm as a special case when the total number of stages S = 1. In our case, for datasets used at stages 1, . . . , S − 1, we use random sampling of full training data to get subsets of data required at these initial stages, while stage S has full data. For the number of top configurations k used to initialize each following stage, we know the larger k is, the better results in the next stage since Bayesian Optimization relies on good initial knowledge to fit good regression models <ref type="bibr" target="#b8">(Feurer et al., 2015)</ref>. However, larger k value also leads to high computation cost at the next stage, since these initial settings will have to be evaluated first. In practice, the number of stages S and the value of k depend on the quantity of the data and the quality of stage-wise model. In our experiments, we empirically choose their values to be S = 2 and k = 3 which result in a good balance between accuracy and speed on the given datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>We empirically evaluate the algorithm on two tasks: classification and question answering. For classification we use the Yelp dataset <ref type="bibr">(Yelp, 2014)</ref> which is a customer review dataset. Each review contains a star/rating (1-5) for a business, and the task is to predict the rating based on the textual in- formation in the review. The training data contains half-million feature vectors, and unique unigrams are used as features (after standard stop-word re-moval and stemming <ref type="bibr" target="#b13">(Manning et al., 2008)</ref>). For question answering (QA), the task is to identify correct answers for a given question. We use a commercial QA dataset containing about 3800 unique training questions and a total of 900, 000 feature vectors. Each feature vector corresponds to an answer candidate for a given question, the vec- tor consists of a binary label (1=correct, 0=incor- rect) and values from standard unigram/bigram, syntactic, and linguistic features used in typical QA applications <ref type="bibr" target="#b17">(Voorhees et al., 2011</ref>). Both QA and Yelp datasets contain independent training, validation, and test data, from which the machine learning models are built, accuracies are evalu- ated, and test results are reported, respectively.</p><p>We evaluate our multi-stage method against two methods: 1) state-of-the-art Bayesian Opti- mization for hyper-parameter learning <ref type="bibr" target="#b14">(Snoek et al., 2012)</ref>), and 2) the same Bayesian Optimiza- tion but only applied on a small subset of data for speed. For experiments, we consider learn- ing hyper-parameters for two machine learning algorithms: SVM implementation for classifica- tion <ref type="bibr" target="#b7">(Fan et al., 2008</ref>) and boosted regression trees for question answering <ref type="bibr" target="#b9">(Ganjisaffar et al., 2011</ref>) as shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Accuracy vs time</head><p>Figures 1 and 2 compare the test accuracy of our proposed multi-stage Bayesian optimization as a function of tuning time for QA and Yelp, respec- tively. The state-of-the-art Bayesian optimiza- tion ( <ref type="bibr" target="#b14">Snoek et al., 2012</ref>) is applied on full train- ing data, and the fast variant of Bayesian Opti- mization is applied with 30% of training data (ran- domly sampled from full dataset). The top-1 and classification accuracies on test data are reported on the y-axis for QA and Yelp, respectively, and the tuning time is reported on the x-axis. For fair- ness of comparison, the multi-stage method uses the same 30% training data at the initial stage, and full training data at the subsequent stage.</p><p>From these figures, while in general both of the comparison methods produce more effective results when given more time, the multi-stage method consistently achieves higher test accuracy than the other two methods across all optimiza- tion time values. For example, best test accu- racy is achieved by the multi-stage algorithm at time (45 min) for the QA task, while both the full Bayesian Optimization and the subset variant  can only achieve a fraction of the best value at the same time value. We also note in general the multi-stage algorithm approaches the upper bound more rapidly as more time is given. This shows that the new algorithm is superior across a wide range of time values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expected accuracy and cost per iteration</head><p>To investigate the average accuracy and cost per iteration achieved by different methods across dif- ferent time points, we compare their mean ex- pected accuracy (according to precision@1 for QA and classification accuracy for Yelp) in Ta- ble 2, and their average speed in <ref type="table" target="#tab_2">Table 3</ref>. In terms of average accuracy, we see that the state- of-the-art Bayesian optimization on full training data and the multi-stage algorithm achieve similar test accuracy, and they both outperform the sub-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA Yelp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayes opt on small subset 3.2 min 0.4 min</head><p>Bayes opt on full data 7.8 min 1.2 min Multi-stage algorithm 6 min 0.6 min set variant of Bayesian Optimization. However, in terms of time per iteration, the full Bayesian Optimization is the most expensive, taking more than twice amount of time over subset variant al- gorithm, while the multi-stage is 23% and 50% faster than standard Bayesian Optimization on QA and Yelp <ref type="table" target="#tab_2">(Table 3)</ref>, respectively, while maintain- ing the same accuracy as full Bayesian Optimiza- tion. This demonstrates the multi-stage approach achieves a good balance between the two baselines and can simultaneously delivers good speedup and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduced a multi-stage optimization algo- rithm for hyper-parameter optimization. The pro- posed algorithm breaks the problem into multiple stages with increasingly amounts of data for effi- cient optimization. We demonstrated its improved performance as compared to the state-of-the-art Bayesian optimization algorithm and fast variants of Bayesian optimization on sentiment classifica- tion and QA tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Model-based Bayesian Optimization (Brochu et al., 2010) starts with an initial set of hyper- parameter settings λ 1 , . . . λ n , where each set- ting denotes a set of assignments to all hyper- parameters. These initial settings are then eval- uated on the validation data and their accuracies are recorded. The algorithm then proceeds in rounds to iteratively fit a probabilistic regression model V to the recorded accuracies. A new hyper- parameter configuration is then suggested by the regression model V with the help of acquisition function (Brochu et al., 2010). Then the accu- racy of the new setting is evaluated on validation data, which leads to the next iteration. A common acquisition function is the expected improvement, EI (Brochu et al., 2010), over best validation accu- racy seen so far L * :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Yelp classification: test accuracy vs tuning time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average test accuracy for QA (preci-
sion@1) and Yelp dataset (classif. accuracy). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Average time (min) per iteration.</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Collaborative hyperparameter tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matyas</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2013: Proceedings of the 30th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Balazs Kegls, and Michele Sebag</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for HyperParameter Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balazs</forename><surname>Kegl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2011: Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">M</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1012.2599v1</idno>
		<imprint>
			<date type="published" when="2010-12-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surrogate Benchmarks for Hyperparameter Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meta-Learning and Algorithm Selection Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient Benchmarking of Hyperparameter Optimizers via Surrogates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2015: Twenty-ninth AAAI Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1871" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Initializing Bayesian Hyperparameter Optimization via Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2015: Twenty-ninth AAAI Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasser</forename><surname>Ganjisaffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2011: Proceedings of the 34th international ACM SIGIR conference on Research and development in Information</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sequential Model-Based Optimization for General Algorithm Configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Hoos and Kevin LeytonBrown</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Efficient Approach for Assessing Hyperparameter Importance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2014: Proceedings of the 31st International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Holger Hoos and Kevin Leyton-brown</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperopt-Sklearn: Automatic Hyperparameter Configuration for Scikit-Learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIPY 2014: Proceedings of the 13th Python In Science Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2012: Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-Task Bayesian Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2013: Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD 2013: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The TREC question answering track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="http://www.yelp.com/dataset_challenge" />
	</analytic>
	<monogr>
		<title level="j">Yelp Academic Challenge Dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Transfer Learning Method for Automatic Hyperparameter Tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 2014: International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lazy Paired Hyper-Parameter Tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Bilenkoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2013: Twenty-third International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
