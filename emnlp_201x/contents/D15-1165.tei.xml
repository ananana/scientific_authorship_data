<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparison between Count and Neural Network Models Based on Joint Translation and Reordering Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Guta</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparison between Count and Neural Network Models Based on Joint Translation and Reordering Sequences</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of n-gram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrase-based systems by up to 2.2 BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Standard phrase-based machine translation ( <ref type="bibr" target="#b31">Och et al., 1999;</ref><ref type="bibr" target="#b39">Zens et al., 2002;</ref><ref type="bibr" target="#b24">Koehn et al., 2003)</ref> uses relative frequencies of phrase pairs to esti- mate a translation model. The phrase table is ex- tracted from a bilingual text aligned on the word level, using e.g. GIZA ++ ( <ref type="bibr" target="#b30">Och and Ney, 2003)</ref>. Al- though the phrase pairs capture internal dependen- cies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguisti- cally consistent falls on the language model (LM). This work proposes word-based translation models that are potentially capable of capturing long-range dependencies. We do this in two steps: First, given bilingual sentence pairs and the asso- ciated word alignments, we convert the informa- tion into uniquely defined linear sequences. These sequenecs encode both word reordering and trans- lation information. Thus, they are referred to as joint translation and reordering (JTR) sequences. Second, we train an n-gram model with modi- fied Kneser-Ney smoothing <ref type="bibr" target="#b6">(Chen and Goodman, 1998</ref>) on the resulting JTR sequences. This yields a model that fuses interdepending reordering and translation dependencies into a single framework.</p><p>Although JTR n-gram models are closely re- lated to the operation sequence model (OSM) ( <ref type="bibr" target="#b14">Durrani et al., 2013b)</ref>, there are three main dif- ferences. To begin with, the OSM employs min- imal translation units (MTUs), which are essen- tially atomic phrases. As the MTUs are extracted sentence-wise, a word can potentially appear in multiple MTUs. In order to avoid overlapping translation units, we define the JTR sequences on the level of words. Consequently, JTR se- quences have smaller vocabulary sizes than OSM sequences and lead to models with less sparsity. Moreover, we argue that JTR sequences offer a simpler reordering approach than operation se- quences, as they handle reorderings without the need to predict gaps. Finally, when used as an additional model in the log-linear framework of phrase-based decoding, an n-gram model trained on JTR sequences introduces only one single fea- ture to be tuned, whereas the OSM additionally uses 4 supportive features ( <ref type="bibr" target="#b14">Durrani et al., 2013b</ref>). Experimental results confirm that this simplifica- tion does not make JTR models less expressive, as their performance is on par with the OSM.</p><p>Due to data sparsity, increasing the n-gram or- der of count-based models beyond a certain point becomes useless. To address this, we resort to neu-ral networks (NNs), as they have been successfully applied to machine translation recently <ref type="bibr" target="#b35">(Sundermeyer et al., 2014;</ref><ref type="bibr" target="#b11">Devlin et al., 2014</ref>). They are able to score any word combination without re- quiring additional smoothing techniques. We ex- periment with feed-forward and recurrent trans- lation networks, benefiting from their smoothing capabilities. To this end, we split the linear se- quence into two sequences for the neural transla- tion models to operate on. This is possible due to the simplicity of the JTR sequence. We show that the count and NN models perform well on their own, and that combining them yields even better results.</p><p>In this work, we apply n-gram models with modified Kneser-Ney smoothing during phrase- based decoding and neural JTR models in rescor- ing. However, using a phrase-based system is not required by the model, but only the initial step to demonstrate the strength of JTR models, which can be applied independently of the underlying de- coding framework. While the focus of this work is on the development and comparison of the models, the long-term goal is to decode using JTR mod- els without the limitations introduced by phrases, in order to exploit the full potential of JTR mod- els. The JTR models are estimated on word align- ments, which we obtain using GIZA ++ in this pa- per. The future aim is to also generate improved word alignments by a joint optimization of both the alignments and the models, similar to the train- ing of IBM models <ref type="bibr" target="#b3">(Brown et al., 1990;</ref><ref type="bibr" target="#b4">Brown et al., 1993</ref>). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>In order to address the downsides of the phrase translation model, various approaches have been taken. <ref type="bibr" target="#b27">Mariño et al. (2006)</ref> proposed a bilingual language model (BILM) that operates on bilin- gual n-grams, with an own n-gram decoder re- quiring monotone alignments. The lexical re- ordering model introduced in <ref type="bibr" target="#b37">(Tillmann, 2004</ref>) was integrated into phrase-based decoding. <ref type="bibr" target="#b9">Crego and Yvon (2010)</ref> adapted the approach to BILMs. The bilingual n-grams are further advanced in <ref type="bibr" target="#b29">(Niehues et al., 2011</ref>), where they operate on non- monotone alignments within a phrase-based trans- lation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. <ref type="bibr" target="#b12">Durrani et al. (2011)</ref> developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single frame- work. It used an own decoder that was based on n- grams of MTUs and predicted single translation or reordering operations. This was further advanced in ( <ref type="bibr" target="#b13">Durrani et al., 2013a</ref>) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In ( <ref type="bibr" target="#b14">Durrani et al., 2013b</ref>), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system ( <ref type="bibr" target="#b25">Koehn et al., 2007)</ref>. Both the BILM ( <ref type="bibr" target="#b34">Stewart et al., 2014</ref>) and the OSM ( <ref type="bibr" target="#b15">Durrani et al., 2014</ref>) can be smoothed using word classes. <ref type="bibr" target="#b21">Guta et al. (2015)</ref> introduced the extended trans- lation model (ETM), which operates on the word level and augments the IBM models by an addi- tional bilingual word pair and a reordering opera- tion. It is implemented into the log-linear frame- work of a phrase-based decoder and shown to be competitive with a 7-gram OSM.</p><p>The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a sin- gle model. The ETM, however, features separate models for the translation of individual words and reorderings and provides an explicit treatment of multiple alignments. As they operate on linear se- quences, JTR count models can be implemented using existing toolkits for n-gram language mod- els, e.g. the KenLM toolkit ( <ref type="bibr" target="#b22">Heafield et al., 2013</ref>).</p><p>An HMM approach for word-to-phrase align- ments was presented in <ref type="bibr" target="#b10">(Deng and Byrne, 2005)</ref>, showing performance similar to IBM Model 4 on the task of bitext alignment.  propose several models which rely only on the in- formation provided by the source side and pre- dict reorderings. Contrastingly, JTR models in- corporate target information as well and predict both translations and reorderings jointly in a sin- gle framework. <ref type="bibr" target="#b40">Zhang et al. (2013)</ref> explore different Markov chain orderings for an n-gram model on MTUs in rescoring. <ref type="bibr" target="#b16">Feng and Cohn (2013)</ref> present an- other generative word-based Markov chain trans- lation model which exploits a hierarchical Pitman- Yor process for smoothing, but it is only applied to induce word alignments. Their follow-up work <ref type="bibr" target="#b18">(Feng et al., 2014</ref>) introduces a Markov-model on MTUs, similar to the OSM described above.</p><p>Recently, neural machine translation has emerged as an alternative to phrase-based decod- ing, where NNs are used as standalone models to decode source input. In ( <ref type="bibr" target="#b36">Sutskever et al., 2014</ref>), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network. The network did not have any explicit treatment of alignments. <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref> introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no mod- ifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in <ref type="bibr" target="#b11">(Devlin et al., 2014</ref>), while the recurrent models are based on <ref type="bibr" target="#b35">(Sundermeyer et al., 2014</ref>). Further recent research on applying NN models for extended context was carried out in ( <ref type="bibr" target="#b26">Le et al., 2012;</ref><ref type="bibr" target="#b1">Auli et al., 2013;</ref><ref type="bibr" target="#b23">Hu et al., 2014</ref>). All of these works focus on lexical context and ignore the reordering aspect covered in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JTR Sequences</head><p>The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and re- ordering (JTR) tokens g K 1 . Formally, the sequence</p><formula xml:id="formula_0">g K 1 ( f J 1 , e I 1 , b I 1 )</formula><p>is a uniquely defined interpretation of a given source sentence f J 1 , its translation e I 1 and the inverted alignment b I 1 , where b i denotes the ordered sequence of source positions j aligned to target position i. We drop the explicit mention of ( f J 1 , e I 1 , b I 1 ) to allow for a better readability. Each JTR token is either an aligned bilingual word pair f , e or a reordering class ∆ j j . Unaligned words on the source and target side are processed as if they were aligned to the empty word ε. Hence, an unaligned source word f gener- ates the token f , ε, and an unaligned target word e the token ε, e.</p><p>Each word of the source and target sentences is to appear in the corresponding JTR sequence ex- actly once. For multiply-aligned target words e, the first source word f that is aligned to e gener- ates the token f , e. All other source words f , that are also aligned to e, are processed as if they were aligned to the artificial word σ . Thus, each of these f generates a token f , σ . The same approach is applied to multiply-aligned source</p><formula xml:id="formula_1">Algorithm 1 JTR Conversion Algorithm 1: procedure JTRCONVERSION( f J 1 , e I 1 , b I 1 ) 2: g K 1 ← / 0 3: // last translated source position j 4: j ← 0 5: for i ← 1 to I do 6:</formula><p>if e i is unaligned then 7:</p><p>// align e i to the empty word ε 8:</p><formula xml:id="formula_2">APPEND(g K 1 , ε, e i ) 9:</formula><p>continue 10:</p><p>// e i is aligned to at least one source word 11:</p><p>j ← first source position in b i 12:</p><p>if j = j then 13:</p><p>// e i is aligned to the same f j as e i−1 14:</p><formula xml:id="formula_3">APPEND(g K 1 , σ , e i ) 15: continue 16: if j = j + 1 then 17: // alignment step is non-monotone 18: REORDERINGS( f J 1 , b I 1 , g K 1 , j , j) 19:</formula><p>// 1-to-1 translation: f j is aligned to e i 20:</p><formula xml:id="formula_4">APPEND(g K 1 , f j , e i ) 21: j ← j 22:</formula><p>// generate all other f j that are also 23:</p><p>// aligned to the current target word e i 24:</p><p>for all remaining j in b i do 25:</p><formula xml:id="formula_5">APPEND(g K 1 , f j , σ ) 26: j ← j 27:</formula><p>// check last alignment step at sentence end 28:</p><formula xml:id="formula_6">if j = J then 29: // last alignment step is non-monotone 30: REORDERINGS( f J 1 , b I 1 , g K 1 , j , J + 1) 31: return g K 1 32</formula><p>: 33: // called when a reordering class is appended 34:</p><formula xml:id="formula_7">procedure REORDERINGS( f J 1 , b I 1 , g K 1 , j , j) 35:</formula><p>// check if the predecessor is unaligned 36:</p><p>if f j−1 is unaligned then 37:</p><p>// get unaligned predecessors 38:</p><formula xml:id="formula_8">f j−1 j 0 ← unaligned predecessors of f j 39:</formula><p>// check if the alignment step to the first 40:</p><p>// unaligned predecessor is monotone 41:</p><formula xml:id="formula_9">if j 0 = j + 1 then 42:</formula><p>// non-monotone: add reordering class 43:</p><formula xml:id="formula_10">APPEND(g K 1 , ∆ j , j 0 ) 44:</formula><p>// translate unaligned predecessors by ε 45:</p><p>for f ← f j 0 to f j−1 do 46:</p><formula xml:id="formula_11">APPEND(g K 1 , f , ε) 47: else 48:</formula><p>// non-monotone: add reordering class 49:</p><formula xml:id="formula_12">APPEND(g K 1 , ∆ j , j )</formula><p>words. Similar to <ref type="bibr" target="#b16">Feng and Cohn (2013)</ref>, we clas- sify the reordered source positions j and j by ∆ j j :</p><formula xml:id="formula_13">∆ j j =      step backward (←), j = j − 1 jump forward (), j &gt; j + 1 jump backward (), j &lt; j − 1.</formula><p>The reordering classes are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence Conversion</head><p>Algorithm 1 presents the formal conversion of a bilingual sentence pair and its alignment into the corresponding JTR sequence g K 1 . At first, g K 1 is initialized by an empty sequence (line 2). For each target position i = 1, . . . , I it is extended by at least one token. During the generation process, we store the last visited source position j (line 4). If a tar- get word e i is</p><p>• unaligned, we align it to the empty word ε and append ε, e i to the current g K 1 (line 8), • if it is aligned to the same f j as e i−1 , we only add σ , e i (line 14), • otherwise we append f j , e i (line 20) and • in case there are more source words aligned to e i , we additionally append f j , σ for each of these (line 24).</p><p>Before a token f j , e i is generated, we have to check whether the alignment step from j to j is monotone (line 16). In case it is not, we have to deal with reorderings (line 34). We define that a token f j−1 , ε is to be generated right before the generation of the token containing f j . Thus, if f j−1 is not aligned, we first determine the con- tiguous sequence of unaligned predecessors f j−1 j 0 (line 38). Next, if the step from j to j 0 is not monotone, we add the corresponding reordering class (line 43). Afterwards we append all f j 0 , ε to f j−1 , ε. If f j−1 is aligned, we do not have to process unaligned source words and only append the corresponding reordering class (line 49). <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the generation steps of a JTR sequence, whose result is presented in Ta- ble 1. The alignment steps are denoted by the ar- rows connecting the alignment points. The first dashed alignment point indicates the ε, , token that is generated right after the Feld, field to- ken. The second dashed alignment point indicates the ein, ε token, which corresponds to the un- aligned source word ein. Note, that the ein, ε  token has to be generated right before ., . is generated. Therefore, there is no forward jump from Code, code to ., ., but a monotone step to ein, ε followed by ., ..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training of Count Models</head><p>As the JTR sequence g K 1 is a unique interpretation of a bilingual sentence pair and its alignment, the probability p( f J 1 , e I 1 , b I 1 ) can be computed as:</p><formula xml:id="formula_14">p( f J 1 , e I 1 , b I 1 ) = p(g K 1 ).<label>(1)</label></formula><p>The probability of g K 1 can be factorized and ap- proximated by an n-gram model.</p><formula xml:id="formula_15">p(g K 1 ) = K ∏ k=1 p(g k |g k−1 k−n+1 )<label>(2)</label></formula><p>Within this work, we first estimate the Viterbi alignment for the bilingual training data using GIZA ++ ( <ref type="bibr" target="#b30">Och and Ney, 2003)</ref>. Secondly, the con- version presented in Algorithm 1 is applied to ob- tain the JTR sequences, on which we estimate an n-gram model with modified Kneser-Ney smooth- ing as described in <ref type="bibr" target="#b6">(Chen and Goodman, 1998</ref>) us- ing the KenLM toolkit 1 ( <ref type="bibr" target="#b22">Heafield et al., 2013</ref>). </p><note type="other">s k t k 1 δ 2 im, in im in 3 σ , the σ the 4 δ 5 Befehl, Command Befehl Command 6 ← δ ← 7 Feld, field Feld field 8 ε, , ε , 9 δ 10 geben, enter geben enter 11 Sie, σ Sie σ 12 δ 13 Ihren, your Ihren your 14 Code, code Code code 15 ein, ε ein ε 16 ., .</note><p>. . <ref type="table">Table 1</ref>: The left side of this table presents the JTR tokens g k corresponding to <ref type="figure" target="#fig_2">Figure 2</ref>. The right side shows the source and target tokens s k and t k obtained from the JTR tokens g k . They are used for the training of NNs (cf. Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integration into Phrase-based Decoding</head><p>Basically, each phrase table entry is annotated with both the word alignment information, which also allows to identify unaligned source words, and the corresponding JTR sequence. The JTR model is added to the log-linear framework as an additional n-gram model. Within the phrase-based decoder, we extend each search state such that it additionally stores the JTR model history.</p><p>In comparison to the OSM, the JTR model does not predict gaps. Local reorderings within phrases are handled implicitly. On the other hand, we rep- resent long-range reorderings between phrases by the coverage vector and limit them by reordering constraints.</p><p>Phrase-pairs ending with unaligned source words at their right boundary prove to be a prob- lem during decoding. As shown in Subsection 3.1, the conversion from word alignments to JTR se- quences assumes that each token corresponding to an unaligned source word is generated immedi- ately before the token corresponding to the closest aligned source position to its right. However, if a phrase ends with an unaligned f j as its rightmost source word, the generation of the f j , ε token has to be postponed until the next word f j+1 is to be translated or, even worse, f j+1 has already been translated before.</p><p>To address this issue, we constrained the phrase table extraction to discard entries with unaligned source tokens at the right boundary. For IWSLT De→En, this led to a baseline weaker by 0.2 BLEU than the one described in Section 5. In order to have an unconstrained and fair baseline, we there- after removed this constraint and forced such dele- tion tokens to be generated at the end of the se- quence. Hence, we accept that the JTR model might compute the wrong score in these special cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Networks</head><p>Usually, smoothing techniques are applied to count-based models to handle unseen events. A neural network does not suffer from this, as it is able to score unseen events without additional smoothing techniques. In the following, we will describe how to adapt JTR sequences to be used with feed-forward and recurrent NNs.</p><p>The first thing to notice is the vocabulary size, mainly determined by the number of bilingual word pairs, which constituted atomic units in the count-based models. NNs that compute probabil- ity values at the output layer evaluate a softmax function that produces normalized scores that sum up to unity. The softmax function is given by:</p><formula xml:id="formula_16">p(e i |e i−1 1 ) = e o e i (e i−1 1 ) ∑ |V | w=1 e o w (e i−1 1 )<label>(3)</label></formula><p>where o e i and o w are the raw unnormalized output layer values for the words e i and w, respectively, and |V | is the vocabulary size. The output layer is a function of the context e i−1 1 . Computing the denominator is expensive for large vocabularies, as it requires computing the output for all words. Therefore, we split JTR tokens g k and use indi- vidual words as input and output units, such that the NN receives jumps, source and target words as input and outputs target words and jumps. Hence, the resulting neural model is not a LM, but a trans- lation model with different input and output vo- cabularies. A JTR sequence g K 1 is split into its source and target parts s K 1 and t K 1 . The construc- tion of the JTR source sequence s K 1 proceeds as follows: Whenever a bilingual pair is encountered, the source word is kept and the target word is dis- carded. In addition, all jump classes are replaced by a special token δ . The JTR target sequence t K 1 is constructed similarly by keeping the target words and dropping source words, and the jump classes are also kept. <ref type="table">Table 1</ref> shows the JTR source and target sequences corresponding to JTR sequence of  Due to the design of the JTR sequence, pro- ducing the source and target JTR sequences is straightforward. The resulting sequences can then be used with existing NN architectures, without further modifications to the design of the net- works. This results in powerful models that re- quire little effort to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feed-forward Neural JTR</head><p>First, we will apply a feed-forward NN <ref type="figure">(FFNN)</ref> to the JTR sequence. FFNN models resemble count- based models in using a predefined limited context size, but they do not encounter the same smooth- ing problems. In this work, we use a FFNN similar to that proposed in <ref type="bibr" target="#b11">(Devlin et al., 2014</ref>), defined as:</p><formula xml:id="formula_17">p(t K 1 |s K 1 ) ≈ K ∏ k=1 p(t k |t k−1 k−n , s k k−n ).<label>(4)</label></formula><p>It scores the JTR target word t k at position k us- ing the current source word s k , and the history of n JTR source words. In addition, the n JTR target words preceding t k are used as context. The FFNN computes the score by looking up the vector em- beddings of the source and target context words, concatenating them, then evaluating the rest of the network. We reduce the output layer to a short- list of the most frequent words, and compute word class probabilities for the remaining words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent Neural JTR</head><p>Unlike feed-forward NNs, recurrent NNs (RNNs) enable the use of unbounded context. Following <ref type="bibr" target="#b35">(Sundermeyer et al., 2014</ref>), we use bidirectional recurrent NNs (BRNNs) to capture the full JTR source side. The BRNN uses the JTR target side as well as the full JTR source side as context, and it is given by:</p><formula xml:id="formula_18">p(t K 1 |s K 1 ) = K ∏ k=1 p(t k |t k−1 1 , s K 1 )<label>(5)</label></formula><p>This equation is realized by a network that uses forward and backward recurrent layers to capture the complete source sentence. By a forward layer we imply a recurrent hidden layer that processes a given sequence from left to right, while a back- ward layer does the processing backwards, from right to left. The source sentence is basically split at a given position k, then past and future represen- tations of the sentence are recursively computed by the forward and backward layers, respectively. To include the target side, we provide the forward layer with the target input t k−1 as well, that is, we aggregate the embeddings of the input source word s k and the input target word t k−1 before they are fed into the forward layer. Due to recurrency, the forward layer encodes the parts (t k−1 1 , s k 1 ), and the backward layer encodes s K k , and together they encode (t k−1 1 , s K 1 ), which is used to score the out- put target word t k . For the sake of comparison to FFNN and count models, we also experiment with a recurrent model that does not include future source information, this is obtained by replacing the term s K 1 with s k 1 in Eq. 5. It will be referred to as the unidirectional recurrent neural network (URNN) model in the experiments.</p><p>Note that the JTR source and target sides include jump information, therefore, the RNN model described above explicitly models reorder- ing. In contrast, the models proposed in <ref type="bibr" target="#b35">(Sundermeyer et al., 2014</ref>) do not include any jumps, and hence do not provide an explicit way of includ- ing word reordering. In addition, the JTR RNN models do not require the use of IBM-1 lexica to resolve multiply-aligned words. As discussed in Section 3, these cases are resolved by aligning the multiply-aligned word to the first word on the op- posite side.</p><p>The integration of the NNs into the decoder is not trivial, due to the dependence on the target context. In the case of RNNs, the context is un- bounded, which would affect state recombination, and lead to less variety in the beam used to prune the search space. Therefore, the RNN scores are computed using approximations instead <ref type="bibr" target="#b1">(Auli et al., 2013;</ref><ref type="bibr" target="#b0">Alkhouli et al., 2015</ref>). In ( <ref type="bibr" target="#b0">Alkhouli et al., 2015)</ref>, it is shown that approximate RNN inte- gration into the phrase-based decoder has a slight advantage over n-best rescoring. Therefore, we apply RNNs in rescoring in this work, and to al- low for a direct comparison between FFNNs and RNNs, we apply FFNNs in rescoring as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We perform experiments on the large- scale <ref type="bibr">IWSLT 2013</ref>   <ref type="table" target="#tab_2">108M  109M  106M  108M  78M  86M  Vocabulary  836K  792K  814K  773K  384K  817K   Table 2</ref>: Statistics for the bilingual training data of the IWSLT 2013 German→English, WMT 2015 German→English, and the DARPA BOLT Chinese→English translation tasks.</p><p>( <ref type="bibr" target="#b30">Och and Ney, 2003)</ref>. We use a standard phrase- based translation system ( <ref type="bibr" target="#b24">Koehn et al., 2003)</ref>. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit ( <ref type="bibr" target="#b22">Heafield et al., 2013)</ref>. The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 × 100 nodes, the first hidden layer 1000 and the sec- ond 500. The RNNs have LSTM architectures. The URNN has 2 hidden layers while the BRNN has one forward, one backward and one addi- tional hidden layer. All layers have 200 nodes, while the output layer is class-factored using 2000 classes. For the count-based JTR model and OSM we tuned the n-gram size on the tuning set of each task. For the full data, 7-grams were used for the IWSLT and WMT tasks, and 8-grams for BOLT. When using in-domain data, smaller n-gram sizes were used. All rescoring experiments used 1000- best lists without duplicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks description</head><p>The domain of IWSLT consists of lecture-type talks presented at TED conferences which are also available online <ref type="bibr">4</ref> . All systems are optimized on the dev2010 corpus, named dev here. Some of the OSM and JTR systems are trained on the TED portions of the data containing 138K sen- tences. To estimate the 4-gram LM, we addi- tionally make use of parts of the Shuffled News, LDC English Gigaword and 10 9 -French-English corpora, selected by a cross-entropy difference cri- terion <ref type="bibr" target="#b28">(Moore and Lewis, 2010)</ref>. In total, 1.7 bil- lion running words are taken for LM training. The BOLT Chinese→English task is evaluated on the "discussion forum" domain. The 5-gram LM is trained on 2.9 billion running words in total. The in-domain data consists of a subset of 67.8K sen- tences and we used a set of 1845 sentences for tun- ing. The evaluation set test1 contains 1844 and test2 1124 sentences. For the WMT task, we used the target side of the bilingual data and all monolingual data to train a pruned 5-gram LM on a total of 4.4 billion running words. We concate- nated the newstest2011 and newstest2012 corpora for tuning the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We start with the IWSLT 2013 German→ English task, where we compare between the different JTR and OSM models. The results are shown in Ta- ble 3. When comparing the in-domain n-gram JTR model trained using Kneser-Ney smoothing (KN) to OSM, we observe that the n-gram KN JTR model improves the baseline by 1.4 BLEU on both test and eval11. The OSM model performs similarly, with a slight disadvantage on eval11. In comparison, the FFNN of Eq. <ref type="formula" target="#formula_17">(4)</ref>    part of the source input when scoring target words. This information is not used by the KN model. Moreover, the BRNN is able to score word com- binations unseen in training, while the KN model uses backing off to score unseen events. When training the KN, FFNN, and OSM mod- els on the full data, we observe less gains in com- parison to in-domain data training. However, com- bining the KN models trained on in-domain and full data gives additional gains, which suggests that although the in-domain model is more adapted to the task, it still can gain from out-of-domain data. Adding the FFNN on top improves the com- bination. Note here that the FFNN sees the same information as the KN model, but the difference is that the NN operates on the word level rather than the word-pair level. Second, the FFNN is able to handle unseen sequences by design, without the need for the backing off workaround. The BRNN improves the combination more than the FFNN, as the model captures an unbounded source and target history in addition to an unbounded future source context. Combining the KN, FFNN and BRNN JTR models leads to an overall gain of 2.2 BLEU on both dev and test.</p><p>Next, we present the BOLT Chinese→English results, shown in <ref type="table" target="#tab_3">Table 4</ref>. Comparing n-gram KN JTR and OSM trained on the in-domain data shows they perform equally well on test1, im- proving the baseline by 0.7 BLEU, with a slight ad- vantage for the JTR model on test2. The feed- forward and the recurrent in-domain networks yield the same results in comparison to each other. Training the OSM and JTR models on the full data yields slightly worse results than in-domain train- ing. However, combining the two types of training improves the results. This is shown when adding the in-domain KN JTR model on top of the model trained on full data, improving it by up to 0.4 BLEU. Rescoring with the feed-forward and the recurrent network improves this even further, sup- porting the previous observation that the n-gram KN JTR and NNs complement each other. The combination of the 4 models yields an overall im- provement of 1.2-1.4 BLEU.</p><p>Finally, we compare KN JTR and OSM models on the WMT German→English task in <ref type="table">Table 5</ref>. The two models perform almost similar to each other. The JTR model improves the baseline by up to 0.7 BLEU. Rescoring the KN JTR with the FFNN improves it by up to 0.3 BLEU leading to an overall improvement between 0.5 and 1.0 BLEU.   <ref type="table">Table 5</ref>: Results measured in BLEU for the WMT German→English task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>To investigate the effect of including jump infor- mation in the JTR sequence, we trained a BRNN using jump classes and another excluding them. The BRNNs were used in rescoring. Below, we demonstrate the difference between the systems:</p><p>source: wir kommen später noch auf diese Leute zurück . reference: We'll come back to these people later . Hypothesis 1: JTR source: wir kommen δ zurück δ später noch auf diese Leute δ . JTR target: we come back later σ to these people . Hypothesis 2: JTR source: wir kommen später noch auf diese Leute zurück . JTR target: we come later σ on these guys back .</p><p>Note the German verb "zurückkommen", which is split into "kommen" and "zurück". German places "kommen" at the second position and "zurück" towards the end of the sentence. Unlike German, the corresponding English phrase "come back" has the words adjacent to each other. We found that the system including jumps prefers the correct translation of the verb, as shown in Hy- pothesis 1 above. The system translates "kom- men" to "come", jumps forward to "zurück", translates it to "back", then jumps back to continue translating the word "später". In contrast, the sys- tem that excludes jump classes is blind to this sep- aration of words. It favors Hypothesis 2 which is a strictly monotone translation of the German sen- tence. This is also reflected by the BLEU scores, where we found the system including jump classes outperforming the one without by up to 0.8 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a method that converts bilingual sentence pairs and their word alignments into joint translation and reordering (JTR) sequences. They combine interdepending lexical and alignment de- pendencies into a single framework. A main ad- vantage of JTR sequences is that a variety of mod- els can be trained on them. Here, we have esti- mated n-gram models with modified Kneser-Ney smoothing, FFNN and RNN architectures on JTR sequences.</p><p>We compared our count-based JTR model to the OSM, both used in phrase-based decoding, and showed that the JTR model performed at least as good as OSM, with a slight advantage for JTR. In comparison to the OSM, the JTR model operates on words, leading to a smaller vocabulary size. Moreover, it utilizes simpler reordering structures without gaps and only requires one log-linear fea- ture to be tuned, whereas the OSM needs 5. Due to the flexibility of JTR sequences, we can ap- ply them also to FFNNs and RNNs. Utilizing two count models and applying both networks in rescoring gains the overall highest improvement over the phrase-based system by up to 2.2 BLEU, on the German→English IWSLT task. The com- bination outperforms OSM by up to 1.2 BLEU on the BOLT Chinese→English tasks.</p><p>The JTR models are not dependent on the phrase-based framework, and one of the long- term goals is to perform standalone decoding with the JTR models independently of phrase-based systems. Without the limitations introduced by phrases, we believe that JTR models could per- form even better. In addition, we aim to use JTR models to obtain the alignment, which would then be used to train the JTR models in an iterative manner, achieving consistency and hoping for im- proved models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the different reordering classes in JTR sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: This example illustrates the JTR sequence g K 1 for a German→English sentence pair including the word-to-word alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results measured in BLEU for the IWSLT 
German→English task. 

train data test1 test2 

baseline 
18.1 
17.0 

+OSM 
indomain 
18.8 
17.2 
+FFNN 
indomain 
18.6 
17.6 
+BRNN 
indomain 
18.6 
17.6 
+KN 
indomain 
18.8 
17.5 

+OSM 
full 
18.5 
17.2 
+FFNN 
full 
18.4 
17.4 
+KN 
full 
18.8 
17.3 

+KN 
indomain 
19.0 
17.7 
+FFNN 
full 
19.2 
18.3 
+RNN indomain 
19.3 
18.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results measured in BLEU for the BOLT 
Chinese→English task. 

URNN is that the latter captures the unbounded 
source and target history that extends until the be-
ginning of the sentences, giving it an advantage 
over the FFNN. The performance of the URNN 
can be improved by including the future part of the 
source sentence, as described in Eq. (5), resulting 
in the BRNN model. Next, we explore whether the 
models are additive. When rescoring the n-gram 
KN JTR output with the BRNN, an additional im-
provement of 0.6 BLEU is obtained. There are two 
reasons for this: The BRNN includes the future 

</table></figure>

			<note place="foot" n="1"> https://kheafield.com/code/kenlm/</note>

			<note place="foot" n="2"> http://www.iwslt2013.org 3 http://www.statmt.org/wmt15/</note>

			<note place="foot" n="4"> http://www.ted.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has received funding from the Euro-pean Union's Horizon 2020 research and innova-tion programme under grant agreement n o 645452 (QT21). This material is partially based upon work supported by the DARPA BOLT project un-der Contract No. HR0011-12-C-0015. Any opin-ions, findings and conclusions or recommenda-tions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigations on phrase-based decoding with recurrent neural network language and translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Rietig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2015 Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint Language and Translation Modeling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Seattle, USA, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, Calefornia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Statistical Approach to Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">A Della</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrick</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<title level="m">The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993-06" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<meeting><address><addrLine>Lake Tahoe, CA, USA, December</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Empirical Study of Smoothing Techniques for Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998-08" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Group, Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unpacking and transforming feature functions: New ways to smooth phrase tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT Summit XIII</title>
		<meeting><address><addrLine>Xiamen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="269" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual Meeting of the Association for Computational Linguistics:shortpapers</title>
		<meeting><address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving reordering with linguistically informed bilingual n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010: Posters)</title>
		<meeting>the 23rd International Conference on Computational Linguistics (Coling 2010: Posters)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="197" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hmm word and phrase alignment for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A joint sequence translation model with integrated reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1045" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model with minimal translation units, but decode with phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can markov models over minimal translation units help phrase-based smt?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="399" to="405" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Investigating the usefulness of generalized word representations in smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A markov model of machine translation using non-parametric bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Advancements in reordering models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Assoc. for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factored markov translation with robust modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkai</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple and effective hierarchical phrase reordering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on</title>
		<meeting>the Conference on</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Natural Language Processing, EMNLP &apos;08</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extended translation models in phrase-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Guta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2015 Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03)</title>
		<meeting>the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03)<address><addrLine>Edmonton, Alberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<title level="m">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<meeting><address><addrLine>Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantine, and Evan Herbst; Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continuous Space Translation Models with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Hai Son Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">N-gram-based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>José B Mariño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrì</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Fonollosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Costa-Jussà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="527" to="549" />
			<date type="published" when="2006" />
			<publisher>December</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Intelligent Selection of Language Model Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Short Papers)</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Proceedings of the Sixth Workshop on Statistical Machine Translation, chapter Wider Context by Using Bilingual Language Models in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved Alignment Models for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>Joint SIGDAT Conf. on Empirical Methods in Natural Language essing and Very Large Corpora<address><addrLine>University of Maryland, College Park, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 41th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
		<idno>RC22176 (W0109-022</idno>
	</analytic>
	<monogr>
		<title level="j">P.O. Box</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">IBM Research Report</note>
	<note>IBM Research Division</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coarse split and lump bilingual languagemodels for richer source information in smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darelene</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Joanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMTA</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Translation Modeling with Bidirectional Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuebker</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unigram orientation model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short &apos;04</title>
		<meeting>HLT-NAACL 2004: Short Papers, HLTNAACL-Short &apos;04<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation with word class models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Rietig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1377" to="1381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Phrase-Based Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th German Conf. on Artificial Intelligence (KI2002)</title>
		<meeting><address><addrLine>Aachen, Germany, September</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond left-to-right: Multiple decomposition structures for smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
