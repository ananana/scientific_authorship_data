<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stack-based Multi-layer Attention for Transition-based Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stack-based Multi-layer Attention for Transition-based Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1677" to="1682"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although sequence-to-sequence (seq2seq) network has achieved significant success in many NLP tasks such as machine translation and text summarization, simply applying this approach to transition-based dependency parsing cannot yield a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM and head selection. In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information. In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning models have been proven very ef- fective in solving various NLP problems such as language modeling, machine translation and syn- tactic parsing. For dependency parsing, one line of research aims to incrementally integrate dis- tributed word representations into classic depen- dency parsing <ref type="bibr" target="#b2">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b17">Weiss et al., 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr" target="#b4">Cross and Huang, 2016;</ref><ref type="bibr" target="#b9">Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b5">Dozat and Manning, 2016)</ref>. Another line of research at- tempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model <ref type="bibr" target="#b6">(Dyer et al., 2015;</ref><ref type="bibr" target="#b20">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Wiseman and Rush, 2016)</ref>. Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization ( <ref type="bibr" target="#b3">Cho et al., 2014;</ref><ref type="bibr" target="#b14">Rush et al., 2015)</ref>. Unfortunately, to our best knowledge, sim- ply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection <ref type="bibr" target="#b6">(Dyer et al., 2015;</ref><ref type="bibr" target="#b20">Zhang et al., 2017)</ref>.</p><p>One issue with the simple seq2seq neural net- work for dependency parsing is that structural lin- guistic information, which plays a key role in clas- sic transition-based or graph-based dependency parsing models, cannot be explicitly employed. For example, classic transition-based parsing al- gorithm utilizes a stack to manage the heads of partial sub-trees and leverages these evidents for action selection, while such information is missing from current seq2seq models. Another problem is related to the limit of the conventional attention network being used in seq2seq network, which is unable to capture dependencies between words in the input. As a matter of fact, various types of fea- tures (word unigram, bigram, trigram, . . . ) tradi- tionally adopted by transition-based parsing algo- rithm are usually ignored by the current attention mechanism, but they are very important to capture word dependencies in generated partial trees.</p><p>In this paper, we propose a stack-based multi- layer attention mechanism to solve the above problems. To simulate the stack used in the transition-based dependency parsing, we intro- duce two binary vectors, one indicates whether a word is pushed into the stack, and another indi- cates whether a word is popped out from it. To model the complex structural information, we pro- pose a multi-layer attention based on the stack information, previous action and input sentence.</p><p>The multi-layer attention aims to capture multiple word dependencies in partial trees for action pre- diction.</p><p>We evaluate our model on English and Chinese datasets. Experimental results show that our pro- posed model can significantly outperform the ba- sic seq2seq model with 1.87 UAS (English) and 1.61 UAS (Chinese), matching the state-of-the-art parsing performance. With 4 models ensembled, we obtain further improvements with accuracies of 94.16 UAS (English) and 87.97 UAS (Chinese).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Model for</head><p>Sequence-to-Sequence Learning</p><p>In this work, we follow the encoder-decoder archi- tecture proposed by <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>. The whole architecture can be divided into three com- ponents: encoder, decoder and attention.</p><p>Encoder: The encoder reads in the source sen- tence X = (x 1 , x 2 , ... , x T ) and transforms it into a sequence of hidden states h = (h 1 , h 2 , ... , h T ), using a bi-directional recurrent neural network that is usually implemented as Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b3">Cho et al., 2014)</ref> or Long Short- Term Memory (LSTM) networks <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>Attention Mechanism: The context vec- tor c i is a weighted sum of the hidden states (h 1 , h 2 , ... , h T ) with the coefficients α i,1 , α i,2 , ... , α i,T computed by</p><formula xml:id="formula_0">α i,t = exp (e i,t ) k exp (e i,k )<label>(1)</label></formula><formula xml:id="formula_1">e i,t = v a tanh(W a z i−1 + U a h t )<label>(2)</label></formula><p>where v a ,W a , U a are the weight matrices.</p><p>Decoder: The decoder uses another recurrent neural network to generate a corresponding target sequence Y = (y 1 , y 2 , ... , y T ) based on the en- coded sequence of hidden state h. At each time i, the conditional probability of target symbol y i is computed by</p><formula xml:id="formula_2">z i = RNN([y i−1 ; c i ], z i−1 ) (3) p(y i |y &lt;i , h) = softmax(g(y i−1 , z i , c i ))<label>(4)</label></formula><p>where g is a non-linear function, z i is the i th hid- den state of the decoder, and it is calculated condi- tional on the previous hidden state z i−1 , previous target symbol y i−1 and source context vector c i .  3 Sequence-to-Sequence Parsing Model</p><formula xml:id="formula_3">ℎ 1 ℎ 2 ℎ í µí± ℎ 1 ℎ 2 ℎ í µí± … … [w 1 , í</formula><p>Transition-based dependency parsing conceptual- izes the process of transforming a sentence into a dependency tree as a sequence of actions. It can be formulated as a sequence-to-sequence problem, and seq2seq framework can be applied. Compared with other tasks, such as machine translation, de- pendency parsing not only considers the previous action and input sentence, but also requires many structure information, such as the subtree structure during the parsing. Such information plays an im- portant role in transition-based dependency pars- ing, so traditional methods adopt a stack to save structure information and design different type of features (word unigram, bigram, trigram, . . . ) to model them. However, vanilla seq2seq models have no explicit structure to model these neces- sary structure information. To better leverage the structure information, we extend the basic seq2seq architecture with a simulated stack and multi-layer attention, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The main structure (encoder, decoder and attention part in <ref type="figure" target="#fig_0">Figure 1</ref>) of our parsing model is detailed below.</p><p>Encoder: As shown in the encoder part of <ref type="figure">Fig-ure</ref> 1, to utilize POS tag information, each word w i is additionally represented by x i , the concate- nation of two vectors corresponding to w i 's lexical and POS tag t i embedding:</p><formula xml:id="formula_4">x i = [W e * e(w i ); W t * e(t i )]</formula><p>, where e(w i ) and e(t i ) are one-hot vector representations of token w i and its POS tag t i , W e and W t are embedding matrices. The rest part of the encoder is the same with the basic seq2seq model. Attention Mechanism: We improve the atten- tion part in two aspects: introduction of stack in- formation and multi-layer attention structure.</p><p>Stack information, which plays an essential role in the conventional algorithm, is simulated with two binary vectors s = (s 1 , . . . , s T ) and r = (r 1 , . . . , r T ) to record the state of each word w i and initialized to zero. When parser pushes word w i into stack, s i is assigned to 1, while r i is as- signed to 1 only if word w i is removed from stack. Intuitively, at each time step i in the decoding phase, stack information serves as an additional input to the attention model, which provides com- plementary information of that the source words is in the stack or not. We expect the stack infor- mation would guide the attention model to focus more on words in the stack. More formally, the coefficients α 1 , α 2 , ... , α T used in attention mech- anism can be rewritten as</p><formula xml:id="formula_5">α i,t = exp (e i,t ) * (1 − r t ) k exp (e i,k ) * (1 − r k )<label>(5)</label></formula><formula xml:id="formula_6">e i,t = v a tanh(W a z i−1 + U a h t + S a s t ) (6)</formula><p>where S a is the weight matrix. To extract complex structure information to help action prediction, we apply a l-layers net- work structure for attention mechanism as shown in the attention part of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><note type="other">To further en- hance connection between adjacent layers, we re- place the state z i−1 in Equation 6 by the concate- nation of z i−1 and context vector c m−1 i at each layer m(m &gt; 1). The Equation 6 can be rewritten as:</note><formula xml:id="formula_7">e m i,t = v a tanh(W m a [z i−1 ; c m−1 i ] + U a h t + S a s t )<label>(7)</label></formula><p>where W m is the weight matrix. With this net- work structure, we obtain different context vec- tors (c 1 i , c 2 i , . . . , c l i ), and the final context vector c i , which is considered as complex context infor- mation, is replaced by the concatenation of those vectors:</p><formula xml:id="formula_8">c i = [c 1 i ; c 2 i ; . . . ; c l i ].</formula><p>Decoder: Unlike machine translation and text summarization in which seq2seq model is widely applied, a sequence of action in dependency pars- ing must satisfy some constraints so that they can generate a dependency tree. Following the arc- standard algorithm <ref type="bibr" target="#b12">(Nivre, 2004</ref>), the precondition can be categorized as 1) SHIFT(SH): There exists at least one word that is not pushed into the stack; 2) LEFT-ARC(LR(d)) and RIGHT-ARC(RR(d)): There are at least two words in the stack. These two constraints can be defined as indicator func- tions</p><formula xml:id="formula_9">I(y i ) =    0 y i = SH, W c ≤ 0 0 y i = LR(d) or RR(d), S c &lt; 2 1 otherwise (8)</formula><p>where S c represents the number of words in the stack and W c is the number of source words that are not pushed into the stack. To introduce these constraints, the conditional probability of each tar- get symbol y i can be rewritten as</p><formula xml:id="formula_10">p(y i |y &lt;i , h) = exp (g i ) * I(y i ) k exp (g k ) * I(y k )<label>(9)</label></formula><p>where g i is the ith element of g(y i−1 , z i , c i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our parsing model on the English and Chinese datasets. Following <ref type="bibr" target="#b6">Dyer et al. (2015)</ref>, Stanford Dependencies (de Marn- effe and Manning, 2008) conversion of the Penn Treebank (PTB) ( <ref type="bibr" target="#b10">Marcus et al., 1993)</ref> and Chinese Treebank 5.1 (CTB) are adopted. We leverage the arc-standard algorithm for our dependency pars- ing. In addition, we limit the vocabulary to con- tain up to 20k most frequent words and convert remaining words into the &lt;unk&gt; token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>For our model, 3-layers GRU is used for encoder and decoder. The dimension of word embedding is 300, the dimension of POS-tag/action embed- ding is 32, and the size of hidden units in GRU is 500. 3-layers attention structure is adopted in our model. Following <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>; <ref type="bibr" target="#b6">Dyer et al. (2015)</ref>, we used 300-dimensional pre- trained GloVe vectors ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>) to initialize our word embedding matrix. Other model parameters are initialized using a normal distribution with a mean of 0 and a variance of  <ref type="table">-SD  CTB  Dev  Test  Dev  Test  UAS LAS UAS LAS UAS LAS UAS LAS  Z&amp;N11  - - 93.00</ref>     <ref type="bibr" target="#b7">(Glorot and Bengio, 2010</ref>). Our models are trained on a Tesla K40m GPU and optimized with vanilla SGD algo- rithm with mini-batch size 64 for English dataset and 32 for Chinese dataset. The initial learning rate is set to 2 and will be halved when unlabeled attachment scores (UAS) on the development set do not increase for 900 batches. To alleviate the gradient exploding problem, we rescale the gradi- ent when its norm exceeds 1. Dropout ( <ref type="bibr" target="#b15">Srivastava et al., 2014</ref>) is applied to our model with the strat- egy recommended in <ref type="bibr" target="#b19">Zaremba et al. (2014)</ref> and the dropout rate is 0.2. For testing, beam search is em- ployed to find the best action sequence with beam size 8. For evaluation, we report unlabeled (UAS) and labeled attachment scores (LAS) on the devel- opment and test sets. Following <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>, the punctuation is excluded from the evaluation. <ref type="table">Table 1</ref> lists the accuracies of our parsing mod- els, compared to other state-of-the-art parsers. For the baseline, seq2seq model employs the same encoder and decoder network structure with our model. We can see that our proposed model can significantly outperform the basic seq2seq model with 1.87 UAS (English) and 1.61 UAS (Chinese) improvements on the test set. This demonstrates the effectiveness of our proposed multi-layer attention mechanism. Besides, our model achieves better UAS accuracy than Z&amp;N11, C&amp;M14, ConBSO and Dyer15 on development and test set, while slightly lower than Weiss15, K&amp;G16 and DENSE. Weiss15 adopts a structured training procedure which can be easiely applied to our model as well, and it will further improve the performance of our model. K&amp;G16 uses 11 bidi- rectional LSTM vectors as features, which will be fed to a transition-based parser. It suggests a new direction that combines our model with feature en- gineering of the traditional transition-based parser to gain better performance. DENSE formalizes dependency parsing as head selection and applies MST algorithms to correct non-tree outputs, while our model doesn't require any post-processing at test time. <ref type="bibr" target="#b5">Dozat and Manning (2016)</ref> use deep bi- affine attention instead of traditional attention in the graph-based architectures of K&amp;G16, achiev- ing 95.74 UAS and 89.30 UAS on PTB-SD and CTB datasets respectively. For ensemble, we train 4 models using the same network with different random initialization. When we ensemble these 4 models, we simply average the output probabil- ities from different models and obtain the better result with accuracies of 94.16 UAS (English) and 87.97 UAS (Chinese) as shown in the Table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of l</head><p>The hyper-parameter l represents the number of layers in our proposed multi-layer attention mech-  <ref type="table">Table 2</ref>: Impact of l on English PTB dataset.</p><p>anism. Larger l would bring more capacity, but lead to more computational complexity and aggra- vate the risk of over-fitting.</p><p>We conduct a group of experiments to investi- gate the impact of l. The results are shown in Ta- ble 2. Seq2seq model can be viewed as a special case of our model without any stack information. With l = 1, we can see that the introduction of stack information can strongly improve the pars- ing performance, especially for LAS. When l is small (l &lt; 4), the general trend is that larger l leads to better result. However, further increasing l bring slightly damages to the parsing performance due to the over-fitting problem.</p><p>Although larger l would bring more capacity, multiple layers structure will double the training time compared with the vanilla seq2seq. In our implementation, our model costs about 500 sec- onds for a round of training data on English PTB dataset, while the vanilla costs about 260 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Results</head><p>We perform some ablation experiments in order to quantify the effect of the different components on our models. As shown in <ref type="table">Table 3</ref>, the POS-tag information plays the most important role in our model. We note that, different from <ref type="bibr" target="#b6">Dyer et al. (2015)</ref>, we don't utilize an external word embed- ding to tackle OOV problem, and it may cause our model to be more dependent on the POS-tag infor- mation. For s and r vectors, same as discussion in last section, we find that the introduction of stack information can strongly improve the parsing per- formance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In order to leverage structure information for seq2seq based dependency parsing, in this pa- per, we propose a stack based multi-layer atten- tion method, in which, stack is simulated with two binary vectors, and multi-layer attention is in-  <ref type="table">Table 3</ref>: Impact of the different components on English PTB dataset.</p><p>troduced to capture multiple word dependencies in partial trees. Experimental results demonstrate that our proposed model significantly outperforms the basic seq2seq model, and achieves a state-of- the-art parsing performance.</p><p>In the future, we plan to apply our approach in more languages and other transition-based system, such as arc-eager or arc-hybrid. Another direction we are interested in is to train our model with com- plex training approaches proposed in <ref type="bibr" target="#b17">Weiss et al. (2015)</ref> and <ref type="bibr" target="#b0">Andor et al. (2016)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of sequence-tosequence parsing model. SH, LR(d), RR(d) denote the SHIFT, LEFT-ARC(d), RIGHT-ARC(d) transitions in arc-standard algorithm and d is arclabel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>Results of various state-of-the-art parsing systems on English dataset (PTB with Stanford De- pendencies) and Chinese dataset (CTB). The numbers reported from different systems are taken from: Z&amp;N11 (Zhang and Nivre, 2011); C&amp;M14 (Chen and Manning, 2014); ConBSO (Wiseman and Rush, 2016); Dyer15 (Dyer et al., 2015); Weiss15 (Weiss et al., 2015); K&amp;G16 (Kiperwasser and Goldberg, 2016); DENSE (Zhang et al., 2017). 6/(d row + d col ), where d row and d col are the number of rows and columns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>89.10 91.84 88.84 86.21 83.80 85.80 83.53 Our model 93.65 91.52 93.71 91.60 87.28 85.30 87.41 85.40 Ensemble 94.24 92.01 94.16 92.13 88.06 86.30 87.97 86.18</figDesc><table>90.95 
-
-
86.00 84.40 
C&amp;M14 
92.20 89.70 91.80 89.60 84.00 82.40 83.90 82.40 
ConBSO 
-
-
91.57 87.26 
-
-
-
-
Dyer15 
93.20 90.90 93.10 90.90 87.20 85.90 87.20 85.70 
Weiss15 
-
-
93.99 92.05 
-
-
-
-
K&amp;G16 
-
-
93.99 91.90 
-
-
87.60 86.10 
DENSE 
94.30 91.95 94.10 91.90 87.35 85.85 87.84 86.15 
seq2seq 
92.02 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We appreciate Dongdong Zhang, Nan Yang, Shuangzhi Wu and Qingyu Zhou for the fruitful discussions. We also thank the anonymous re-viewers for their careful reading of our paper and insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<idno>abs/1603.06042</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<pubPlace>Slav Petrov, and Michael Collins</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Learning Representations</title>
		<meeting>the Third International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental parsing with minimal features using bi-directional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1603.04351</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Stanford typed dependencies manual</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
