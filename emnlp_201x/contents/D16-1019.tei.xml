<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Embedding Knowledge Graphs and Logical Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Computer Network Emergency Response Technical Team Coordination Center of China</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049, 100029</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100093</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Embedding Knowledge Graphs and Logical Rules</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="192" to="202"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest. Most existing methods perform the embedding task using only fact triples. Logical rules, although containing rich background information, have not been well studied in this task. This paper proposes a novel method of jointly embedding knowledge graphs and logical rules. The key idea is to represent and model triples and rules in a unified framework. Specifically, triples are represented as atomic formulae and modeled by the translation assumption , while rules represented as complex formulae and modeled by t-norm fuzzy logics. Embedding then amounts to minimizing a global loss over both atomic and complex for-mulae. In this manner, we learn embeddings compatible not only with triples but also with rules, which will certainly be more predictive for knowledge acquisition and inference. We evaluate our method with link prediction and triple classification tasks. Experimental results show that joint embedding brings significant and consistent improvements over state-of-the-art methods. Particularly, it enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of our method to learn more predictive embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) provide rich structured in- formation and have become extremely useful re- sources for many NLP related applications like * Corresponding author: Quan <ref type="bibr">Wang.</ref> word sense disambiguation <ref type="bibr" target="#b34">(Wasserman-Pritsker et al., 2015</ref>) and information extraction <ref type="bibr" target="#b10">(Hoffmann et al., 2011)</ref>. A typical KG represents knowledge as multi-relational data, stored in triples of the for- m (head entity, relation, tail entity), e.g., <ref type="bibr">(Paris, Capital-Of, France)</ref>. Although powerful in representing structured data, the symbolic nature of such triples makes KGs, especially large-scale KGs, hard to manipulate.</p><p>Recently, a promising approach, namely knowl- edge graph embedding, has been proposed and suc- cessfully applied to various <ref type="bibr">KGs (Nickel et al., 2012;</ref><ref type="bibr" target="#b29">Socher et al., 2013;</ref><ref type="bibr" target="#b4">Bordes et al., 2014</ref>). The key idea is to embed components of a KG including en- tities and relations into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the KG. The embeddings contain rich semantic information about entities and relations, and can significantly enhance knowledge acquisition and inference ).</p><p>Most existing methods perform the embedding task based solely on fact triples ( <ref type="bibr" target="#b32">Wang et al., 2014;</ref><ref type="bibr" target="#b23">Nickel et al., 2016</ref>). The only re- quirement is that the learned embeddings should be compatible with those facts. While logical rules con- tain rich background information and are extreme- ly useful for knowledge acquisition and inference <ref type="bibr" target="#b12">(Jiang et al., 2012;</ref><ref type="bibr" target="#b24">Pujara et al., 2013</ref>), they have not been well studied in this task.  and <ref type="bibr" target="#b35">Wei et al. (2015)</ref> tried to leverage both em- bedding methods and logical rules for KG comple- tion. In their work, however, rules are modeled sep- arately from embedding methods, serving as post- processing steps, and thus will not help to obtain better embeddings. <ref type="bibr" target="#b28">Rocktäschel et al. (2015)</ref> recent- ly proposed a joint model which injects first-order logic into embeddings. But it focuses on the rela- tion extraction task, and creates vector embeddings for entity pairs rather than individual entities. Since entities do not have their own embeddings, relations between unpaired entities cannot be effectively dis- covered ( <ref type="bibr" target="#b6">Chang et al., 2014)</ref>.</p><p>In this paper we introduce KALE, a new approach that learns entity and relation Embeddings by joint- ly modeling Knowledge And Logic. Knowledge triples are taken as atoms and modeled by the trans- lation assumption, i.e., relations act as translations between head and tail entities ( . A triple (e i , r k , e j ) is scored by ∥e i + r k − e j ∥ 1 , where e i , r k , and e j are the vector embeddings for entities and relations. The score is then mapped to the unit interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> to indicate the truth value of that triple. Logical rules are taken as complex for- mulae constructed by combining atoms with logical connectives (e.g., ∧ and ⇒), and modeled by t-norm fuzzy logics <ref type="bibr">(Hájek, 1998)</ref>. The truth value of a rule is a composition of the truth values of the constituen- t atoms, defined by specific logical connectives. In this way, KALE represents triples and rules in a uni- fied framework, as atomic and complex formulae re- spectively. <ref type="figure" target="#fig_0">Figure 1</ref> gives a simple illustration of the framework. After unifying triples and rules, KALE minimizes a global loss involving both of them to obtain entity and relation embeddings. The learned embeddings are therefore compatible not only with triples but also with rules, which will definitely be more predictive for knowledge acquisition and in- ference.</p><p>The main contributions of this paper are summa- rized as follows. (i) We devise a unified framework that jointly models triples and rules to obtain more predictive entity and relation embeddings. The new framework KALE is general enough to handle any type of rules that can be represented as first-order logic formulae. (ii) We evaluate KALE with link prediction and triple classification tasks on WordNet <ref type="bibr" target="#b19">(Miller, 1995)</ref> and <ref type="bibr">Freebase (Bollacker et al., 2008)</ref>. Experimental results show significant and consistent improvements over state-of-the-art methods. Partic- ularly, joint embedding enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of KALE to learn more predictive embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years have seen rapid growth in KG em- bedding methods. Given a KG, such methods aim to encode its entities and relations into a continu- ous vector space, by using neural network architec- tures ( <ref type="bibr" target="#b29">Socher et al., 2013;</ref><ref type="bibr" target="#b4">Bordes et al., 2014</ref>), matrix/tensor factorization tech- niques <ref type="bibr" target="#b21">(Nickel et al., 2011;</ref><ref type="bibr" target="#b26">Riedel et al., 2013;</ref><ref type="bibr" target="#b6">Chang et al., 2014</ref>), or Bayesian clustering strate- gies ( <ref type="bibr" target="#b13">Kemp et al., 2006;</ref><ref type="bibr" target="#b37">Xu et al., 2006;</ref><ref type="bibr" target="#b31">Sutskever et al., 2009</ref>). Among these methods, <ref type="bibr">TransE (Bordes et al., 2013</ref>), which models relations as translating op- erations, achieves a good trade-off between predic- tion accuracy and computational efficiency. Various extensions like TransH ( <ref type="bibr" target="#b32">Wang et al., 2014</ref>) and Tran- sR ( <ref type="bibr" target="#b16">Lin et al., 2015b</ref>) are later proposed to further enhance the prediction accuracy of TransE. Most ex- isting methods perform the embedding task based solely on triples contained in a KG. Some recent work tries to further incorporate other types of infor- mation available, e.g., relation paths ( <ref type="bibr" target="#b20">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b15">Lin et al., 2015a;</ref><ref type="bibr" target="#b17">Luo et al., 2015)</ref>, relation type-constraints <ref type="bibr">(Krompaßet al., 2015)</ref>, entity type- s ( , and entity descriptions ( <ref type="bibr" target="#b38">Zhong et al., 2015)</ref>, to learn better embeddings.</p><p>Logical rules have been widely studied in knowl- edge acquisition and inference, usually on the basis of Markov logic networks ( <ref type="bibr" target="#b25">Richardson and Domingos, 2006;</ref><ref type="bibr" target="#b5">Bröcheler et al., 2010;</ref><ref type="bibr" target="#b24">Pujara et al., 2013;</ref><ref type="bibr" target="#b0">Beltagy and Mooney, 2014)</ref>. Recently, there has been growing interest in combining logical rules and embedding models.  and <ref type="bibr" target="#b35">Wei et al. (2015)</ref> tried to utilize rules to refine predictions made by embedding models, via integer linear pro- gramming or Markov logic networks. In their work, however, rules are modeled separately from embed- ding models, and will not help obtain better embed- dings. <ref type="bibr" target="#b28">Rocktäschel et al. (2015)</ref> proposed a joint model that injects first-order logic into embeddings. But their work focuses on relation extraction, cre- ating vector embeddings for entity pairs, and hence fails to discover relations between unpaired entities. This paper, in contrast, aims at learning more pre- dictive embeddings by jointly modeling knowledge and logic. Since each entity has its own embedding, our approach can successfully make predictions be- tween unpaired entities, providing greater flexibility for knowledge acquisition and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Jointly Embedding Knowledge and Logic</head><p>We first describe the formulation of joint embed- ding. We are given a KG containing a set of triples K = {(e i , r k , e j )}, with each triple composed of t- wo entities e i , e j ∈ E and their relation r k ∈ R. Here E is the entity vocabulary and R the relation set. Besides the triples, we are given a set of logical rules L, either specified manually or extracted auto- matically. A logical rule is encoded, for example, in the form of ∀x, y : (x, r s , y) ⇒ (x, r t , y), stat- ing that any two entities linked by relation r s should also be linked by relation r t . Entities and relations are associated with vector embeddings, denoted by e, r ∈ R d , representing their latent semantics. The proposed method, KALE, aims to learn these em- beddings by jointly modeling knowledge triples K and logical rules L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>To enable joint embedding, a key ingredient of KALE is to unify triples and rules, in terms of first- order logic <ref type="bibr" target="#b27">(Rocktäschel et al., 2014;</ref><ref type="bibr" target="#b28">Rocktäschel et al., 2015)</ref>. A triple (e i , r k , e j ) is taken as a ground atom which applies a relation r k to a pair of entities e i and e j . Given a logical rule, it is first instantiated with concrete entities in the vocabulary E, resulting in a set of ground rules. For example, a universal- ly quantified rule ∀x, y : (x, Capital-Of, y) ⇒ (x, Located-In, y) might be instantiated with the concrete entities of Paris and France, giving the ground rule (Paris, Capital-Of, France) ⇒ (Paris, Located-In, France). 1 A ground rule can then be interpreted as a complex formula, con- structed by combining ground atoms with logical connectives (e.g. ∧ and ⇒).</p><p>Let F denote the set of training formulae, both atomic (triples) and complex (ground rules). KALE further employs a truth function I : F → [0, 1] to assign a soft truth value to each formula, indicating how likely a triple holds or to what degree a ground rule is satisfied. The truth value of a triple is deter- mined by the corresponding entity and relation em- beddings. The truth value of a ground rule is deter- mined by the truth values of the constituent triples, via specific logical connectives. In this way, KALE models triples and rules in a unified framework. See <ref type="figure" target="#fig_0">Figure 1</ref> for an overview. Finally, KALE minimizes a global loss over the training formulae F to learn entity and relation embeddings compatible with both triples and rules. In what follows, we describe the key components of KALE, including triple model- ing, rule modeling, and joint learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Triple Modeling</head><p>To model triples we follow TransE ( , as it is simple and efficient while achieving state-of-the-art predictive performance. Specifically, given a triple (e i , r k , e j ), we model the relation em- bedding r k as a translation between the entity em- beddings e i and e j , i.e., we want e i + r k ≈ e j when the triple holds. The intuition here originates from linguistic regularities such as France − Paris = Germany − Berlin ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>). In relational data, such analogy holds because of the certain relation Capital-Of, through which we will get Paris + Capital-Of = France and Berlin + Capital-Of = Germany. Then, we score each triple on the basis of ∥e i + r k − e j ∥ 1 , and define its soft truth value as</p><formula xml:id="formula_0">I (e i , r k , e j ) = 1 − 1 3 √ d ∥e i + r k − e j ∥ 1 , (1)</formula><p>where d is the dimension of the embedding space. It is easy to see that I (e i , r k , e j ) ∈ [0, 1] with the constraints ∥e i ∥ 2 ≤ 1, ∥e j ∥ 2 ≤ 1, and ∥r k ∥ 2 ≤</p><p>1. 2 I (e i , r k , e j ) is expected to be large if the triple holds, and small otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rule Modeling</head><p>To model rules we use t-norm fuzzy logics <ref type="bibr">(Hájek, 1998)</ref>, which define the truth value of a complex for- mula as a composition of the truth values of its con- stituents, through specific t-norm based logical con- nectives. We follow <ref type="bibr" target="#b28">Rocktäschel et al. (2015)</ref> and use the product t-norm. The compositions associat- ed with logical conjunction (∧), disjunction (∨), and negation (¬) are defined as follow:</p><formula xml:id="formula_1">I(f 1 ∧ f 2 ) = I(f 1 )·I(f 2 ), I(f 1 ∨ f 2 ) = I(f 1 ) + I(f 2 ) − I(f 1 )·I(f 2 ), I(¬f 1 ) = 1 − I(f 1 ),</formula><p>where f 1 and f 2 are two constituent formulae, either atomic or complex. Given these compositions, the truth value of any complex formula can be calculat- ed recursively, e.g.,</p><formula xml:id="formula_2">I(¬f 1 ∧ f 2 ) = I(f 2 ) − I(f 1 )·I(f 2 ), I(f 1 ⇒ f 2 ) = I(f 1 )·I(f 2 ) − I(f 1 ) + 1.</formula><p>This paper considers two types of rules. The first type is ∀x, y : (x, r s , y) ⇒ (x, r t , y). Given a ground rule f (e m , r s , e n ) ⇒ (e m , r t , e n ), the truth value is calculated as:</p><formula xml:id="formula_3">I(f )=I(e m , r s , e n )·I(e m , r t , e n ) −I(e m , r s , e n ) + 1,<label>(2)</label></formula><p>where I(·,·,·) is the truth value of a constituent triple, defined by Eq. (1). The second type is ∀x, y, z :</p><formula xml:id="formula_4">(x, r s 1 , y) ∧ (y, r s 2 , z) ⇒ (x, r t , z). Given a ground rule f (e ℓ , r s 1 , e m ) ∧ (e m , r s 2 , e n ) ⇒ (e ℓ , r t , e n ),</formula><p>the truth value is:</p><formula xml:id="formula_5">I(f )=I(e ℓ , r s 1 , e m )·I(e m , r s 2 , e n )·I(e ℓ , r t , e n ) −I(e ℓ , r s 1 , e m )·I(e m , r s 2 , e n ) + 1.<label>(3)</label></formula><p>The larger the truth values are, the better the ground rules are satisfied. It is easy to see that besides these two types of rules, the KALE framework is general enough to handle any rules that can be represented as first-order logic formulae. The investigation of other types of rules will be left for future work.</p><formula xml:id="formula_6">2 Note that 0 ≤ ∥ei + r k − ej∥ 1 ≤ ∥ei∥ 1 + ∥r k ∥ 1 + ∥ej∥ 1 ≤ 3 √ d,</formula><p>where the last inequality holds because</p><formula xml:id="formula_7">∥x∥ 1 = ∑ i |xi| ≤ √ d ∑ i x 2 i = √ d ∥x∥ 2 for any x ∈ R d , according to the Cauchy-Schwarz inequality.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint Learning</head><p>After unifying triples and rules as atomic and com- plex formulae, we minimize a global loss over this general representation to learn entity and relation embeddings. We first construct a training set F con- taining all positive formulae, including (i) observed triples, and (ii) ground rules in which at least one constituent triple is observed. Then we minimize a margin-based ranking loss, enforcing positive for- mulae to have larger truth values than negative ones:</p><formula xml:id="formula_8">min {e},{r} ∑ f + ∈F ∑ f − ∈N f + [ γ − I(f + ) + I(f − ) ] + , s.t. ∥e∥ 2 ≤ 1, ∀e ∈ E; ∥r∥ 2 ≤ 1, ∀r ∈ R.<label>(4)</label></formula><p>Here f + ∈ F is a positive formula, f − ∈ N f + a negative one constructed for f + , γ a margin sepa- rating positive and negative formulae, and</p><formula xml:id="formula_9">[x] + max{0, x}. If f + (e i , r k , e j )</formula><p>is a triple, we con- struct f − by replacing either e i or e j with a random entity e ∈ E, and calculate its truth value according to Eq. (1). For example, we might generate a neg- ative instance (Paris, Capital-Of, Germany) for the triple (Paris, Capital-Of, France). If</p><formula xml:id="formula_10">f + (e m , r s , e n ) ⇒ (e m , r t , e n ) or (e ℓ , r s 1 , e m ) ∧ (e m , r s 2 , e n ) ⇒ (e ℓ , r t , e n )</formula><p>is a ground rule, we con- struct f − by replacing r t in the consequent with a random relation r ∈ R, and calculate its truth value according to Eq. (2) or Eq. (3). For example, given a ground rule (Paris, Capital-Of, France) ⇒ (Paris, Located-In, France), a possible neg- ative instance (Paris, Capital-Of, France)⇒ (Paris, Has-Spouse, France) could be gener- ated. We believe that most instances (both triples and ground rules) generated in this way are truly negative. Stochastic gradient descent in mini-batch mode is used to carry out the minimization. To satis- fy the ℓ 2 -constraints, e and r are projected to the unit ℓ 2 -ball before each mini-batch. Embeddings learned in this way are required to be compatible with not only triples but also rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussions</head><p>Complexity. We compare KALE with several state- of-the-art embedding methods in space complexity and time complexity (per iteration) during learning. of the embedding space, and n e /n r /n t /n g is the num- ber of entities/relations/triples/ground rules. The re- sults indicate that incorporating additional rules will not significantly increase the space or time complex- ity of KALE, keeping the model complexity almost the same as that of TransE (optimal among the meth- ods listed in the table). But please note that KALE needs to ground universally quantified rules before learning, which further requires O(n u n t /n r ) in time complexity. Here, n u is the number of universally quantified rules, and n t /n r is the averaged number of observed triples per relation. During grounding, we select those ground rules with at least one triple observed. Grounding is required only once before learning, and is not included during the iterations.</p><p>Extensions. Actually, our approach is quite general. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We empirically evaluate KALE with two tasks: (i) link prediction and (ii) triple classification. We further create logical rules for each dataset, in the form of ∀x, y : (x, r s , y) ⇒ (x, r t , y) or ∀x, y, z : (x, r s 1 , y) ∧ (y, r s 2 , z) ⇒ (x, r t , z). To do so, we first run TransE to get entity and relation embeddings, and calculate the truth value for each of such rules according to Eq. (2) or Eq. (3). Then we rank all such rules by their truth values and man- ually filter those ranked at the top. We finally create 47 rules on FB122, and 14 on WN18 (see <ref type="table" target="#tab_2">Table 2</ref> for examples). The rules are then instantiated with con- crete entities (grounding). Ground rules in which at least one constituent triple is observed in the train- ing set are used in joint learning.</p><note type="other">Dataset # Ent # Rel # Train/Valid/Test-I/Test-II # Rule</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FB122 9,738 122 91,638 9,595 5,057 6,186 78,488 WN18 40,943 18 141,442 5,000 1,394 3,606 119,222</head><p>Note that some of the test triples can be inferred by directly applying these rules on the training set (pure logical inference). On each dataset, we fur- ther split the test set into two parts, test-I and test-II. The former contains triples that cannot be directly inferred by pure logical inference, and the latter the remaining test triples. <ref type="table" target="#tab_1">Table 3</ref> gives some statistics of the datasets, including the number of entities, re- lations, triples in training/validation/test-I/test-II set, and ground rules.</p><p>Comparison settings. As baselines we take the em- bedding techniques of TransE, TransH, and Tran- sR. TransE models relation embeddings as transla- tion operations between entity embeddings. TransH ∀x, y : /sports/athlete/team(x, y) ⇒ /sports/sports team/player(y, x) ∀x, y : /location/country/capital(x, y) ⇒ /location/location/contains(x, y) ∀x, y, z : /people/person/nationality(x, y) ∧ /location/country/official language(y, z) ⇒ /people/person/languages(x, z) ∀x, y, z : /country/administrative divisions(x, y) ∧ /administrative division/capital(y, z) ⇒ /country/second level divisions <ref type="bibr">(x, z)</ref> ∀x, y : hypernym(x, y) ⇒ hyponym(y, x) ∀x, y : instance hypernym(x, y) ⇒ instance hyponym(y, x) ∀x, y : synset domain topic of(x, y) ⇒ member of domain topic(y, x) and TransR are extensions of TransE. They further allow entities to have distinct embeddings when in- volved in different relations, by introducing relation- specific hyperplanes and projection matrices respec- tively. All the three methods have been demonstrat- ed to perform well on WordNet and Freebase data.</p><p>We further test our approach in three different sce- narios. (i) KALE-Trip uses triples alone to perform the embedding task, i.e., only the training triples are included in the optimization Eq. (4). It is a linear- ly transformed version of TransE. The only differ- ence is that relation embeddings are normalized in KALE-Trip, but not in TransE. (ii) KALE-Pre first repeats pure logical inference on the training set and adds inferred triples as additional training data, until no further triples can be inferred. Both original and inferred triples are then included in the optimization. For example, given a logical rule ∀x, y : (x, r s , y) ⇒ (x, r t , y), a new triple (e i , r t , e j ) can be inferred if (e i , r s , e j ) is observed in the training set, and both triples will be used as training instances for embed- ding. (iii) KALE-Joint is the joint learning scenari- o, which considers both training triples and ground rules in the optimization. In the aforementioned example, training triple (e i , r s , e j ) and ground rule (e i , r s , e j ) ⇒ (e i , r t , e j ) will be used in the train- ing process of KALE-Joint, without explicitly in- corporating triple (e i , r t , e j ). TransE. However, to ensure fair comparison, we ran- domly initialize all the methods in our experiments. For all the methods, we create 100 mini-batches on each dataset, and tune the embedding dimension d in {20, 50, 100}. For TransE, TransH, and Tran- sR which score a triple by a distance in R + , we tune the learning rate η in {0.001, 0.01, 0.1}, and the margin γ in {1, 2, 3, 4}. For KALE which s- cores a triple (as well as a ground rule) by a soft truth value in the unit interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, we set the learn- ing rate η in {0.01, 0.02, 0.05, 0.1}, and the mar- gin γ in {0.1, 0.12, 0.15, 0.2}. KALE allows triples and rules to have different weights, with the former fixed to 1, and the latter (denoted by λ) selected in {0.001, 0.01, 0.1, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Link Prediction</head><p>This task is to complete a triple (e i , r k , e j ) with e i or e j missing, i.e., predict e i given (r k , e j ) or predict e j given (e i , r k ). Evaluation protocol. We follow the same evalua- tion protocol used in TransE ( ). For each test triple (e i , r k , e j ), we replace the head entity e i by every entity e ′ i in the dictionary, and cal- culate the truth value (or distance) for the corrupted triple (e ′ i , r k , e j ). Ranking the truth values in de- scending order (or the distances in ascending order), we get the rank of the correct entity e i . Similarly, we can get another rank by corrupting the tail entity e j . Aggregated over all the test triples, we report three metrics: (i) the mean reciprocal rank (MRR), (ii) the median of the ranks (MED), and (iii) the proportion of ranks no larger than n (HITS@N). We do not re- port the averaged rank (i.e., the "Mean Rank" metric used by ), since it is usually sen- sitive to outliers <ref type="bibr" target="#b23">(Nickel et al., 2016)</ref>.</p><p>Note that a corrupted triple may exist in KGs, which should also be taken as a valid triple. Consid- er a test triple (Paris, Located-In, France) Test-I <ref type="table" target="#tab_0">Test-II  Test-ALL   MRR MED  HITS@N (%)  MRR MED  HITS@N (%)  MRR MED  HITS@N (%)   3  5  10  3  5  10  3  5</ref>     and a possible corruption (Lyon, Located-In, France). Both triples are valid. In this case, rank- ing Lyon before the correct answer Paris should not be counted as an error. To avoid such phenome- na, we follow  and remove those corrupted triples which exist in either the training, validation, or test set before getting the ranks. That means, we remove Lyon from the candidate list be- fore getting the rank of Paris in the aforemen- tioned example. We call the original setting "raw" and the new setting "filtered".</p><p>Optimal configurations. For each of the method- s to be compared, we tune its hyperparameters in the ranges specified in Section 4.1, and select a best model that leads to the highest filtered MRR score on the validation set (with a total of 500 epochs over Results. <ref type="table" target="#tab_3">Table 4 and Table 5</ref> show the results in the raw setting and filtered setting respectively. On each dataset we report the metrics on three sets: test-I, test-II, and the whole test set (denoted by test-all). Test-I contains test triples that cannot be directly in- ferred by performing pure logical inference on the training set, and hence might be intrinsically more d- ifficult for the rules. The remaining test triples (i.e., the directly inferable ones) are included in Test-II. These triples have either been used directly as train-  ing instances in KALE-Pre, or encoded explicitly in training ground rules in KALE-Joint, making this set trivial for the rules to some extent. From the result- s, we can see that in both settings: (i) KALE-Pre and KALE-Joint outperform (or at least perform as well as) the other methods which use triples alone on almost all the test sets, demonstrating the superi- ority of incorporating logical rules. (ii) On the test-I sets which contain triples beyond the scope of pure logical inference, KALE-Joint performs significant- ly better than KALE-Pre. On these sets KALE-Joint can still beat all the baselines by a significant margin in most cases, while KALE-Pre can hardly outper- form KALE-Trip. It demonstrates the capability of the joint embedding scenario to learn more predic- tive embeddings, through which we can make better predictions even beyond the scope of pure logical inference. (iii) On the test-II sets which contain di- rectly inferable triples, KALE-Pre can easily beat all the baselines (even KALE-Joint). That means, for triples covered by pure logical inference, it is trivial to improve the performance by directly incorporat- ing them as training instances.</p><p>To better understand how the joint embedding s- cenario can learn more predictive embeddings, on each dataset we further split the test-I set into two parts. Given a triple (e i , r k , e j ) in the test-I set, we assign it to the first part if relation r k is covered by the rules, and the second part otherwise. We call the two parts Test-Incl and Test-Excl respectively. Ta- ble 6 compares the performance of KALE-Trip and KALE-Joint on the two parts. The results show that KALE-Joint outperforms KALE-Trip on both parts, but the improvements on Test-Incl are much more significant than those on Test-Excl. Take the fil- tered setting on WN18 as an example. On Test-Incl, KALE-Joint increases the metric MRR by 55.7%, decreases the metric MED by 26.9%, and increas- es the metric HITS@10 by 38.2%. On Test-Excl, however, MRR rises by 3.1%, MED remains the same, and HITS@10 rises by only 0.3%. This obser- vation indicates that jointly embedding triples and rules helps to learn more predictive embeddings, es- pecially for those relations that are used to construct the rules. This might be the main reason that KALE- Joint can make better predictions even beyond the scope of pure logical inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Triple Classification</head><p>This task is to verify whether an unobserved triple (e i , r k , e j ) is correct or not. Evaluation protocol. We take the following evalu- ation protocol similar to that used in TransH ( <ref type="bibr" target="#b32">Wang et al., 2014</ref>). We first create labeled data for evalua- tion. For each triple in the test or validation set (i.e., a positive triple), we construct 10 negative triples for it by randomly corrupting the entities, 5 at the head position and the other 5 at the tail position. <ref type="bibr">6</ref> To make the negative triples as difficult as possible, we corrupt a position using only entities that have ap- peared in that position, and further ensure that the corrupted triples do not exist in either the training, validation, or test set. We simply use the truth values (or distances) to classify triples. Triples with large truth values (or small distances) tend to be predict- ed as positive. To evaluate, we first rank the triples associated with each specific relation (in descending order according to their truth values, or in ascending order according to the distances), and calculate the average precision for that relation. We then report on the test sets the mean average precision (MAP) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP (Test-I/II/ALL) MAP (Test-I/II/ALL) TransE</head><p>0.552 0.852 0.634 0.592 0.993 0.958 TransH 0.576 0.758 0.641 0.604 0.978 0.947 TransR 0.572 0.699 0.619 0.412 0.854 0.836 KALE-Trip 0.578 0.829 0.652 0.618 0.995 0.953 KALE-Pre 0.575 0.916 0.668 0.620 0.997 0.964 KALE-Joint 0.599 0.870 0.677 0.627 0.997 0.961  Results. <ref type="table" target="#tab_7">Table 7</ref> shows the results on the test-I, test- II, and test-all sets of our datasets. From the results, we can see that: (i) KALE-Pre and KALE-Joint out- perform the other methods which use triples alone on almost all the test sets, demonstrating the superi- ority of incorporating logical rules. (ii) KALE-Joint performs better than KALE-Pre on the test-I sets, i.e., triples that cannot be directly inferred by per- forming pure logical inference on the training set. This observation is similar to that observed in the link prediction task, demonstrating that the joint em- bedding scenario can learn more predictive embed- dings and make predictions beyond the capability of pure logical inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose a new method for joint- ly embedding knowledge graphs and logical rules, referred to as KALE. The key idea is to represent and model triples and rules in a unified framework. Specifically, triples are represented as atomic for- mulae and modeled by the translation assumption, while rules as complex formulae and by the t-norm fuzzy logics. A global loss on both atomic and com- plex formulae is then minimized to perform the em- bedding task. Embeddings learned in this way are compatible not only with triples but also with rules, which are certainly more useful for knowledge ac- quisition and inference. We evaluate KALE with the link prediction and triple classification tasks on WordNet and Freebase data. Experimental result- s show that joint embedding brings significant and consistent improvements over state-of-the-art meth- ods. More importantly, it can obtain more predic- tive embeddings and make better predictions even beyond the scope of pure logical inference.</p><p>For future work, we would like to (i) Investigate the efficacy of incorporating other types of logical rules such as ∀x, y, z : (x, Capital-Of, y) ⇒ ¬(x, Capital-Of, z). (ii) Investigate the possibil- ity of modeling logical rules using only relation em- beddings as suggested by <ref type="bibr" target="#b7">Demeester et al. (2016)</ref>, e.g., modeling the above rule using only the embed- ding associated with Capital-Of. This avoids grounding, which might be time and space ineffi- cient especially for complicated rules. (iii) Inves- tigate the use of automatically extracted rules which are no longer hard rules and tolerant of uncertainty.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Simple illustration of KALE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(i) Besides TransE, a variety of embedding method- s, e.g., those listed in Table 1, can be used for triple modeling (Section 3.2), as long as we further define a mapping f : R → [0, 1] to map original scores to soft truth values. (ii) Besides the two types of rules introduced in Section 3.3, other types of rules can also be handled as long as they can be represented as first-order logic formulae. (iii) Besides the prod- uct t-norm, other types of t-norm based fuzzy logics can be used for rule modeling (Section 3.3), e.g., the Łukasiewicz t-norm used in probabilistic soft log- ic (Bröcheler et al., 2010) and the minimum t-norm used in fuzzy description logic (Stoilos et al., 2007). (iv) Besides the pairwise ranking loss, other type- s of loss functions can be designed for joint learn- ing (Section 3.4), e.g., the pointwise squared loss or the logarithmic loss (Rocktäschel et al., 2014; Rock- täschel et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Among the method- s, TransE/TransH/TransR and KALE-Trip use only triples, while KALE-Pre/KALE-Joint further incor- porates rules, before or during embedding. Implementation details. We use the code provid- ed by Bordes et al. (2013) for TransE 4 , and the code provided by Lin et al. (2015b) for TransH and Tran- sR 5 . KALE is implemented in Java. Note that Lin et al. (2015b) initialized TransR with the results of 4 https://github.com/glorotxa/SME 5 https://github.com/mrlyk423/relation extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the training data). The optimal configurations for KALE are: d = 100, η = 0.05, γ = 0.12, and λ = 1 on FB122; d = 50, η = 0.05, γ = 0.2, and λ = 0.1 on WN18. To better see and understand the effects of rules, we use the same configuration for KALE-Trip, KALE-Pre, and KALE-Joint on each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FB122WN18</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Optimal configurations.</head><label></label><figDesc>The hyperparameters of each method are again tuned in the ranges specified in Section 4.1, and the best models are selected by maximizing MAP on the validation set. The optimal configurations for KALE are: d = 100, η = 0.1, γ = 0.2, and λ = 0.1 on FB122; d = 100, η = 0.1, γ = 0.2, and λ = 0.001 on WN18. Again, we use the same configuration for KALE-Trip, KALE-Pre, and KALE-Joint on each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 shows the results, where d is the dimension</head><label>1</label><figDesc></figDesc><table>Method 

Complexity (Space/Time) 

SE (Bordes et al., 2011) 
ned+2nrd 2 
O(ntd 2 ) 
LFM (Jenatton et al., 2012) ned+nrd 2 
O(ntd 2 ) 
TransE (Bordes et al., 2013) ned+nrd 
O(ntd) 
TransH (Wang et al., 2014) ned+2nrd 
O(ntd) 
TransR (Lin et al., 2015b) 
ned+nr(d 2 +d) O(ntd 2 ) 

KALE (this paper) 
ned+nrd 
O(ntd+ngd) 

Table 1: Complexity of different embedding methods. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics of datasets. 

4.1 Experimental Setup 

Datasets. We use two datasets: WN18 and FB122. 
WN18 is a subgraph of WordNet containing 18 rela-
tions. FB122 is composed of 122 Freebase relations 
regarding the topics of "people", "location", and "s-
ports", extracted from FB15K. Both WN18 and F-
B15K are released by Bordes et al. (2013) 3 . Triples 
on each dataset are split into training/validation/test 
sets, used for model training, parameter tuning, and 
evaluation respectively. For WN18 we use the o-
riginal data split, and for FB122 we extract triples 
associated with the 122 relations from the training, 
validation, and test sets of FB15K. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of rules created. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Link prediction results on the test-I, test-II, and test-all sets of FB122 and WN18 (raw setting). 

Test-I 
Test-II 
Test-ALL 

MRR MED 
HITS@N (%) 
MRR MED 
HITS@N (%) 
MRR MED 
HITS@N (%) 

3 
5 
10 
3 
5 
10 
3 
5 
10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Link prediction results on the test-I, test-II, and test-all sets of FB122 and WN18 (filtered setting). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison between KALE-Trip and KALE-Joint on Test-Incl and Test-Excl of FB122 and WN18. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Triple classification results on the test-I, test-II, and 

test-all sets of FB122 and WN18. 

aggregated over different relations. 

</table></figure>

			<note place="foot" n="1"> Our approach actually takes as input rules represented in first-order logic, i.e., those with quantifiers such as ∀. But it could be hard to deal with quantifiers, so we use ground rules, i.e., propositional statements during learning.</note>

			<note place="foot" n="3"> https://everest.hds.utc.fr/doku.php?id=en:smemlj12</note>

			<note place="foot" n="6"> Previous work typically constructs only a single negative case for each positive one. We empirically found such a balanced classification task too simple for our datasets. So we consider a highly unbalanced setting, with a positive-to-negative ratio of 1:10, for which the previously used metric accuracy is no longer suitable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their insightful comments and suggestions. This re-search is supported by the National Natural Science Foundation of China (grant No. 61402465) and the Strategic Priority Research Program of the Chinese Academy of Sciences (grant No. XDA06030200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient markov logic inference for natural language semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence-Workshop on Statistical Relational Artificial Intelligence</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence-Workshop on Statistical Relational Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 27th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A semantic matching energy function for learning with multi-relational data. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic similarity logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bröcheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilyana</forename><surname>Mihalkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 26th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1568" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regularizing relation representations by first-order implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies-Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies-Workshop on Automated Knowledge Base Construction</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantically smooth knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The metamathematics of fuzzy logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Hájek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 26th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to refine an automatically extracted knowledge base using markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangpu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 12th IEEE International Conference on Data Mining</title>
		<meeting>12th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="912" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning systems of concepts with an infinite relational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 21st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Type-constrained representation learning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Semantic Web Conference</title>
		<meeting>the 14th International Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-dependent knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1656" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="156" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">P</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorizing yago: Scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">P</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
		<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Semantic Web Conference</title>
		<meeting>the 12th International Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="107" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Low-dimensional embeddings of logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational LinguisticsWorkshop on Semantic Parsing</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational LinguisticsWorkshop on Semantic Parsing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 27th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reasoning with very expressive fuzzy description logics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Stoilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><forename type="middle">B</forename><surname>Stamou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="273" to="320" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Vassilis Tzouvaras, and Ian Horrocks</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 23rd Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1859" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to identify the best contexts for knowledge-based wsd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Wasserman-Pritsker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1662" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale knowledge base completion: inferring via grounding network sampling over selected instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengya</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Infinite hidden relational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 22nd Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="544" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aligning knowledge and text embeddings by entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
