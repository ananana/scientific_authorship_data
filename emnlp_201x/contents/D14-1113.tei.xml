<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1059" to="1069"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type-ignoring poly-semy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scal-ability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing words by dense, real-valued vector embeddings, also commonly called "distributed representations," helps address the curse of di- mensionality and improve generalization because they can place near each other words having sim- ilar semantic and syntactic roles. This has been shown dramatically in state-of-the-art results on language modeling ( <ref type="bibr" target="#b1">Bengio et al, 2003;</ref><ref type="bibr" target="#b18">Mnih and Hinton, 2007)</ref> as well as improvements in other natural language processing tasks <ref type="bibr" target="#b3">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b30">Turian et al, 2010</ref>). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent consider- able interest in the CBOW and Skip-gram models <ref type="bibr">*</ref> The first two authors contributed equally to this paper. of Mikolov et al (2013a); Mikolov et al (2013b)- relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine.</p><p>There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters <ref type="bibr" target="#b2">(Brown et al, 1992</ref>) have become common input features for many tasks, such as named entity extraction ( <ref type="bibr" target="#b17">Miller et al, 2004;</ref><ref type="bibr" target="#b22">Ratinov and Roth, 2009</ref>) and parsing ( <ref type="bibr" target="#b9">Koo et al, 2008;</ref><ref type="bibr" target="#b29">Täckström et al, 2012</ref>). In comparison to Brown clusters, the vector em- beddings have the advantages of substantially bet- ter scalability in their training, and intriguing po- tential for their continuous and multi-dimensional interrelations. In fact, <ref type="bibr" target="#b21">Passos et al (2014)</ref> present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skip- gram that injects supervision with lexicons. Sim- ilarly <ref type="bibr" target="#b0">Bansal et al (2014)</ref> show results in depen- dency parsing using Skip-gram embeddings. They have also recently been applied to machine trans- lation ( <ref type="bibr" target="#b31">Zou et al, 2013;</ref><ref type="bibr" target="#b16">Mikolov et al, 2013c)</ref>.</p><p>A notable deficiency in this prior work is that each word type (e.g. the word string plant) has only one vector representation-polysemy and hononymy are ignored. This results in the word plant having an embedding that is approximately the average of its different contextual seman- tics relating to biology, placement, manufactur- ing and power generation. In moderately high- dimensional spaces a vector can be relatively "close" to multiple regions at a time, but this does not negate the unfortunate influence of the triangle inequality 2 here: words that are not synonyms but are synonymous with different senses of the same word will be pulled together. For example, pollen and refinery will be inappropriately pulled to a dis-tance not more than the sum of the distances plant- pollen and plant-refinery. Fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encum- brance of these illegitimate triangle inequalities.</p><p>Discovering embeddings for multiple senses per word type is the focus of work by <ref type="bibr" target="#b24">Reisinger and Mooney (2010a)</ref> and <ref type="bibr" target="#b8">Huang et al (2012)</ref>. They both pre-cluster the contexts of a word type's to- kens into discriminated senses, use the clusters to re-label the corpus' tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by em- ploying an earlier pass of non-discriminated em- bedding learning to obtain vectors used to rep- resent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the cluster- ing. Other weaknesses include their fixed num- ber of sense per word type, and the computational expense of the two-step process-the Huang et al (2012) method took one week of computation to learn multiple embeddings for a 6,000 subset of the 30,000 vocabulary on a corpus containing close to billion tokens. <ref type="bibr">3</ref> This paper presents a new method for learn- ing vector-space embeddings for multiple senses per word type, designed to provide several ad- vantages over previous approaches. (1) Sense- discriminated vectors are learned jointly with the assignment of token contexts to senses; thus we can use the emerging sense representation to more accurately perform the clustering. (2) A non- parametric variant of our method automatically discovers a varying number of senses per word type. (3) Efficient online joint training makes it fast and scalable. We refer to our method as Multiple-sense Skip-gram, or MSSG, and its non- parametric counterpart as NP-MSSG.</p><p>Our method builds on the Skip-gram model ( <ref type="bibr" target="#b14">Mikolov et al, 2013a</ref>), but maintains multiple vectors per word type. During online training with a particular token, we use the average of its context words' vectors to select the token's sense that is closest, and perform a gradient update on that sense. In the non-parametric version of our method, we build on facility location <ref type="bibr" target="#b13">(Meyerson, 2001)</ref>: a new cluster is created with probability proportional to the distance from the context to the nearest sense.</p><p>We present experimental results demonstrating the benefits of our approach. We show quali- tative improvements over single-sense <ref type="bibr">Skip-gram and Huang et al (2012)</ref>, comparing against word neighbors from our parametric and non-parametric methods. We present quantitative results in three tasks. On both the SCWS and WordSim353 data sets our methods surpass the previous state-of- the-art. The Google Analogy task is not espe- cially well-suited for word-sense evaluation since its lack of context makes selecting the sense dif- ficult; however our method dramatically outper- forms Huang et al (2012) on this task. Finally we also demonstrate scalabilty, learning multiple senses, training on nearly a billion tokens in less than 6 hours-a 27x improvement on Huang et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Much prior work has focused on learning vector representations of words; here we will describe only those most relevant to understanding this pa- per. Our work is based on neural language mod- els, proposed by <ref type="bibr" target="#b1">Bengio et al (2003)</ref>, which extend the traditional idea of n-gram language models by replacing the conditional probability table with a neural network, representing each word token by a small vector instead of an indicator variable, and estimating the parameters of the neural network and these vectors jointly. Since the <ref type="bibr" target="#b1">Bengio et al (2003)</ref> model is quite expensive to train, much re- search has focused on optimizing it. <ref type="bibr" target="#b3">Collobert and Weston (2008)</ref> replaces the max-likelihood char- acter of the model with a max-margin approach, where the network is encouraged to score the cor- rect n-grams higher than randomly chosen incor- rect n-grams. <ref type="bibr" target="#b18">Mnih and Hinton (2007)</ref> replaces the global normalization of the Bengio model with a tree-structured probability distribution, and also considers multiple positions for each word in the tree.</p><p>More relevantly, Mikolov et al (2013a) and Mikolov et al (2013b) propose extremely com- putationally efficient log-linear neural language models by removing the hidden layers of the neu- ral networks and training from larger context win- dows with very aggressive subsampling. The goal of the models in Mikolov et al (2013a) and <ref type="bibr" target="#b15">Mikolov et al (2013b)</ref> is not so much obtain- ing a low-perplexity language model as learn- ing word representations which will be useful in downstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector repre- sentations using Canonical Correlation Analysis (CCA).</p><p>Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition <ref type="bibr" target="#b19">(Neelakantan and Collins, 2014;</ref><ref type="bibr" target="#b21">Passos et al, 2014;</ref><ref type="bibr" target="#b30">Turian et al, 2010</ref>), dependency parsing ( <ref type="bibr" target="#b0">Bansal et al, 2014</ref>), chunking ( <ref type="bibr" target="#b30">Turian et al, 2010;</ref><ref type="bibr" target="#b4">Dhillon and Ungar, 2011</ref>), sentiment analysis ( <ref type="bibr">Maas et al, 2011</ref>), para- phrase detection ( <ref type="bibr" target="#b27">Socher et al, 2011</ref>) and learning representations of paragraphs and documents ( <ref type="bibr" target="#b10">Le and Mikolov, 2014</ref>). The word clusters obtained from Brown clustering ( <ref type="bibr" target="#b2">Brown et al, 1992</ref>) have similarly been used as features in named entity recognition ( <ref type="bibr" target="#b17">Miller et al, 2004;</ref><ref type="bibr" target="#b22">Ratinov and Roth, 2009</ref>) and dependency parsing ( <ref type="bibr" target="#b9">Koo et al, 2008</ref>), among other tasks.</p><p>There is considerably less prior work on learn- ing multiple vector representations for the same word type. Reisinger and Mooney (2010a) intro- duce a method for constructing multiple sparse, high-dimensional vector representations of words. <ref type="bibr" target="#b8">Huang et al (2012)</ref> extends this approach incor- porating global document context to learn mul- tiple dense, low-dimensional embeddings by us- ing recursive neural networks. Both the meth- ods perform word sense discrimination as a pre- processing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: Skip-gram model</head><p>The Skip-gram model learns word embeddings such that they are useful in predicting the sur- rounding words in a sentence. In the Skip-gram model, v(w) ∈ R d is the vector representation of the word w ∈ W , where W is the words vocabu- lary and d is the embedding dimensionality.</p><p>Given a pair of words (w t , c), the probability that the word c is observed in the context of word w t is given by,</p><formula xml:id="formula_0">P (D = 1|v(w t ), v(c)) = 1 1 + e −v(wt) T v(c) (1)</formula><p>The probability of not observing word c in the con- text of w t is given by,</p><formula xml:id="formula_1">P (D = 0|v(w t ), v(c)) = 1 − P (D = 1|v(w t ), v(c))</formula><p>Given a training set containing the sequence of word types w 1 , w 2 , . . . , w T , the word embeddings are learned by maximizing the following objective function:</p><formula xml:id="formula_2">J(θ) = (wt,ct)∈D + c∈ct log P (D = 1|v(w t ), v(c)) + (wt,c t )∈D − c ∈c t log P (D = 0|v(w t ), v(c ))</formula><p>where w t is the t th word in the training set, c t is the set of observed context words of word w t and c t is the set of randomly sampled, noisy con- text words for the word w t . D + consists of the set of all observed word-context pairs (w t , c t )</p><formula xml:id="formula_3">(t = 1, 2 . . . , T ). D − consists of pairs (w t , c t ) (t = 1, 2 . . . , T ) where c</formula><p>t is the set of randomly sampled, noisy context words for the word w t .</p><p>For each training word w t , the set of context words c t = {w t−Rt , . . . , w t−1 , w t+1 , . . . , w t+Rt } includes R t words to the left and right of the given word as shown in <ref type="figure">Figure 1</ref>. R t is the window size considered for the word w t uniformly randomly sampled from the set {1, 2, . . . , N }, where N is the maximum context window size.</p><p>The set of noisy context words c t for the word w t is constructed by randomly sampling S noisy context words for each word in the context c t . The noisy context words are randomly sampled from the following distribution,</p><formula xml:id="formula_4">P (w) = p unigram (w) 3/4 Z<label>(2)</label></formula><p>where p unigram (w) is the unigram distribution of the words and Z is the normalization constant.  <ref type="figure">Figure 1</ref>: Architecture of the Skip-gram model with window size R t = 2. Context c t of word w t consists of w t−1 , w t−2 , w t+1 , w t+2 .</p><p>and let each sense of word have its own embed- ding, and induce the senses by clustering the em- beddings of the context words around each token. The vector representation of the context is the av- erage of its context words' vectors. For every word type, we maintain clusters of its contexts and the sense of a word token is predicted as the cluster that is closest to its context representation. After predicting the sense of a word token, we perform a gradient update on the embedding of that sense. The crucial difference from previous approaches is that word sense discrimination and learning em- beddings are performed jointly by predicting the sense of the word using the current parameter es- timates. In the MSSG model, each word w ∈ W is associated with a global vector v g (w) and each sense of the word has an embedding (sense vec- tor) v s (w, k) (k = 1, 2, . . . , K) and a context clus- ter with center µ(w, k) (k = 1, 2, . . . , K). The K sense vectors and the global vectors are of dimen- sion d and K is a hyperparameter.</p><p>Consider the word w t and let c t = {w t−Rt , . . . , w t−1 , w t+1 , . . . , w t+Rt } be the set of observed context words. The vector repre- sentation of the context is defined as the average of the global vector representation of the words in the context. Let v context (c t ) = 1 2 * Rt c∈ct v g (c) be the vector representation of the context c t . We use the global vectors of the context words instead of its sense vectors to avoid the computational complexity associated with predicting the sense of the context words. We predict s t , the sense  </p><formula xml:id="formula_5">s t = arg max k=1,2,...,K sim(µ(w t , k), v context (c t )) (3)</formula><p>The hard cluster assignment is similar to the k- means algorithm. The cluster center is the aver- age of the vector representations of all the contexts which belong to that cluster. For sim we use co- sine similarity in our experiments.</p><p>Here, the probability that the word c is observed in the context of word w t given the sense of the word w t is,</p><formula xml:id="formula_6">P (D = 1|s t ,v s (w t , 1), . . . , v s (w t , K), v g (c)) = P (D = 1|v s (w t , s t ), v g (c)) = 1 1 + e −vs(wt,st) T vg(c)</formula><p>The probability of not observing word c in the con- text of w t given the sense of the word w t is,</p><formula xml:id="formula_7">P (D = 0|s t ,v s (w t , 1), . . . , v s (w t , K), v g (c)) = P (D = 0|v s (w t , s t ), v g (c)) = 1 − P (D = 1|v s (w t , s t ), v g (c))</formula><p>Given a training set containing the sequence of word types w 1 , w 2 , ..., w T , the word embeddings are learned by maximizing the following objective Algorithm 1 Training Algorithm of MSSG model 1: Input: w 1 , w 2 , ..., w T , d, K, N . 2: Initialize v s (w, k) and v g (w), ∀w ∈ W, k ∈ {1, . . . , K} randomly, µ(w, k) ∀w ∈ W, k ∈ {1, . . . , K} to 0. 3: for t = 1, 2, . . . , T do 4:</p><p>R t ∼ {1, . . . , N }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>c t = {w t−Rt , . . . , w t−1 , w t+1 , . . . , w t+Rt } 6:</p><formula xml:id="formula_8">v context (c t ) = 1 2 * Rt c∈ct v g (c)</formula><p>7:</p><formula xml:id="formula_9">s t = arg max k=1,2,...,K { sim(µ(w t , k), v context (c t ))} 8:</formula><p>Update context cluster center µ(w t , s t ) since context c t is added to context cluster s t of word w t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>c t = N oisy Samples(c t )</p><p>10:</p><p>Gradient update on v s (w t , s t ), global vec- tors of words in c t and c t . 11: end for 12: Output: v s (w, k), v g (w) and context cluster centers µ(w, k), ∀w ∈ W, k ∈ {1, . . . , K} function:</p><formula xml:id="formula_10">J(θ) = (wt,ct)∈D + c∈ct log P (D = 1|v s (w t , s t ), v g (c))+ (wt,c t )∈D − c ∈c t log P (D = 0|v s (w t , s t ), v g (c ))</formula><p>where w t is the t th word in the sequence, c t is the set of observed context words and c t is the set of noisy context words for the word w t . D + and D − are constructed in the same way as in the Skip- gram model.</p><p>After predicting the sense of word w t , we up- date the embedding of the predicted sense for the word w t (v s (w t , s t )), the global vector of the words in the context and the global vector of the randomly sampled, noisy context words. The con- text cluster center of cluster s t for the word w t (µ(w t , s t )) is updated since context c t is added to the cluster s t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Non-Parametric MSSG model (NP-MSSG)</head><p>The MSSG model learns a fixed number of senses per word type. In this section, we describe a non-parametric version of MSSG, the NP-MSSG model, which learns varying number of senses per word type. Our approach is closely related to the online non-parametric clustering procedure de- scribed in <ref type="bibr" target="#b13">Meyerson (2001)</ref>. We create a new clus- ter (sense) for a word type with probability propor- tional to the distance of its context to the nearest cluster (sense).</p><p>Each word w ∈ W is associated with sense vec- tors, context clusters and a global vector v g (w) as in the MSSG model. The number of senses for a word is unknown and is learned during training. Initially, the words do not have sense vectors and context clusters. We create the first sense vector and context cluster for each word on its first occur- rence in the training data. After creating the first context cluster for a word, a new context cluster and a sense vector are created online during train- ing when the word is observed with a context were the similarity between the vector representation of the context with every existing cluster center of the word is less than λ, where λ is a hyperparameter of the model.</p><p>Consider the word w t and let c t = {w t−Rt , . . . , w t−1 , w t+1 , . . . , w t+Rt } be the set of observed context words. The vector repre- sentation of the context is defined as the average of the global vector representation of the words in the context. Let v context (c t ) = 1 2 * Rt c∈ct v g (c) be the vector representation of the context c t . Let k(w t ) be the number of context clusters or the number of senses currently associated with word w t . s t , the sense of word w t when k(w t ) &gt; 0 is given by</p><formula xml:id="formula_11">s t =      k(w t ) + 1, if max k=1,2,...,k(wt) {sim (µ(w t , k), v context (c t ))} &lt; λ k max , otherwise<label>(4)</label></formula><p>where µ(w t , k) is the cluster center of the k th cluster of word w t and k max = arg max k=1,2,...,k(wt) sim(µ(w t , k), v context (c t )).</p><p>The cluster center is the average of the vector representations of all the contexts which belong to that cluster. If s t = k(w t ) + 1, a new context cluster and a new sense vector are created for the word w t .</p><p>The NP-MSSG model and the MSSG model described previously differ only in the way word sense discrimination is performed. The objec- tive function and the probabilistic model associ- ated with observing a (word, context) pair given the sense of the word remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Time (in hours) <ref type="table" target="#tab_2">Huang et al  168  MSSG 50d  1  MSSG-300d  6  NP-MSSG-50d</ref> 1.83 NP-MSSG-300d 5 Skip-gram-50d</p><p>0.33 Skip-gram-300d</p><p>1.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al <ref type="formula" target="#formula_4">(2012)</ref>, which is the April 2010 snap- shot of the Wikipedia corpus <ref type="bibr" target="#b26">(Shaoul and Westbury, 2010</ref>). It contains approximately 2 million articles and 990 million tokens. In all our experi- ments we remove all the words with less than 20 occurrences and use a maximum context window (N ) of length 5 (5 words before and after the word occurrence). We fix the number of senses (K) to be 3 for the MSSG model unless otherwise speci- fied. Our hyperparameter values were selected by a small amount of manual exploration on a vali- dation set. In NP-MSSG we set λ to -0.5. The Skip-gram model, MSSG and NP-MSSG models sample one noisy context word (S) for each of the observed context words. We train our models us- ing AdaGrad stochastic gradient decent ( <ref type="bibr" target="#b5">Duchi et al, 2011</ref>) with initial learning rate set to 0.025. Similarly to <ref type="bibr" target="#b8">Huang et al (2012)</ref>, we don't use a regularization penalty. Below we describe qualitative results, display- ing the embeddings and the nearest neighbors of each word sense, and quantitative experiments in two benchmark word similarity tasks. <ref type="table" target="#tab_2">Table 1</ref> shows time to train our models, com- pared with other models from previous work. All these times are from single-machine implementa- tions running on similar-sized corpora. We see that our model shows significant improvement in the training time over the model in <ref type="bibr" target="#b8">Huang et al (2012)</ref>, being within well within an order-of- magnitude of the training time for Skip-gram mod- els. <ref type="table">Table 2</ref>: Nearest neighbors of each sense of each word, by cosine similarity, for different algo- rithms. Note that the different senses closely cor- respond to intuitions regarding the senses of the given word types. <ref type="table">Table 2</ref> shows qualitatively the results of dis- covering multiple senses by presenting the near- est neighbors associated with various embeddings. The nearest neighbors of a word are computed by comparing the cosine similarity between the em- bedding for each sense of the word and the context embeddings of all other words in the vocabulary. Note that each of the discovered senses are indeed semantically coherent, and that a reasonable num- ber of senses are created by the non-parametric method. <ref type="table">Table 3</ref> shows the nearest neighbors of the word plant for Skip-gram, MSSG , NP-MSSG and Haung's model <ref type="figure" target="#fig_0">(Huang et al, 2012</ref>). <ref type="bibr">Skipgram</ref> plants, flowering, weed, fungus, biomass MS -SG plants, tubers, soil, seed, biomass refinery, reactor, coal-fired, factory, smelter asteraceae, fabaceae, arecaceae, lamiaceae, eri- caceae NP MS -SG plants, seeds, pollen, fungal, fungus factory, manufacturing, refinery, bottling, steel fabaceae, legume, asteraceae, apiaceae, flowering power, coal-fired, hydro-power, hydroelectric, re- finery Hua -ng et al insect, capable, food, solanaceous, subsurface robust, belong, pitcher, comprises, eagles food, animal, catching, catch, ecology, fly seafood, equipment, oil, dairy, manufacturer facility, expansion, corporation, camp, co. treatment, skin, mechanism, sugar, drug facility, theater, platform, structure, storage natural, blast, energy, hurl, power matter, physical, certain, expression, agents vine, mute, chalcedony, quandong, excrete <ref type="table">Table 3</ref>: Nearest Neighbors of the word plant for different models. We see that the discovered senses in both our models are more semantically coherent than Huang et al (2012) and NP-MSSG is able to learn reasonable number of senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Nearest Neighbors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Word Similarity</head><p>We evaluate our embeddings on two related datasets: the WordSim-353 ( <ref type="bibr" target="#b6">Finkelstein et al, 2001</ref>) dataset and the Contextual Word Similari- ties (SCWS) dataset Huang et al <ref type="bibr">(2012)</ref>. WordSim-353 is a standard dataset for evaluat- ing word vector representations. It consists of a list of pairs of word types, the similarity of which is rated in an integral scale from 1 to 10. Pairs include both monosemic and polysemic words. These scores to each word pairs are given with- out any contextual information, which makes them tricky to interpret.</p><p>To overcome this issue, Stanford's Contextual Word Similarities (SCWS) dataset was developed by <ref type="bibr" target="#b8">Huang et al (2012)</ref>. The dataset consists of 2003 word pairs and their sentential contexts. It consists of 1328 noun-noun pairs, 399 verb-verb pairs, 140 verb-noun, 97 adjective-adjective, 30 noun-adjective, 9 verb-adjective, and 241 same- word pairs. We evaluate and compare our embed- dings on both WordSim-353 and SCWS word sim- ilarity corpus.</p><p>Since it is not trivial to deal with multiple em- beddings per word, we consider the following sim- ilarity measures between words w and w given their respective contexts c and c , where P (w, c, k) is the probability that w takes the k th sense given the context c, and d <ref type="figure">(v s (w, i), v s (w , j)</ref>) is the sim- ilarity measure between the given embeddings v s (w, i) and v s (w , j).</p><p>The avgSim metric,</p><formula xml:id="formula_12">avgSim(w, w ) = 1 K 2 K i=1 K j=1 d (v s (w, i), v s (w , j)) ,</formula><p>computes the average similarity over all embed- dings for each word, ignoring information from the context. To address this, the avgSimC metric,</p><formula xml:id="formula_13">avgSimC(w, w ) = K j=1 K i=1 P (w, c, i)P (w , c , j) × d (v s (w, i), v s (w , j))</formula><p>weighs the similarity between each pair of senses by how well does each sense fit the context at hand.</p><p>The globalSim metric uses each word's global context vector, ignoring the many senses:</p><formula xml:id="formula_14">globalSim(w, w ) = d (v g (w), v g (w )) .</formula><p>Finally, localSim metric selects a single sense for each word based independently on its context and computes the similarity by</p><formula xml:id="formula_15">localSim(w, w ) = d (v s (w, k), v s (w , k )) ,</formula><p>where k = arg max i P (w, c, i) and k = arg max j P (w , c , j) and P (w, c, i) is the prob- ability that w takes the i th sense given context c. The probability of being in a cluster is calculated as the inverse of the cosine distance to the cluster center <ref type="figure" target="#fig_0">(Huang et al, 2012)</ref>.</p><p>We report the Spearman correlation between a model's similarity scores and the human judge- ments in the datasets. <ref type="table" target="#tab_5">Table 5</ref> shows the results on WordSim-353 task. C&amp;W refers to the language model by <ref type="bibr" target="#b3">Collobert and Weston (2008)</ref> and HLBL model is the method described in <ref type="bibr" target="#b18">Mnih and Hinton (2007)</ref>. On WordSim-353 task, we see that our model per- forms significantly better than the previous neural network model for learning multi-representations per word <ref type="figure" target="#fig_0">(Huang et al, 2012</ref>). Among the meth- ods that learn low-dimensional and dense repre- sentations, our model performs slightly better than Skip-gram. <ref type="table" target="#tab_4">Table 4</ref> shows the results for the SCWS task. In this task, when the words are Model globalSim avgSim avgSimC localSim</p><formula xml:id="formula_16">TF-IDF 26.3 - - - Collobort &amp; Weston-50d 57.0 - - - Skip-gram-50d 63.4 - - - Skip-gram-300d</formula><p>65.2 - - - Pruned TF-IDF 62.5 60.4 60.5 - Huang et al-50d</p><p>58      introduced by Mikolov et al (2013a) where both MSSG and NP-MSSG models achieve 64% accu- racy compared to 12% accuracy by <ref type="bibr" target="#b8">Huang et al (2012)</ref>. Skip-gram which is the state-of-art model for this task achieves 67% accuracy. <ref type="figure" target="#fig_1">Figure 3</ref> shows the distribution of number of senses learned per word type in the NP-MSSG model. We learn the multiple embeddings for the same set of approximately 6000 words that were used in Huang et al (2012) for all our experiments to ensure fair comparision. These approximately 6000 words were choosen by Huang et al. mainly from the top 30,00 frequent words in the vocab- ulary. This selection was likely made to avoid the noise of learning multiple senses for infre- quent words. However, our method is robust to noise, which can be seen by the good performance of our model that learns multiple embeddings for the top 30,000 most frequent words. We found that even by learning multiple embeddings for the top 30,000 most frequent words in the vocubu- lary, MSSG model still achieves state-of-art result on SCWS task with an avgSimC score of 69.2 as shown in <ref type="table" target="#tab_7">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. The model jointly performs word sense discrimination and embedding learning, and non-parametrically estimates the number of senses per word type. Our method achieves new state- of-the-art results in the word similarity in con- text task and learns multiple senses, training on close to billion tokens in less than 6 hours. The global vectors, sense vectors and cluster centers of our model and code for learning them are avail- able at https://people.cs.umass.edu/ ˜ arvind/emnlp2014wordvectors. In fu- ture work we plan to use the multiple embeddings per word type in downstream NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of Multi-Sense Skip-gram (MSSG) model with window size R t = 2 and K = 3. Context c t of word w t consists of w t−1 , w t−2 , w t+1 , w t+2. The sense is predicted by finding the cluster center of the context that is closest to the average of the context vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The plot shows the distribution of number of senses learned per word type in NP-MSSG model</figDesc><graphic url="image-1.png" coords="8,307.28,345.35,172.80,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Training Time Results. First five model 
reported in the table are capable of learning mul-
tiple embeddings for each word and Skip-gram 
is capable of learning only single embedding for 
each word. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results in the SCWS task. The numbers are Spearmans correlation ρ × 100 
between each model's similarity judgments and the human judgments, in context. First three models 
learn only a single embedding per model and hence, avgSim, avgSimC and localSim are not reported 
for these models, as they'd be identical to globalSim. Both our parametric and non-parametric models 
outperform the baseline models, and our best model achieves a score of 69.3 in this task. NP-MSSG 
achieves the best results when globalSim, avgSim and localSim similarity measures are used. The best 
results according to each metric are in bold face. 

Model 
ρ × 100 
HLBL 
33.2 
C&amp;W 
55.3 
Skip-gram-300d 
70.4 
Huang et al-G 
22.8 
Huang et al-M 
64.2 
MSSG 50d-G 
60.6 
MSSG 50d-M 
63.2 
MSSG 300d-G 
69.2 
MSSG 300d-M 
70.9 
NP-MSSG 50d-G 
61.5 
NP-MSSG 50d-M 
62.4 
NP-MSSG 300d-G 
69.1 
NP-MSSG 300d-M 
68.6 
Pruned TF-IDF 
73.4 
ESA 
75 
Tiered TF-IDF 
76.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on the WordSim-353 dataset. 
The table shows the Spearmans correlation ρ be-
tween the model's similarities and human judg-
ments. G indicates the globalSim similarity mea-
sure and M indicates avgSim measure.The best 
results among models that learn low-dimensional 
and dense representations are in bold face. Pruned 
TF-IDF (Reisinger and Mooney, 2010a), ESA 
(Gabrilovich and Markovitch, 2007) and Tiered 
TF-IDF (Reisinger and Mooney, 2010b) construct 
spare, high-dimensional representations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table - 4.</head><label>-</label><figDesc></figDesc><table>The previous state-of-art model (Huang 
et al, 2012) on this task achieves 65.7% using 
the avgSimC measure, while the MSSG model 
achieves the best score of 69.3% on this task. The 
results on the other metrics are similar. For a 
fixed embedding dimension, the model by Huang 
et al (2012) has more parameters than our model 
since it uses a hidden layer. The results show 
that our model performs better than Huang et al 
(2012) even when both the models use 50 dimen-
sional vectors and the performance of our model 
improves as we increase the number of dimensions 
to 300. 
We evaluate the models in a word analogy task (a) 

(b) 

Figure 4: Figures (a) and (b) show the effect of varying embedding dimensionality and number of senses 
respectively of the MSSG Model on the SCWS task. 

Model 
Task 
Sim ρ × 100 
Skip-gram WS-353 globalSim 
70.4 
MSSG 
WS-353 globalSim 
68.4 
MSSG 
WS-353 
avgSim 
71.2 
NP MSSG WS-353 globalSim 
68.3 
NP MSSG WS-353 
avgSim 
69.66 
MSSG 
SCWS 
localSim 
59.3 
MSSG 
SCWS globalSim 
64.7 
MSSG 
SCWS 
avgSim 
67.2 
MSSG 
SCWS avgSimC 
69.2 
NP MSSG 
SCWS 
localSim 
60.11 
NP MSSG 
SCWS globalSim 
65.3 
NP MSSG 
SCWS 
avgSim 
67 
NP MSSG 
SCWS avgSimC 
68.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Experiment results on WordSim-353 and 
SCWS Task. Multiple Embeddings are learned for 
top 30,000 most frequent words in the vocabulary. 
The embedding dimension size is 300 for all the 
models for this task. The number of senses for 
MSSG model is 3. 

</table></figure>

			<note place="foot" n="2"> For distance d, d(a, c) ≤ d(a, b) + d(b, c).</note>

			<note place="foot" n="3"> Personal communication with authors Eric H. Huang and Richard Socher.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Center for Intelligent Information Retrieval and in part by DARPA under agreement number FA8750-13-2-0020. The U.S. Government is authorized to re-produce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<title level="m">Tailoring Continuous Word Representations for Dependency Parsing. Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Class-based N-gram models of natural language Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<title level="m">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. International Conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-View Learning of Word Embeddings via CCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive sub-gradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2001" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Word Representations via Global Context and Multiple Word Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simple Semi-supervised Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<title level="m">Distributed Representations of Sentences and Documents. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning Word Vectors for Sentiment Analysis Association for Computational Linguistics (ACL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyerson</surname></persName>
		</author>
		<title level="m">IEEE Symposium on Foundations of Computer Science. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Name tagging with word clusters and discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jethran</forename><surname>Guinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zamanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Dictionaries for Named Entity Recognition using Minimal Supervision</title>
	</analytic>
	<monogr>
		<title level="m">European Chapter of the Association for Computational Linguistics (EACL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lexicon Infused Phrase Embeddings for Named Entity Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design Challenges and Misconceptions in Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic and Static Prototype Vectors for Semantic Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCNLP)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<publisher>NAACL-HLT</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A mixture model with sharing for lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Westbury lab wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shaoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Westbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings for Phrase-Based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
