<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Structured Weighted Violations Perceptron Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rotem</forename><surname>Dror</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Structured Weighted Violations Perceptron Algorithm</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="469" to="478"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a new struc-tured prediction algorithm that generalizes the Collins Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the update rule of SWVP explicitly exploits the internal structure of the predicted labels. We prove the convergence of SWVP for linearly separable training sets, provide mistake and generalization bounds, and show that in the general case these bounds are tighter than those of the CSP special case. In synthetic data experiments with data drawn from an HMM, various variants of SWVP substantially outperform its CSP special case. SWVP also provides encouraging initial dependency parsing results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The structured perceptron ( <ref type="bibr" target="#b2">(Collins, 2002</ref>), hence- forth denoted CSP) is a prominent training algo- rithm for structured prediction models in NLP, due to its effective parameter estimation and simple im- plementation. It has been utilized in numerous NLP applications including word segmentation and POS tagging <ref type="bibr" target="#b23">(Zhang and Clark, 2008)</ref>, dependency pars- ing ( <ref type="bibr" target="#b10">Koo and Collins, 2010;</ref><ref type="bibr" target="#b7">Goldberg and Elhadad, 2010;</ref><ref type="bibr" target="#b12">Martins et al., 2013)</ref>, semantic parsing <ref type="bibr" target="#b22">(Zettlemoyer and Collins, 2007)</ref> and information extrac- tion ( <ref type="bibr" target="#b8">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b19">Reichart and Barzilay, 2012)</ref>, if to name just a few.</p><p>Like some training algorithms in structured pre- diction (e.g. structured SVM ( <ref type="bibr" target="#b20">Taskar et al., 2004;</ref><ref type="bibr" target="#b21">Tsochantaridis et al., 2005</ref>), MIRA <ref type="bibr" target="#b3">(Crammer and Singer, 2003)</ref> and <ref type="bibr">LaSo (Daumé III and Marcu, 2005)</ref>), CSP considers in its update rule the differ- ence between complete predicted and gold standard labels (Sec. 2). Unlike others (e.g. factored MIRA ( <ref type="bibr" target="#b15">McDonald et al., 2005b;</ref><ref type="bibr" target="#b14">McDonald et al., 2005a</ref>) and dual-loss based methods ( <ref type="bibr" target="#b16">Meshi et al., 2010)</ref>) it does not exploit the structure of the predicted label. This may result in valuable information being lost.</p><p>Consider, for example, the gold and predicted de- pendency trees of <ref type="figure" target="#fig_0">Figure 1</ref>. The substantial differ- ence between the trees may be mostly due to the dif- ference in roots (are and worse, respectively). Pa- rameter update w.r.t this mistake may thus be more useful than an update w.r.t the complete trees.</p><p>In this work we present a new perceptron algo- rithm with an update rule that exploits the struc- ture of a predicted label when it differs from the gold label (Section 3). Our algorithm is called The Structured Weighted Violations Perceptron (SWVP) as its update rule is based on a weighted sum of up- dates w.r.t violating assignments and non-violating assignments: assignments to the input example, de- rived from the predicted label, that score higher (for violations) and lower (for non-violations) than the gold standard label according to the current model.</p><p>Our concept of violating assignment is based on <ref type="bibr" target="#b9">Huang et al. (2012)</ref> that presented a variant of the CSP algorithm where the argmax inference problem is replaced with a violation finding function. Their update rule, however, is identical to that of the CSP algorithm. Importantly, although CSP and the above variant do not exploit the internal structure of the predicted label, they are special cases of SWVP.</p><p>In Section 4 we prove that for a linearly separable training set, SWVP converges to a linear separator of the data under certain conditions on the parameters of the algorithm, that are respected by the CSP spe- cial case. We further prove mistake and generaliza- tion bounds for SWVP, and show that in the general case the SWVP bounds are tighter than the CSP's.</p><p>In Section 5 we show that SWVP allows ag- gressive updates, that exploit only violating assign- ments derived from the predicted label, and more balanced updates, that exploit both violating and non-violating assignments. In experiments with syn- thetic data generated by an HMM, we demonstrate that various SWVP variants substantially outper- form CSP training. We also provide initial encour- aging dependency parsing results, indicating the po- tential of SWVP for real world NLP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Collins Structured Perceptron</head><p>In structured prediction the task is to find a mapping f : X → Y, where y ∈ Y is a structured object rather than a scalar, and a feature mapping φ(x, y) :</p><formula xml:id="formula_0">X × Y(x) → R d is given. In this work we denote Y(x) = {y |y ∈ D Y Lx },</formula><p>where L x , a scalar, is the size of the allowed output sequence for an input x and D Y is the domain of y i for every i ∈ {1, . . . L x }. <ref type="bibr">1</ref> Our results, however, hold for the general case of an output space with variable size vectors as well.</p><p>The CSP algorithm (Algorithm 1) aims to learn a parameter (or weight) vector w ∈ R d , that sepa- rates the training data, i.e. for each training example (x, y) it holds that: y = arg max y ∈Y(x) w · φ(x, y ). To find such a vector the algorithm iterates over the training set examples and solves the above in- ference (argmax) problem. If the inferred label y * differs from the gold label y the update w = w + ∆φ(x, y, y * ) is performed. For linearly separa- ble training data (see definition 4), CSP is proved to converge to a vector w separating the training data. <ref type="bibr" target="#b0">Collins and Roark (2004)</ref> and Huang et al. (2012) expanded the CSP algorithm by proposing various alternatives to the argmax inference problem which is often intractable in structured prediction problems (e.g. in high-order graph-based dependency parsing <ref type="bibr" target="#b13">(McDonald and Pereira, 2006)</ref>). The basic idea is re- placing the argmax problem with the search for a vi- olation: an output label that the model scores higher <ref type="bibr">1</ref> In the general case Lx is a set of output sizes, which may be finite or infinite (as in constituency parsing <ref type="bibr" target="#b1">(Collins, 1997)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Structured Perceptron (CSP)</head><formula xml:id="formula_1">Input: data D = {x i , y i } n i=1 , feature mapping φ Output: parameter vector w ∈ R d Define: ∆φ(x, y, z) φ(x, y) − φ(x, z) 1: Initialize w = 0. 2: repeat 3: for each (x i , y i ) ∈ D do 4: y * = arg max y ∈Y(x i ) w · φ(x i , y )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>if y * = y i then 6:</p><formula xml:id="formula_2">w = w + ∆φ(x i , y i , y * ) 7:</formula><p>end if 8: end for 9: until Convergence than the gold standard label. The update rule in these CSP variants is, however, identical to the CSP's. We, in contrast, propose a novel update rule that exploits the internal structure of the model's prediction re- gardless of the way this prediction is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Structured Weighted Violations</head><p>Perceptron (SWVP)</p><p>SWVP exploits the internal structure of a predicted label y * = y for a training example (x, y) ∈ D, by updating the weight vector with respect to sub- structures of y * . We start by presenting the funda- mental concepts at the basis of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Concepts</head><p>Sub-structure Sets We start with two fundamen- tal definitions: (1) An individual sub-structure of a structured object (or label) y ∈ D Y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lx</head><p>, denoted with J, is defined to be a subset of indexes J ⊆ [L x ]; 2 and (2) A set of substructures for a training example (x, y), denoted with JJ x , is defined as JJ x ⊆ 2 <ref type="bibr">[Lx]</ref> .</p><p>Mixed Assignment We next define the concept of a mixed assignment: Definition 1. For a training pair (x, y) and a pre- dicted label y * ∈ Y(x), y * = y, a mixed assign- ment (M A) vector denoted as m J (y * , y) is defined with respect to J ∈ JJ x as follows:</p><formula xml:id="formula_3">m J k (y * , y) = y * k k ∈ J y k else</formula><p>That is, a mixed assignment is a new label, de- rived from the predicted label y * , that is identical to y * in all indexes in J and to y otherwise. For sim- plicity we denote m J (y * , y) = m J when the refer- ence y * and y labels are clear from the context. Consider, for example, the trees of <ref type="figure" target="#fig_0">Figure 1</ref>, as- suming that the top tree is y, the middle tree is y * and J = <ref type="bibr">[2,</ref><ref type="bibr">5]</ref>. <ref type="bibr">3</ref> In the m J (y * , y) (bottom) tree the heads of all the words are identical to those of the top tree, except for the heads of mistakes and of then.</p><p>Violation The next central concept is that of a vi- olation, originally presented by <ref type="bibr" target="#b9">Huang et al. (2012)</ref>: Definition 2. A triple (x, y, y * ) is said to be a vio- lation with respect to a training example (x, y) and a parameter vector w if for y * ∈ Y(x) it holds that y * = y and w · ∆φ(x, y, y * ) ≤ 0.</p><p>The SWVP algorithm distinguishes between M As that are violations, and ones that are not. For a triplet (x, y, y * ) and a set of substructures JJ x ⊆ 2 <ref type="bibr">[Lx]</ref> we provide the following notations:</p><formula xml:id="formula_4">I(y * , y, JJx) v = {J ∈ JJx|m J = y, w·∆φ(x, y, m J ) ≤ 0} I(y * , y, JJx) nv = {J ∈ JJx|m J = y, w·∆φ(x, y, m J ) &gt; 0}</formula><p>This notation divides the set of substructures into two subsets, one consisting of the substructures that yield violating MAs and one consisting of the substructures that yield non-violating MAs. Here again when the reference label y * and the set JJ x are known we denote: I(y * , y, JJ x ) v = I v , I(y * , y, JJ x ) nv = I nv and I = I v ∪ I nv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Violations</head><p>The key idea of SWVP is the exploitation of the internal structure of the pre- dicted label in the update rule. For this aim at each iteration we define the set of substructures, JJ x , and then, for each J ∈ JJ x , update the parameter vector, w, with respect to the mixed assignments, M A J 's. This is a more flexible setup compared to CSP, as we can update with respect to the predicted output (if it is a violation, as is promised if inference is per- formed via argmax), if we wish to do so, as well as with respect to other mixed assignments.</p><p>Naturally, not all mixed assignments are equally important for the update rule. Hence, we weigh the different updates using a weight vector γ. This pa- per therefore extends the observation of <ref type="bibr" target="#b9">Huang et al. (2012)</ref> that perceptron parameter update can be per- formed w.r.t violations (Section 2), by showing that w can actually be updated w.r.t linear combinations of mixed assignments, under certain conditions on the selected weights. <ref type="bibr">3</ref> We index the dependency tree words from 1 onwards.</p><p>Some mistakes are worse than others.</p><p>Some mistakes are worse than others.</p><p>Some mistakes are worse than others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm</head><p>With these definitions we can present the SWVP al- gorithm (Algorithm 2). SWVP is in fact a family of algorithms differing with respect to two decisions that can be made at each pass over each training ex- ample (x, y): the choice of the set JJ x and the im- plementation of the SETGAMMA function.</p><p>SWVP is very similar to CSP except for in the update rule. Like in CSP, the algorithm iterates over the training data examples and for each example it first predicts a label according to the current param- eter vector w (inference is discussed in Section 4.2, property 2). The main difference from CSP is in the update rule (lines 6-12). Here, for each substructure in the substructure set, J ∈ JJ x , the algorithm gen- erates a mixed assignment m J (lines 7-9). Then, w is updated with a weighted sum of the mixed assign- ments (line 11), unlike in CSP where the update is held w.r.t the predicted assignment only.</p><p>The γ(m J ) weights assigned to each of the ∆φ(x, y, m J ) updates are defined by a SETGAMMA function (line 10). Intuitively, a γ(m J ) weight should be higher the more the mixed assignment is assumed to convey useful information that can guide the update of w in the right direction. In Sec- tion 4 we detail the conditions on SETGAMMA un- der which SWVP converges, and in Section 5 we describe various SETGAMMA implementations.</p><p>Going back to the example of <ref type="figure" target="#fig_0">Figure 1</ref>, one would assume (Sec. 1) that the head word prediction for worse is pivotal to the substantial difference between the two top trees (UAS of 0.2). CSP does not directly exploit this observation as it only updates its param- eter vector with respect to the differences between complete assignments: w = w + ∆φ(x, y, z).</p><p>In contrast, SWVP can exploit this observation in various ways. For example, it can generate a mixed assignment for each of the erroneous arcs where all other words are assigned their correct arc (according to the gold tree) except for that specific arc which is kept as in the bottom tree. Then, higher weights can be assigned to errors that seem more central than others. We elaborate on this in the next two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 The Structured Weighted Violations Perceptron</head><formula xml:id="formula_5">Input: data D = {x i , y i } n i=1 , feature mapping φ Output: parameter vector w ∈ R d Define: ∆φ(x, y, z) φ(x, y) − φ(x, z) 1: Initialize w = 0. 2: repeat 3: for each (x i , y i ) ∈ D do 4: y * = arg max y ∈Y(x i ) w · φ(x i , y )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>if y * = y i then 6:</p><formula xml:id="formula_6">Define: JJ x i ⊆ 2 [L x i ] 7:</formula><p>for J ∈ JJ x i do 8:</p><formula xml:id="formula_7">Define: m J s.t. m J k = y * k k ∈ J y i k else 9:</formula><p>end for 10:</p><formula xml:id="formula_8">γ = SETGAMMA() 11</formula><p>:</p><formula xml:id="formula_9">w = w + J∈I v ∪I nv γ(m J )∆φ(x i , y i , m J )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13: end for 14: until Convergence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theory</head><p>We start this section with the convergence conditions on the γ vector which weighs the mixed assignment updates in the SWVP update rule (line 11). Then, using these conditions, we describe the relation be- tween the SWVP and the CSP algorithms. After that, we prove the convergence of SWVP and anal- yse the derived properties of the algorithm.</p><p>γ Selection Conditions Our main observation in this section is that SWVP converges under two con- ditions: (a) the training set D is linearly separable; and (b) for any parameter vector w achievable by the algorithm, there exists (x, y) ∈ D with JJ x ⊆ 2 <ref type="bibr">[Lx]</ref> , such that for the predicted output y * = y, SETGAMMA returns a γ weight vector that respects the γ selection conditions defined as follows: Definition 3. The γ selection conditions for the SWVP algorithm are (I = I v ∪ I nv ):</p><formula xml:id="formula_10">(1) J∈I γ(m J ) = 1. γ(m J ) ≥ 0, ∀J ∈ I. (2) w · J∈I γ(m J )∆φ(x i , y i , m J ) ≤ 0.</formula><p>With this definition we are ready to prove the fol- lowing property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SWVP Generalizes the CSP Algorithm We now</head><p>show that the CSP algorithm is a special case of SWVP. CSP can be derived from SWVP when tak- ing: JJ x = {[L x ]}, and γ(m <ref type="bibr">[Lx]</ref> ) = 1 for every (x, y) ∈ D. With these parameters, the γ selection conditions hold for every w and y * . Condition (1) holds trivially as there is only one γ coefficient and it is equal to 1. Condition (2) holds as y * = m <ref type="bibr">[Lx]</ref> and</p><formula xml:id="formula_11">hence I = {[L x ]} and w · J∈I ∆φ(x, y, m J ) ≤ 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convergence for Linearly Separable Data</head><p>Here we give the theorem regarding the convergence of the SWVP in the separable case. We first define:</p><formula xml:id="formula_12">Definition 4. A data set D = {x i , y i } n i=1</formula><p>is linearly separable with margin δ &gt; 0 if there exists some vector u with u 2 = 1 such that for all i:</p><formula xml:id="formula_13">u · ∆φ(x i , y i , z) ≥ δ, ∀z ∈ Y(x i ). Definition 5. The radius of a data set D = {x i , y i } n i=1</formula><p>is the minimal scalar R s.t for all i:</p><formula xml:id="formula_14">∆φ(x i , y i , z) ≤ R, ∀z ∈ Y(x i ).</formula><p>We next extend these definitions:</p><formula xml:id="formula_15">Definition 6. Given a data set D = {x i , y i } n i=1 and a set JJ = {JJ x i ⊆ 2 [L x i ] |(x i , y i ) ∈ D}, D is</formula><p>linearly separable w.r.t JJ, with margin δ JJ &gt; 0 if there exists a vector u with u 2 = 1 such that:</p><formula xml:id="formula_16">u · ∆φ(x i , y i , m J (z, y i )) ≥ δ JJ for all i, z ∈ Y(x i ), J ∈ JJ x i . Definition 7. The mixed assignment radius w.r.t JJ of a data set D = {x i , y i } n i=1</formula><p>is a constant R JJ s.t for all i it holds that:</p><formula xml:id="formula_17">∆φ(x i , y i , m J (z, y i )) ≤ R JJ , ∀z ∈ Y(x i ), J ∈ JJ x i .</formula><p>With these definitions we can make the following observation (proof in A): Observation 1. For linearly separable data D and a set JJ, every unit vector u that separates the data with margin δ, also separates the data with respect to mixed assignments with JJ, with margin δ JJ ≥ δ. Likewise, it holds that R JJ ≤ R.</p><p>We can now state our convergence theorem. While the proof of this theorem resembles that of the CSP <ref type="bibr" target="#b2">(Collins, 2002</ref>), unlike the CSP proof the SWVP proof relies on the γ selection conditions pre- sented above and on the Jensen inequality. Theorem 1. For any dataset D, linearly separable with respect to JJ with margin δ JJ &gt; 0, the SWVP algorithm terminates after t ≤ (R JJ ) 2 (δ JJ ) 2 steps, where R JJ is the mixed assignment radius of D w.r.t. JJ.</p><p>Proof. Let w t be the weight vector before the t th update, thus w 1 = 0. Suppose the t th update occurs on example (x, y), i.e. for the predicted output y * it holds that y * = y. We will bound w t+1 2 from both sides. First, it follows from the update rule of the algorithm that:</p><formula xml:id="formula_18">w t+1 = w t + J∈I v ∪I nv γ(m J )∆φ(x, y, m J ).</formula><p>For simplicity, in this proof we will use the notation I v ∪ I nv = I. Hence, multiplying each side of the equation by u yields:</p><formula xml:id="formula_19">u · w t+1 = u · w t + u · J∈I γ(m J )∆φ(x, y, m J ) = u · w t + J∈I γ(m J )u · ∆φ(x, y, m J ) ≥ u · w t + J∈I γ(m J )δ JJ (margin property) ≥ u · w t + δ JJ ≥ . . . ≥ tδ JJ .</formula><p>The last inequality holds because J∈I γ(m J ) = 1. From this we get that w t+1 2 ≥ (δ JJ ) 2 t 2 since u=1. Second,</p><formula xml:id="formula_20">w t+1 2 = w t + J∈I γ(m J )∆φ(x, y, m J ) 2 = w t 2 + J∈I γ(m J )∆Φ(x, y, m J ) 2 + 2w t · J∈I γ(m J )∆Φ(x, y, m J ).</formula><p>From γ selection condition (2) we get that:</p><formula xml:id="formula_21">w t+1 2 ≤ w t 2 + J∈I γ(m J )∆Φ(x, y, m J ) 2 ≤ w t 2 + J∈I γ(m J )∆Φ(x, y, m J ) 2 ≤ w t 2 + (R JJ ) 2 . (radius property)</formula><p>The inequality one before the last results from the Jensen inequality which holds due to (a) γ selection condition (1); and (b) the squared norm function be- ing convex. From this we finally get:</p><formula xml:id="formula_22">w t+1 2 ≤ w t 2 + (R JJ ) 2 ≤ . . . ≤ t(R JJ ) 2 .</formula><p>Combining the two steps we get:</p><formula xml:id="formula_23">(δ JJ ) 2 t 2 ≤ w t+1 2 ≤ t(R JJ ) 2 .</formula><p>From this it is easy to derive the upper bound in the theorem: t ≤ (R JJ ) 2 (δ JJ ) 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence Properties</head><p>We next point on three properties of the SWVP al- gorithm, derived from its convergence proof:</p><p>Property 1 (tighter iterations bound) The con- vergence proof of CSP <ref type="bibr" target="#b2">(Collins, 2002</ref>) is given for a vector u that linearly separates the data, with mar- gin δ and for a data radius R. Following observation 1, it holds that in our case, u also linearly separates the data with respect to mixed assignments with a set JJ and with margin δ JJ ≥ δ. Together with the definition of R JJ ≤ R we get that: (R JJ ) 2 (δ JJ ) 2 ≤ R 2 δ 2 . This means that the bound on the number of updates made by SWVP is tighter than the bound of CSP.</p><p>Property 2 (inference) From the γ selection con- ditions it holds that any label from which at least one violating MA can be derived through JJ x is suitable for an update. This is because in such a case we can choose, for example, a SETGAMMA function that assigns the weight of 1 to that MA, and the weight of 0 to all other MAs.</p><p>Algorithm 2 employs the argmax inference func- tion, following the basic reasoning that it is a good choice to base the parameter update on. Importantly, if the inference function is argmax and the algorithm performs an update (y * = y), this means that y * , the output of the argmax function, is a violating MA by definition. However, it is obvious that solving the in- ference problem and the optimal γ assignment prob- lems jointly may result in more informed parameter (w) updates. We leave a deeper investigation of this issue to future research.</p><p>Property 3 (dynamic updates) The γ selec- tion conditions paragraph states two conditions ((a) and (b)) under which the convergence proof holds. While it is trivial for SETGAMMA to generate a γ vector that respects condition (a), if there is a pa- rameter vector w' achievable by the algorithm for which SETGAMMA cannot generate γ that respects condition (b), SWVP gets stuck when reaching w'. This problem can be solved with dynamic up- dates. A deep look into the convergence proof re- veals that the set JJ x and the SETGAMMA func- tion can actually differ between iterations. While this will change the bound on the number of it- erations, it will not change the fact that the algo- rithm converges if the data is linearly separable. This makes SWVP highly flexible as it can always back off to the CSP setup of JJ x = {[L x ]}, and ∀(x, y) ∈ D : γ(m <ref type="bibr">[Lx]</ref> ) = 1, update its parameters and continue with its original JJ and SETGAMMA when this option becomes feasible. If this does not happen, the algorithm can continue till convergence with the CSP setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mistake and Generalization Bounds</head><p>The following bounds are proved: the number of updates in the separable case (see Theorem 1); the number of mistakes in the non-separable case (see Appendix B); and the probability to misclassify an unseen example (see supplementary material). It can be shown that in the general case these bounds are tighter than those of the CSP special case. We next discuss variants of SWVP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Passive Aggressive SWVP</head><p>Here we present types of update rules that can be implemented within SWVP. Such rule types are de- fined by: (a) the selection of γ, which should respect the γ selection conditions (see Definition 3) and (b) the selection of JJ = {JJ x ⊆ 2 <ref type="bibr">[Lx]</ref> |(x, y) ∈ D}, the substructure sets for the training examples.</p><p>γ Selection A first approach we consider is the ag- gressive approach 4 where only mixed assignments that are violations {m J : J ∈ I v } are exploited (i.e. for all J ∈ I nv , γ(m J ) = 0). Note, that in this case condition (2) of the γ selection conditions trivially holds as:</p><formula xml:id="formula_24">w · J∈I v γ(m J )∆φ(x, y, m J ) ≤ 0.</formula><p>The only remaining requirement is that condition (1) also holds, i.e. that J∈I v γ(m J ) = 1. The opposite, passive approach, exploits only non-violating MA's {m J : J ∈ I nv }. How- ever, such γ assignments do not respect γ selection condition (2), as they yield: w · J∈I nv γ(m J )∆φ(x, y, m J ) ≤ 0 which holds if and only if for every J ∈ I nv , γ(m J ) = 0 that in turn contradicts condition (1).</p><p>Finally, we can take a balanced approach which gives a positive γ coefficient for at least one violat- ing MA and at least one positive γ coefficient for a non-violating MA. This approach is allowed by SWVP as long as both γ selection conditions hold.</p><p>We implemented two weighting methods, both based on the concept of margin:</p><formula xml:id="formula_25">(1) Weighted Margin (WM): γ(m J ) = |w·∆φ(x,y,m J )| β J ∈JJx |w·∆φ(x,y,m J )| β (2) Weighted Margin Rank (WMR): γ(m J ) = |JJx|−r |JJx| β .</formula><p>where r is the rank of |w · ∆φ(x, y, m J (y * , y))| among the |w · ∆φ(x, y, m J (y * , y))| values for J ∈ JJ x . Both schemes were implemented twice, within a balanced approach (denoted as B) and an aggressive approach (denoted as A). <ref type="bibr">5</ref> The aggressive schemes respect both γ selection conditions. The balanced schemes, however, respect the first condition but not necessarily the second. Since all models that employ the balanced weighting schemes converged after at most 10 iterations, we did not impose this condition (which we could do by, e.g., excluding terms for J ∈ I nv till condition (2) holds).</p><p>JJ Selection Another choice that strongly affects the updates made by SWVP is that of JJ. A choice of JJ x = 2 <ref type="bibr">[Lx]</ref> , for every (x, y) ∈ D results in an update rule which considers all possible mixing as- signments derived from the predicted label y * and the gold label y. Such an update rule, however, re- quires computing a sum over an exponential number of terms <ref type="bibr">(2 Lx )</ref> and is therefore highly inefficient.</p><p>Among the wide range of alternative approaches, in this paper we exploit single difference mixed as- signments. In this approach we define: JJ = {JJ x = {{1}, {2}, . . . {L x }}|(x, y) ∈ D}. For a training pair (x, y) ∈ D, a predicted label y * and J = {j} ∈ JJ x , we will have:</p><formula xml:id="formula_26">m J k (y * , y) = y k k = j y * k k = j</formula><p>Under this approach for the pair (x, y) ∈ D only L x terms are summed in the SWVP update rule. We leave a further investigation of JJ selection ap- proaches to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Synthetic Data We experiment with syn- thetic data generated by a linear-chain, first-order Hidden Markov Model <ref type="bibr">(HMM, (Rabiner and Juang, 1986)</ref>).</p><p>Our learning al- gorithm is a liner-chain conditional random field (CRF, ( <ref type="bibr" target="#b11">Lafferty et al., 2001</ref>)):</p><formula xml:id="formula_27">P (y|x) = 1 Z(x) i=1:Lx exp(w · φ(y i−1 , y i , x)) (where Z(x)</formula><p>is a normalization factor) with binary indicator fea- tures {x i , y i , y i−1 , (x i , y i ), (y i , y i−1 ), (x i , y i , y i−1 )} for the triplet (y i , y i−1 , x).</p><p>A dataset is generated by iteratively sampling K items, each is sampled as follows. We first sam- ple a hidden state, y 1 , from a uniform prior distri- bution. Then, iteratively, for i = 1, 2, . . . , L x we sample an observed state from the emission prob- ability and (for i &lt; L x ) a hidden state from the transition probability. We experimented in 3 setups. In each setup we generated 10 datasets that were subsequently divided to a 7000 items training set, a 2000 items development set and a 1000 items test set. In all datasets, for each item, we set L x = 8. We experiment in three conditions: (1) simple(++), learnable(+++), (2) simple(++), learnable(++) and (3) simple(+), learnable(+). <ref type="bibr">6</ref> For each dataset (3 setups, 10 datasets per setup) we train variants of the SWVP algorithm differing in the γ selection strategy (WM or WMR, Section 5), being aggressive (A) or passive (B), and in their β parameter (β = {0.5, 1, . . . , 5}). Training is done on the training subset and the best performing vari- ant on the development subset is applied to the test subset. For CSP no development set is employed as there is no hyper-parameter to tune. We report averaged accuracy (fraction of observed states for which the model successfully predicts the hidden state value) across the test sets, together with the standard deviation.</p><p>Dependency Parsing We also report initial de- pendency parsing results. We implemented our algo- rithms within the TurboParser <ref type="figure" target="#fig_0">(Martins et al., 2013)</ref>. That is, every other aspect of the parser: feature set, probabilistic pruning algorithm, inference algo- rithm etc., is kept fixed but training is performed with SWVP. We compare our results to the parser performance with CSP training (which comes with the standard implementation of the parser).</p><p>We experiment with the datasets of the CoNLL 2007 shared task on multilingual dependency pars- ing ( <ref type="bibr" target="#b17">Nilsson et al., 2007)</ref>, for a total of 9 languages. We followed the standard train/test split of these dataset. For SWVP, we randomly sampled 1000 sen- tences from each training set to serve as develop- ment sets and tuned the parameters as in the syn- thetic data experiments. CSP is trained on the train- ing set and applied to the test set without any devel- opment set involved. We report the Unlabeled At- tachment Score (UAS) for each language and model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Synthetic Data <ref type="table" target="#tab_1">Table 1</ref> presents our results. In all three setups an SWVP algorithm is superior. Av- eraged accuracy differences between the best per- forming algorithms and CSP are: 3.72 (B-WMR, (simple(++), learnable(+++))), 5.29 (B-WM, (sim- ple(++), learnable(++))) and 5.18 (A-WM, (sim- ple(+), learnable(+))). In all setups SWVP outper- forms CSP in terms of averaged performance (ex- cept from B-WMR for (simple(+), learnable(+))). Moreover, the weighted models are more stable than CSP, as indicated by the lower standard deviation of their accuracy scores. Finally, for the more sim- ple and learnable datasets the SWVP models outper- form CSP in the majority of cases (7-10/10).</p><p>We measure generalization from development to test data in two ways. First, for each SWVP algo- rithm we count the number of times its β parame- ter results in an algorithm that outperforms the CSP on the development set but not on the test set (not shown in the table). Of the 120 comparisons re- ported in the table (4 SWVP models, 3 setups, 10 comparisons per model/setup combination) this hap- pened once (A-MV, (simple(++), learnable(+++)).</p><p>Second, we count the number of times the best de- velopment set value of the β hyper-parameter is also the best value on the test set, or the test set accu- racy with the best development set β is at most 0.5% lower than that with the best test set β. The Gener-  Gener. is the number of times the best β hyper-parameter value on the development set is also the best value on the test set, or the test set accuracy with the best development set β is at most 0.5% lower than that with the best test set β.  alization column of the table shows that this has not happened in all of the 120 runs of SWVP.</p><formula xml:id="formula_28">simple(++), learnable(+++) simple(++), learnable(++) simple(+), learnable(+) Model Acc. (std) #</formula><p>Dependency Parsing Results are given in <ref type="table" target="#tab_3">Table  2</ref>. For the SWVP trained models we report three numbers: (a) B-WM is the standard setup where the β hyper parameter is tuned on the development data; (b) For Top B-WM we first selected the models with a UAS score within 0.1% of the best development data result, and of these we report the UAS of the model that performs best on the test set; and (c) Test B-WM reports results when β is tuned on the test set. This measure provides an upper bound on SWVP with our simplistic JJ (Section 5).</p><p>Our results indicate the potential of SWVP. De- spite our simple JJ set, Top B-WM and Test B-WM improve over CSP in 5/9 and 6/9 cases in first order parsing, respectively, and in 7/9 cases in second or- der parsing. In the latter case, Test B-WM improves the UAS over CSP in 0.22% on average across lan- guages. Unfortunately, SWVP still does not gener- alize well from train to test data as indicated, e.g., by the modest improvements B-WM achieves over CSP in only 5 of 9 languages in second order parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We presented the Structured Weighted Violations Perceptron (SWVP) algorithm, a generalization of the Structured Perceptron (CSP) algorithm that ex- plicitly exploits the internal structure of the pre- dicted label in its update rule. We proved the conver- gence of the algorithm for linearly separable training sets under certain conditions on its parameters, and provided generalization and mistake bounds.</p><p>In experiments we explored only very simple con- figurations of the SWVP parameters -γ and JJ. Nevertheless, several of our SWVP variants out- performed the CSP special case in synthetic data experiments. In dependency parsing experiments, SWVP demonstrated some improvements over CSP, but these do not generalize well. While we find these results somewhat encouraging, they emphasize the need to explore the much more flexible γ and JJ selection strategies allowed by SWVP (Sec. 4.2). In future work we will hence develop γ and JJ selec- tion algorithms, where selection is ideally performed jointly with inference (property 2, Sec. 4.2), to make SWVP practically useful in NLP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof Observation 1.</head><p>Proof. For every training example (x, y) ∈ D, it holds that: ∪ z∈Y(x) m J (z, y) ⊆ Y(x). As u sepa- rates the data with margin δ, it holds that:</p><formula xml:id="formula_29">u · ∆φ(x, y, m J (z, y)) ≥ δ JJx , ∀z ∈ Y(x), J ∈ JJ x . u · ∆φ(x, y, z) ≥ δ, ∀z ∈ Y(x).</formula><p>Therefore also δ JJx ≥ δ. As the last inequal- ity holds for every (x, y) ∈ D we get that δ JJ = min (x,y)∈D δ JJx ≥ δ. From the same considerations it holds that R JJ ≤ R. This is because R JJ is the radius of a sub- set of the dataset with radius R (proper subset if</p><formula xml:id="formula_30">∃(x, y) ∈ D, [L x ] /</formula><p>∈ JJ x , non-proper subset oth- erwise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Mistake Bound -Non Separable Case</head><p>Here we provide a mistake bound for the algorithm in the non-separable case. We start with the follow- ing definition and observation: (R + D u,δ ) 2 δ 2 .</p><p>#mistakes − SW V P ≤ min u:u=1,δ&gt;0</p><p>(R JJ + D u,δ ) 2 (δ + r dif f ) 2 .</p><p>As R JJ ≤ R (Observation 1) and r dif f ≥ 0, we get a tighter bound for SWVP. The proof for #mistakes-CSP is given at <ref type="bibr" target="#b2">(Collins, 2002</ref>). The proof for #mistakes-SWVP is given below.</p><p>Proof. We transform the representation φ(x, y) ∈ R d into a new representation ψ(x, y) ∈ R d+n as fol- lows: for i = 1, ..., d : ψ i (x, y) = φ i (x, y), for j = 1, ..., n : ψ d+j (x, y) = ∆ if (x, y) = (x j , y j ) and 0 otherwise, where ∆ &gt; 0 is a parameter. Given a u, δ pair define v ∈ R d+n as follows: for i = 1, ..., d : v i = u i , for j = 1, ..., n : v d+j = j ∆ . Under these definitions we have:</p><formula xml:id="formula_31">v · ψ(x i , y i ) − v · ψ(x i , z) ≥ δ, ∀i, z ∈ Y(x i ).</formula><p>For every i, z ∈ Y(x i ), J ∈ JJ x i :</p><formula xml:id="formula_32">v · ψ(x i , y i ) − v · ψ(x i , m J (z, y i )) ≥ δ + r dif f . ψ(x i , y i ) − ψ(x i , m J (z, y i )) 2 ≤ (R JJ ) 2 + ∆ 2 .</formula><p>Last, we have,</p><formula xml:id="formula_33">v 2 = u 2 + n i=1 2 i ∆ 2 = 1 + D 2 u,δ ∆ 2 .</formula><p>We get that the vector v v linearly separates the data with respect to single decision assignments with margin We minimize this w.r.t ∆, which gives: ∆ = R JJ D u,δ , and obtain the result guaranteed in the theorem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example parse trees: gold tree (y, top), predicted tree (y * , middle) with arcs differing from the gold's marked with a dashed line, and m J (y * , y) for J = [2, 5] (bottom tree).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6</head><label></label><figDesc>Denoting Dx = [Cx], Dy = [Cy], and a permuta- tion of a vector v with perm(v), the parameters of the dif- ferent setups are: (1) simple(++), learnable(+++): Cx = 5, Cy = 3, P (y |y) = perm(0.7, 0.2, 0.1), P (x|y) = perm(0.75, 0.1, 0.05, 0.05, 0.05). (2) simple(++), learn- able(++): Cx = 5, Cy = 3, P (y |y) = perm(0.5, 0.3, 0.2), P (x|y) = perm(0.6, 0.15, 0.1, 0.1, 0.05). (3) sim- ple(+), learnable(+): Cx = 20 , Cy = 7 , P (y |y) = perm(0.7, 0.2, 0.1, 0, . . . , 0)), P (x|y) = perm(0.4, 0.2, 0.1, 0.1, 0.1, 0, . . . , 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 8 .</head><label>8</label><figDesc>Given an example (x i , y i ) ∈ D, for a u, δ pair define: r i = u · φ(x i , y i ) − max z∈Y(x i ) u · φ(x i , z) i = max{0, δ − r i } r i JJ = u · φ(x i , y i )− max z∈Y(x i ),J∈JJ x i u · φ(x i , m J (z, y i )) Finally define: D u,δ = n i=1 2 i Observation 2. For all i: r i ≤ r i JJ . Observation 2 easily follows from Definition 8. Following this observation we denote: r dif f = min i {r i JJ − r i } ≥ 0 and present the next theorem: Theorem 2. For any training sequence D, for the first pass over the training set of the CSP and the SWVP algorithms respectively, it holds that: #mistakes − CSP ≤ min u:u=1,δ&gt;0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Overall Synthetic Data Results. A-and B-denote an aggressive and a balanced approaches, respectively. Acc. (std) is 

the average and the standard deviation of the accuracy across 10 test sets. # Wins is the number of test sets on which the SWVP 

algorithm outperforms CSP. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>First and second order dependency parsing UAS results for CSP trained models, as well as for models trained with SWVP 

with a balanced γ selection (B) and with a weighted margin (WM) strategy. For explanation of the B-WM, Top B-WM, and Test 

B-WM see text. For each language and parsing order we highlight the best result in bold font, but this do not include results from 

Test B-WM as it is provided only as an upper bound on the performance of SWVP. 

</table></figure>

			<note place="foot" n="2"> We use the notation [n] = {1, 2,. .. n}.</note>

			<note place="foot" n="4"> We borrow the term passive-aggressive from (Crammer et al., 2006), despite the substantial difference between the works.</note>

			<note place="foot" n="5"> For the aggressive approach the equations for schemes (1) and (2) are changed such that JJx is replaced with I(y * , y, JJx) v .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The second author was partly supported by a re-search grant from the GIF Young Scientists' <ref type="bibr">Program (No. I-2388</ref><ref type="bibr">-407.6/2015</ref>: Syntactic Parsing in Context.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ultraconservative online algorithms for multiclass problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="951" to="991" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shai ShalevShwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passiveaggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning as search optimization: Approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large margin classification using the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="296" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT 2010</title>
		<meeting>of NAACL-HLT 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="742" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured perceptron with inexact search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prc. of ACL short papers</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLPHLT</title>
		<meeting>of EMNLPHLT</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning efficiently with approximate inference via dual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Meshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The conll 2007 shared task on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL shared task session of EMNLP-CoNLL</title>
		<meeting>the CoNLL shared task session of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
	<note>sn</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An introduction to hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biing-Hwang</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASSP Magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi event extraction guided by global constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT 2012</title>
		<meeting>of NAACL-HLT 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint word segmentation and pos tagging using a single perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="888" to="896" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
