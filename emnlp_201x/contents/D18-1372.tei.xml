<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Explanation Analysis on Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseo</forename><surname>Son</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University Stony Brook</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Bayas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University Stony Brook</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Andrew</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University Stony Brook</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Explanation Analysis on Social Media</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3350" to="3359"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3350</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Understanding causal explanations-reasons given for happenings in one&apos;s life-has been found to be an important psychological factor linked to physical and mental health. Causal explanations are often studied through manual identification of phrases over limited samples of personal writing. Automatic identification of causal explanations in social media, while challenging in relying on contextual and sequential cues, offers a larger-scale alternative to expensive manual ratings and opens the door for new applications (e.g. studying prevailing beliefs about causes, such as climate change). Here, we explore automating causal explanation analysis, building on discourse parsing, and presenting two novel subtasks: causality detection (determining whether a causal explanation exists at all) and causal explanation identification (identifying the specific phrase that is the explanation). We achieve strong accuracies for both tasks but find different approaches best: an SVM for causality prediction (F 1 = 0.791) and a hierarchy of Bidirectional LSTMs for causal explanation identification (F 1 = 0.853). Finally , we explore applications of our complete pipeline (F 1 = 0.868), showing demographic differences in mentions of causal explanation and that the association between a word and sentiment can change when it is used within a causal explanation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Explanations of happenings in one's life, causal explanations, are an important topic of study in so- cial, psychological, economic, and behavioral sci- ences. For example, psychologists have analyzed people's causal explanatory style ( <ref type="bibr" target="#b18">Peterson et al., 1988)</ref> and found strong negative relationships with depression, passivity, and hostility, as well as pos- itive relationships with life satisfaction, quality of life, and length of life ( <ref type="bibr" target="#b22">Scheier et al., 1989;</ref><ref type="bibr" target="#b2">Carver and Gaines, 1987;</ref><ref type="bibr" target="#b18">Peterson et al., 1988)</ref>.</p><p>To help understand the significance of causal explanations, consider how they are applied to measuring optimism (and its converse, pes- simism) <ref type="bibr" target="#b18">(Peterson et al., 1988)</ref>. For example, in "My parser failed because I always have bugs.", the emphasized text span is considered a causal explanation which indicates pessimistic personal- ity -a negative event where the author believes the cause is pervasive. However, in "My parser failed because I barely worked on the code.", the expla- nation would be considered a signal of optimistic personality -a negative event for which the cause is believed to be short-lived.</p><p>Language-based models which can detect causal explanations from everyday social media language can be used for more than automating optimism detection. Language-based assessments would enable other large-scale downstream tasks: tracking prevailing causal beliefs (e.g., about cli- mate change or autism), better extracting process knowledge from non-fiction (e.g., gravity causes objects to move toward one another), or detecting attribution of blame or praise in product or service reviews ("I loved this restaurant because the fish was cooked to perfection").</p><p>In this paper, we introduce causal explanation analysis and its subtasks of detecting the presence of causality (causality prediction) and identifying explanatory phrases (causal explanation identifi- cation). There are many challenges to achiev- ing these task. First, the ungrammatical texts in social media incur poor syntactic parsing results which drastically affect the performance of dis- course relation parsing pipelines 1 . Many causal relations are implicit and do not contain any dis- course markers (e.g., 'because'). Further, Explicit causal relations are also more difficult in social media due to the abundance of abbreviations and variations of discourse connectives (e.g., 'cuz' and 'bcuz').</p><p>Prevailing approaches for social media analy- ses, utilizing traditional linear models or bag of words models (e.g., SVM trained with n-gram, part-of-speech (POS) tags, or lexicon-based fea- tures) alone do not seem appropriate for this task since they simply cannot segment the text into meaningful discourse units or discourse arguments 2 such as clauses or sentences rather than random consecutive token sequences or specific word to- kens. Even when the discourse units are clear, parsers may still fail to accurately identify dis- course relations since the content of social media is quite different than that of newswire which is typically used for discourse parsing.</p><p>In order to overcome these difficulties of dis- course relation parsing in social media, we sim- plify and minimize the use of syntactic parsing re- sults and capture relations between discourse ar- guments, and investigate the use of a recursive neural network model (RNN). Recent work has shown that RNNs are effective for utilizing dis- course structures for their downstream tasks <ref type="bibr" target="#b5">(Ji and Smith, 2017;</ref><ref type="bibr" target="#b0">Bhatia et al., 2015;</ref><ref type="bibr" target="#b24">Wieting et al., 2015;</ref><ref type="bibr" target="#b16">Paulus et al., 2014</ref>), but they have yet to be directly used for discourse relation predic- tion in social media. We evaluated our model by comparing it to off-the-shelf end-to-end discourse relation parsers and traditional models. We found that the SVM and random forest classifiers work better than the LSTM classifier for the causality <ref type="bibr">1</ref> Off-the-shelf Penn Discourse Treebank (PDTB) end-to- end parsers perform poorly on our Facebook causal predic- tion dataset (see <ref type="table" target="#tab_2">Table 3)</ref> 2 Each discourse relation theory uses a different term for minimal discourse text spans: 'Elementary Discourse Unit (EDU)' in RST and 'Discourse Argument' in PDTB. We will call it 'Discourse Argument' in this paper, since we adapted the PDTB text segmentation method. detection, while the LSTM classifier outperforms other models for identifying causal explanation.</p><p>The contributions of this work include: (1) the proposal of models for both (a) causality predic- tion and (b) causal explanation identification, (2) the extensive evaluation of a variety of models from social media classification models and dis- course relation parsers to RNN-based application models, demonstrating that feature-based models work best for causality prediction while RNNs are superior for the more difficult task of causal ex- planation identification, (3) performance analysis on architectural differences of the pipeline and the classifier structures, (4) exploration of the applica- tions of causal explanation to downstream tasks, and (5) release of a novel, anonymized causality Facebook dataset along with our causality predic- tion and causal explanation identification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Identifying causal explanations in documents can be viewed as discourse relation parsing. The Penn Discourse Treebank (PDTB) ( <ref type="bibr" target="#b21">Prasad et al., 2007</ref>) has a 'Cause' and 'Pragmatic Cause' dis- course type under a general 'Contingency' class and Rhetorical Structure Theory (RST) <ref type="bibr" target="#b12">(Mann and Thompson, 1987)</ref> has a 'Relations of Cause'. In most cases, the development of discourse parsers has taken place in-domain, where researchers have used the existing annotations of discourse argu- ments in newswire text (e.g. Wall Street Jour- nal) from the discourse treebank and focused on exploring different features and optimizing vari- ous types of models for predicting relations <ref type="bibr" target="#b15">Park and Cardie, 2012;</ref><ref type="bibr" target="#b26">Zhou et al., 2010)</ref>. In order to further develop automated sys- tems, researchers have proposed end-to-end dis- course relation parsers, building models which are trained and evaluated on the annotated PDTB and RST Discourse Treebank (RST DT). These cor- pora consist of documents from Wall Street Jour- nal (WSJ) which are much more well-organized and grammatical than social media texts <ref type="bibr" target="#b1">(Biran and McKeown, 2015;</ref><ref type="bibr" target="#b10">Lin et al., 2014;</ref><ref type="bibr" target="#b4">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b3">Feng and Hirst, 2014)</ref>.</p><p>Only a few works have attempted to parse dis- course relations for out-of-domain problems such as text categorizations on social media texts; Ji and Bhatia used models which are pretrained with RST DT for building discourse structures from movie reviews, and Son adapted the PDTB discourse re-lation parsing approach for capturing counterfac- tual conditionals from tweets ( <ref type="bibr" target="#b0">Bhatia et al., 2015;</ref><ref type="bibr" target="#b5">Ji and Smith, 2017;</ref>. These works had substantial differences to what propose in this paper. First, Ji and Bhatia used a pretrained model (not fully optimal for some parts of the given task) in their pipeline; Ji's model performed worse than the baseline on the categorization of legisla- tive bills, which is thought to be due to legisla- tive discourse structures differing from those of the training set (WSJ corpus). Bhatia also used a pretrained model finding that utilizing discourse relation features did not boost accuracy <ref type="bibr" target="#b0">(Bhatia et al., 2015;</ref><ref type="bibr" target="#b5">Ji and Smith, 2017)</ref>. Both Bhatia and Son used manual schemes which may limit the coverage of certain types of positive samples- Bhatia used a hand-crafted schema for weighting discourse structures for the neural network model and Son manually developed seven surface forms of counterfactual thinking for the rule-based sys- tem ( <ref type="bibr" target="#b0">Bhatia et al., 2015;</ref>). We use social-media-specific features from pretrained models which are directly trained on tweets and we avoid any hand-crafted rules except for those included in the existing discourse argument ex- traction techniques.</p><p>The automated systems for discourse relation parsing involve multiple subtasks from segment- ing the whole text into discourse arguments to classifying discourse relations between the argu- ments. Past research has found that different types of models and features yield varying performance for each subtask. Some have optimized models for discourse relation classification (i.e. given a doc- ument indicating if the relation existing) without discourse argument parsing using models such as Naive-Bayes or SVMs, achieve relatively stronger accuracies but a simpler task than that associated with discourse arguments <ref type="bibr" target="#b15">(Park and Cardie, 2012;</ref><ref type="bibr" target="#b26">Zhou et al., 2010;</ref>). Researchers who, instead, tried to build the end-to-end parsing pipelines considered a wider range of approaches including sequence models and RNNs <ref type="bibr" target="#b1">(Biran and McKeown, 2015;</ref><ref type="bibr" target="#b3">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b4">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b9">Li et al., 2014</ref>). Particularly, when they tried to utilize the discourse struc- tures for out-domain applications, they used RNN- based models and found that those models are advantageous for their downstream tasks ( <ref type="bibr" target="#b0">Bhatia et al., 2015;</ref><ref type="bibr" target="#b5">Ji and Smith, 2017</ref>).</p><p>In our case, for identifying causal explana- tions from social media using discourse structure, we build an RNN-based model for its structural effectiveness in this task (see details in section 3.2). However, we also note that simpler models such as SVMs and logistic regression obtained the state-of-the-art performances for text categoriza- tion tasks in social media ( <ref type="bibr" target="#b11">Lynn et al., 2017;</ref><ref type="bibr" target="#b13">Mohammad et al., 2013</ref>), so we build relatively simple models with different properties for each stage of the full pipeline of our parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We build our model based on PDTB-style dis- course relation parsing since PDTB has a rela- tively simpler text segmentation method; 3 for ex- plicit discourse relations, it finds the presence of discourse connectives within a document and ex- tracts discourse arguments which parametrize the connective while for implicit relations, it consid- ers all adjacent sentences as candidate discourse arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We created our own causal explanation dataset by collecting 3,268 random Facebook status update messages. Three well-trained annotators manually labeled whether or not each message contains the causal explanation and obtained 1,600 causality messages with substantial agreement (κ = 0.61). We used the majority vote for our gold standard. Then, on each causality message, annotators iden- tified which text spans are causal explanations.</p><p>For each task, we used 80% of the dataset for training our model and 10% for tuning the hy- perparameters of our models. Finally, we evalu- ated all of our models on the remaining 10% ( <ref type="table" target="#tab_0">Ta- ble 1 and Table 2</ref>). For causal explanation detec- tion task, we extracted discourse arguments using our parser and selected discourse arguments which most cover the annotated causal explanation text span as our gold standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>We build two types of models. First, we de- velop feature-based models which utilize features of the successful models in social media analysis and causal relation discourse parsing. Then, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Causality Non- <ref type="table" target="#tab_0">Causal Total  Training  1,284  1,330  2,614  Validation 150  177  327  Test  164  163  327  Total  1,598  1,670</ref> 3,268  build a recursive neural network model which uses distributed representation of discourse arguments as this approach can even capture latent proper- ties of causal relations which may exist between distant discourse arguments. We specifically se- lected bidirectional LSTM since the model with the discourse distributional structure built in this form outperformed the traditional models in simi- lar NLP downstream tasks (Ji and Smith, 2017).</p><p>Discourse Argument Extraction As the first step of our pipeline, we use Tweebo parser ( <ref type="bibr" target="#b7">Kong et al., 2014</ref>) to extract syntactic features from mes- sages. Then, we demarcate sentences using punc- tuation (',') tag and periods. Among those sen- tences, we find discourse connectives defined in PDTB annotation along with a Tweet POS tag for conjunction words which can also be a discourse marker. In order to decide whether these connec- tives are really discourse connectives (e.g., I went home, but he stayed) as opposed to simple con- nections of two words (I like apple and banana) we see if verb phrases 4 exist before and after the connective by using dependency parsing results. Although discourse connective disambiguation is a complicated task which can be much improved by syntactic features , we try to minimize effects of syntactic parsing and simplify it since it is highly error-prone in social media. Finally, according to visual inspection, emojis ('E' tag) are crucial for discourse relation in social media so we take them as separate dis- course arguments (e.g.,in "My test result... :(" the sad feeling is caused by the test result, but it can- not be captured by plain word tokens). </p><formula xml:id="formula_0">(DA = [ − → h ; ← − h ])</formula><p>. Then, we feed the sequence of the vector representation of discourse arguments to the Discourse-argument-level LSTM (DA-level LSTM) to make a final prediction with log soft- max function. With this structure, the model can learn the representation of interaction of tokens inside each discourse argument, then capture dis- course relations across all of the discourse argu- Architectural Variants We also explore subsets of the full RNN architecture, specifically with one of the two LSTM layers removed. In the first model variant, we directly input all word embed- dings of a whole message to a BiLSTM layer and make prediction (Word LSTM) without the help of the distributional vector representations of dis- course arguments. In the second model variant, we take the average of all word embeddings of each discourse argument (DA k = 1</p><formula xml:id="formula_1">N k N k i=1 W i ),</formula><p>and use them as inputs to a BiLSTM layer (DA AVG LSTM) as the average vector of embeddings were quite effective for representing the whole se- quence ( <ref type="bibr" target="#b5">Ji and Smith, 2017;</ref><ref type="bibr" target="#b24">Wieting et al., 2015)</ref>. As with the full architectures, for CP both of these variants ends with a many-to-one classification per message, while the CEI model ends with a se- quence of classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment</head><p>Feature Based Model We explored three types of models (RBF SVM, Linear SVM, and Ran- dom Forest Classifier) which have previously been shown empirically useful for the language analy- sis in social media. We filtered out low frequency Word Pairs features as they tend to be noisy and sparse ). Then, we conducted univariate feature selection to restrict all remain- ing features to those showing at least a small rela- tionship with the outcome. Specifically, we keep all features passing a family-wise error rate of α = 60 with the given outcome. After comparing the performance of the optimized version of each model, we also conducted a feature ablation test on the best model in order to see how much each feature contributes to the causality prediction.</p><p>Neural Network Model We used bidirectional LSTMs for causality classification and causal ex- planation identification since the discourse argu- ments for causal explanation can show up either before and after the effected events or results and we want our model to be optimized for both cases. However, there is a risk of overfitting due to the dataset which is relatively small for the high com- plexity of the model, so we added a dropout layer (p=0.3) between the Word-level LSTM and the DA-level LSTM.</p><p>For tuning our model, we explore the dimen- sionality of word vector and LSTM hidden state vectors of discourse arguments of 25, 50, 100, and 200 as pretrained GLOVE vectors were trained in this setting. For optimization, we used Stochastic Gradient Descent (SGD) and Adam ( <ref type="bibr" target="#b6">Kingma and Ba, 2014</ref>) with learning rates 0.01 and 0.001.</p><p>We ignore missing word embeddings because our dataset is quite small for retraining new word embeddings. However, if embeddings are ex- tracted as separate discourse arguments, we used the average of all vectors of all discourse argu- ments in that message. Average embeddings have performed well for representing text sequences in other tasks ( <ref type="bibr" target="#b24">Wieting et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1 (Biran and McKeown, 2015) 0.434 (Lin et al., 2014) 0.638 Linear SVM</head><p>0.791 RBF SVM 0.777 Random Forest 0.771 LSTM 0.758  Model Evaluation We first use state-of-the-art PDTB taggers for our baseline ( <ref type="bibr" target="#b10">Lin et al., 2014;</ref><ref type="bibr" target="#b1">Biran and McKeown, 2015)</ref> for the evaluation of the causality prediction of our models <ref type="bibr" target="#b1">((Biran and McKeown, 2015</ref>) requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message). Then, we compare how models work for each task and disassembled them to inspect how each part of the models can affect their final prediction perfor- mances. We conducted McNemar's test to deter- mine whether the performance differences are sta- tistically significant at p &lt; .05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We investigated various models for both causal- ity detection and explanation identification. Based on their performances on the task, we analyzed the relationships between the types of models and the tasks, and scrutinized further for the best per- forming models. For performance analysis, we re- ported weighted F1 of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Causality Prediction</head><p>In order to classify whether a message contains causal relation, we compared off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers. The off-the-shelf parsers achieved the lowest accuracies ((Biran and McK-  eown, 2015) and ( <ref type="bibr" target="#b10">Lin et al., 2014</ref>) in <ref type="table" target="#tab_2">Table 3</ref>). This result can be expected since 1) these mod- els were trained with news articles and 2) they are trained for all possible discourse relations in ad- dition to causal relations (e.g., contrast, condition, etc). Among our suggested models, SVM and ran- dom forest classifier performed better than LSTM and, in the general trend, the more complex the models were, the worse they performed. This sug- gests that the models with more direct and simpler learning methods with features might classify the causality messages better than the ones more op- timized for capturing distributional information or non-linear relationships of features. <ref type="table" target="#tab_3">Table 4</ref> shows the results of a feature ablation test to see how each feature contributes to causality classification per- formance of the linear SVM classifier. POS tags caused the largest drop in F1. We suspect POS tags played a unique role because discourse con- nectives can have various surface forms (e.g., be- cause, cuz, bcuz, etc) but still the same POS tag 'P'. Also POS tags can capture the occurrences of modal verbs, a feature previously found to be very useful for detecting similar discourse rela- tions ( ). N-gram features caused 0.022 F1 drop while sentiment tags did not af- fect the model when removed. Unlike the previ- ous work where First-Last, First3 and Word pairs tended to gain a large F1 increase for multiclass discourse relation prediction, in our case, they did not affect the prediction performance compared to other feature types such as POS tags or N-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causality Classifier Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Causal Explanation Identification</head><p>In this task, the model identifies causal explana- tions given the discourse arguments of the causal- ity message. We explored over the same mod- els as those we used for causality (sans the out- put layer), and found the almost opposite trend of performances (see <ref type="table" target="#tab_5">Table 5</ref>). The Linear SVM ob-</p><formula xml:id="formula_2">Model CP (F1) CEI (F1) Full LSTM 0.758 0.853 DA AVG LSTM 0.685</formula><p>0.818 Word LSTM 0.694 0.792 <ref type="table">Table 6</ref>: The effect of Word-level LSTM (Word LSTM) and discourse argument LSTM (DA AVG LSTM) for causality prediction (CP) and causal expla- nation identification (CEI). Note that, as described in methods, there are architectural differences for CP and CEI models with the same names, most notably that the output layer is always a single classification for CP and a sequence of classifications for CEI. tained lowest F1 while the LSTM model made the best identification performance. As opposed to the simple binary classification of the causality mes- sages, in order to detect causal explanation, it is more beneficial to consider the relation across dis- course arguments of the whole message and im- plicit distributional representation due to the im- plicit causal relations between two distant argu- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Architectural Variants</head><p>For causality prediction, we experimented with only word tokens in the whole message without help of Word-level LSTM layer (Word LSTM), and F1 dropped by 0.064 (CP in <ref type="table">Table 6</ref>). Also, when we used the average of the sequence of word embeddings of each discourse argument as an input to the DA-level LSTM and it caused F1 drop of 0.073. This suggests that the informa- tion gained from both the interaction of words in and in between discourse arguments help when the model utilizes the distributional representation of the texts.</p><p>For causal explanation identification, in order to test how the LSTM classifier works without its capability of capturing the relations between dis- course arguments, we removed DA-level LSTM layer and ran the LSTM directly on the word em- bedding sequence for each discourse argument for classifying whether the argument is causal expla- nation, and the model had 0.061 F1 drop (Word LSTM in CEI in <ref type="table">Table 6</ref>). Also, when we ran DA- level LSTM on the average vectors of the word se- quences of each discourse argument of messages, F1 decreased to 0.818. This follows the similar pattern observed from other types of models per- formances (i.e., SVMs and Random Forest classi- fiers) that the models with higher complexity for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Prec Rec F1 CP + CEI causal 0.864 0.877 0.868 CP + CEI all 0.842 0.864 0.848 CEI causal Only 0.847 0.788 0.810 CEI all Only 0.836 0.848 0.842 capturing the interaction of discourse arguments tend to identify causal explanation with the higher accuracies.</p><p>For CEI task, we found that when the model ran on the sequence representation of discourse ar- gument (DA AVG LSTM), its performance was higher than the plain sequence of word embed- dings (Word LSTM). Finally, in both subtasks, when the models ran on both Word-level and DA- Level (Full LSTM), they obtained the highest per- formance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Complete Pipeline</head><p>Evaluations thus far zeroed-in on each subtask of causal explanation analysis (i.e. CEI only focused on data already identified to contain causal expla- nations). Here, we seek to evaluate the complete pipeline of CP and CEI, starting from all of test data (those or without causality) and evaluating the final accuracy of CEI predictions. This is intended to evaluate CEI performance under an applied set- ting where one does not already know whether a document has a causal explanation.</p><p>There are several approaches we could take to perform CEI starting from unannotated data. We could simply run CEI prediction by itself (CEI Only) or the pipeline of CP first and then only run CEI on documents predicted as causal (CP + CEI). Further, the CEI model could be trained only on those documents annotated causal (as was done in the previous experiments) or on all train- ing documents including many that are not causal. <ref type="table" target="#tab_6">Table 7</ref> show results varying the pipeline and how CEI was trained. Though all setups per- formed decent (F 1 &gt; 0.81) we see that the pipelined approach, first predicting causality (with the linear SVM) and then predicting causal expla-nations only for those with marked causal (CP + CEI causal ) yielded the strongest results. This also utilized the CEI model only trained on those anno- tated causal. Besides performance, an added ben- efit from this two step approach is that the CP step is less computational intensive of the CEI step and approximately 2/3 of documents will never need the CEI step applied.</p><p>Limitations. We had an inevitable limitation on the size of our dataset, since there is no other causality dataset over social media and the anno- tation required an intensive iterative process. This might have limited performances of more com- plex models, but considering the processing time and the computation load, the combination of the linear model and the RNN-based model of our pipeline obtained both the high performance and efficiency for the practical applications to down- stream tasks. In other words, it's possible the lin- ear model will not perform as well if the training size is increased substantially. However, a linear model could still be used to do a first-pass, com- putationally efficient labeling, in order to short- list social media posts for further labeling from an LSTM or more complex model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Exploration</head><p>Here, we explore the use of causal explanation analysis for downstream tasks. First we look at the relationship between use of causal explanation and one's demographics: age and gender. Then, we consider their use in sentiment analysis for extract- ing the causes of polarity ratings. Research involv- ing human subjects was approved by the Univer- sity of Pennsylvania Institutional Review Board.</p><p>Demographic differences. We first explored variance in number of causality posts by de- mographics. To do this, we used self-authored posts from a random 300 consenting-users of the MyPersonality dataset ( <ref type="bibr" target="#b8">Kosinski et al., 2013</ref>). For each user we calculate a cp ratio, defined as the number of causality predicted posts divided by their total number of posts, indicating the percent- age of their posts which include a causal explana- tion. We then correlated this ratio with real-valued age using Pearson correlation and looked the dif- ferences by dichotomous gender using Cohen's d (the difference in standardized means; only bi- nary gender was available). We found significant (p &lt; .05) moderate-sized associations for both,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE</head><p>Non-CE <ref type="table" target="#tab_0">Top Ngrams Top Ngrams  1  worst  not  2  was  no  3  not  "  4  the worst  asked  5  horrible  she  6  rude  told  7  bad  said  8  overpriced  minutes  9  over  ?  10</ref> slow me <ref type="table">Table 8</ref>: Top words most associated with negative re- views from within causal explanations (CE) and out- side of causal explanation (Non-CE).</p><p>indicating both older individuals and females were likely to use more causal explanations.</p><p>Causality in Sentiment Analysis We explored the application of causality explanation identifi- cation for sentiment analysis using the Yelp po- larity dataset ( <ref type="bibr" target="#b25">Zhang et al., 2015</ref>). We randomly selected 10,000 of both positive and negative re- views and ran our complete pipeline on them to ex- tract the causal explanations from the reviews. We then analyzed the ngrams from (a) causal expla- nation and (b) all other discourse arguments test- ing for associations with polarity. We used the a Bayesian interpretation of the log odds ratio us- ing an informative dirichlet prior defined by <ref type="bibr" target="#b14">Monroe et al. (2008)</ref>. We found difference in the top ngrams depending on whether the argument the ngram originated from was a causal explanation or not (see <ref type="table">Table 8</ref>). Top ngrams for causal expla- nations included more content words (e.g. 'rude', 'overpriced', 'slow') suggesting analyzing causal explanations within reviews can better target the reasons for the negative review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We developed a pipeline for causal explanation analysis over social media text, including both causality prediction and causal explanation iden- tification. We examined a variety of model types and RNN architectures for each part of the pipeline, finding an SVM best for causality pre- diction and a hierarchy of BiLSTMs for causal ex- planation identification, suggesting the later task relies more heavily on sequential information. In fact, we found replacing either layer of the hier-archical LSTM architecture (the word-level or the DA-level) with a an equivalent "bag of features" approach resulted in reduced accuracy. Results of our whole pipeline of causal explanation analysis were found quite strong, achieving an F 1 = 0.868 at identifying discourse arguments that are causal explanations. Finally, we demonstrated use of our models in applications, finding associations between demo- graphics and rate of mentioning causal explana- tions, as well as showing differences in the top words predictive of negative ratings in Yelp re- views. Utilization of discourse structure in social media analysis has been a largely untapped area of exploration, perhaps due to its perceived diffi- culty. We hope the strong results of causal expla- nation identification here leads to the integration of more syntax and deeper semantics into social media analyses and ultimately enables new appli- cations beyond the current state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A casual relation characterizes the connection between two discourse arguments, one of which is the causal explanation.</figDesc><graphic url="image-1.png" coords="1,308.41,222.54,215.99,108.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature</head><label></label><figDesc>Based Models We trained a linear SVM, an rbf SVM, and a random forest with N- gram, charater N-gram, and tweet POS tags, senti- ment tags, average word lengths and word counts from each message as they have a pivotal role in the models for many NLP downstream tasks in so- cial media (Mohammad et al., 2013; Lynn et al., 2017). In addition to these features, we also ex- tracted First-Last, First3 features and Word Pairs from every adjacent pair of discourse arguments since these features were most helpful for causal relation prediction (Pitler et al., 2009). First-Last, First3 features are first and last word and first three words of two discourse arguments of the relation, and Word Pairs are the cross product of words of those discourse arguments. These two features en- able our model to capture interaction between two discourse arguments. (Pitler et al., 2009) reported that these two features along with verbs, modal- ity, context, and polarity (which can be captured by N-grams, sentiment tags and POS tags in our previous features) obtained the best performance for predicting Contingency class to which causal- ity belongs. Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter 5 for each token of ex- tracted discourse arguments from messages. For the distributional representation of discourse ar- guments, we run a Word-level LSTM on the words' embeddings within each discourse argu- ment and concatenate last hidden state vectors of forward LSTM ( − → h ) and backward LSTM ( ← − h ) which is suggested by (Ji and Smith, 2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: LSTM classifier for causality detection and explanation identification</figDesc><graphic url="image-2.png" coords="5,125.97,62.81,345.60,189.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Number of messages containing causality or 
not in our dataset. 

Causality messages CE DA Total DA 
Training 
1,278 
5,606 
Validation 
160 
652 
Test 
160 
757 
Total 
1,598 
7,015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The number of discourse arguments in causal-
ity messages. Across 1,598 total causality messages, 
we found 7,015 discourse arguments (Total DA) and 
the one which covers annotated causal explanation are 
used as causal explanation discourse arguments (CE 
DA) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Causality prediction performance across dif-
ferent predictive models. Bold indicates significant im-
provement over the LSTM 

Model 
F1 
All 
0.791 
-First-Last, First3 
0.788 
-Word Pairs 
0.787 
-POS tags 
0.734 
-(Char + Word) N-grams 0.769 
-Sentiment tags 
0.791 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Feature ablation test of Linear SVM for causality prediction</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Causal explanation identification perfor-
mance. Bold indicates significant imrpovement over 
next best model (p &lt; .05) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>The effect of Linear SVM Cauality model 
(CP) within our pipeline. CEI all : LSTM CEI models 
trained on all messages; CEI causal : LSTM CEI mod-
els trained only on causality messages (CEI causal ); CP 
+ CEI all|causal : the combination of Linear SVM and 
each LSTM model. Bold: significant (p &lt; .05) in-
crease in F1 over the next best model, suggesting the 
two-step approach worked best. 

</table></figure>

			<note place="foot" n="3"> RST parsing builds fully hierarchical discourse tree structures out of the whole span of target text which highly depends on syntactic parsing and exact matching of elementary discourse units which are extremely hard to obtain from social media texts</note>

			<note place="foot" n="4"> minimal discourse unit is verb phrases with very few exceptions (Prasad et al., 2007)</note>

			<note place="foot" n="5"> http://nlp.stanford.edu/data/glove. twitter.27B.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported, in part, by a grant from the Templeton Religion Trust (ID #TRT0048). The funders had no role in study design, data col-lection and analysis, decision to publish, or prepa-ration of the manuscript. We also thank Laura Smith, Yiyi Chen, Greta Jawel and Vanessa Her-nandez for their work in identifying causal expla-nations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Better document-level sentiment analysis from rst discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01599</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pdtb discourse parsing as a tagging task: The two taggers approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimism, pessimism, and postpartum depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Gollin</forename><surname>Carver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive therapy and Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="462" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01829</idno>
		<title level="m">Neural discourse structure for text categorization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A dependency parser for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archna</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Private traits and attributes are predictable from digital records of human behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="5802" to="5805" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A pdtb-styled end-to-end discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="151" to="184" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human centered nlp with user-factor adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseo</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Andrew</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1146" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rhetorical structure theory: A theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California, Information Sciences Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.6242</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fightin&apos;words: Lexical feature selection and evaluation for identifying the content of political conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burt L Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin M</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="372" to="403" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving implicit discourse relation recognition through feature set optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global belief recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2888" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pessimistic explanatory style is a risk factor for physical illness: a thirtyfive-year longitudinal study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Martin E Seligman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaillant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using syntax to disambiguate explicit discourse connectives in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP 2009 Conference Short Papers</title>
		<meeting>the ACL-IJCNLP 2009 Conference Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0 annotation manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dispositional optimism and recovery from coronary artery bypass surgery: the beneficial effects on physical and psychological well-being</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">A</forename><surname>Michael F Scheier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><forename type="middle">F</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">J</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Magovern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles S</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1024</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing counterfactual thinking in social media texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngseo</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anneke</forename><surname>Buffone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Raso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allegra</forename><surname>Larche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Janocko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zembroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="654" to="658" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08198</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting discourse connectives for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1507" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
