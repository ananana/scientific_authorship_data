<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">#TAGSPACE: Semantic Embeddings from Hashtags</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
							<email>spchopra@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">#TAGSPACE: Semantic Embeddings from Hashtags</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1822" to="1827"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe a convolutional neural network that learns feature representations for short textual posts using hashtags as a supervised signal. The proposed approach is trained on up to 5.5 billion words predicting 100,000 possible hashtags. As well as strong performance on the hashtag prediction task itself, we show that its learned representation of text (ignoring the hash-tag labels) is useful for other tasks as well. To that end, we present results on a document recommendation task, where it also outperforms a number of baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hashtags (single tokens often composed of nat- ural language n-grams or abbreviations, prefixed with the character '#') are ubiquitous on social networking services, particularly in short textual documents (a.k.a. posts). Authors use hashtags to diverse ends, many of which can be seen as labels for classical NLP tasks: disambiguation (chips #futurism vs. chips #junkfood); identi- fication of named entities (#sf49ers); sentiment (#dislike); and topic annotation (#yoga). Hashtag prediction is the task of mapping text to its accompanying hashtags. In this work we pro- pose a novel model for hashtag prediction, and show that this task is also a useful surrogate for learning good representations of text.</p><p>Latent representations, or embeddings, are vec- torial representations of words or documents, tra- ditionally learned in an unsupervised manner over large corpora. For example LSA <ref type="bibr" target="#b4">(Deerwester et al., 1990</ref>) and its variants, and more recent neural- network inspired methods like those of <ref type="bibr" target="#b0">Bengio et al. (2006)</ref>,  and word2vec ( <ref type="bibr" target="#b9">Mikolov et al., 2013</ref>) learn word embeddings. In the word embedding paradigm, each word is rep- resented as a vector in R n , where n is a hyper- parameter that controls capacity. The embeddings of words comprising a text are combined using a model-dependent, possibly learned function, pro- ducing a point in the same embedding space. A similarity measure (for example, inner product) gauges the pairwise relevance of points in the em- bedding space.</p><p>Unsupervised word embedding methods train with a reconstruction objective in which the em- beddings are used to predict the original text. For example, word2vec tries to predict all the words in the document, given the embeddings of sur- rounding words. We argue that hashtag predic- tion provides a more direct form of supervision: the tags are a labeling by the author of the salient aspects of the text. Hence, predicting them may provide stronger semantic guidance than unsuper- vised learning alone. The abundance of hashtags in real posts provides a huge labeled dataset for learning potentially sophisticated models.</p><p>In this work we develop a convolutional net- work for large scale ranking tasks, and apply it to hashtag prediction. Our model represents both words and the entire textual post as embeddings as intermediate steps. We show that our method out- performs existing unsupervised (word2vec) and supervised (WSABIE (Weston et al., 2011)) em- bedding methods, and other baselines, at the hash- tag prediction task.</p><p>We then probe our model's generality, by trans- fering its learned representations to the task of per- sonalized document recommendation: for each of M users, given N previous positive interactions with documents (likes, clicks, etc.), predict the N + 1'th document the user will positively inter- act with. To perform well on this task, the rep- resentation should capture the user's interest in textual content. We find representations trained on hashtag prediction outperform representations from unsupervised learning, and that our convolu-  <ref type="figure">Figure 1</ref>: #TAGSPACE convolutional network f (w, t) for scoring a (document, hashtag) pair.</p><formula xml:id="formula_0">t (l + K 1) ⇥ d l ⇥ H l ⇥ H H H d d</formula><p>tional architecture performs better than WSABIE trained on the same hashtag task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Some previous work <ref type="bibr" target="#b3">(Davidov et al., 2010;</ref><ref type="bibr" target="#b6">Godin et al., 2013;</ref><ref type="bibr" target="#b11">She and Chen, 2014</ref>) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of <ref type="bibr" target="#b5">Ding et al. (2012)</ref>, which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec ( <ref type="bibr" target="#b9">Mikolov et al., 2013)</ref> as a representative scalable model for un- supervised embeddings. WSABIE ) is a supervised embedding approach that has shown promise in NLP tasks ( <ref type="bibr" target="#b16">Weston et al., 2013;</ref><ref type="bibr" target="#b8">Hermann et al., 2014</ref>). WSABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach.</p><p>Convolutional neural networks (CNNs), in which shared weights are applied across the in- put, are popular in the vision domain and have re- cently been applied to semantic role labeling (Col- lobert et al., 2011) and parsing <ref type="bibr" target="#b2">(Collobert, 2011)</ref>. Neural networks in general have also been applied to part-of-speech tagging, chunking, named en- tity recognition <ref type="bibr" target="#b14">Turian et al., 2010)</ref>, and sentiment detection ( <ref type="bibr" target="#b13">Socher et al., 2013)</ref>. All these tasks involve predicting a limited (2-30) number of labels. In this work, we make use of CNNs, but apply them to the task of rank- ing a very large set of tags. We thus propose a model and training scheme that can scale to this class of problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional Embedding Model</head><p>Our model #TAGSPACE (see <ref type="figure">Figure 1</ref>), like other word embedding models, starts by assigning a d- dimensional vector to each of the l words of an input document w 1 , . . . , w l , resulting in a matrix of size l × d. This is achieved using a matrix of N × d parameters, termed the lookup-table layer ), where N is the vocabulary size. In this work N is 10 6 , and each row of the matrix represents one of the million most frequent words in the training corpus.</p><p>A convolution layer is then applied to the l × d input matrix, which considers all successive win- dows of text of size K, sliding over the docu- ment from position 1 to l. This requires a fur- ther Kd × H weights and H biases to be learned. To account for words at the two boundaries of the document we also apply a special padding vector at both ends. In our experiments K was set to 5 and H was set to 1000. After the convolutional step, a tanh nonlinearity followed by a max op- eration over the l × H features extracts a fixed- size (H-dimensional) global feature vector, which is independent of document size. Finally, another tanh non-linearity followed by a fully connected linear layer of size H ×d is applied to represent the entire document in the original embedding space of d-dimensions.</p><p>Hashtags are also represented using d- dimensional embeddings using a lookup-table. We represent the top 100,000 most frequent tags. For a given document w we then rank any given hashtag t using the scoring function:</p><formula xml:id="formula_1">f (w, t) = e conv (w) · e lt (t)</formula><p>where e conv (w) is the embedding of the document by the CNN just described and e lt (t) is the em- bedding of a candidate tag t. We can thus rank all candidate hashtags via their scores f (w, t), largest first.</p><p>To train the above scoring function, and hence the parameters of the model we minimize a rank- ing loss similar to the one used in WSABIE as a training objective: for each training example, we sample a positive tag, compute f (w, t + ), then sample random tags ¯ t up to 1000 times until f (w, ¯ t) &gt; m + f (w, t + ), where m is the mar- gin. A gradient step is then made to optimize the pairwise hinge loss:</p><formula xml:id="formula_2">L = max{0, m − f (w, t + ) + f (w, ¯ t)}.</formula><p>We use m = 0.1 in our experiments. This loss function is referred to as the WARP loss in  and is used to approximately optimizing the top of the ranked list, useful for metrics like precision and recall@k. In particu- lar, the search for a negative candidate tag means that more energy is spent on improving the rank- ing performance of positive labels already near the top of the ranked list, compared to only randomly sampling of negatives, which would optimize the average rank instead. Minimizing our loss is achieved with parallel stochastic gradient descent using the hogwild al- gorithm (Niu et al., 2011). The lookup-table lay- ers are initialized with the embeddings learned by WSABIE to expedite convergence. This kind of 'pre-training' is a standard trick in the neural net- work literature, see e.g. <ref type="bibr" target="#b12">(Socher et al., 2011</ref>).</p><p>The ranking loss makes our model scalable to 100,000 (or more) hashtags. At each training ex- ample only a subset of tags have to be computed, so it is far more efficient than a standard classifi- cation loss that considers them all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>Our experiments use two large corpora of posts containing hashtags from a popular social net- work. <ref type="bibr">1</ref> The first corpus, which we call people, consists of 201 million posts from individual user accounts, comprising 5.5 billion words.</p><p>The second corpus, which we call pages, con- sists of 35.3 million page posts, comprising 1.6 1 Both corpora were de-identified during collection. billion words. These posts' authorial voice is a public entity, such as a business, celebrity, brand, or product. The posts in the pages dataset are pre- sumably intended for a wider, more general audi- ence than the posts in the people dataset. Both are summarized in <ref type="table">Table 1</ref>.</p><p>Both corpora comprise posts between February 1st and February 17th, 2014. Since we are not at- tempting a multi-language model, we use a simple trigram-based language prediction model to con- sider only posts whose most likely language is En- glish.</p><p>The two datasets use hashtags very differently. The pages dataset has a fatter head, with popular tags covering more examples. The people dataset uses obscure tags more heavily. For example, the top 100 tags account for 33.9% of page tags, but only 13.1% of people tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hashtag prediction</head><p>The hashtag prediction task attempts to rank a post's ground-truth hashtags higher than hash- tags it does not contain. We trained models on both the people and page datasets, and collected precision at 1, recall at 10, and mean rank for 50,000 randomly selected posts withheld from training. A further 50,000 withheld posts are used for selecting hyperparameters. We compare #TAGSPACE with the following models:</p><p>Frequency This simple baseline ignores input text, always ranking hashtags by their frequency in the training data.</p><p>#words This baseline assigns each tag a static score based on its frequency plus a large bonus if it corresponds to a word in the input text. For ex- ample, on input "crazy commute this am", #words ranks #crazy, #commute, #this and #am highest, in frequency order.</p><p>Word2vec We trained the unsupervised model of <ref type="bibr" target="#b9">Mikolov et al. (2013)</ref> on both datasets, treat- ing hashtags the same as all other words. To ap- ply these word embeddings to ranking, we first sum the embeddings of each word in the text (as word2vec does), and then rank hashtags by simi- larity of their embedding to that of the text. <ref type="bibr">2</ref> WSABIE ) is a supervised bilinear embedding model. Each word and tag has an embedding. The words in a text are averaged to produce an embedding of the text, and hash- tags are ranked by similarity to the text embed- ding. That is, the model is of the form:</p><formula xml:id="formula_3">f (w, t) = w U V t</formula><p>where the post w is represented as a bag of words (a sparse vector in R N ), the tag is a one-hot-vector in R N , and U and V are k × N embedding matri- ces. The WARP loss, as described in section 3, is used for training.</p><p>Performance of all these models at hashtag pre- diction is summarized in <ref type="table" target="#tab_4">Tables 3 and 4</ref>. We find similar results for both datasets. The frequency and #words baselines perform poorly across the 2 Note that the unsupervised Word2vec embeddings could be used as input to a supervised classifier, which we did not do. For a supervised embedding baseline we instead use WS- ABIE. WSABIE trains word embeddings U and hashtag em- beddings V in a supervised fashion, whereas Word2vec trains them both unsupervised. Adding supervision to Word2vec would effectively do something in-between: U would still be unsupervised, but V would then be supervised. board, establishing the need to learn from text. Among the learning models, the unsupervised word2vec performs the worst. We believe this is due to it being unsupervised -adding super- vision better optimizes the metric we evaluate. #TAGSPACE outperforms WSABIE at all dimen- sionalities. Due to the relatively large test sets, the results are statistically significant; for example, comparing #TAGSPACE (64 dim) beats Wsabie (64 dim) for the page dataset 56% of the time, and draws 23% of the time in terms of the rank met- ric, and is statistically significant with a Wilcoxon signed-rank test.</p><p>Some example predictions for #TAGSPACE are given for some constructed examples in <ref type="table" target="#tab_2">Table 2</ref>. We also show nearest word embeddings to the posts. Training data was collected at the time of the pax winter storm, explaining predictions for the first post, and Kevin Spacey appears in the show "House of Cards,". In all cases the hash- tags reveal labels that capture the semantics of the posts, not just syntactic similarity of individual words.</p><p>Comparison to Production System We also compare to a proprietary system in production in Facebook for hashtag prediction. It trains a lo- gistic regression model for every hashtag, using a bag of unigrams, bigrams, and trigrams as the    input features. Unlike the other models we con- sider here, this baseline has been trained using a set of approximately 10 million posts. Engineer- ing constraints prevent measuring mean rank per- formance. We present it here as a serious effort at solving the same problem from outside the em- bedding paradigm. On the people dataset this sys- tem achieves 3.47% P@1 and 5.33% R@10. On the pages dataset it obtains 5.97% P@1 and 6.30% R@10. It is thus outperformed by our method. However, we note the differences in experimen- tal setting mean this comparison is perhaps not completely fair (different training sets). We expect performance of linear models such as this to be similar to WSABIE as that has been in the case in other datasets ( <ref type="bibr" target="#b7">Gupta et al., 2014</ref>), but at the cost of more memory usage. Note that models like lo- gistic regression and SVM do not scale well if you have millions of hashtags, which we could handle in our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Personalized document recommendation</head><p>To investigate the generality of these learned rep- resentations, we apply them to the task of recom- mending documents to users based on the user's interaction history. The data for this task comprise anonymized day-long interaction histories for a tiny subset of people on a popular social network-  ing service. For each of the 34 thousand people considered, we collected the text of between 5 and 167 posts that she has expressed previous positive interactions with (likes, clicks, etc.). Given the person's trailing n − 1 posts, we use our models to predict the n'th post by ranking it against 10,000 other unrelated posts, and measuring precison and recall. The score of the n th post is obtained by taking the max similarity over the n − 1 posts. We use cosine similarity between post embeddings in- stead of the inner product that was used for hash- tag training so that the scores are not unduly influ- enced by document length. All learned hashtag models were trained on the people dataset. We also consider a TF-IDF weighted bag-of-words baseline (BoW). The results are given in <ref type="table" target="#tab_7">Table 5</ref>.</p><p>Hashtag-based embeddings outperform BoW and unsupervised embeddings across the board, and #TAGSPACE outperforms WSABIE. The best results come from summing the bag-of-words scores with those of #TAGSPACE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>#TAGSPACE is a convolutional neural network that learns to rank hashtags with a minimum of task-specific assumptions and engineering. It per- forms well, beating several baselines and an in- dustrial system engineered for hashtag prediction. The semantics of hashtags cause #TAGSPACE to learn a representation that captures many salient aspects of text. This representation is general enough to port to the task of personalized docu- ment recommendation, where it outperforms other well-known representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>#TAGSPACE (256 dim) predictions for some example posts.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Hashtag test results for people dataset. 

Method 
dim 
P@1 
R@10 
Rank 
Freq. baseline 
-
4.20% 
1.59% 
11103 
#words baseline 
-
2.63% 
5.05% 
10581 
Word2Vec 
256 
4.66% 
8.15% 
10149 
Word2Vec 
512 
5.26% 
9.33% 
9800 
WSABIE 
64 
24.45% 29.64% 
2619 
WSABIE 
128 27.47% 32.94% 
2325 
WSABIE 
256 29.76% 35.28% 
1992 
WSABIE 
512 30.90% 36.96% 
1184 
#TAGSPACE 
64 
34.08% 38.96% 
1184 
#TAGSPACE 
128 36.27% 41.42% 
1165 
#TAGSPACE 
256 37.42% 43.01% 
1155 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Hashtag test results for pages dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Document recommendation task results. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ledell Wu and Jeff Pasternak for their help with datasets and baselines.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno>number EPFL- CONF-192374</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced sentiment learning using twitter hashtags and smileys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="241" to="249" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic hashtag recommendation for microblogs using topic-specific translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)&apos;12</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using topic models for twitter hashtag recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Slavkovikj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van De Walle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web companion</title>
		<meeting>the 22nd international conference on World Wide Web companion</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="593" to="596" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training highly multiclass classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Maya R Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic frame identification with distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hogwild!: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tomoha: Topic model-based hashtag recommendation on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieying</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the companion publication of the 23rd international conference on World wide web companion</title>
		<meeting>the companion publication of the 23rd international conference on World wide web companion</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="371" to="372" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three</title>
		<meeting>the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
