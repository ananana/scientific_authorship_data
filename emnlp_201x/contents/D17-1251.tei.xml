<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masumi</forename><surname>Shirakawa</surname></persName>
							<email>shirakawa@hapicom.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<country>Japan ‡</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Hara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Maekawa</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">hapicom Inc</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2358" to="2367"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a language-independent data-driven method to exhaustively extract bursty phrases of arbitrary forms (e.g., phrases other than simple noun phrases) from microblogs. The burst (i.e., the rapid increase of the occurrence) of a phrase causes the burst of overlapping N-grams including incomplete ones. In other words, bursty incomplete N-grams inevitably overlap bursty phrases. Thus, the proposed method performs the extraction of bursty phrases as the set cover problem in which all bursty N-grams are covered by a minimum set of bursty phrases. Experimental results using Japanese Twitter data showed that the proposed method outper-formed word-based, noun phrase-based, and segmentation-based methods both in terms of accuracy and coverage.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Background and motivation. Trends on mi- croblogs reflect manifold real-world events includ- ing natural disaster, new product launch, televi- sion broadcasting, public speech, airplane acci- dent, scandal and national holiday. To catch real- world events, not a few researchers and practi- tioners have sought ways to detect trends on mi- croblogs. Trend detection often involves bursty phrase extraction, i.e., extracting phrases of which occurrence rate in microblog texts posted within a certain period of time (and from a certain location) is much higher than that of the normal state. Ex- tracted bursty phrases are directly used as trends as Twitter 1 officially provides, or sent to higher-order processes such as clustering and event labeling. Bursty phrases on microblogs are likely to be noun phrases, but sometimes deviate from such standards. The title of a movie, song, game or any creation can be an arbitrary form like a long and/or general phrase (e.g., Spielberg's movie "catch me if you can", the Beatles' song "let it be") <ref type="bibr" target="#b1">2</ref> . A memorable phrase (e.g., Steve Jobs's phrase "stay hungry, stay foolish") can also be a bursty phrase on microblogs. Even numbers and symbols can be potentially bursty phrases (e.g., "1984" can be a novel, "!!!" can be an artist). Any filtering rule such as stop word removal, part-of-speech (POS) tag restrictions, or length limit can mistakenly fil- ter out bursty phrases.</p><p>Extracting irregularly-formed bursty phrases as described in the previous paragraph is difficult since no restriction can be used anymore to fil- ter out incomplete N-grams. However, they are rare and little influence the overall accuracy even if they are correctly extracted. Not only that, tack- ling such difficult and rare cases easily leads to extracting many incomplete N-grams and deteri- orating the accuracy. Almost all existing work has therefore ignored difficult and rare cases, and con- centrated on extracting simple phrases (e.g., uni- grams, bi-grams, tri-grams, or noun phrases iden- tified by POS taggers). By sacrificing minorities, most bursty phrases can be extracted with high accuracy. Thus, irregularly-formed phrases have been abandoned in bursty phrase extraction (and alike in many text analysis tasks).</p><p>Contributions. In this work, we aim to accu- rately and exhaustively extract bursty phrases of arbitrary forms from microblogs. The challenge here is: How do we avoid extracting bursty incom- plete N-grams without introducing any filtering rule? To solve this challenging problem, we pro- pose a set cover-based method. We found that the <ref type="table">Table 1</ref>: Representative trend detection methods based on bursty phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Unit of process Measure of burst ( <ref type="bibr" target="#b27">Sayyadi et al., 2009)</ref> Noun phrase TF, DF, IDF (O' <ref type="bibr">Connor et al., 2010)</ref> Uni-gram, bi-gram, tri-gram Burstiness ( <ref type="bibr" target="#b22">Mathioudakis and Koudas, 2010</ref>) Uni-gram Burstiness ( <ref type="bibr" target="#b31">Weng and Lee, 2011)</ref> Uni-gram DF-IDF, H-measure (wavelet analysis) <ref type="bibr" target="#b23">(Metzler et al., 2012)</ref> Uni-gram Burstiness ( <ref type="bibr" target="#b19">Li et al., 2012)</ref> N-gram of any length Deviation from Gaussian distribution ( <ref type="bibr" target="#b13">Cui et al., 2012)</ref> Hashtag Deviation from Gaussian distribution ( <ref type="bibr" target="#b10">Benhardus and Kalita, 2013</ref>)-1</p><p>Uni-gram, bi-gram, tri-gram Burstiness ( <ref type="bibr" target="#b10">Benhardus and Kalita, 2013</ref>)-2</p><p>Uni-gram TF-IDF, entropy ( <ref type="bibr">Aiello et al., 2013</ref>)-1</p><p>Uni-gram Burstiness ( <ref type="bibr">Aiello et al., 2013</ref>)-2</p><p>Uni-gram, bi-gram, tri-gram DF-IDF ( <ref type="bibr" target="#b8">Abdelhaq et al., 2013)</ref> Uni-gram Deviation from Gaussian distribution ( <ref type="bibr" target="#b28">Schubert et al., 2014)</ref> Uni-gram (pair) Deviation from Gaussian distribution <ref type="bibr" target="#b16">(Feng et al., 2015)</ref> Hashtag Deviation from Gaussian distribution burst (i.e., the rapid increase of occurrence) of a phrase causes the burst of overlapping incomplete N-grams. For example, if phrase "let it be" bursts, the occurrence of some overlapping N-grams such as "let it", "let it be is", and "it be is" inevitably in- creases, possibly generating bursty incomplete N- grams. Given that bursty incomplete N-grams al- ways accompany overlapping bursty phrases, we can avoid extracting bursty incomplete N-grams using the set cover problem <ref type="bibr" target="#b12">(Chvátal, 1979)</ref>. The proposed set cover-based method finds a minimum set of bursty phrases that cover all bursty N-grams including incomplete ones. Because the set cover problem is NP-complete, the proposed method ap- proximately solves it by iteratively choosing an N- gram that most covers remaining bursty N-grams. The advantages of the proposed method are as follows. 1) Exhaustive. The proposed method can extract bursty (contiguous) phrases of arbi- trary forms. In our experiment, the coverage has been shown to be larger than that of word-based, noun phrase-based, and segmentation-based meth- ods. 2) Accurate. With adequate preprocess- ing of auto-generated texts, the proposed method achieved 99.3% of precision for top 10 bursty phrases and 97.3% for top 50 bursty phrases, which were even higher than the comparative methods. 3) Language-independent. Because the proposed method processes texts as a sequence of characters (or words), it works in any languages including those without word boundary such as Japanese. 4) Purely data-driven. The proposed method only requires raw microblog texts and does not need external resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Much work has been focused on trend detection or event detection from microblogs. Majority of rep- resentative trend detection methods (summarized in <ref type="table">Table 1</ref>) start with extracting bursty phrases, of- ten followed by clustering bursty phrases in order to link them to real-world events. Others first build clusters of words (uni-grams) by using word co- occurrence ( <ref type="bibr" target="#b25">Pervin et al., 2013</ref>) or topic models ( <ref type="bibr">Aiello et al., 2013;</ref><ref type="bibr" target="#b14">Diao et al., 2012;</ref><ref type="bibr" target="#b18">Lau et al., 2012</ref>) and then apply burst detection for clusters. Also, there are different approaches such as the document-based approach ( <ref type="bibr">Aiello et al., 2013)</ref>, sketch-based model <ref type="bibr" target="#b32">(Xie et al., 2013)</ref> and bursty biterm topic model ( <ref type="bibr" target="#b33">Yan et al., 2015)</ref>.</p><p>It is noteworthy that most methods in <ref type="table">Table 1</ref> only focus on uni-grams, short N-grams (up to tri-grams), or noun phrases (or rely on hashtags). This is because majority of bursty phrases con- form to such simple forms. The rest of bursty phrases are rare but their forms are irregular and difficult to stereotype by using rules. Trying to ex- tract such irregularly-formed phrases easily leads to the deterioration of the precision due to incor- rect extraction. Also, the recall can hardly be in- creased since they are only a small portion of all bursty phrases. To balance the precision and re- call of bursty phrase extraction, focusing on sim- ple phrases and ignoring rare cases is a reasonable strategy. In the field of trend detection on mi- croblogs, this ignoring-minority strategy has be- come a de facto standard. However, it always fails to extract irregularly-formed bursty phrases. In this work, we tackle the challenge of extracting bursty phrases without any restriction of forms.</p><p>Among methods in <ref type="table">Table 1</ref>, <ref type="bibr" target="#b19">Li et al. (Li et al., 2012</ref>) have only attempted to extract bursty phrases of arbitrary forms. Their method, Twevent, applies text segmentation (or chunk- ing) before measuring the degree of the burst. Every microblog text is represented as a se- quence of word N-grams called segments. N- gram length, Symmetric Conditional Probability (SCP) <ref type="bibr" target="#b29">(da Silva and Lopes, 1999)</ref>, and seman- tic resources extracted from Wikipedia are inte- grated to obtain the best segmentation results. Ow- ing to a good segmentation algorithm, it can po- tentially detect bursty phrases other than noun phrases, uni-grams, bi-grams, and tri-grams with high accuracy. However, it is still possible to miss irregularly-formed bursty phrases because they are likely to be segmented incorrectly. Our set cover- based method guarantees that all bursty N-grams including irregularly-formed ones must be cov- ered by extracted bursty phrases. Thus, it is un- likely to miss irregularly-formed bursty phrases.</p><p>One more thing to note in <ref type="table">Table 1</ref> is that how to measure the degree of the burst can be largely classified into a few groups: burstiness, TF-IDF- based measures, and distribution-based methods. The simplest approach is burstiness, which is the ratio of the occurrence rate in target and reference document sets. Reference document set is usu- ally constructed from past microblog texts. TF- IDF-based measures compute term frequency (TF) or document frequency (DF) in the target docu- ment set and inverse document frequency (IDF) in the reference document set. Distribution-based methods generally measure how much the ob- served frequency deviates from the distribution of the normal state using standard scores (z-scores). The Poisson distribution is proper to represent the number of occurrence of phrases, but the Gaussian distribution is often used as its approximation due to computational reasons. We in this work primar- ily adopt a Gaussian distribution-based approach and use the z-score as the measure of the burst because it reasonably works well for the different magnitude of the number of occurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bursty Phrase Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We formalize the problem of bursty phrase extrac- tion from microblogs. Target document set D T is a set of microblog messages posted within a certain period of time (e.g., one day, three hours). Refer- ence document set D R is a set of microblog mes- sages posted before the target time. Both are usu- ally limited to certain locations or languages. Each document is a sequence of characters (or words). The objective here is to extract bursty complete phrases from D T as much as possible using D R as the normal state. The output format is a list of bursty N-grams L = [g 1 , g 2 , · · · , g |L| ] (g i is an N- gram or a sequence of characters) ranked by the degree of the burst. The accuracy and coverage of top K (K is a user-specified parameter) bursty phrases are important evaluation criteria.</p><p>The degree of the burst for N-grams in D T is defined as the z-score when the Gaussian distribu- tion is estimated from D R . While most N-grams occur once in a single microblog message, a few N-grams are repeatedly used in it. We thus em- ploy the document frequency-based z-score as the degree of the burst. The z-score of N-gram g is specifically computed as</p><formula xml:id="formula_0">zscore(g) = df (g) − µ(g) σ(g)<label>(1)</label></formula><p>where df (g) is the document frequency of g in D T , and µ(g) and σ(g) are respectively the mean and standard deviation for the document frequency of g estimated from D R . Given that the Gaus- sian distribution used here is the approximation of the Poisson distribution, σ(g) is approximated by √ µ(g). To smooth µ(g) when g never occurs in D R , we add δ = 1 to µ(g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basic Idea</head><p>To exhaustively extract bursty phrases without any restriction of their forms, we have to refrain from using filtering rules such as stop word removal, POS tag restrictions, and length limit. Without filtering rules, there are far more bursty incom- plete N-grams than bursty complete phrases. It is challenging to extract bursty phrases including irregularly-formed ones and at the same time to avoid extracting bursty incomplete N-grams. Is there any evident difference between bursty in- complete N-grams and bursty phrases of irregular forms?</p><p>We scrutinized Twitter data and found the fol- lowing fact: Bursty incomplete N-grams always accompany overlapping bursty phrases provided that the definition of the burst is appropriate. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Greedy Set Cover Algorithm for Bursty Phrase Extraction</head><p>Input:</p><formula xml:id="formula_1">Target document set D T , reference document set D R Output: Ranked list of bursty phrases L 1 Initialize L; 2 Get a set of valid N-grams G V = {g i } satisfying burstiness(g i ) ≥ 1 + ϵ; 3 Get a set of bursty N-grams G B = {g i } ⊂ G V satisfying zscore(g i ) ≥ θ and zscore tf (g i ) ≥ θ; 4 Discard trivial N-grams from G V ; 5 while G B ̸ = ∅ do 6</formula><p>Select g ∈ G V that most cover the occurrence of bursty N-grams in G B ;</p><p>7</p><p>Get a set of longer N-grams</p><formula xml:id="formula_2">G g ⊂ G V containing g; 8</formula><p>Determine a set of containment N-grams G C ⊂ G g for g;</p><formula xml:id="formula_3">9 if G C = ∅ then 10</formula><p>Push g into L;</p><p>11</p><p>Negate the occurrence of all N-grams in G B and G V where g overlaps;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>Delete g i ∈ G B if it does not satisfy zscore tf (g i ) ≥ θ; explain this phenomenon in due order. When many microblog users intensively use a certain phrase, it becomes a bursty phrase. Here, the increment of the occurrence of the phrase con- tributes to the increment of the occurrence of over- lapping N-grams. Consequently, (incomplete) N- grams that overlap the phrase can also burst. Thus, bursty incomplete N-grams always have their orig- inal bursty phrases that overlap each other. Based on the phenomenon described in the pre- vious paragraph, bursty incomplete N-grams cease bursting if their original bursty phrases disappear from microblog texts. In other words, all bursty N-grams including incomplete ones can be cov- ered (overlapped) by bursty phrases. Given that bursty phrases cause bursty incomplete N-grams but the reverse was not true, we can formalize the extraction of bursty phrases as the set cover prob- lem <ref type="bibr" target="#b12">(Chvátal, 1979</ref>) where a minimum number of bursty phrases are selected to cover all bursty N- grams. When all selected bursty phrases are re- moved from microblog texts, it is guaranteed that there is no bursty N-gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed Algorithm</head><p>Algorithm 1 is a pseudo-code of the greedy set cover algorithm for bursty phrase extraction. As formulated in Section 3.1, the input data is target D T and reference document sets D R of microblog messages. The output is a ranked list of bursty</p><formula xml:id="formula_4">phrases L = [g 1 , g 2 , · · · , g |L| ].</formula><p>Basically, Algo- rithm 1 iteratively selects an N-gram that most covers the occurrence of bursty N-grams (Line 6) until all bursty N-grams are covered. In the fol- lowing, we describe points of the Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Bursty N-grams and Valid N-grams</head><p>A set of bursty N-grams G B in Algorithm 1 (Line 3) corresponds to the universe of the set cover problem which should be all covered. Bursty N- grams satisfy z-score threshold θ both in document frequency-based (Eq. (1)) and term frequency- based z-scores (Eq. <ref type="formula" target="#formula_5">(2)</ref>).</p><formula xml:id="formula_5">zscore tf (g) = tf (g) − µ tf (g) σ tf (g)<label>(2)</label></formula><p>Here, tf (g) is the term frequency of g in D T , and µ tf (g) and σ tf (g) are respectively the mean and standard deviation for the term frequency of g es- timated from D R . The term frequency-based z- score is used to judge whether a bursty N-gram in G B still bursts when its occurrences are partly covered. This is required to handle N-grams re- peatedly occurring in a single microblog message.</p><p>Valid N-grams in G V (Line 2) are qualified to be bursty phrases to cover G B . We differenti- ate bursty N-grams and valid N-grams (specifi- cally, G B ⊂ G V ) to handle threshold problems.</p><p>Namely, it is possible that bursty incomplete N- grams satisfy the z-score threshold but their origi- nal bursty phrases do not satisfy the threshold. The criterion of the valid N-gram is defined by using burstiness.</p><formula xml:id="formula_6">burstiness(g) = df (g) df R (g) · |D R | |D T |<label>(3)</label></formula><p>Here, df R (g) is the document frequency of N- gram g in D R . To avoid division by zero, smooth- ing term δ = 1 is added to df R (g). When burstiness(g) satisfies threshold 1 + ϵ (i.e., the occurrence of g actually increases), g becomes a valid N-gram. To reduce pointless processes, triv- ial N-grams that can never be phrases (e.g., start- ing or ending with spaces, occurring only as a part of a sole longer N-gram) are discarded (Line 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Occurrence-based Set Covering</head><p>Whereas the standard set cover problem assumes that each item is atomic, i.e., the state of an item is either not covered or covered, the proposed method manages the state of covering by using all occurrences of bursty N-grams. When a valid N-gram is selected (Line 10), the occurrence of all N-grams in G B and G V that the valid N-gram overlaps is negated (Line 11). Here, a bursty N- gram in G B is completely covered if the term frequency-based z-score computed from remain- ing occurrences of the N-gram does not satisfy the threshold (Line 12).</p><p>Occurrence-based set covering can solve the case when a bursty N-gram is covered by multi- ple bursty phrases. That is, the bursty N-gram is not completely covered even if one of the bursty phrases is selected. For example, given two bursty phrases "let it be" and "let it go" (a song), incom- plete N-gram "let it" ceases bursting only when the occurrence of both phrases is negated. Also, it can handle partially overlapping N-grams (e.g., "let it be" and "be is") based on the number of overlaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Containment N-grams</head><p>N-grams that are contained in multiple phrases should be carefully treated in the set cover prob- lem. Shorter N-grams are likely to be contained in more phrases and chosen in the set cover problem even if they are incomplete. For example, when phrases "let it be" and "let it go" burst, shared incomplete N-gram "let it" is likely to cover the occurrence of bursty N-grams more than the two phrases. To prevent selecting shared incomplete N-grams, we determine containment relations be- tween an N-gram and longer N-grams contain- ing it ( <ref type="bibr">Lines 7, 8)</ref>. We define longer N-grams in containment relations as containment N-grams. Containment relations negate the occurrence of the shorter N-gram where containment N-grams overlap <ref type="bibr">(Line 14)</ref>. Containment relation is in- spired by the idea of rectified frequency used in a segmentation-based quality phrase mining method ( <ref type="bibr" target="#b21">Liu et al., 2015)</ref>, though how to rectify the fre- quency is different. Note that containment rela- tions do not necessarily mean that shorter N-grams are incomplete because both shorter and longer N- grams can be phrases (e.g., "new york" and "new york times").</p><p>How to determine containment relations is de- signed not to contradict the greedy set cover al- gorithm. Briefly, containment relations hold when only the containment N-grams among longer N- grams can cover the occurrence of bursty N-grams more than the shorter N-gram owing to the con- tainment relations. That is, we find a stable state of containment relations. To find a stable state, we initially define temporal containment relations and then iteratively find a set of containment N-grams so that containment relations become stable.</p><p>Initial containment N-grams are determined us- ing burst context variety, which is an extension of accessor variety <ref type="bibr" target="#b15">(Feng et al., 2004</ref>). Accessor va- riety roughly measures how much an N-gram is likely to be a phrase. It specifically counts the number of distinct characters (or words) before or after the N-gram and employs the minimum one. The drawback of accessor variety is that it han- dles left and right contexts independently. We thus modify it as context variety in which both left and right contexts are simultaneously counted. Con- text variety can also be calculated using the set cover problem. In particular, a left or right char- acter (or word) that most covers the occurrence of the N-gram is iteratively selected until all the oc- currences are covered. Burst context variety is a further extension of context variety to count the number of contexts only for additional term fre- quencies given the mean of the term frequency. When the burst context variety of an N-gram is not greater than that of a longer N-gram, we extract the context (i.e., left or right character) and define all longer N-grams having the context as initial con- tainment N-grams.</p><p>There are two minute settings for measuring burst context variety. One is that the start and end of every line are all unique and should be counted as distinct contexts. The other is that symbols <ref type="bibr" target="#b2">3</ref> should be ignored when checking left and right contexts of the N-gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Reranking Bursty Phrases</head><p>The output L is finally reranked by using actual z-scores (Line 17), which are different from z- scores calculated from raw document frequency. The actual z-score is calculated from the num- ber of occurrences of bursty N-grams that N-gram g ∈ L actually covered during the set cover pro- cess. Since a single valid N-gram usually cov- ers multiple bursty N-grams, the z-score for ev- ery covered bursty N-gram is recalculated and the maximum is used as the actual z-score. When the maximum z-score exceeds the original z-score, the original z-score is preserved. Without reranking, incomplete N-grams that covered very few occur- rences of bursty N-grams may be overestimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluated the proposed method using two weeks of Japanese Twitter data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset.</head><p>We created a dataset using Twit- ter Streaming API statuses/sample. We chose Japanese as a language because it was one of pop- ular languages used in Twitter and because it has no word boundary and finding phrases is diffi- cult compared to space-delimited languages such as English. We specifically collected 15 days of tweets from September 30 to <ref type="bibr">October 14, 2016 4</ref> . For each day from October 1 to 14, we extracted bursty phrases in reference to the previous day.</p><p>To maximally alleviate the influence of auto- generated contents such as tweets posted by bots and spammers, we filtered out them. Detecting bots and spammers ( <ref type="bibr" target="#b11">Chu et al., 2010;</ref><ref type="bibr" target="#b30">Subrahmanian et al., 2016</ref>) is a nontrivial research task and out of the scope of this paper. In this experiment, we used simple but effective heuristics. First, we only used tweets posted by Twitter official clients 5 because they were mainly used by normal users. Second, we discarded tweets including URLs be- cause most spammers tried to lure users to visit their websites. Third, retweets (actions to propa- gate someone's tweets) were discarded. The max- imum and minimum numbers of remaining tweets per day were 326,002 (Oct. 2) and 253,044 <ref type="bibr">(Oct. 13)</ref>, respectively. Additionally, we deleted hash- tags (starting by #) and mentions (starting by @) from tweets. Note that the degree of the burst for URLs, hashtags, and mentions can be indepen- dently measured. We concentrated on extracting bursty phrases from plain texts.</p><p>Ground truth. To create the ground truth, we mixed N-grams extracted with all methods and then manually annotated each N-gram by check- ing its real usage in tweets. While most N-grams were easily judged as complete phrase (i.e., cor- rect) or incomplete N-grams (i.e., incorrect), a few N-grams were difficult to judge (e.g., a last name that was not frequently used to indicate the person in tweets). We annotated such N-grams as maybe correct and regarded as correct in the strict case and incorrect in the loose case when measuring evaluation metrics.</p><p>Evaluated methods. In the proposed method (Proposed), we processed Japanese texts as se- quences of characters. Default threshold parame- ters θ and ϵ were set to 10 and 0.5, respectively. Comparative methods included the word-based method (Word), noun phrase-based method (NP), and segmentation-based method (Segment) ( <ref type="bibr" target="#b19">Li et al., 2012)</ref>. Because these methods require word breaking, we used MeCab ( <ref type="bibr" target="#b17">Kudo et al., 2004</ref>) (ver- sion 0.996, ipadic as a dictionary), a Japanese morphological analyzer. The word-based method uses all self-sufficient words as candidate phrases. The noun phrase-based method regards concate- nated successive nouns as a candidate phrase. In word-based and noun phrase-based methods, the dictionary or vocabulary significantly affects the performance. Thus, we also used neologd 6 ( <ref type="bibr" target="#b26">Sato et al., 2017</ref>) (version v0.0.5 updated at May 2, 2016), a neologism dictionary extracted from many language resources on the web, as an ad- ditional dictionary (+Dic). The segmentation- based method detects segments (chunks) from a sequence of words as candidate phrases. The segmentation model was constructed using three   months of tweets (from July 1 to Sept. <ref type="bibr">30,</ref><ref type="bibr">2016)</ref> and Japanese Wikipedia dump data (as of Oct. 1, 2016). Evaluation metrics. We employed precision and min-z-score of top K bursty phrases (K was set to 10, 20, 30, 40, or 50) as evaluation metrics. We measured the precision both in strict and loose cases based on the ground truth labels. The min- z-score (the minimum of the z-score, computed from raw document frequency) was introduced to evaluate how much the top K output ranked by z- scores included highly bursty phrases. To increase the min-z-score, all the top K phrases should have high z-scores and hence highly bursty phrases should not be ignored. Thus the min-z-score can evaluate the coverage of extracted bursty phrases using a fixed size of the output. Higher precision and min-z-score indicate that the method can more accurately and exhaustively extract bursty phrases. <ref type="table" target="#tab_0">Tables 2, 3</ref> show the precision of bursty phrase extraction. It was surprisingly that the proposed method achieved higher precision than noun- phrase based methods, which were supposed to be safety by sacrificing irregularly-formed phrases. The precision of the proposed method for top 50 bursty phrases was 97.3% (correct phrases were 681 out of 700) in the strict case and 99.1% (694 out of 700) in the loose case. The precision for top 10 bursty phrases was 99.3% (139 out of 140) even in the strict case. The results demonstrate that the burst information alone can accurately find the boundary of bursty phrases using the set cover problem. Error cases of the proposed method were largely classified into two: base sequences of di- versified expressions and phrases with strongly- correlated attached characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Results: Precision</head><p>In comparative methods, the accuracy tended to be high when noun phrases were used and the dic- tionary was well defined. Especially, the use of the neologism dictionary boosted the precision. The segmentation-based method also marked moder- ately high precision. <ref type="table" target="#tab_2">Table 4</ref> shows the min-z-score of bursty phrase ex- traction. The proposed method achieved higher min-z-score than the comparative methods. This was because the proposed method extracted bursty phrases regardless of their forms. Noun phrase- based methods tended to miss highly bursty phrases of irregular forms. Therefore the min- z-score of extracted top K bursty phrases be- came small. Among comparative methods, the segmentation-based method best achieved the min-z-score since it did not restrict the form of phrases. However, it was still possible to miss very irregular phrases due to segmentation mis- takes and the min-z-score was less than that of the proposed method. The use of the neologism dic- tionary increased the min-z-score as well as the precision, indicating that it had no negative effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Results: Coverage</head><p>To intuitively assess the coverage of the pro- posed method, we manually counted the num- ber of bursty phrases that were extracted with the proposed method (when K = 10, i.e., 139 correct phrases) but completely missed with the comparative methods. Here, we regarded com- pletely missed when neither of the bursty phrase, overlapping N-grams including incomplete ones nor orthographic variants were extracted in top    <ref type="table" target="#tab_3">Table 5</ref> shows the results. Although the percentages were small, any com- parative method completely missed some highly bursty phrases that were extracted with the pro- posed method. Note that the proposed method did not completely miss top 10 bursty phrases of com- parative methods at all since the set cover problem inevitably covered all bursty N-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Influence of Parameter Settings</head><p>We changed threshold parameters θ and ϵ to eval- uate their influence on performance. <ref type="table" target="#tab_5">Tables 6, 7</ref> show the performance results with different pa- rameter settings. We confirmed that both parame- ters, especially ϵ, hardly affected the precision and min-z-score. The results indicate that the thresh- old parameters can be roughly set based on data. <ref type="table" target="#tab_6">Table 8</ref> shows top 10 bursty phrases (all of them are correct) on Oct. 1. This day contained many irregularly-formed phrases; phrases containing hi- ragana 7 characters (rank 5, 7, 8, 10), other than noun phrases (rank 5, 7), and containing symbols (rank 3). Especially, (rank 5) and (rank 7) were troublesome since they con- tain hiragana characters and at the same time they are other than noun phrases. Even with the neol-  ogism dictionary, the noun phrase-based method extracted but missed . To demonstrate the language-independent na- ture of the proposed method, we also applied it to English with character-level processing <ref type="bibr" target="#b6">8</ref> . <ref type="table">Ta- ble 9</ref> shows an example of bursty phrases ex- tracted from English tweets. Whereas an incom- plete phrase (rank 2) was extracted due to auto- generated contents that could not be eliminated from data, other top bursty phrases were correctly extracted. The proposed method also extracted a very long Internet meme (rank 5), which burst along with its counterpart "anyone that knows me knows i love _________." (rank 17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed a language-independent data-driven method to accurately and exhaustively extract bursty phrases of arbitrary forms from microblogs. We found that bursty incomplete N-grams always <ref type="table">Table 9</ref>: Example of top 10 bursty phrases in <ref type="bibr">English (Oct. 1, 2016, PST)</ref>. Note that these bursty phrases were generated by processing En- glish tweets in character level. accompany overlapping bursty phrases by ascer- taining the mechanism why incomplete N-grams burst. Based on the findings, the proposed method solves the extraction of bursty phrases as the set cover problem where a minimum set of bursty phrases covers all bursty N-grams including in- complete ones. We confirmed from experimen- tal results that the proposed method outperformed noun phrase-based and segmentation-based meth- ods both in terms of the accuracy and coverage. The source code of the proposed method is pub- licly available <ref type="bibr" target="#b7">9</ref> .</p><p>The future work includes the reduction or esti- mation of the computation time and memory us- age. They increase as the target document set grows or, specifically the number of occurrences of bursty N-grams increases. Also, handling auto- generated contents is an important issue. The pro- posed method should be used with effective meth- ods of identifying auto-generated contents, bots, and spammers in microblogs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://twitter.com/.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>13 else 14 Negate the occurrence of g where containment N-grams g i ∈ G C overlap; 15 end 16 end 17 Rerank L based on the actual z-score;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6 :</head><label>6</label><figDesc>Average precision of top K bursty phrases with different parameter settings in the proposed method (strict case).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 .</head><label>1</label><figDesc>LizQuen InSydney Oct30 (maybe an auto- generated phrase used like a tag) 2. for The 100 most beautiful faces in 2016 (maybe an auto-generated sequence used to vote a celebrity for the ranking) 3. Lamar Jackson (American football player) 4. Clemson (American football team) 5. quote with what you think the answer is and copy this tweet to see what people say about you (Internet meme) 6. Tennessee (American football team) 7. Louisville (American football team) 8. Milner (American football player) 9. FSU (American football team) 10. Louis Walsh (talent manager)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : Average precision of top K bursty phrases (strict case).</head><label>2</label><figDesc></figDesc><table>Method 
Top10 Top20 Top30 Top40 Top50 
Word 
0.700 0.714 0.731 0.743 0.743 
Word+Dic 0.929 0.907 0.895 0.907 0.904 
NP 
0.936 0.929 0.936 0.927 0.930 
NP+Dic 
0.971 0.968 0.962 0.955 0.954 
Segment 
0.921 0.918 0.905 0.918 0.914 
Proposed 
0.993 0.993 0.981 0.979 0.973 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average precision of top K bursty 
phrases (loose case). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 : Average min-z-score of top K bursty phrases.</head><label>4</label><figDesc></figDesc><table>Method 
Top10 Top20 Top30 Top40 Top50 
Word 
41.5 
25.7 
20.4 
17.2 
15.3 
Word+Dic 44.7 
29.0 
22.5 
19.3 
16.7 
NP 
42.6 
26.4 
20.4 
16.7 
14.6 
NP+Dic 
42.7 
28.8 
21.0 
17.6 
15.3 
Segment 
45.0 
31.2 
24.3 
20.5 
18.0 
Proposed 
50.1 
33.5 
24.8 
21.5 
19.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 : Number of bursty phrases extracted with the proposed method (K = 10) but completely missed with comparative methods.</head><label>5</label><figDesc></figDesc><table>Word Word+Dic 
NP 
NP+Dic Segment 
17/139 
7/139 
10/139 
4/139 
8/139 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 7 : Average min-z-score of top K bursty phrases with different parameter settings in the proposed method.</head><label>7</label><figDesc></figDesc><table>Parameters Top10 Top20 Top30 Top40 Top50 
θ=5, ϵ=0.2 
50.6 
33.4 
24.9 
21.4 
19.2 
θ=5, ϵ=0.5 
50.6 
33.4 
24.8 
21.4 
19.2 
θ=5, ϵ=1.0 
50.6 
33.4 
24.9 
21.4 
19.3 
θ=10, ϵ=0.2 50.1 
33.5 
24.8 
21.5 
19.4 
θ=10, ϵ=0.5 50.1 
33.5 
24.8 
21.5 
19.4 
θ=10, ϵ=1.0 50.2 
33.5 
24.9 
21.5 
19.5 
θ=15, ϵ=0.2 50.2 
33.5 
25.1 
21.8 
19.4 
θ=15, ϵ=0.5 50.2 
33.5 
25.1 
21.8 
19.5 
θ=15, ϵ=1.0 50.2 
33.5 
25.1 
21.8 
19.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Example of top 10 bursty phrases in 
Japanese (Oct. 1, 2016). </table></figure>

			<note place="foot" n="2"> We used old but popular titles for illustrative purposes.</note>

			<note place="foot" n="3"> In our experiments, we used isalnum (or iswalnum for wide characters) in C++ standard library to distinguish words and symbols. 4 We considered that a day changed at 4 am JST. 5 https://about.twitter.com/products/ list.</note>

			<note place="foot" n="6"> https://github.com/neologd/ mecab-ipadic-neologd.</note>

			<note place="foot" n="7"> In Japanese, hiragana is mainly used for auxiliary words and thus difficult to break into words or phrases.</note>

			<note place="foot" n="8"> Of course, we can process English in word level.</note>

			<note place="foot" n="9"> https://github.com/iwnsew/ sc-bursty-phrase.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is partially supported by the Grant-in-Aid for Scientific Research (A)(2620013) of the Ministry of Education, Culture, Sports, Science and Technology, Japan, and JST, Strategic Interna-tional Collaborative Research Program, SICORP.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sandaime, generally meaning &quot;third</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anri</forename><surname>Sakaguchi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Vens</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>idol group</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pri-rai, short name of a live concert) 5. (Utsucchatta, generally meaning &quot;it was reflected</title>
		<imprint/>
	</monogr>
	<note>short name of a TV program</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tandoku, generally meaning &quot;singleness</title>
		<imprint/>
	</monogr>
	<note>implying a live concert without supporting acts</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Shiyagare, generally a phrase used in an imperative sentence</title>
		<imprint/>
	</monogr>
	<note>short name of a TV program</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Utapri, short name of an anime)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">WORKING (short name of an anime) 10. (Utapri, short name of an anime)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EvenTweet: Online Localized Event Detection from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Abdelhaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sengstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1326" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Petkos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symeon</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayse</forename><surname>Skraba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Göker</surname></persName>
		</author>
		<title level="m">Ioannis Kompatsiaris, and Alejandro Jaimes. 2013. Sensing Trending Topics in Twitter. IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1268" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Streaming Trend Detection in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Benhardus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jugal</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Web Based Communities</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="139" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gianvecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushil</forename><surname>Jajodia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Computer Security Applications Conference (ACSAC)</title>
		<meeting>Annual Computer Security Applications Conference (ACSAC)<address><addrLine>Bot, or Cyborg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Greedy Heuristic for the SetCovering Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Václav</forename><surname>Chvátal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="235" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discover Breaking Events with Popular Hashtags in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1794" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding Bursty Topics from Microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="536" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accessor Variety Criteria for Chinese Word Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">STREAMCUBE: Hierarchical Spatio-temporal Hashtag Clustering for Event Exploration over the Twitter Stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Engineering (ICDE)</title>
		<meeting>IEEE International Conference on Data Engineering (ICDE)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1561" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Applying Conditional Random Fields to Japanese Morphological Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On-line Trend Analysis with Topic Models: #twitter trends detection topic model online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics (COLING)</title>
		<meeting>International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1519" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Twevent: Segment-based Event Detection from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwitaman</forename><surname>Datta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tweets</surname></persName>
		</author>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining Quality Phrases from Massive Text Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data (SIGMOD)</title>
		<meeting>ACM SIGMOD International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1729" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TwitterMonitor: Trend Detection over the Twitter Stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathioudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Koudas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data (SIGMOD)</title>
		<meeting>ACM SIGMOD International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1155" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured Event Retrieval over Microblog Archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congxing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="646" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TweetMotif: Exploratory Search and Topic Summarization for Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International AAAI Conference on Weblogs and Social Media (ICWSM)</title>
		<meeting>International AAAI Conference on Weblogs and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast, Scalable, and Context-Sensitive Detection of Trending Topics in Microblog Post Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nargis</forename><surname>Pervin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anindya</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debra</forename><surname>Vandermeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implementation of a Word Segmentation Dictionary Called mecab-ipadic-NEologd and Study on How to Use It Effectively for Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshinori</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiichi</forename><surname>Hashimoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meeting of the Association for Natural Language Processing</title>
		<meeting>Meeting of the Association for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Event Detection and Tracking in Social Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sayyadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Maykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International AAAI Conference on Weblogs and Social Media (ICWSM)</title>
		<meeting>International AAAI Conference on Weblogs and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="311" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SigniTrend: Scalable Detection of Emerging Topics in Textual Streams by Hashed Significance Thresholds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="871" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Local Maxima Method and a Fair Dispersion Normalization for Extracting Multi-word Units from Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquim</forename><surname>Ferreira Da Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos&amp;apos;e Gabriel Pereira</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meeting on Mathematics of Language (MOL)</title>
		<meeting>Meeting on Mathematics of Language (MOL)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="369" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The DARPA Twitter Bot Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Azaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skylar</forename><surname>Durst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="38" to="46" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Event Detection in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bu-Sung</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International AAAI Conference on Weblogs and Social Media (ICWSM)</title>
		<meeting>International AAAI Conference on Weblogs and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TopicSketch: Real-Time Bursty Topic Detection from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="837" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Probabilistic Model for Bursty Topic Discovery in Microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="353" to="359" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
