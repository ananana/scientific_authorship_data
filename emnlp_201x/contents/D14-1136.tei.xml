<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Parsing Using Content and Context: A Case Study from Requirements Elicitation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Weiss</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaarit</forename><surname>Natan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smadar</forename><surname>Szekely</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ilia Pogrebezky Interdisciplinary Center Herzliya</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Weizmann Institute Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Weizmann Institute Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Weizmann Institute Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Weizmann Institute Rehovot</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Parsing Using Content and Context: A Case Study from Requirements Elicitation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1296" to="1307"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a model for the automatic semantic analysis of requirements elicitation documents. Our target semantic representation employs live sequence charts, a multi-modal visual language for scenario-based programming, which can be directly translated into executable code. The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context. We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static (entities, properties) and dynamic (behavioral scenarios) requirements in the document.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Requirements elicitation is a process whereby a system analyst gathers information from a stake- holder about a desired system (software or hard- ware) to be implemented. The knowledge col- lected by the analyst may be static, referring to the conceptual model (the entities, their properties, the possible values) or dynamic, referring to the behavior that the system should follow (who does what to whom, when, how, etc). A stakeholder in- terested in the system typically has a specific static and dynamic domain in mind, but he or she cannot necessarily prescribe any formal models or code artifacts. The term requirements elicitation we use here refers to a piece of discourse in natural lan- guage, by means of which a stakeholder commu- nicates their desiderata to the system analyst.</p><p>The role of a system analyst is to understand the different requirements and transform them into formal constructs, formal diagrams or executable code. Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain. Studies in software engi- neering aim to develop intuitive symbolic systems with which human agents can encode require- ments that would then be unambiguously trans- lated into a formal model ( <ref type="bibr" target="#b31">Fuchs and Schwitter, 1995;</ref><ref type="bibr" target="#b27">Bryant and Lee, 2002</ref>).</p><p>More recently,  de- fined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) <ref type="bibr" target="#b29">(Damm and Harel, 2001;</ref><ref type="bibr" target="#b36">Harel and Marelly, 2003)</ref>, a formal language for specifying the dynamic behavior of reactive systems. How- ever, the grammar that underlies this language fragment is highly ambiguous, and all disam- biguation has to be conducted manually by a hu- man agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism <ref type="bibr" target="#b42">(Kuhn, 2014)</ref>.</p><p>In this paper we accept the ambiguity of re- quirements descriptions as a premise, and aim to answer the following question: can we automati- cally recover a formal representation of the com- plete system -one that best reflects the human- perceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing <ref type="bibr" target="#b51">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b45">Liang et al., 2011;</ref><ref type="bibr" target="#b23">Artzi and Zettlemoyer, 2013;</ref><ref type="bibr" target="#b44">Liang and Potts, 2014</ref>), use differ- ent formalisms and various kinds of learning sig- nals for statistical semantic parsing. In particu- lar, the model of <ref type="bibr" target="#b43">Lei et al. (2013)</ref> induces input parsers from format descriptions. However, rarely do these models take into account the entire docu- ment's context.</p><p>The key idea we promote here is that discourse context provides substantial disambiguating infor- mation for sentence analysis. We suggest a novel <ref type="figure">Figure 1</ref>: An LSC scenario: "When the user clicks the button, the display color must change to red." model for integrated sentence-level and discourse- level processing, in a joint generative probabilistic framework. The input for the requirements elici- tation task is given in a simplified, yet highly am- biguous, fragment of English, as specified in . The output, in contrast, is a sequence of unambiguous and well-formed live sequence charts (LSC) <ref type="bibr" target="#b29">(Damm and Harel, 2001;</ref><ref type="bibr" target="#b36">Harel and Marelly, 2003</ref>) describing the dynamic behavior of the system, tied to a single shared code-base called a system model (SM).</p><p>Our solution takes the form of a hidden markov model (HMM) where emission probabilities re- flect the grammaticality and interpretability of tex- tual requirements via a probabilistic grammar and transition probabilities model the overlap between SM snapshots of a single, shared, domain. Using efficient viterbi decoding, we search for the best sequence of domain snapshots that has most likely generated the entire requirements document. We empirically show that such an integrated model consistently outperforms a sentence-based model learned from the same set of data.</p><p>The remainder of this document is organized as follows. In Section 2 we describe the task, and spell out our formal assumptions concerning the input and the output. In Section 3 we present our target semantic representation and a specially tailored notion of grounding for anchoring the requirements in a code-base. In Section 4 we develop our sentence-based and discourse-based models, and in Section 5 we evaluate the models on various case studies. In Section 6 we discuss applications and future extensions, and in Sec- tion 7 we summarize and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parsing Requirements Elicitation Documents: Task Description</head><p>There is an inherent discrepancy between the in- put and the output of the software engineering pro- cess. The input, system requirements, is specified in a natural, informal, language. The output, the system, is ultimately implemented in a formal un- ambiguous programming language. Can we auto- matically recover such a formal representation of a complete system from a set of requirements? In this work we explore this challenge empirically.</p><p>The Input. We assume a scenario-based pro- gramming paradigm (a.k.a behavioral program- ming (BP) ( <ref type="bibr" target="#b38">Harel et al., 2012)</ref>) in which system development is seen as a process whereby humans describe the expected behavior of the system by means of "short-stories", formally called scenar- ios <ref type="bibr" target="#b40">(Harel, 2001</ref>). We further assume that a given requirements document describes exactly one sys- tem, and that each sentence describes a single, possibly complex, scenario. The requirements we aim to parse are given in a simplified form of En- glish (specifically, the English fragment described in ). Contrary to strictly formal specification languages, which are closed and unambiguous, this fragment of English em- ploys an open-ended lexicon and exhibits exten- sive syntactic and semantic ambiguity. 1</p><p>The Output. Our target semantic representation employs live sequence charts (LSC), a diagram- matic formal language for scenario-based pro- gramming <ref type="bibr" target="#b29">(Damm and Harel, 2001</ref>). Formally, LSCs are an extension of the well-known UML message sequence diagrams ( <ref type="bibr" target="#b35">Harel and Maoz, 2006</ref>), and they have a direct translation into ex- ecutable code <ref type="bibr" target="#b36">(Harel and Marelly, 2003)</ref>. <ref type="bibr">2</ref> Using LSC diagrams for software modelling enjoys the advantages of being easily learnable , intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to exe- cution ( <ref type="bibr" target="#b37">Harel et al., 2002</ref>) and verification ( <ref type="bibr" target="#b39">Harel et al., 2013</ref>). The LSC language is particularly suited for representing natural language require- ments, since its basic formal constructs, scenar- ios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics <ref type="bibr" target="#b46">(Parsons, 1990)</ref>.</p><p>Live Sequence Charts and Code Artifacts. A live sequence chart (LSC) is a diagram that de- scribes a possible or necessary run of a specified system. In a single LSC diagram, entities are rep- resented as vertical lines called lifelines, and inter- actions between entities are represented using hor- izontal arrows between lifelines called messages, connecting a sender to a receiver. Messages may refer to other entities (or properties of entities) as arguments. Time in LSCs proceeds from top to bottom, imposing a partial order on the execution of messages. LSC messages can be hot (red, "must happen") or cold (blue, "may happen"). A mes- sage may have an execution status, which desig- nates it as monitored (dashed arrow, "wait for") or executed (full arrow, "execute"). The LSC lan- guage also encompasses conditions and control structures, and it allows defining requirements in terms of the negation of charts. <ref type="figure">Figure 1</ref> illustrates the LSC for the scenario "When the user clicks the button, the display color must change to red.". The respective system model (SM) is a code-base hierarchy containing the classes USER, BUTTON, DISPLAY, the method BUTTON.CLICK() and the property DISPLAY.COLOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Formal Settings</head><p>In the text-to-code generation task, we aim to im- plement a prediction function f : D → M, such that D ∈ D is a piece of discourse consisting of an ordered set of requirements D = d 1 , d 2 ...d n , and f (D) = M ∈ M is a code-base hierarchy that grounds the semantic interpretation of D; we de- note this by M sem(d 1 , ..., d n ). We now define the objects D, M , and describe how to construct the semantic interpretation function (sem(.)). We then spell out the notion of grounding ().</p><p>Surface Structures: Let Σ be a finite lexicon and let L req ⊆ Σ * be a language for specifying requirements. We assume the sentences in L req have been generated by a context-free grammar G = N , Σ, S ∈ N , RR, where N is a set of non- terminals, Σ is the aforementioned lexicon, S ∈ N is the start symbol and R is a set of context-free rules {A → α|A ∈ N , α ∈ (N ∪ Σ) * }. For each utterance u ∈ L req , we can find a sequential appli- cation of rules that generates it: u = r 1 • ...</p><p>• r k ; ∀i : r i ∈ R. We call such a sequence a deriva- tion of u. These derivations may be graphically depicted as parse trees, where the utterance u de- fines the sequence of tree terminals in the leaves.</p><p>We define T req to be the set of trees strongly generated by G, and utilize an auxiliary yield function yield : T req → L req returning the leaves of the given tree t ∈ .L req . Different parse-trees can generate the same utterance, so the task of an- alyzing the structure of an utterance u ∈ L req is modeled via a function syn : L req → T req that returns the correct, human-perceived, parse of u.</p><p>Semantic Structures: Our target semantic rep- resentation of a requirement d ∈ L req is a dia- grammatic structure called a live sequence chart (LSC). The LSC formal definition we provide here is based on the appendix of <ref type="bibr" target="#b36">Harel and Marelly (2003)</ref>, but rephrased in set-theoretic, event-based, terms. We defined this alternative formalization in order to make LSCs compatible with Neo- Davidsonian, event-based, semantic theories. As a result, this form of LSC formalization is well- suited for representing the semantics of natural language utterances.</p><p>Let us assume that L is a dictionary of entities (lifelines), A is a dictionary of actions, P is a dic- tionary of attribute names and V a dictionary of attribute values. The set of simple events in the LSC formal system is defined as follows:</p><formula xml:id="formula_0">E active ⊂ L × A × L × (L × P × V ) * ×{hot, cold} × {executed, monitored} where e = l 1 , a, l 2 , {l i : p i : v i } k i=3</formula><p>, temp, exe and l i ∈ L, a ∈ A, p i ∈ P, temp ∈ {hot, cold}, exe ∈ {executed, monitored}. The event e is called a message in which an action a is carried over from a sender l 1 to a receiver l 2 . <ref type="bibr">3</ref> The set {l i : p i : v i } k i=3 depicts a set of attribute:value pairs provided as arguments to action a. The tem- perature temp marks the modality of the action (may, must), and the status exe distinguishes ac- tions to be taken from actions to be waited for.</p><p>An event e can also refer to a state, where a logical expression is being evaluated over a set of property:value pairs. We call such an event a con- dition, and specify the set of possible conditions as follows:</p><formula xml:id="formula_1">E cond ⊂ Exp × (L × P × V ) * ×{hot, cold} × {executed, monitored}</formula><p>Specifically, e = exp, {l : p : v} k i=0 , temp, exe is a condition to be evaluated, where l i ∈ L, p i ∈ P, v i ∈ V, temp ∈ {hot, cold} and exe ∈ {executed, monitored} are as specified above. The condition exp ∈ Exp is a first-order logic for- mula using the usual operators (∨, ∧, →, ¬, ∃, ∀). The set {l : p : v} k i=0 depicts a (possibly empty) set of attribute:value pairs that participates as pred- icates in exp. Executing a condition, that is, evalu- ating the logical expression specified by exp, also has a modality (may/must) and an execution status (performed/waited for).</p><p>The LSC language further allows us to define more complex events by combining partially or- dered sets of events with control structures.</p><formula xml:id="formula_2">E complex ⊂ N × E cond × {{E c , &lt;||E c , &lt; is a poset }</formula><p>N is a set of non-negative integers, E cond is a set of conditions as described above, and each ele- ment E c , &lt; is a partially ordered set of events. This structure allows us to derive three kinds of control structures:</p><p>• e = #, ∅, E, &lt; is a loop in which E, &lt; is executed # times.</p><p>• e = 0, cond, E, &lt; is a conditioned exe- cution. If cond holds, E, &lt; is executed.</p><p>•</p><formula xml:id="formula_3">e = #, {cond} # i=1 , {{E c , &lt;} # i=1 is a switch: in case i, if the condition i holds, E c , &lt; i is executed.</formula><p>Definition 1 (LSC) An LSC c = E, &lt; is a partially ordered set of events such that</p><formula xml:id="formula_4">∀e ∈ E : e ∈ E active ∨ e ∈ E cond ∨ e ∈ E complex</formula><p>Grounded Semantics: The information repre- sented in the LSC provides the recipe for a rig- orous construction of the code-base that will im- plement the program. This code-base is said to ground the semantic representation. For exam- ple, if our target programming language is an Object-Oriented programming language such as Java, then the code-base will include the objects, the methods and the properties that are minimally required for executing the scenario that is repre- sented by the LSC. We refer to this code-base as a system model (henceforth, SM), and define it as follows.</p><p>Definition 2: (SM) Let L m be a set of imple- mented objects, A m a set of implemented meth- ods, P m a set of arguments and V m argument values. We further define the auxiliary functions methods : A m → L m , props : P m → L m and values : V m → L m × P m , for identifying the entity l ∈ L m that implements the method a ∈ A m , the entity l ∈ L m that contains the property p ∈ P m , and the entity property l, p ∈ L m × P m that assumes that value v ∈ V m , respectively. A system model (SM) is a tuple m representing the implemented architecture.</p><formula xml:id="formula_5">m = L m , A m , P m , V m ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>methods, props, values</head><p>Analogously to interpretation functions in logic and natural language semantics, we assume here an implementation function, denoted <ref type="bibr">[[.]</ref>], which maps each formal entity in the LSC semantic rep- resentation to its instantiation in the code-base. Using this function we define a notion of ground- ing that captures the fact that a certain code-base permits the execution of a given LSC c.</p><p>Definition 3(a): (Grounding) Let M be the set of system models and let C be the set of LSC charts. We say that m grounds c = E, &lt;, and write m c, if ∀e ∈ E : m e, where:</p><formula xml:id="formula_6">• if e ∈ E active then m e ⇔ [[l 1 ]], [[l 2 ]] ∈ L &amp; [[a]] ∈ methods([[l 2 ]]) &amp; ∀i : l : p : v i ⇒ [[l]] ∈ L m &amp;[[p]] ∈ props[[l]]&amp;v ∈ values([[l]], [[p]]) • if e ∈ E cond then m e ⇔ ∀i : l : p : v i ⇒ [[l]] ∈ L m &amp;[[p]] ∈ props[[l]]&amp;v ∈ values([[l]], [[p]]) • if e = #, e s , E c , &lt; ∈ E complex then m e ⇔ m e s &amp; ∀e ∈ E c : m e</formula><p>We have thus far defined how the semantics of a single LSC can be grounded in a single SM. In the real world, however, a requirements document typically contains multiple different requirements, but it is interpreted as a complete whole. The de- sired SM is then one that represents a single do- main shared by all the specified requirements. Let us then assume a document d = d 1 , ..., d n con- taining n requirements, where ∀i : d i ∈ L req , and let be a unification operation that returns the for- mal unification of two SMs if such exists, and an empty SM otherwise. We define a discourse in- terpretation function sem(d) that returns a single SM for the entire document, where different men- tions across sentences may share the same refer- ence. The discourse interpretation function sem can be as simple as unifying all individual SMs for d i , and asserting that all elements that have the same name in different SMs refer to a single ele- ment in the overall SM. Or, it can be as complex as taking into account synonyms ("clicks the button" and "presses the button"), anaphora ("when the user clicks the button, it changes colour"), bind- ing ("when the user clicks any button, this button is highlighted"), and so on. We can now define the grounding of an entire requirements document.</p><formula xml:id="formula_7">Definition 3(b): (Grounding) Let d = d 1 ...d n</formula><p>be a requirements document and let m = m 1 ...m n be a sequence of system models. M = m, is a sequence of models and a unification operation, and M sem(d) if and only if ∀i :</p><formula xml:id="formula_8">m i sem(d i ) and ((m 1 m 2 ).... m n ) sem(d 1 , ...., d n ).</formula><p>In this work we assume that sem(d) is a simple discourse interpretation function, where entities, methods, properties, etc. that are referred to using the same name in different local SMs refer to a sin- gle element in the overall code-base. This simple assumption already carries a substantial amount of disambiguating information concerning individual requirements. For example, assume that we have seen a "click" method over a "button" object in sentence i. This may help us disambiguate future attachment ambiguity, favoring structures where a "button" is attached to "click" over other at- tachment alternatives. Our goal is then to model discourse-level context for supporting the accurate semantic analysis of individual requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Probabilistic Modeling</head><p>In this section we set out to explicitly model the requirement's context, formally captured as a document-level SM, in order to support the accu- rate disambiguation of the requirements' content. We first specify our probabilistic content model, a sentence-level model which is based on a prob- abilistic grammar augmented with compositional semantic rules. We then specify our probabilistic context model, a document-level sequence model that takes into account the content as well as the relation between SMs at different time points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence-Based Modeling</head><p>The task of our sentence-based model is to learn a function that maps each requirement sentence to its correct LSC diagram and SM snapshot. In a nutshell, we do this via a (partially lexi- calized) probabilistic context-free grammar aug- mented with a semantic interpretation function.</p><p>More formally, given a discourse D = d</p><note type="other">1 ...d n we think of each d i as having been generated by a probabilistic context-free grammar (PCFG) G. The syntactic analysis of d i may be ambiguous, so we first implement a syntactic analysis function syn : L req → T req using a probabilistic model that selects the most likely syntax tree t of each d individually. We can simplify syn(d), with d constant with respect to the maximization:</note><formula xml:id="formula_9">syn(d) = argmax t∈Treq P (t|d) = argmax t∈Treq P (t,d) p(d) = argmax t∈Treq P (t, d) = argmax t∈{t|t∈Treq,yield(t)=d} P (t)</formula><p>Because of the context-freeness assumption, it holds that P (t) = r∈der(t) P (r), where der(t) returns the rules that derive t. The resulting proba- bility distribution P : T req → [0, 1] defines a prob- abilistic language model over all requirements d ∈ L req , i.e., d∈Lreq t∈Treq,yield(t)=d P (t) = 1. We assume a function sem : T → C mapping syntactic parse trees to semantic constructs in the LSC language. Syntactic parse trees are complex entities, assigning structures to the flat sequences of words. The principle of compositionality as- serts that the meaning of a complex syntactic en- tity is a function of the meaning of its parts and their mode of combination. Here, the semantics of a tree t ∈ T req is derived compositionally from the interpretation of the rules in the grammar G. We overload the sem notation to define sem : R → C as a function assigning rules to LSC constructs (events or parts of events), <ref type="bibr">4</ref> withˆ•withˆwithˆ• merging the resulting sets of events. Our sentence-based com- positional semantics is summarized as:</p><formula xml:id="formula_10">sem(u) = sem(syn(u)) = sem(r 1 • ... • r n ) = sem(r 1 )ˆ •...ˆ •sem(r n ) = c 1 ˆ •...ˆ •c n = c 4</formula><p>Here, it suffices to say that sem maps edges in the syntax tree to functions in the API of an existing LSC editor. For example: sem(N P → DET N N ) = f CreateObject(DET.sem, N N.sem). We specify the function sem in the supplementary materials. The code of sem is available as part of PlayGo (www.playgo.co).</p><p>For a single chart c, one can easily construct an implementation for every entity, action and prop- erty in the chart. Then, by design, we get an SM m such that m c. To construct the SM of the entire discourse in the sentence-based model we simply return f (d 1 , ..., d n ) = n i=1 m i where ∀i : m i sem(syn(d i )) and unifies different mentions of the same string to a single element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discourse-Based Modeling</head><p>We assume a given document D ∈ D and aim to find the most probable system model M ∈ M that satisfies the requirements. We assume that M re- flects a single domain that the stakeholders have in mind, and we are provided with an ambiguous nat- ural language evidence, an elicited discourse D, in which they convey it. We instantiate this view as a noisy channel model <ref type="bibr" target="#b47">(Shannon, 1948)</ref>, which pro- vides the foundation for many NLP applications, such as speech recognition ( <ref type="bibr" target="#b24">Bahl et al., 1983</ref>) and machine translation ( <ref type="bibr" target="#b26">Brown et al., 1993)</ref>.</p><p>According to the noisy channel model, when a signal is received it does not uniquely identify the message being sent. A probabilistic model is then used to decode the original message. In our case, the signal is the discourse and the message is the overall system model. In formal terms, we want to find a model M that maximises the following:</p><formula xml:id="formula_11">f (D) = argmax M ∈M P (M |D)</formula><p>We can simplify further, using Bayes law, where D is constant with respect to the maximisation.</p><formula xml:id="formula_12">f (D) = argmax M ∈M P (M |D) = argmax M ∈M P (D|M )×P (M ) P (D) = argmax M ∈M P (D|M ) × P (M )</formula><p>We would thus like to estimate two types of prob- ability distributions, P (M ) over the source and P (D|M ) over the channel. Both M and D are structured objects with com- plex internal structure. In order to assign prob- abilities to objects involving such complex struc- tures it is customary to break them down into sim- pler, more basic, events. We know that D = d 1 , d 2 , ..., d n is composed of n individual sen- tences, each representing a certain aspect of the model M . We assume a sequence of snapshots of M that correspond to the timestamps 1...n, that is: m 1 , m 2 , ..., m n ∈ M where ∀i : m i sem(d i ). The complete SM is given by the union of the different snapshots reflected in different require- ments, i.e., M = i m i . We then rephrase:</p><formula xml:id="formula_13">P (M ) = P (m 1 , ..., m n ) P (D|M ) = P (d 1 , ...., d n |m 1 , ..., m n )</formula><p>These events may be seen as points in a high di- mensional space. In actuality, they are too com- plex and would be too hard to estimate directly. We then define two independence assumptions. First, we assume that a system model snapshot at time i depends only on k previous snapshots (a stationary distribution). Secondly, we assume that each sentence i depends only on the SM snapshot at time i. We now get:</p><formula xml:id="formula_14">P (m 1 ...m n ) ≈ i P (m i |m i−1 ...m i−k ) P (d 1 ...d n |m 1 ...m n ) ≈ i P (d i |m i )</formula><p>Furthermore, assuming bi-gram transitions, our objective function is now represented as follows:</p><formula xml:id="formula_15">f (D) = argmax M ∈M n i=1 P (m i |m i−1 )P (d i |m i )</formula><p>Note that m 0 may be empty if the system is im- plemented from scratch, and non-empty if the re- quirements assume an existing code-base, which makes p(m 1 |m 0 ) a non-trivial transition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Decoding</head><p>Our model is in essence a Hidden Markov Model in which states capture SM snapshots, state- transition probabilities model transitions between SM snapshots, and emission probabilities model the verbal description of each state. To implement this, we need to implement a decoding algorithm that searches through all possible state sequences, and a training algorithm that can automatically learn the values of the still rather complex param- eters P (m i |m i−1 ), P (d i |m i ) from data.</p><formula xml:id="formula_16">f (D) = argmax M ∈M decoding n i=1 P (m i |m i−1 )P (d i |m i ) training</formula><p>Training: We assume a supervised training set- ting in which we are given a set of examples anno- tated by a human expert. For instance, these can be requirements an analyst has formulated and en- coded using an LSC editor, while manually pro- viding disambiguating information. We are pro- vided with a set of pairs {D i , M i } n i=1 containing n documents, where each of the pairs in i = 1..n is a tuple set {d ij , t ij , c ij , m ij } n i j=1 . For all i, j, it holds that t ij = syn(d ij ), c ij = sem(t ij ), and m ij sem(syn(d ij )). The union of the n i SM snapshots yields the entire model j m i j = M i , that satisfies the set of requirements M i sem(d i1 , ..., d in i ).</p><p>(i) Emission Parameters Our emission parame- ters P (d i |m i ) represent the probability of a verbal description of a requirement given an SM snap- shot which grounds the semantics of the descrip- tion. A single SM may result from different syn- tactic derivations. We calculate this probability by marginalizing over the syntactic trees that are grounded in the same SM snapshot.</p><formula xml:id="formula_17">P (d, m) P (m) = t∈{t|yield(t)=d,mmsem(t)} P (t)</formula><p>t∈{t|t∈Treq,mmsem(t)} P (t)</p><p>The probability of P (t) is estimated using a tree- bank PCFG <ref type="bibr" target="#b28">(Charniak, 1996)</ref>, based on all pairs d ij , t ij in the annotated corpus. We estimate rule probabilities using maximum-likelihood es- timates, and use simple smoothing for unknown lexical items, using rare-words distributions.</p><p>(ii) Transition Parameters Our transition pa- rameters P (m i |m i−1 ) represent the amount of overlap between the SM snapshots. We look at the current and the previous system model, and aim to estimate how likely the current SM is given the previous one. There are different assumptions that may underly this probability distribution, reflect- ing different principles of human communication.</p><p>We first define a generic estimator as follows:</p><formula xml:id="formula_18">ˆ P (m i |m j ) = gap(m i , m j ) m j gap(m i , m j )</formula><p>where gap(.) quantifies the information sharing between SM snapshots. Regardless of the im- plementation of gap, it can be easily shown thatˆP thatˆ thatˆP is a conditional probability distribution wherêwherê P : M × M → [0, 1] and, for all m i , m j , :</p><formula xml:id="formula_19">m j ˆ P (m i |m j ) = 1.</formula><p>(For efficiency reasons, we consider M to be a restricted universe that is con- sidered be the decoder, as specified shortly.)</p><p>We define different gap implementations, re- flecting different assumptions about the discourse. Our first assumption here is that different SM snapshots refer to the same conceptual world, so there should be a large overlap between them. We call this the max-overlap assumption. A second assumption is that, in collaborative communica- tion, a new requirement will only be stated if it <ref type="table">Table 1</ref>: Quantifying the gap between snapshots. set(m i ) is a set of nodes marked by path to root.</p><formula xml:id="formula_20">Transition: gap(m curr , m prev ) max-overlap |set(mcurr)∩set(mprev)| |set(mcurr)| max-expansion 1 − |set(mcurr)∩set(m prev) | |set(mprev)∪set(mcurr)| min-distance 1 − ted(mprev,mcurr) |set(mprev)|+|set(mcurr)|</formula><p>provides new information, akin to <ref type="bibr" target="#b33">Grice (1975)</ref>. This is the max-expansion assumption. An addi- tional assumption prefers "easy" transitions over "hard" ones, this is the min-distance assumption. The different gap calculations are listed in <ref type="table">Table 1</ref>.</p><p>Decoding An input document contains n re- quirements. Our decoding algorithm considers the N-best syntactic analyses for each requirement. At each time step i = 1...n we assume N, states rep- resenting the semantics of the N best syntax trees, retrieved via a CKY chart parser. Thus, setting N = 1 is equal to a sentence-based model, in which for each sentence we simply select the most likely tree according to a probabilistic grammar, and construct a semantic representation for it.</p><p>For each document of length n, we assume that our entire universe of system models M is com- posed of N × n SM snapshots, reflecting the N most-likely analyses of n sentences, as provided by the probabilistic syntactic model. (As shall be seen shortly, even with this restricted 5 universe ap- proximating M, our discourse-based model pro- vides substantial improvements over a sentence- based model).</p><p>Our discourse-based model is an HMM where each requirement is an observed signal, and each i = 1..N is a state representing the SM that grounds the i th best tree. Because of the Markov independence assumption our setup satis- fies the optimal subproblem and overlapping prob- lem properties, and we can use efficient viterbi de- coding to exhaustively search through the differ- ent state sequences, and find the most probable sequence that has generated the sequence of re- quirements according to our discourse-based prob- abilistic model. The overall complexity decoding a document with n sentences of which max length is l, using a grammar G of size |G| and a fixed N , is given by:</p><formula xml:id="formula_21">O(n × l 3 × |G| 3 + l 2 × N 2 × n + n 3 × N 2 )</formula><p>We can break this expression down as follows: (i) In O(n × l 3 × |G| 3 ) we generate N best trees for each one of the n requirements, using a CKY chart <ref type="bibr" target="#b50">(Younger, 1967)</ref>. (ii) In O(l 2 × N 2 × n) we create the universe M based on the N best trees of the n requirements, and calculate N × N transitions. (iii) In O((N ×n) 2 ×n) = O(N 2 ×n 3 ) we decode the n × N grid using Viterbi (1967) decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Goal. We set out to evaluate the accuracy of a se- mantic parser for requirements documents, in the two modes of analysis presented above. Our eval- uation methodology is as standardly assumed in machine learning and NLP: given a set of anno- tated examples -that is, given a set of require- ments documents, where each requirement is an- notated with its correct LSC representation and each document is associated with a complete SM -we partition this set into a training set and a test set that are disjoint. We train our statistical model on the examples in the training set and automati- cally analyze the requirements in the test set. We then compare the predicted semantic analyses of the test set with the human-annotated (henceforth, gold) semantic analyses of this test set, and empir- ically quantify our prediction accuracy.</p><p>Metrics. Our semantic LSC objects have the form of a tree (reflecting the sequence of nested events in our scenarios). Therefore, we can use standard tree evaluation metrics, such as ParseE- val ( <ref type="bibr" target="#b25">Black et al., 1992)</ref>, to evaluate the accuracy of the prediction. Overall, we define three metrics to evaluate the accuracy of the LSC trees:</p><p>POS: the POS metric is the percentage of part-of-speech tags predicted correctly. LSC-F1: F1 is the harmonic means of the precision and recall of the predicted tree. LSC-EM: EM is 1 if the predicted tree is an exact match to the gold tree, and 0 otherwise.</p><p>In the case of SM trees, as opposed to the LSC trees, we cannot assume identity of the yield be- tween the gold and parse trees for the same sen-   SM-TED: TED is the normalized edit dis- tance between the predicted and gold SM trees, subtracted from a unity. SM-EM: EM is 1 if the predicted SM is an exact match with the gold SM, 0 otherwise.</p><p>Data. We have a small seed of correctly anno- tated requirements-specification case studies that describe simple reactive systems in the LSC lan- guage. Each document contains a sequence of requirements, each of which is annotated with the correct LSC diagram. The entire program is grounded in a java implementation. As training data, we use the case studies provided by . <ref type="table" target="#tab_1">Table 2</ref> lists the case studies and basic statistics concerning these data. As our annotated seed is quite small, it is hard to generalize from it to unseen examples. In particu- lar, we are not guaranteed to have observed all pos- sible structures that are theoretically permitted by the assumed grammar. To cope with this, we cre- ate a synthetic set of examples using the grammar of  in generation mode, and randomly generate trees t ∈ T req .</p><p>The grammar we use to generate the synthetic examples clearly over-generates. That is to say, it creates many trees that do not have a sound in- terpretation. In fact, only 3000 our of 10000 gen- erated examples turn out to have a sound seman- tic interpretation grounded in an SM. Nonetheless, these data allow us to smooth the syntactic distri- butions that are observed in the seed, and increase the coverage of the grammar learned from it.</p><p>Results. <ref type="table" target="#tab_2">Table 3</ref> presents the results for pars- ing the Phone document, our development set, with the sentence-based model, varying the train- ing data. We see that despite the small size of the seed, adding it to our set if synthetics examples substantially improves results over a model trained on synthetic examples only.</p><p>In our next experiment, we provide empirical upper-bounds and lower-bounds for the discourse- based model. <ref type="table" target="#tab_4">Table 4</ref> presents the results of the discourse-based model for N &gt; 1 on the Phone example. Gen-Only presents the results of the discourse-based model with a PCFG learned from synthetic trees only, incorporating transitions obeying the max-overlap assumption. Already here, we see a mild improvement for N &gt; 1 rel- ative to the N = 1 results, indicating that even a weak signal such as the overlap between neighbor- ing sentences already improves sentence disam- biguation in context. We next present the results of an Oracle experiment, where every requirement is assigned the highest scoring tree in terms of LSC- F1 with respect to the gold tree, keeping the same transitions. Again we see that results improve with N , indicating that the syntactic model alone does not provide optimal disambiguation. These re- sults provides an upper bound on the parser perfor- mance for each N . Gen+Seed presents results of the discourse-based model where the PCFG inter- polates the seed set and the synthetic train set, with max-overlap transitions. Here, we see larger im- provements over the synthetic-only PCFG. That is, modeling grammaticality of individual sentences improves the interpretation of the document.</p><p>Next we compare the performance for differ- ent implementations of the gap(m i , m j ) function. We estimate probability distributions that reflect each of the assumptions we discussed, and add an additional method called hybrid, in which we interpolate the max-expansion and max-overlap estimates (equal weights). In <ref type="table" target="#tab_5">Table 5</ref>, the trend from the previous experiment persists. Notably, the hybrid model provides a larger error reduc- tion than its components used separately, indicat- ing that in order to capture discourse context we may need to balance possibly conflicting factors. In no emissions we rely solely on the probability of state transitions, and again increasing N leads to improvement. This result confirms that con- text is indispensable for sentence interpretation - even when probabilities for the sentence's seman-  We finally perform a cross-fold experiment in which we leave one document out as a test set and take the rest as our seed. The results are pro- vided in <ref type="table" target="#tab_6">Table 6</ref>. The discourse-based model out- performs the sentence-based model N = 1 in all cases. Moreover, the drop in N = 128 for Phone seems incidental to this set, and the other cases level off beforehand. Despite our small seed, the persistent improvement on all metrics is consistent with our hypothesis that modeling the interpreta- tion process within the discourse has substantial benefits for automatic understanding of the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications and Discussion</head><p>The statistical models we present here are ap- plied in the context of PlayGo, 7 a comprehensive tool for behavioral, scenario-based, programming. PlayGo now provides two modes of playing-in natural language requirements: interactive play-in, where a user manually disambiguates the analyses of the requirements , and statistical play-in, where disambiguation decisions are taken using our discourse-based model.</p><p>The fragment of English we use is very ex- pressive. It covers not only entities and predi- cates, but also temporal and aspectual information, modalities, and program flow. Beyond that, we as- sume an open-ended lexicon. Overall, we are not only translating English sentences into executable LSCs -we provide a fully generative model for translating a complete document (text) into a com- plete system model (code).</p><p>This text-to-code problem may be thought of as a machine translation (MT) problem, where one aims to translate sentences in English to the formal language of LSCs. However, standard statistical MT techniques rely on the assumption that textual requirements and code are aligned at a sentence level. Creating a formal model that aligns text and code on a sentence-by-sentence basis is precisely our technical contribution in Section 3.</p><p>To our knowledge, modeling syntax and dis- course processing via a fully joint generative model, where a discourse level HMM is in- terleaved with PCFG sentence-based emissions, is novel. By plugging in different models for p(d|m), different languages may be parsed. This method may further be utilized for relating content and context in other tasks: parsing and document- level NER, parsing and document-level IE, etc. To do so, one only needs to redefine the PCFG (emis- sions) and state-overlap (transition) parameters, as appropriate for their data. 8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The requirements understanding task presents an exciting challenge for CL/NLP. We ought to au- tomatically discover the entities in the discourse, the actions they take, conditions, temporal con- straints, and execution modalities. Furthermore, it requires us to extract a single ontology that satis- fies all individual requirements. The contributions of this paper are three-fold: we formalize the text- to-code prediction task, propose a semantic rep- resentation with well-defined grounding, and em- pirically evaluate models for this prediction. We show consistent improvement of discourse-based over sentence-based models, in all case studies. In the future, we intend to extend this model for interpreting requirements in un-restricted, or less- restricted, English, endowed with a more sophisti- cated discourse interpretation function.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Seed Gold-Annotated Requirements</head><label>2</label><figDesc></figDesc><table>N=1 POS LSC-F1 LSC-EM SM-TED SM-EM 
Gen-Only 85.52 
84.40 
9.52 
84.25 
9.52 
Gen+Seed 91.59 
88.05 
14.29 
85.17 
14.29 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Sentence-Based modeling: Accuracy re-
sults on the Phone development set. 

tence, 6 so we cannot use ParseEval. Therefore, we 
implement a distance-based metrics in the spirit of 
Tsarfaty et al. (2012). Then, to evaluate the accu-
racy of the SM, we use two kinds of scores: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>84.40 85.35 86.31 87.51 88.81 89.30 89.51 LSC-EM 9.52 9.52 14.29 14.29 14.29 14.29 14.29 SM-TED 84.25 85.94 89.14 91.90 92.81 93.31 92.70 SM-EM 9.52 19.05 33.33 33.33 33.33 38.10 33.33 Gen+Seed POS 91.78 92.95 93.54 93.35 94.32 94.52 93.93 LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73 LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86 SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76 SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38 Oracle POS 91.98 93.54 94.91 95.30 96.09 96.67 96.87 LSC-F1 88.73 91.33 93.19 94.39 95.11 95.91 96.70 LSC-EM 23.81 42.86 61.90 61.90 66.67 76.19 76.19 SM-TED 86.54 91.28 94.28 94.88 96.24 97.51 98.80 SM-EM 23.81 42.86 66.67 71.43 76.19 76.19 76.19</figDesc><table>System N=2 
4 
8 
16 
32 
64 
128 
Gen-Only 
POS 85.52 86.30 87.67 88.45 88.85 88.85 88.85 
LSC-F1 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Discourse-Based Modeling: Accuracy re- sults on the Phone dev set. The Oracle selects the highest scoring LSC tree among the N-candidates, providing an upper bound on accuracy. Gen-Only selects the most probable tree, relying on synthetic examples only, providing a lower bound. tics (content) are entirely absent.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Discourse-Based modeling: Experiments on the Phone development set. Estimation proce- dure for transition probabilities. All experiments use the Gen+Seed emission probablities.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Cross-Fold Validation for N=1..128. 
Seed+Generated emissions, Hybrid transitions. 

</table></figure>

			<note place="foot" n="1"> Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2 It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branching temporal logic called CTL* (Kugler et al., 2005).</note>

			<note place="foot" n="3"> The LSC language also distinguishes static lifelines from dynamically-bound lifelines. For brevity, we omit this from the formal description of events, and simply assert that it may be listed as one of the properties of the relevant lifeline.</note>

			<note place="foot" n="5"> This restriction is akin to pseudo-likelihood estimation, as in Arnold and Strauss (1991). In pseudo-likelihood estimation, instead of normalizing over the entire set of elements, one uses a subset that reflects only the possible outcomes. Here, instead of summing SM probabilities over all possible sentences in the language, we sum up the SM analyses of the sentences observed in the document only. This estimation could also be addressed via, e.g., sampling methods.</note>

			<note place="foot" n="6"> This is because the LSC trees are predicted bottom up and the SM trees are predicted top-down.</note>

			<note place="foot" n="7"> www.playgo.co.</note>

			<note place="foot" n="8"> Our code, annotated data, four case studies, and the LSC</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Shahar Maoz, Rami Marelly, Yoav Goldberg and three anonymous reviewers for their insightful comments on an earlier draft. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Dist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pos</forename></persName>
		</author>
		<idno>91.98 92.76 93.54 93.35 94.32 94.52 93.93</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lsc-Em</forename><surname>23</surname></persName>
		</author>
		<idno>47.62 47.62 47.62 47.62 47.62</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Ted</forename></persName>
		</author>
		<idno>86.54 91.71 94.38 93.81 95.57 96.43 94.53</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Em</forename><surname>23</surname></persName>
		</author>
		<idno>57.14 57.14 57.14 57.14 57.14</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Ted</forename></persName>
		</author>
		<idno>85.49 90.78 93.59 93.02 94.81 95.69 93.76</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Em</forename></persName>
		</author>
		<idno>19.05 38.10 52.38 52.38 52.38 52.38 52.38</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Expand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pos</forename></persName>
		</author>
		<idno>91.98 92.76 93.74 93.54 94.32 94.52 93.93</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lsc-Em</forename><surname>23</surname></persName>
		</author>
		<idno>47.62 47.62 47.62 47.62 47.62</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Ted</forename></persName>
		</author>
		<idno>86.54 91.93 93.75 93.18 94.79 95.66 93.75</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Em</forename><surname>23</surname></persName>
		</author>
		<idno>57.14 57.14 57.14 57.14 57.14</idno>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Ted</forename></persName>
		</author>
		<idno>85.49 90.78 93.66 93.09 94.87 95.75 93.83</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Em</forename></persName>
		</author>
		<idno>19.05 38.10 57.14 57.14 57.14 57.14 57.14 No Emissions POS 91.78 91.98 92.37 92.37 92.17 92.76 93.15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Ted</forename><surname>85</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Em</forename></persName>
		</author>
		<idno>19.05 19.05 23.81 23.81 23.81 23.81 23.81</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Data Set N=1 32 64 128 Baby Monitor POS 94</title>
		<idno>96.07 96.07 96.07</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lsc-Em</forename></persName>
		</author>
		<idno>14.29 21.43 21.43 21.43</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Ted</forename></persName>
		</author>
		<idno>88.63 91.11 91.11 91.11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Em</forename></persName>
		</author>
		<idno>28.57 50.00 50.00 50.00 Chess POS 92.63 93.68 93.68 93.68</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lsc-Em</forename></persName>
		</author>
		<idno>11.11 11.11 11.11</idno>
		<imprint>
			<biblScope unit="volume">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Ted</forename></persName>
		</author>
		<idno>94.90 97.10 97.10 97.10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sm-Em</forename></persName>
		</author>
		<idno>61.11 66.67 66.67 66.67 Phone POS 91.59 94.72 94.91 94.32</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wristwatch</surname></persName>
		</author>
		<idno>POS 34.23 34.45 34.45 34.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pseudolikelihood Estimation: Some Examples. Sankhy¯ a: The Indian Journal of Statistics, Series B</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-level grammar as an object-oriented requirements specification language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS&apos;02</title>
		<meeting>the 35th Annual Hawaii International Conference on System Sciences (HICSS&apos;02<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">280</biblScope>
		</imprint>
	</monogr>
	<note>HICSS &apos;02</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tree-bank grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1031" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LSCs: Breathing life into message sequence charts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Damm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Form. Methods Syst. Des</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="80" />
			<date type="published" when="2001-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On visualization and comprehension of scenario-based programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE 19th International Conference on Program Comprehension, ICPC &apos;11</title>
		<meeting>the 2011 IEEE 19th International Conference on Program Comprehension, ICPC &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attempto: Controlled natural language for requirements specifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwitter</surname></persName>
		</author>
		<editor>Markus P. J. Fromherz, Marc Kirschenbaum, and Anthony J. Kusalik</editor>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating executable scenarios from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing &apos;09</title>
		<meeting>the 10th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing &apos;09<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="456" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Logic and conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax and Semantics</title>
		<editor>P. Cole and J. L. Morgan</editor>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
	<note>Speech Acts</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On teaching visual formalisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Softw</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Assert and negate revisited: Modal semantics for UML sequence diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 International Workshop on Scenarios and State Machines: Models, Algorithms, and Tools, SCESM &apos;06</title>
		<meeting>the 2006 International Workshop on Scenarios and State Machines: Models, Algorithms, and Tools, SCESM &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Come, Let&apos;s Play: Scenario-Based Programming Using LSCs and the Play-Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Smart play-out of behavioral requirements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kugler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pnueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Formal Methods in Computer-Aided Design, FMCAD &apos;02</title>
		<meeting>the 4th International Conference on Formal Methods in Computer-Aided Design, FMCAD &apos;02<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="378" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Behavioral programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On composing and proving the correctness of reactive behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mizrahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Software (EMSOFT), 2013 Proceedings of the International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Sept</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From play-in scenarios to code: An achievable dream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="2001-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal logic for scenario-based specifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kugler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pnueli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS&apos;05</title>
		<meeting>the 11th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS&apos;05<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="445" to="460" />
		</imprint>
	</monogr>
	<note>Bontemps</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey and classification of controlled natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="170" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From natural language specifications to program input parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bringing machine learning and compositional semantics together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Reviews of Linguistics</title>
		<imprint>
			<biblScope unit="page">0</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Events in the Semantics of English: A study in subatomic semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parsons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Crossframework evaluation for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">W. Daelemans, M. Lapata, and L. M` arquez</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theor</title>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recognition and parsing of context-free languages in time n3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
