<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Neural Templates for Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Neural Templates for Text Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3174" to="3187"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3174</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While neural, encoder-decoder models have had significant empirical success in text generation , there remain several unaddressed problems with this style of generation. Encoder-decoder models are largely (a) uninterpretable, and (b) difficult to control in terms of their phrasing or content. This work proposes a neural generation system using a hidden semi-markov model (HSMM) decoder, which learns latent, discrete templates jointly with learning to generate. We show that this model learns useful templates, and that these templates make generation both more interpretable and controllable. Furthermore, we show that this approach scales to real data sets and achieves strong performance nearing that of encoder-decoder text generation models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the continued success of encoder-decoder models for machine translation and related tasks, there has been great interest in extending these methods to build general-purpose, data-driven nat- ural language generation (NLG) systems <ref type="bibr">(Mei et al., 2016;</ref><ref type="bibr">Dušek and Jurcıcek, 2016;</ref><ref type="bibr">Lebret et al., 2016;</ref><ref type="bibr">Chisholm et al., 2017;</ref><ref type="bibr">Wiseman et al., 2017)</ref>. These encoder-decoder models <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> use a neural encoder model to represent a source knowledge base, and a decoder model to emit a textual description word-by-word, condi- tioned on the source encoding. This style of gen- eration contrasts with the more traditional division of labor in NLG, which famously emphasizes ad- dressing the two questions of "what to say" and "how to say it" separately, and which leads to sys- tems with explicit content selection, macro-and micro-planning, and surface realization compo- nents <ref type="bibr">(Reiter and Dale, 1997;</ref><ref type="bibr" target="#b6">Jurafsky and Martin, 2014)</ref>.  Generation dataset ( <ref type="bibr">Novikova et al., 2017)</ref>. Knowledge base x (top) contains 6 records, andˆyandˆ andˆy (middle) is a system gen- eration; records are shown as type <ref type="bibr">[value]</ref>. An induced neural template (bottom) is learned by the system and em- ployed in generatingˆygeneratingˆ generatingˆy. Each cell represents a segment in the learned segmentation, and "blanks" show where slots are filled through copy attention during generation.</p><formula xml:id="formula_0">| . | It'</formula><p>Encoder-decoder generation systems appear to have increased the fluency of NLG outputs, while reducing the manual effort required. However, due to the black-box nature of generic encoder- decoder models, these systems have also largely sacrificed two important desiderata that are often found in more traditional systems, namely (a) in- terpretable outputs that (b) can be easily controlled in terms of form and content.</p><p>This work considers building interpretable and controllable neural generation systems, and pro- poses a specific first step: a new data-driven gen- eration model for learning discrete, template-like structures for conditional text generation. The core system uses a novel, neural hidden semi- markov model (HSMM) decoder, which provides a principled approach to template-like text gener- ation. We further describe efficient methods for training this model in an entirely data-driven way by backpropagation through inference. Generat- ing with the template-like structures induced by the neural HSMM allows for the explicit repre- sentation of what the system intends to say (in the form of a learned template) and how it is attempt- ing to say it (in the form of an instantiated tem- plate).</p><p>We show that we can achieve performance com- petitive with other neural NLG approaches, while making progress satisfying the above two desider- ata. Concretely, our experiments indicate that we can induce explicit templates (as shown in <ref type="figure" target="#fig_0">Figure  1</ref>) while achieving competitive automatic scores, and that we can control and interpret our gener- ations by manipulating these templates. Finally, while our experiments focus on the data-to-text regime, we believe the proposed methodology rep- resents a compelling approach to learning discrete, latent-variable representations of conditional text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A core task of NLG is to generate textual descrip- tions of knowledge base records. A common ap- proach is to use hand-engineered templates ( <ref type="bibr" target="#b10">Kukich, 1983;</ref><ref type="bibr">McKeown, 1992;</ref><ref type="bibr">McRoy et al., 2000</ref>), but there has also been interest in creating tem- plates in an automated manner. For instance, many authors induce templates by clustering sen- tences and then abstracting templated fields with hand-engineered rules ( <ref type="bibr" target="#b0">Angeli et al., 2010;</ref>, or with a pipeline of other automatic approaches ( <ref type="bibr">Wang and Cardie, 2013)</ref>.</p><p>There has also been work in incorporating prob- abilistic notions of templates into generation mod- els ( <ref type="bibr">Liang et al., 2009;</ref><ref type="bibr" target="#b9">Konstas and Lapata, 2013)</ref>, which is similar to our approach. However, these approaches have always been conjoined with dis- criminative classifiers or rerankers in order to ac- tually accomplish the generation ( <ref type="bibr" target="#b0">Angeli et al., 2010;</ref><ref type="bibr" target="#b9">Konstas and Lapata, 2013)</ref>. In addition, these models explicitly model knowledge base field selection, whereas the model we present is fundamentally an end-to-end model over genera- tion segments.</p><p>Recently, a new paradigm has emerged around neural text generation systems based on machine translation <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>). Most of this work has used unconstrained black-box encoder- decoder approaches. There has been some work on discrete variables in this context, including ex- tracting representations <ref type="bibr">(Shen et al., 2018)</ref>, incor- porating discrete latent variables in text model- ing ( <ref type="bibr">Yang et al., 2018)</ref>, and using non-HSMM seg- mental models for machine translation or summa- rization ( <ref type="bibr">Yu et al., 2016;</ref><ref type="bibr">Wang et al., 2017;</ref><ref type="bibr" target="#b5">Huang et al., 2018)</ref>. <ref type="bibr">Dai et al. (2017)</ref> develop an approx- imate inference scheme for a neural HSMM using RNNs for continuous emissions; in contrast we maximize the exact log-marginal, and use RNNs to parameterize a discrete emission distribution. Finally, there has also been much recent interest in segmental RNN models for non-generative tasks in NLP ( <ref type="bibr">Tang et al., 2016;</ref><ref type="bibr" target="#b8">Kong et al., 2016;</ref><ref type="bibr">Lu et al., 2016)</ref>.</p><p>The neural text generation community has also recently been interested in "controllable" text gen- eration ( <ref type="bibr" target="#b4">Hu et al., 2017)</ref>, where various aspects of the text (often sentiment) are manipulated or transferred <ref type="bibr">(Shen et al., 2017;</ref><ref type="bibr">Zhao et al., 2018;</ref><ref type="bibr" target="#b5">Li et al., 2018)</ref>. In contrast, here we focus on control- ling either the content of a generation or the way it is expressed by manipulating the (latent) template used in realizing the generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview: Data-Driven NLG</head><p>Our focus is on generating a textual description of a knowledge base or meaning representation. Following standard notation ( <ref type="bibr">Liang et al., 2009;</ref><ref type="bibr">Wiseman et al., 2017)</ref>, let x = {r 1 . . . r J } be a collection of records. A record is made up of a type (r.t), an entity (r.e), and a value (r.m). For example, a knowledge base of restaurants might have a record with r.t = Cuisine, r.e = Denny's, and r.m = American. The aim is to generate an adequate and fluent text descriptionˆy descriptionˆ descriptionˆy 1:T = ˆ y 1 , . . . , ˆ y T of x. Concretely, we consider the E2E Dataset ( <ref type="bibr">Novikova et al., 2017</ref>) and the WikiBio Dataset ( <ref type="bibr">Lebret et al., 2016)</ref>. We show an example E2E knowledge base x in the top of <ref type="figure" target="#fig_0">Figure 1</ref>. The top of <ref type="figure" target="#fig_3">Figure 2</ref> shows an exam- ple knowledge base x from the WikiBio dataset, where it is paired with a reference text y = y 1:T at the bottom.</p><p>The dominant approach in neural NLG has been   <ref type="bibr">March 1914</ref><ref type="bibr">-21 November 1987</ref> was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist.".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Modeling for Constrained Sentence generation</head><p>Conditional language models are a popular choice to generate sentences. We introduce a table- conditioned language model for constraining text generation to include elements from fact tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language model</head><p>Given a sentence s = w 1 , . . . , w T with T words from vocabulary W, a language model estimates:</p><formula xml:id="formula_1">P (s) = T t=1 P (w t |w 1 , . . . , w t−1 ) .<label>(1)</label></formula><p>Let c t = w t−(n−1) , . . . , w t−1 be the sequence of n − 1 context words preceding w t . An n-gram lan- guage model makes an order n Markov assumption,</p><formula xml:id="formula_2">P (s) ≈ T t=1 P (w t |c t ) .<label>(2)</label></formula><p>3.   to use an encoder network over x and then a condi- tional decoder network to generate y, training the whole system in an end-to-end manner. To gener- ate a description for a given example, a black-box network (such as an RNN) is used to produce a dis- tribution over the next word, from which a choice is made and fed back into the system. The entire distribution is driven by the internal states of the neural network.</p><p>While effective, relying on a neural decoder makes it difficult to understand what aspects of x are correlated with a particular system output. This leads to problems both in controlling fine- grained aspects of the generation process and in interpreting model mistakes.</p><p>As an example of why controllability is im- portant, consider the records in <ref type="figure" target="#fig_0">Figure 1</ref>. Given these inputs an end-user might want to generate an output meeting specific constraints, such as not mentioning any information relating to customer rating. Under a standard encoder-decoder style model, one could filter out this information either from the encoder or decoder, but in practice this would lead to unexpected changes in output that might propagate through the whole system.</p><p>As an example of the difficulty of interpret- ing mistakes, consider the following actual gen- eration from an encoder-decoder style system for the records in <ref type="figure" target="#fig_3">Figure 2</ref>: "frederick parker-rhodes <ref type="bibr">(21 november 1914 -2 march 1987)</ref> was an en- glish mycology and plant pathology, mathematics at the university of uk." In addition to not being fluent, it is unclear what the end of this sentence is even attempting to convey: it may be attempt- ing to convey a fact not actually in the knowledge base (e.g., where Parker-Rhodes studied), or per- haps it is simply failing to fluently realize infor- mation that is in the knowledge base (e.g., Parker- Rhodes's country of residence).</p><p>Traditional NLG systems <ref type="bibr" target="#b10">(Kukich, 1983;</ref><ref type="bibr">McKeown, 1992;</ref><ref type="bibr">Belz, 2008;</ref><ref type="bibr">Gatt and Reiter, 2009)</ref>, in contrast, largely avoid these problems. Since they typically employ an explicit planning component, which decides which knowledge base records to focus on, and a surface realization component, which realizes the chosen records, the intent of the system is always explicit, and it may be modified to meet constraints.</p><p>The goal of this work is to propose an approach to neural NLG that addresses these issues in a prin- cipled way. We target this goal by proposing a new model that generates with template-like ob- jects induced by a neural HSMM (see <ref type="figure" target="#fig_0">Figure 1</ref>). Templates are useful here because they represent a fixed plan for the generation's content, and be- cause they make it clear what part of the genera- tion is associated with which record in the knowl- edge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Background: Semi-Markov Models</head><p>What does it mean to learn a template? It is nat- ural to think of a template as a sequence of typed text-segments, perhaps with some segments acting as the template's "backbone" ( <ref type="bibr">Wang and Cardie, 2013)</ref>, and the remaining segments filled in from the knowledge base.</p><p>A natural probabilistic model conforming with this intuition is the hidden semi-markov model (HSMM) ( <ref type="bibr">Gales and Young, 1993;</ref><ref type="bibr">Ostendorf et al., 1996</ref>), which models latent segmentations in an output sequence. Informally, an HSMM is much like an HMM, except emissions may last multiple time-steps, and multi-step emissions need not be independent of each other conditioned on the state.</p><p>We briefly review HSMMs following <ref type="bibr">Murphy (2002)</ref>. Assume we have a sequence of ob- served tokens y 1 . . . y T and a discrete, latent state z t ∈ {1, . . . , K} for each timestep. We addition-ally use two per-timestep variables to model multi- step segments: a length variable l t ∈ {1, . . . , L} specifying the length of the current segment, and a deterministic binary variable f t indicating whether a segment finishes at time t. We will consider in particular conditional HSMMs, which condition on a source x, essentially giving us an HSMM de- coder.</p><p>An HSMM specifies a joint distribution on the observations and latent segmentations. Letting θ denote all the parameters of the model, and using the variables introduced above, we can write the corresponding joint-likelihood as follows</p><formula xml:id="formula_3">p(y, z, l, f | x; θ) = T −1 t=0 p(z t+1 , l t+1 | z t , l t , x) ft × T t=1 p(y t−lt+1:t | z t , l t , x) ft ,</formula><p>where we take z 0 to be a distinguished start- state, and the deterministic f t variables are used for excluding non-segment log probabilities. We</p><formula xml:id="formula_4">further assume p(z t+1 , l t+1 | z t , l t , x) factors as p(z t+1 | z t , x) × p(l t+1 | z t+1 ).</formula><p>Thus, the likeli- hood is given by the product of the probabilities of each discrete state transition made, the proba- bility of the length of each segment given its dis- crete state, and the probability of the observations in each segment, given its state and length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Neural HSMM Decoder</head><p>We use a novel, neural parameterization of an HSMM to specify the probabilities in the likeli- hood above. This full model, sketched out in <ref type="figure">Fig- ure 3</ref>, allows us to incorporate the modeling com- ponents, such as LSTMs and attention, that make neural text generation effective, while maintaining the HSMM structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameterization</head><p>Since our model must condition on x, let r j ∈ R d represent a real embedding of record r j ∈ x, and let x a ∈ R d represent a real embedding of the en- tire knowledge base x, obtained by max-pooling coordinate-wise over all the r j . It is also useful to have a representation of just the unique types of records that appear in x, and so we also define x u ∈ R d to be the sum of the embeddings of the unique types appearing in x, plus a bias vector and followed by a ReLU nonlinearity. <ref type="figure">Figure 3</ref>: HSMM factor graph (under a known segmenta- tion) to illustrate parameters. Here we assume z1 is in the "red" state (out of K possibilities), and transitions to the "blue" state after emitting three words. The transition model, shown as T , is a function of the two states and the neural en- coded source x. The emission model is a function of a "red" RNN model (with copy attention over x) that generates words 1, 2 and 3. After transitioning, the next word y4 is generated by the "blue" RNN, but independently of the previous words.</p><formula xml:id="formula_5">x z 1 RNN y 1 y 2 y 3 y 4 RNN z 4 T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transition Distribution</head><p>The transition distribu- tion p(z t+1 | z t , x) may be viewed as a K × K ma- trix of probabilities, where each row sums to 1. We define this matrix to be</p><formula xml:id="formula_6">p(z t+1 | z t , x) ∝ AB + C(x u )D(x u ),</formula><p>where A ∈ R K×m 1 , B ∈ R m 1 ×K are state embed- dings, and where C : R d → R K×m 2 and D : R d → R m 2 ×K are parameterized non-linear func- tions of x u . We apply a row-wise softmax to the resulting matrix to obtain the desired probabilities.</p><p>Length Distribution We simply fix all length probabilities p(l t+1 | z t+1 ) to be uniform up to a maximum length L. 1</p><p>Emission Distribution The emission model models the generation of a text segment condi- tioned on a latent state and source information, and so requires a richer parameterization. Inspired by the models used for neural NLG, we base this model on an RNN decoder, and write a segment's probability as a product over token-level probabil- ities,</p><formula xml:id="formula_7">p(y t−lt+1:t | z t = k, l t = l, x) = lt i=1 p(y t−lt+i | y t−lt+1:t−lt+i−1 , z t = k, x) × p(&lt;/seg&gt; | y t−lt+1:t , z t = k, x) × 1 {lt = l} ,</formula><p>where &lt;/seg&gt; is an end of segment token. The RNN decoder uses attention and copy-attention over the embedded records r j , and is conditioned on z t = k by concatenating an embedding corre- sponding to the k'th latent state to the RNN's in- put; the RNN is also conditioned on the entire x by initializing its hidden state with x a .</p><p>More concretely, let h k i−1 ∈ R d be the state of an RNN conditioned on x and z t = k (as above) run over the sequence y t−lt+1:t−lt+i−1 . We let the model attend over records r j using h k i−1 (in the style of <ref type="bibr">Luong et al. (2015)</ref>), producing a context vector c k i−1 . We may then obtain scores v i−1 for each word in the output vocabulary,</p><formula xml:id="formula_8">v i−1 = W tanh(g k 1 • [h k i−1 , c k i−1 ]),</formula><p>with parameters g k 1 ∈ R 2d and W ∈ R V ×2d . Note that there is a g k 1 vector for each of K discrete states. To additionally implement a kind of slot filling, we allow emissions to be directly copied from the value portion of the records r j using copy attention ( <ref type="bibr">Gülçehre et al., 2016;</ref><ref type="bibr">Gu et al., 2016;</ref><ref type="bibr">Yang et al., 2016)</ref>. Define copy scores,</p><formula xml:id="formula_9">ρ j = r T j tanh(g k 2 • h k i−1 ),</formula><p>where g k 2 ∈ R d . We then normalize the output- vocabulary and copy scores together, to arrive at</p><formula xml:id="formula_10">v i−1 = softmax([v i−1 , ρ 1 , . . . , ρ J ]),</formula><p>and thus</p><formula xml:id="formula_11">p(y t−lt+i = w | y t−lt+1:t−lt+i−1 , z t = k, x) = v i−1,w + j:r j .m = w v i−1,V +j .</formula><p>An Autoregressive Variant The model as spec- ified assumes segments are independent condi- tioned on the associated latent state and x. While this assumption still allows for reasonable perfor- mance, we can tractably allow interdependence between tokens (but not segments) by having each next-token distribution depend on all the previ- ously generated tokens, giving us an autoregres- sive HSMM. For this model, we will in fact use p(y t−lt+i = w | y 1:t−lt+i−1 , z t = k, x) in defining our emission model, which is easily implemented by using an additional RNN run over all the pre- ceding tokens. We will report scores for both non-autoregressive and autoregressive HSMM de- coders below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning</head><p>The model requires fitting a large set of neu- ral network parameters. Since we assume z, l, and f are unobserved, we marginalize over these variables to maximize the log marginal-likelihood of the observed tokens y given x. The HSMM marginal-likelihood calculation can be carried out efficiently with a dynamic program analogous to either the forward-or backward-algorithm famil- iar from HMMs <ref type="bibr">(Rabiner, 1989)</ref>. It is actually more convenient to use the backward-algorithm formulation when using RNNs to parameterize the emission distributions, and we briefly review the backward recurrences here, again following Murphy (2002). We have:</p><formula xml:id="formula_12">β t (j) = p(y t+1:T | z t = j, f t = 1, x) = K k=1 β * t (k) p(z t+1 = k | z t = j) β * t (k) = p(y t+1:T | z t+1 = k, f t = 1, x) = L l=1 β t+l (k) p(l t+1 = l | z t+1 = k) p(y t+1:t+l | z t+1 = k, l t+1 = l) ,</formula><p>with base case β T (j) = 1. We can now obtain the marginal probability of y as</p><formula xml:id="formula_13">p(y | x) = K k=1 β * 0 (k) p(z 1 = k),</formula><p>where we have used the fact that f 0 must be 1, and we therefore train to maximize the log-marginal likelihood of the observed y:</p><formula xml:id="formula_14">ln p(y | x; θ) = ln K k=1 β * 0 (k) p(z 1 = k).<label>(1)</label></formula><p>Since the quantities in (1) are obtained from a dynamic program, which is itself differentiable, we may simply maximize with respect to the pa- rameters θ by back-propagating through the dy- namic program; this is easily accomplished with automatic differentiation packages, and we use pytorch ( <ref type="bibr">Paszke et al., 2017</ref>) in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Extracting Templates and Generating</head><p>After training, we could simply condition on a new database and generate with beam search, as is stan- dard with encoder-decoder models. However, the structured approach we have developed allows us to generate in a more template-like way, giving us more interpretable and controllable generations.</p><p>[ subscripted numbers indicate the corresponding latent state. From this we can extract a template with S = 17 segments; compare with the template used at the bottom of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><note type="other">The Golden Palace] 55 [is a] 59 [coffee shop] 12 [providing]</note><p>First, note that given a database x and refer- ence generation y we can obtain the MAP assign- ment to the variables z, l, and f with a dynamic program similar to the Viterbi algorithm familiar from HMMs. These assignments will give us a typed segmentation of y, and we show an example Viterbi segmentation of some training text in <ref type="figure">Fig- ure 4</ref>. Computing MAP segmentations allows us to associate text-segments (i.e., phrases) with the discrete labels z t that frequently generate them. These MAP segmentations can be used in an ex- ploratory way, as a sort of dimensionality reduc- tion of the generations in the corpus. More im- portantly for us, however, they can also be used to guide generation.</p><p>In particular, since each MAP segmentation im- plies a sequence of hidden states z, we may run a template extraction step, where we collect the most common "templates" (i.e., sequences of hid- den states) seen in the training data. Each "tem- plate" z (i) consists of a sequence of latent states, with</p><formula xml:id="formula_15">z (i) = z (i) 1 , . . . z (i)</formula><p>S representing the S distinct segments in the i'th extracted template (recall that we will technically have a z t for each time-step, and so z (i) is obtained by collapsing adjacent z t 's with the same value); see <ref type="figure">Figure 4</ref> for an example template (with S = 17) that can be extracted from the E2E corpus. The bottom of <ref type="figure" target="#fig_0">Figure 1</ref> shows a visualization of this extracted template, where dis- crete states are replaced by the phrases they fre- quently generate in the training data.</p><p>With our templates z (i) in hand, we can then restrict the model to using (one of) them during generation. In particular, given a new input x, we may generate by computingˆy</p><formula xml:id="formula_16">computingˆ computingˆy (i) = arg max y p(y , z (i) | x),<label>(2)</label></formula><p>which gives us a generationˆygenerationˆ generationˆy (i) for each extracted template z (i) . For example, the generation in <ref type="figure" target="#fig_0">Fig- ure 1</ref> is obtained by maximizing (2) with x set to the database in <ref type="figure" target="#fig_0">Figure 1</ref> and z (i) set to the template extracted in <ref type="figure">Figure 4</ref>. In practice, the arg max in (2) will be intractable to calculate exactly due to the use of RNNs in defining the emission distribu- tion, and so we approximate it with a constrained beam search. This beam search looks very similar to that typically used with RNN decoders, except the search occurs only over a segment, for a par- ticular latent state k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>Returning to the discussion of controllability and interpretability, we note that with the proposed model (a) it is possible to explicitly force the gen- eration to use a chosen template z (i) , which is it- self automatically learned from training data, and (b) that every segment in the generatedˆygeneratedˆ generatedˆy (i) is typed by its corresponding latent variable. We ex- plore these issues empirically in Section 7.1. We also note that these properties may be use- ful for other text applications, and that they offer an additional perspective on how to approach la- tent variable modeling for text. Whereas there has been much recent interest in learning continuous latent variable representations for text (see Sec- tion 2), it has been somewhat unclear what the la- tent variables to be learned are intended to capture. On the other hand, the latent, template-like struc- tures we induce here represent a plausible, proba- bilistic latent variable story, and allow for a more controllable method of generation.</p><p>Finally, we highlight one significant possible is- sue with this model -the assumption that seg- ments are independent of each other given the cor- responding latent variable and x. Here we note that the fact that we are allowed to condition on x is quite powerful. Indeed, a clever encoder could capture much of the necessary interdependence between the segments to be generated (e.g., the correct determiner for an upcoming noun phrase) in its encoding, allowing the segments themselves to be decoded more or less independently, given x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Data and Methods</head><p>Our experiments apply the approach outlined above to two recent, data-driven NLG tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>Experiments use the E2E ( <ref type="bibr">Novikova et al., 2017)</ref> and WikiBio ( <ref type="bibr">Lebret et al., 2016</ref>) datasets, ex- amples of which are shown in <ref type="figure" target="#fig_0">Figures 1 and 2</ref>, respectively. The former dataset, used for the mately 50K total examples, and uses 945 distinct word types, and the latter dataset contains approx- imately 500K examples and uses approximately 400K word types. Because our emission model uses a word-level copy mechanism, any record with a phrase consisting of n words as its value is replaced with n positional records having a single word value, following the preprocessing of <ref type="bibr">Lebret et al. (2016)</ref>. For example, "type[coffee shop]" in <ref type="figure" target="#fig_0">Figure 1</ref> becomes "type-1[coffee]" and "type- 2 <ref type="bibr">[shop]</ref>."</p><p>For both datasets we compare with published encoder-decoder models, as well as with direct template-style baselines. The E2E task is eval- uated in terms of BLEU ( <ref type="bibr">Papineni et al., 2002</ref>), NIST ( <ref type="bibr">Belz and Reiter, 2006</ref>), ROUGE <ref type="bibr">(Lin, 2004</ref>), <ref type="bibr">CIDEr (Vedantam et al., 2015)</ref>, and ME- TEOR ( <ref type="bibr">Banerjee and Lavie, 2005</ref>). <ref type="bibr">2</ref> The bench- mark system for the task is an encoder-decoder style system followed by a reranker, proposed by <ref type="bibr">Dušek and Jurcıcek (2016)</ref>. We compare to this baseline, as well as to a simple but competitive non-parametric template-like baseline ("SUB" in tables), which selects a training sentence with records that maximally overlap (without including extraneous records) the unseen set of records we wish to generate from; ties are broken at random. Then, word-spans in the chosen training sentence are aligned with records by string-match, and re- placed with the corresponding fields of the new set of records. <ref type="bibr">3</ref> The WikiBio dataset is evaluated in terms of BLEU, NIST, and ROUGE, and we compare with the systems and baselines implemented by <ref type="bibr">Lebret et al. (2016)</ref>, which include two neural, encoder- decoder style models, as well as a Kneser-Ney, templated baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model and Training Details</head><p>We first emphasize two additional methodological details important for obtaining good performance.</p><p>Constraining Learning We were able to learn more plausible segmentations of y by constraining the model to respect word spans y t+1:t+l that ap- pear in some record r j ∈ x. We accomplish this by giving zero probability (within the backward re-currences in Section 5) to any segmentation that splits up a sequence y t+1:t+l that appears in some r j , or that includes y t+1:t+l as a subsequence of another sequence. Thus, we maximize (1) subject to these hard constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increasing the Number of Hidden States</head><p>While a larger K allows for a more expressive la- tent model, computing K emission distributions over the vocabulary can be prohibitively expen- sive. We therefore tie the emission distribution be- tween multiple states, while allowing them to have a different transition distributions.</p><p>We give additional architectural details of our model in the Supplemental Material; here we note that we use an MLP to embed r j ∈ R d , and a 1- layer LSTM <ref type="bibr" target="#b2">(Hochreiter and Schmidhuber, 1997</ref>) in defining our emission distributions. In order to reduce the amount of memory used, we restrict our output vocabulary (and thus the height of the ma- trix W in Section 5) to only contain words in y that are not present in x; any word in y present in x is assumed to be copied. In the case where a word y t appears in a record r j (and could therefore have been copied), the input to the LSTM at time t+1 is computed using information from r j ; if there are multiple r j from which y t could have been copied, the computed representations are simply averaged.</p><p>For all experiments, we set d = 300 and L = 4. At generation time, we select the 100 most com- mon templates z (i) , perform beam search with a beam of size 5, and select the generation with the highest overall joint probability.</p><p>For our E2E experiments, our best non- autoregressive model has 55 "base" states, dupli- cated 5 times, for a total of K = 275 states, and our best autoregressive model uses K = 60 states, without any duplication. For our WikiBio exper- iments, both our best non-autoregressive and au- toregressive models uses 45 base states duplicated 3 times, for a total of K = 135 states. In all cases, K was chosen based on BLEU performance on held-out validation data. Code implementing our models is available at https://github.com/ harvardnlp/neural-template-gen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Our results on automatic metrics are shown in <ref type="table" target="#tab_6">Tables 1 and 2</ref>. In general, we find that the templated baselines underperform neural models, whereas our proposed model is fairly competi- tive with neural models, and sometimes even out-  <ref type="bibr">(2016)</ref>, which forms the baseline for the E2E challenge, a non-parametric, substitution-based baseline (see text), and our HSMM models (denoted "NTemp" and "NTemp+AR" for the non-autoregressive and autoregressive versions, resp.) on the validation and test portions of the E2E dataset. "ROUGE" is ROUGE-L. Models are evaluated using the of- ficial E2E NLG Challenge scoring scripts.   <ref type="formula" target="#formula_1">(2016)</ref>, their templated baseline, and our HSMM models (denoted "NTemp" and "NTemp+AR" for the non- autoregressive and autoregressive versions, resp.) on the test portion of the WikiBio dataset. Models marked with a † are from <ref type="bibr">Lebret et al. (2016)</ref>, and following their methodology we use ROUGE-4. Bottom: state-of-the-art seq2seq-style re- sults from <ref type="bibr">Liu et al. (2018).</ref> performs them. On the E2E data, for example, we see in <ref type="table" target="#tab_8">Table 1</ref> that the SUB baseline, despite having fairly impressive performance for a non- parametric model, fares the worst. The neural HSMM models are largely competitive with the encoder-decoder system on the validation data, de- spite offering the benefits of interpretability and controllability; however, the gap increases on test. <ref type="table" target="#tab_6">Table 2</ref> evaluates our system's performance on the test portion of the WikiBio dataset, compar- ing with the systems and baselines implemented by <ref type="bibr">Lebret et al. (2016)</ref>. Again for this dataset we see that their templated Kneser-Ney model under- performs on the automatic metrics, and that neu- ral models improve on these results. Here the HSMMs are competitive with the best model of <ref type="bibr">Lebret et al. (2016)</ref>, and even outperform it on ROUGE. We emphasize, however, that recent, so- phisticated approaches to encoder-decoder style  <ref type="table">Table 3</ref>: Impact of varying the template z (i) for a single x from the E2E validation data; generations are annotated with the segmentations of the chosen z (i) . Results were obtained using the NTemp+AR model from <ref type="table" target="#tab_8">Table 1.</ref> database-to-text generation have since surpassed the results of <ref type="bibr">Lebret et al. (2016)</ref> and our own, and we show the recent seq2seq style results of <ref type="bibr">Liu et al. (2018)</ref>, who use a somewhat larger model, at the bottom of Table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Qualitative Evaluation</head><p>We now qualitatively demonstrate that our gener- ations are controllable and interpretable.</p><p>Controllable Diversity One of the powerful as- pects of the proposed approach to generation is that we can manipulate the template z (i) while leaving the database x constant, which allows for easily controlling aspects of the generation. In <ref type="table">Ta- ble 3</ref> we show the generations produced by our model for five different neural template sequences z (i) , while fixing x. There, the segments in each generation are annotated with the latent states de- termined by the corresponding z (i) . We see that these templates can be used to affect the word- ordering, as well as which fields are mentioned in the generated text. Moreover, because the discrete states align with particular fields (see below), it is generally simple to automatically infer to which fields particular latent states correspond, allowing users to choose which template best meets their re- quirements. We emphasize that this level of con- trollability is much harder to obtain for encoder- decoder models, since, at best, a large amount of sampling would be required to avoid generating around a particular mode in the conditional distri- bution, and even then it would be difficult to con- trol the sort of generations obtained. kenny warren name: kenny warren, birth date: 1 april 1946, birth name: kenneth warren deutscher, birth place: brooklyn, new york, occupation: ventriloquist, comedian, author, notable work: book -the revival of ventriloquism in america  <ref type="table">Table 4</ref>: Impact of varying the template z (i) for a single x from the WikiBio validation data; generations are annotated with the segmentations of the chosen z (i) . Results were obtained using the NTemp model from <ref type="table" target="#tab_6">Table 2</ref>.</p><p>Interpretable States Discrete states also pro- vide a method for interpreting the generations pro- duced by the system, since each segment is explic- itly typed by the current hidden state of the model. <ref type="table">Table 4</ref> shows the impact of varying the template z (i) for a single x from the WikiBio dataset. While there is in general surprisingly little stylistic varia- tion in the WikiBio data itself, there is variation in the information discussed, and the templates cap- ture this. Moreover, we see that particular discrete states correspond in a consistent way to particular pieces of information, allowing us to align states with particular field types. For instance, birth names have the same hidden state (132), as do names (117), nationalities (82), birth dates (101), and occupations <ref type="bibr">(20)</ref>.</p><p>To demonstrate empirically that the learned states indeed align with field types, we calculate the average purity of the discrete states learned for both datasets in <ref type="table" target="#tab_10">Table 5</ref>. In particular, for each discrete state for which the majority of its gen- erated words appear in some r j , the purity of a state's record type alignment is calculated as the percentage of the state's words that come from the most frequent record type the state represents. This calculation was carried out over training ex- amples that belonged to one of the top 100 most frequent templates. <ref type="table" target="#tab_10">Table 5</ref> indicates that discrete states learned on the E2E data are quite pure. Dis- crete states learned on the WikiBio data are less pure, though still rather impressive given that there are approximately 1700 record types represented in the WikiBio data, and we limit the number of states to 135. Unsurprisingly, adding autoregres- siveness to the model decreases purity on both datasets, since the model may rely on the autore- gressive RNN for typing, in addition to the state's identity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We have developed a neural, template-like gen- eration model based on an HSMM decoder, which can be learned tractably by backpropagat- ing through a dynamic program. The method al- lows us to extract template-like latent objects in a principled way in the form of state sequences, and then generate with them. This approach scales to large-scale text datasets and is nearly competi- tive with encoder-decoder models. More impor- tantly, this approach allows for controlling the diversity of generation and for producing inter- pretable states during generation. We view this work both as the first step towards learning dis- crete latent variable template models for more dif- ficult generation tasks, as well as a different per- spective on learning latent variable text models in general. Future work will examine encouraging the model to learn maximally different (or mini- mal) templates, which our objective does not ex- plicitly encourage, templates of larger textual phe- nomena, such as paragraphs and documents, and hierarchical templates. A Supplemental Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Model and Training Details</head><p>Computing r j A record r j is represented by embedding a feature for its type, its position, and its word value in R d , and applying an MLP with ReLU nonlinearity <ref type="bibr">(Nair and Hinton, 2010)</ref> to form r j ∈ R d , similar to <ref type="bibr">Yang et al. (2016)</ref> and <ref type="bibr">Wiseman et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM Details</head><p>The initial cell and hidden- state values for the decoder LSTM are given by Q 1 x a and tanh(Q 2 x a ), respectively, where</p><formula xml:id="formula_17">Q 1 , Q 2 ∈ R d×d .</formula><p>When a word y t appears in a record r j , the input to the LSTM at time t + 1 is computed using an MLP with ReLU nonlinearity over the concatena- tion of the embeddings for r j 's record type, word value, position, and a feature for whether it is the final position for the type. If there are multiple r j from which y t could have been copied, the com- puted representations are averaged. At test time, we use the MAP r j to compute the input, even if there are multiple matches. For y t which could not have been copied, the input to the LSTM at time t + 1 is computed using the same MLP over y t and three dummy features.</p><p>For the autoregressive HSMM, an additional 1- layer LSTM with d hidden units is used. We ex- perimented with having the autoregressive HSMM consume either tokens y 1:t in predicting y t+1 , or the average embedding of the field types corre- sponding to copied tokens in y 1:t . The former worked slightly better for the WikiBio dataset (where field types are more ambiguous), while the latter worked slightly better for the E2E dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transition</head><p>Distribution The function C(x u ), which produces hidden state em- beddings conditional on the source, is de- fined as C(x u ) = U 2 (ReLU <ref type="figure" target="#fig_0">(U 1 x u )</ref>), where U 1 ∈ R m 3 ×d and U 2 ∈ R K×m 2 ×m 3 ; D(x) is de- fined analogously. For all experiments, m 1 = 64, m 2 = 32, and m 3 = 64.</p><p>Optimization We train with SGD, using a learn- ing rate of 0.5 and decaying by 0.5 each epoch after the first epoch in which validation log- likelihood fails to increase. When using an au- toregressive HSMM, the additional LSTM is op- timized only after the learning rate has been de- cayed. We regularize with Dropout ( <ref type="bibr">Srivastava et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional Learned Templates</head><p>In <ref type="table">Tables 6 and 7</ref> we show visualizations of addi- tional templates learned on the E2E and WikiBio data, respectively, by both the non-autoregressive and autoregressive HSMM models presented in the paper. For each model, we select a set of five dissimilar templates in an iterative way by greed- ily selecting the next template (out of the 200 most frequent) that has the highest percentage of states that do not appear in the previously selected tem- plates; ties are broken randomly. Individual states within a template are visualized using the three most common segments they generate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example template-like generation from the E2E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Wikipedia infobox of Frederick Parker-Rhodes. The introduction of his article reads: "Frederick Parker-Rhodes (21 March 1914-21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist.".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Frederick</head><label></label><figDesc>Parker-Rhodes (21 March 1914 -21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example from the WikiBio dataset (Lebret et al., 2016), with a database x (top) for Frederick ParkerRhodes and corresponding reference generation y (bottom).</figDesc><graphic url="image-1.png" coords="3,100.26,65.00,165.13,204.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>3 [Indian] 50 [food] 1 [in the] 17 [£20- 25] 26 [price range] 16 [.] 2 [It is] 8 [located in the] 25 [riverside] 40 [.] 53 [Its customer rating is] 19 [high] 23 [.] 2 Figure 4: A sample Viterbi segmentation of a training text;</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>BLEU NIST ROUGE CIDEr METEOR</head><label>BLEU</label><figDesc></figDesc><table>Validation 

D&amp;J 
69.25 8.48 
72.57 
2.40 
47.03 
SUB 
43.71 6.72 
55.35 
1.41 
37.87 
NTemp 
64.53 7.66 
68.60 
1.82 
42.46 
NTemp+AR 
67.07 7.98 
69.50 
2.29 
43.07 

Test 

D&amp;J 
65.93 8.59 
68.50 
2.23 
44.83 
SUB 
43.78 6.88 
54.64 
1.39 
37.35 
NTemp 
55.17 7.14 
65.70 
1.70 
41.91 
NTemp+AR 
59.80 7.56 
65.01 
1.95 
38.75 

Table 1: Comparison of the system of Dušek and Jurcıcek 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Top: comparison of the two best neural systems of Lebret et al.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Travellers Rest Beefeater name[Travellers Rest Beefeater], customerRating[3 out of 5], area[riverside], near[Raja Indian Cuisine]</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>1 . [kenneth warren deutscher]132 [ ( ]75 [born]89 [april 1, 1946]101 [ ) ]67 [is an american]82 [author]20 [and]1 [ventriloquist and comedian]69 [.]88 2. [kenneth warren deutscher]132 [ ( ]75 [born]89 [april 1, 1946]101 [ ) ]67 [is an american]82 [author]20</head><label>1</label><figDesc></figDesc><table>[best known for his]95 [the revival of ventriloquism]96 [.]88 
3. [kenneth warren]16 ["kenny" warren]117 [ ( ]75 [born]89 [april 1, 1946]101 [ ) ]67 [is an american]127 
[ventriloquist, comedian]28 [.]133 
4. [kenneth warren]16 ["kenny" warren]117 [ ( ]75 [born]89 [april 1, 1946]101 [ ) ]67 [is a]104 [new york]98 [author]20 [.]133 
5. [kenneth warren deutscher]42 [is an american]82 [ventriloquist, comedian]118 [based in]15 [brooklyn, new york]84 [.]88 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Empirical analysis of the average purity of dis-

crete states learned on the E2E and WikiBio datasets, for the 
NTemp and NTemp+AR models. Average purities are given 
as percents, and standard deviations follow in parentheses. 
See the text for full description of this calculation. 

</table></figure>

			<note place="foot" n="1"> We experimented with parameterizing the length distribution, but found that it led to inferior performance. Forcing the length probabilities to be uniform encourages the model to cluster together functionally similar emissions of different lengths, while parameterizing them can lead to states that specialize to specific emission lengths.</note>

			<note place="foot" n="2"> We use the official E2E NLG Challenge scoring scripts at https://github.com/tuetschek/e2e-metrics. 3 For categorical records, like &quot;familyFriendly&quot;, which cannot easily be aligned with a phrase, we simply select only candidate training sentences with the same categorical value.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>SW gratefully acknowledges the support of a Siebel Scholars award. AMR gratefully acknowl-edges the support of NSF CCF-1704834, Intel Re-search, and Amazon AWS Research grants.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|.</head><p>3. |    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|.</head><p>1. |   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|.</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domain-independent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References Gabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="502" to="512" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptable semantic clustering in statistical nlg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Howald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravikumar</forename><surname>Kondadadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schilder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)-Long Papers</title>
		<meeting>the 10th International Conference on Computational Semantics (IWCS 2013)-Long Papers</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards neural phrasebased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Pearson London</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A statistical nlg framework for aggregated planning and realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kondadadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Howald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schilder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmental recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A global model for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="305" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design of a knowledge-based report generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="145" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Golden Palace Browns Cambridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>| The Waterman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
