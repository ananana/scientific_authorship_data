<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Paraphrase for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<email>li.dong@ed.ac.uk, J.Mallinson@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="department" key="dep3">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="department" key="dep3">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
							<email>sivar@stanford.edu, mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="department" key="dep3">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="department" key="dep3">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Paraphrase for Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="875" to="886"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-to-end using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enabling computers to automatically answer ques- tions posed in natural language on any domain or topic has been the focus of much research in re- cent years. Question answering (QA) is challeng- ing due to the many different ways natural lan- guage expresses the same information need. As a result, small variations in semantically equivalent questions, may yield different answers. For exam- ple, a hypothetical QA system must recognize that the questions "who created microsoft" and "who started microsoft" have the same meaning and that they both convey the founder relation in order to retrieve the correct answer from a knowledge base.</p><p>Given the great variety of surface forms for se- mantically equivalent expressions, it should come as no surprise that previous work has investigated the use of paraphrases in relation to question an- swering. There have been three main strands of research. The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing. <ref type="bibr" target="#b3">Berant and Liang (2014)</ref> use a template-based method to heuristi- cally generate canonical text descriptions for can- didate logical forms, and then compute paraphrase scores between the generated texts and input ques- tions in order to rank the logical forms. Another strand of work uses paraphrases in the context of neural question answering models ( <ref type="bibr">Bordes et al., 2014a,b;</ref><ref type="bibr" target="#b10">Dong et al., 2015</ref>). These models are typ- ically trained on question-answer pairs, and em- ploy question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases.</p><p>The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation <ref type="bibr">(Duboue and ChuCarroll, 2006</ref>), lexical and phrasal rules from the Paraphrase Database ( <ref type="bibr" target="#b25">Narayan et al., 2016)</ref>, as well as rules mined from Wiktionary ( <ref type="bibr" target="#b9">Chen et al., 2016</ref>) and large-scale paraphrase corpora <ref type="bibr" target="#b12">(Fader et al., 2013)</ref>. A common problem with the gen- erated paraphrases is that they often contain in- appropriate candidates. Hence, treating all para- phrases as equally felicitous and using them to an- swer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer <ref type="bibr" target="#b11">(Duboue and Chu-Carroll, 2006;</ref><ref type="bibr" target="#b25">Narayan et al., 2016)</ref>. Problematically, the sepa- rate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned   <ref type="figure">Figure 1</ref>: We use three different methods to generate candidate paraphrases for input q. The question and its paraphrases are fed into a neural model which scores how suitable they are. The scores are normalized and used to weight the results of the question answering model. The entire system is trained end-to-end using question-answer pairs as a supervision signal.</p><p>to the question answering tasks at hand. Based on the large variety of possible transformations that can generate paraphrases, it seems likely that the kinds of paraphrases that are useful would de- pend on the QA application of interest <ref type="bibr" target="#b5">(Bhagat and Hovy, 2013)</ref>. <ref type="bibr" target="#b13">Fader et al. (2014)</ref> use features that are defined over the original question and its rewrites to score paraphrases. Examples include the pointwise mutual information of the rewrite rule, the paraphrase's score according to a lan- guage model, and POS tag features. In the context of semantic parsing, <ref type="bibr" target="#b9">Chen et al. (2016)</ref> also use the ID of the rewrite rule as a feature. However, most of these features are not informative enough to model the quality of question paraphrases, or cannot easily generalize to unseen rewrite rules.</p><p>In this paper, we present a general framework for learning paraphrases for question answering tasks. Given a natural language question, our model estimates a probability distribution over candidate answers. We first generate paraphrases for the question, which can be obtained by one or several paraphrasing systems. A neural scoring model predicts the quality of the generated para- phrases, while learning to assign higher weights to those which are more likely to yield correct an- swers. The paraphrases and the original question are fed into a QA model that predicts a distribution over answers given the question. The entire sys- tem is trained end-to-end using question-answer pairs as a supervision signal. The framework is flexible, it does not rely on specific paraphrase or QA models. In fact, this plug-and-play functional- ity allows to learn specific paraphrases for differ- ent QA tasks and to explore the merits of different paraphrasing models for different applications.</p><p>We evaluate our approach on QA over Free- base and text-based answer sentence selection. We employ a range of paraphrase models based on the Paraphrase Database (PPDB; <ref type="bibr" target="#b29">Pavlick et al. 2015</ref>), neural machine translation <ref type="bibr" target="#b21">(Mallinson et al., 2016)</ref>, and rules mined from the WikiAn- swers corpus <ref type="bibr" target="#b13">(Fader et al., 2014)</ref>. Results on three datasets show that our framework consistently im- proves performance; it achieves state-of-the-art re- sults on GraphQuestions and competitive perfor- mance on two additional benchmark datasets us- ing simple QA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Let q denote a natural language question, and a its answer. Our aim is to estimate p (a|q), the condi- tional probability of candidate answers given the question. We decompose p (a|q) as:</p><formula xml:id="formula_0">p (a|q) = q ∈Hq∪{q} p ψ a|q QA Model p θ q |q Paraphrase Model (1)</formula><p>where H q is the set of paraphrases for question q, ψ are the parameters of a QA model, and θ are the parameters of a paraphrase scoring model. As shown in <ref type="figure">Figure 1</ref>, we first generate candi- date paraphrases H q for question q. Then, a neu- ral scoring model predicts the quality of the gen- erated paraphrases, and assigns higher weights to the paraphrases which are more likely to obtain Input: what be the zip code of the largest car manufacturer what be the zip code of the largest vehicle manufacturer PPDB what be the zip code of the largest car producer PPDB what be the postal code of the biggest automobile manufacturer NMT what be the postcode of the biggest car manufacturer NMT what be the largest car manufacturer 's postal code Rule zip code of the largest car manufacturer Rule <ref type="table">Table 1</ref>: Paraphrases obtained for an input ques- tion from different models (PPDB, NMT, Rule). Words are lowercased and stemmed.</p><p>the correct answers. These paraphrases and the original question simultaneously serve as input to a QA model that predicts a distribution over an- swers for a given question. Finally, the results of these two models are fused to predict the answer. In the following we will explain how p (q |q) and p (a|q ) are estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Paraphrase Generation</head><p>As shown in Equation (1), the term p (a|q) is the sum over q and its paraphrases H q . Ide- ally, we would generate all the paraphrases of q. However, since this set could quickly become in- tractable, we restrict the number of candidate para- phrases to a manageable size. In order to in- crease the coverage and diversity of paraphrases, we employ three methods based on: (1) lexical and phrasal rules from the Paraphrase Database ( <ref type="bibr" target="#b29">Pavlick et al., 2015)</ref>; (2) neural machine trans- lation models (Sutskever et al., 2014; Bahdanau et al., 2015); and (3) paraphrase rules mined from clusters of related questions <ref type="bibr" target="#b13">(Fader et al., 2014</ref>). We briefly describe these models below, however, there is nothing inherent in our framework that is specific to these, any other paraphrase generator could be used instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">PPDB-based Generation</head><p>Bilingual pivoting <ref type="bibr" target="#b1">(Bannard and Callison-Burch, 2005</ref>) is one of the most well-known approaches to paraphrasing; it uses bilingual parallel corpora to learn paraphrases based on techniques from phrase-based statistical machine translation (SMT, <ref type="bibr" target="#b18">Koehn et al. 2003</ref>). The intuition is that two English strings that translate to the same foreign string can be assumed to have the same meaning.  <ref type="figure">Figure 2</ref>: Overview of NMT-based paraphrase generation. NMT 1 (green) translates ques- tion q into pivots g 1 . . . g K which are then back- translated by NMT 2 (blue) where K decoders jointly predict tokens at each time step, rather than only conditioning on one pivot and independently predicting outputs.</p><p>leading to the creation of PPDB ( <ref type="bibr" target="#b15">Ganitkevitch et al., 2013)</ref>, a large-scale paraphrase database containing over a billion of paraphrase pairs in 24 different languages. <ref type="bibr" target="#b29">Pavlick et al. (2015)</ref> fur- ther used a supervised model to automatically la- bel paraphrase pairs with entailment relationships based on natural logic <ref type="bibr" target="#b20">(MacCartney, 2009)</ref>. In our work, we employ bidirectionally entailing rules from PPDB. Specifically, we focus on lexical (sin- gle word) and phrasal (multiword) rules which we use to paraphrase questions by replacing words and phrases in them. An example is shown in <ref type="table">Table 1</ref> where we substitute car with vehicle and manufacturer with producer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">NMT-based Generation</head><p>Mallinson et al. (2016) revisit bilingual pivoting in the context of neural machine translation (NMT, <ref type="bibr" target="#b39">Sutskever et al. 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al. 2015)</ref> and present a paraphrasing model based on neural net- works. At its core, NMT is trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilin- gual corpus. Paraphrases can be obtained by trans- lating an English string into a foreign language and then back-translating it into English. NMT- based pivoting models offer advantages over con- ventional methods such as the ability to learn con- tinuous representations and to consider wider con- text while paraphrasing.</p><p>In our work, we select German as our pivot following <ref type="bibr" target="#b21">Mallinson et al. (2016)</ref> who show that it outperforms other languages in a wide range of paraphrasing experiments, and pretrain two NMT systems, English-to-German (EN-DE) and Source Target the average size of what be average size be locate on which continent what continent be a part of language speak in what be the official language of what be the money in what currency do use <ref type="table">Table 2</ref>: Examples of rules used in the rule-based paraphrase generator.</p><p>German-to-English (DE-EN). A naive implemen- tation would translate a question to a German string and then back-translate it to English. How- ever, using only one pivot can lead to inaccu- racies as it places too much faith on a single translation which may be wrong. Instead, we translate from multiple pivot sentences <ref type="bibr" target="#b21">(Mallinson et al., 2016</ref>). As shown in <ref type="figure">Figure 2</ref>, question q is translated to K-best German pivots,</p><formula xml:id="formula_1">G q = {g 1 , . . . , g K }.</formula><p>The probability of generating para- phrase q = y 1 . . . y |q | is decomposed as:</p><formula xml:id="formula_2">p q |G q = |q | t=1 p (y t |y &lt;t , G q ) = |q | t=1 K k=1 p (g k |q) p (y t |y &lt;t , g k )<label>(2)</label></formula><p>where y &lt;t = y 1 , . . . , y t−1 , and |q | is the length of q . Probabilities p (g k |q) and p (y t |y &lt;t , g k ) are computed by the EN-DE and DE-EN models, re- spectively. We use beam search to decode tokens by conditioning on multiple pivoting sentences.</p><p>The results with the best decoding scores are con- sidered candidate paraphrases. Examples of NMT paraphrases are shown in <ref type="table">Table 1</ref>. Compared to PPDB, NMT-based paraphrases are syntax-agnostic, operating on the surface level without knowledge of any underlying grammar. Furthermore, paraphrase rules are captured im- plicitly and cannot be easily extracted, e.g., from a phrase table. As mentioned earlier, the NMT- based approach has the potential of perform- ing major rewrites as paraphrases are generated while considering wider contextual information, whereas PPDB paraphrases are more local, and mainly handle lexical variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Rule-Based Generation</head><p>Our third paraphrase generation approach uses rules mined from the WikiAnswers corpus <ref type="bibr" target="#b13">(Fader et al., 2014</ref>) which contains more than 30 mil- lion question clusters labeled as paraphrases by WikiAnswers 1 users. This corpus is a large re- source (the average cluster size is 25), but is rel- atively noisy due to its collaborative nature -45% of question pairs are merely related rather than genuine paraphrases. We therefore followed the method proposed in <ref type="bibr" target="#b12">(Fader et al., 2013</ref>) to har- vest paraphrase rules from the corpus. We first ex- tracted question templates (i.e., questions with at most one wild-card) that appear in at least ten clus- ters. Any two templates co-occurring (more than five times) in the same cluster and with the same arguments were deemed paraphrases. <ref type="table">Table 2</ref> shows examples of rules extracted from the cor- pus. During paraphrase generation, we consider substrings of the input question as arguments, and match them with the mined template pairs. For ex- ample, the stemmed input question in <ref type="table">Table 1</ref> can be paraphrased using the rules ("what be the zip code of ", "what be 's postal code") and ("what be the zip code of ", "zip code of "). If no ex- act match is found, we perform fuzzy matching by ignoring stop words in the question and templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Paraphrase Scoring</head><p>Recall from Equation (1) that p θ (q |q) scores the generated paraphrases q ∈ H q ∪ {q}. We esti- mate p θ (q |q) using neural networks given their successful application to paraphrase identification tasks <ref type="bibr" target="#b37">(Socher et al., 2011;</ref><ref type="bibr" target="#b48">Yin and Schütze, 2015;</ref>. Specifically, the input ques- tion and its paraphrases are encoded as vectors. Then, we employ a neural network to obtain the score s (q |q) which after normalization becomes the probability p θ (q |q).</p><p>Encoding Let q = q 1 . . . q |q| denote an input question. Every word is initially mapped to a d-dimensional vector. In other words, vector q t is computed via q t = W q e (q t ), where W q ∈ R d×|V| is a word embedding matrix, |V| is the vocabulary size, and e (q t ) is a one-hot vector. Next, we use a bi-directional recurrent neural net- work with long short-term memory units (LSTM, Hochreiter and Schmidhuber 1997) as the ques- tion encoder, which is shared by the input ques- tions and their paraphrases. The encoder recur- sively processes tokens one by one, and uses the encoded vectors to represent questions. We com- pute the hidden vectors at the t-th time step via:</p><formula xml:id="formula_3">− → h t = LSTM − → h t−1 , q t , t = 1, . . . , |q| ← − h t = LSTM ← − h t+1 , q t , t = |q|, . . . , 1<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">− → h t , ← − h t ∈ R n .</formula><p>In this work we follow the LSTM function described in <ref type="bibr" target="#b31">Pham et al. (2014)</ref>. The representation of q is obtained by:</p><formula xml:id="formula_5">q = − → h |q| , ← − h 1<label>(4)</label></formula><p>where [·, ·] denotes concatenation, and q ∈ R 2n .</p><p>Scoring After obtaining vector representations for q and q , we compute the score s (q |q) via:</p><formula xml:id="formula_6">s q |q = w s · q, q , q q + b s<label>(5)</label></formula><p>where w s ∈ R 6n is a parameter vector, [·, ·, ·] de- notes concatenation, is element-wise multipli- cation, and b s is the bias. Alternative ways to com- pute s (q |q) such as dot product or with a bilinear term were not empirically better than Equation <ref type="formula" target="#formula_6">(5)</ref> and we omit them from further discussion.</p><p>Normalization For paraphrases q ∈ H q ∪ {q}, the probability p θ (q |q) is computed via:</p><formula xml:id="formula_7">p θ q |q = exp{s (q |q)} r∈Hq∪{q} exp{s (r|q)}<label>(6)</label></formula><p>where the paraphrase scores are normalized over the set H q ∪ {q}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">QA Models</head><p>The framework defined in Equation <ref type="formula">(1)</ref> is rela- tively flexible with respect to the QA model being employed as long as it can predict p ψ (a|q ). We il- lustrate this by performing experiments across dif- ferent tasks and describe below the models used for these tasks.</p><p>Knowledge Base QA In our first task we use the Freebase knowledge base to answer questions. Query graphs for the questions typically contain more than one predicate. For example, to answer the question "who is the ceo of microsoft in 2008", we need to use one relation to query "ceo of mi- crosoft" and another relation for the constraint "in 2008". For this task, we employ the SIMPLE- GRAPH model described in <ref type="bibr" target="#b33">Reddy et al. ( , 2017</ref>, and follow their training protocol and fea- ture design. In brief, their method uses rules to convert questions to ungrounded logical forms, which are subsequently matched against Freebase subgraphs. The QA model learns from question- answer pairs: it extracts features for pairs of ques- tions and Freebase subgraphs, and uses a logistic regression classifier to predict the probability that a candidate answer is correct. We perform entity linking using the Freebasee/KG API on the origi- nal question ( <ref type="bibr" target="#b33">Reddy et al., , 2017</ref>, and gener- ate candidate Freebase subgraphs. The QA model estimates how likely it is for a subgraph to yield the correct answer.</p><p>Answer Sentence Selection Given a question and a collection of relevant sentences, the goal of this task is to select sentences which contain an answer to the question. The assumption is that correct answer sentences have high semantic similarity to the questions ( <ref type="bibr" target="#b49">Yu et al., 2014;</ref><ref type="bibr" target="#b43">Yang et al., 2015;</ref><ref type="bibr" target="#b22">Miao et al., 2016)</ref>. We use two bi- directional recurrent neural networks (BILSTM) to separately encode questions and answer sen- tences to vectors (Equation <ref type="formula" target="#formula_5">(4)</ref>). Similarity scores are computed as shown in Equation <ref type="formula" target="#formula_6">(5)</ref>, and then squashed to (0, 1) by a sigmoid function in order to predict p ψ (a|q ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Inference</head><p>We use a log-likelihood objective for training, which maximizes the likelihood of the correct an- swer given a question: maximize (q,a)∈D log p (a|q)</p><p>where D is the set of all question-answer training pairs, and p (a|q) is computed as shown in Equa- tion (1). For the knowledge base QA task, we pre- dict how likely it is that a subgraph obtains the correct answer, and the answers of some candidate subgraphs are partially correct. So, we use the binary cross entropy between the candidate sub- graph's F1 score and the prediction as the objec- tive function. The RMSProp algorithm <ref type="bibr" target="#b40">(Tieleman and Hinton, 2012</ref>) is employed to solve this non- convex optimization problem. Moreover, dropout is used for regularizing the recurrent neural net- works ( <ref type="bibr" target="#b31">Pham et al., 2014)</ref>. At test time, we generate paraphrases for the question q, and then predict the answer by:</p><formula xml:id="formula_9">ˆ a = arg max a ∈Cq p a |q<label>(8)</label></formula><p>where C q is the set of candidate answers, and p (a |q) is computed as shown in Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We compared our model which we call PARA4QA (as shorthand for learning to paraphrase for ques- tion answering) against multiple previous systems on three datasets. In the following we introduce these datasets, provide implementation details for our model, describe the systems used for compar- ison, and present our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Our model was trained on three datasets, repre- sentative of different types of QA tasks. The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WEBQUESTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Paraphrase Generation Candidate paraphrases were stemmed ( <ref type="bibr" target="#b24">Minnen et al., 2001</ref>) and lower- cased. We discarded duplicate or trivial para- phrases which only rewrite stop words or punc- tuation. For the NMT model, we followed the im- plementation 2 and settings described in <ref type="bibr" target="#b21">Mallinson et al. (2016)</ref>, and used English↔German as the language pair. The system was trained on data released as part of the WMT15 shared transla- tion task (4.2 million sentence pairs). We also had access to back-translated monolingual train- ing data <ref type="bibr" target="#b34">(Sennrich et al., 2016a</ref>). Rare words were 2 github.com/sebastien-j/LV_groundhog split into subword units <ref type="bibr" target="#b35">(Sennrich et al., 2016b</ref>) to handle out-of-vocabulary words in questions. We used the top 15 decoding results as candidate para- phrases. We used the S size package of PPDB 2.0 ( <ref type="bibr" target="#b29">Pavlick et al., 2015</ref>) for high precision. At most 10 candidate paraphrases were considered. We mined paraphrase rules from WikiAnswers ( <ref type="bibr" target="#b13">Fader et al., 2014</ref>) as described in Section 2.1.3. The extracted rules were ranked using the point- wise mutual information between template pairs in the WikiAnswers corpus. The top 10 candidate paraphrases were used.</p><p>Training For the paraphrase scoring model, we used GloVe ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>) vectors 3 pre- trained on Wikipedia 2014 and Gigaword 5 to ini- tialize the word embedding matrix. We kept this matrix fixed across datasets. Out-of-vocabulary words were replaced with a special unknown sym- bol. We also augmented questions with start-of- and end-of-sequence symbols. Word vectors for these special symbols were updated during train- ing. Model hyperparameters were validated on the development set. The dimensions of hid- den vectors and word embeddings were selected from {50, 100, 200} and {100, 200}, respectively. The dropout rate was selected from {0.2, 0.3, 0.4}.</p><p>The BILSTM for the answer sentence selection QA model used the same hyperparameters. Pa- rameters were randomly initialized from a uniform distribution U (−0.08, 0.08). The learning rate and decay rate of RMSProp were 0.01 and 0.95, respectively. The batch size was set to 150. To alleviate the exploding gradient problem ( <ref type="bibr" target="#b28">Pascanu et al., 2013)</ref>, the gradient norm was clipped to 5. Early stopping was used to determine the number of epochs.  <ref type="table" target="#tab_3">Table 3</ref>: Statistics of generated paraphrases across datasets (training set). avg(|q|): average ques- tion length; avg(|q |): average paraphrase length; avg(#q ): average number of paraphrases; cover- age: the proportion of questions that have at least one candidate paraphrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Paraphrase Statistics</head><p>method and the rules extracted from WikiAnswers tend to paraphrase more (i.e., have lower BLEU and higher TER scores) compared to PPDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison Systems</head><p>We compared our framework to previous work and several ablation models which either do not use paraphrases or paraphrase scoring, or are not jointly trained. The first baseline only uses the base QA mod- els described in Section 2.3 (SIMPLEGRAPH and BILSTM). The second baseline (AVGPARA) does not take advantage of paraphrase scoring. The paraphrases for a given question are used while the QA model's results are directly averaged to predict the answers. The third baseline (DATAAUGMENT) employs paraphrases for data augmentation dur- ing training. Specifically, we use the question, its paraphrases, and the correct answer to automati- cally generate new training samples.</p><p>In the fourth baseline (SEPPARA), the para- phrase scoring model is separately trained on para- phrase classification data, without taking question- answer pairs into account. In the experiments, we used the Quora question paraphrase dataset 4 which contains question pairs and labels indicat- ing whether they constitute paraphrases or not. We removed questions with more than 25 tokens and sub-sampled to balance the dataset. We used 90% of the resulting 275K examples for training, and the remaining for development. The paraphrase score s (q |q) (Equation <ref type="formula" target="#formula_6">(5)</ref>) was wrapped by a sigmoid function to predict the probability of a question pair being a paraphrase. A binary cross- entropy loss was used as the objective. The classi- fication accuracy on the dev set was 80.6%.</p><formula xml:id="formula_10">4 goo.gl/kMP46n Method Average F1 (%)</formula><p>GRAPHQ WEBQ SEMPRE ( <ref type="bibr" target="#b2">Berant et al., 2013)</ref> 10.8 35.7 JACANA ( <ref type="bibr" target="#b45">Yao and Van Durme, 2014</ref>  Finally, in order to assess the individual con- tribution of different paraphrasing resources, we compared the PARA4QA model against versions of itself with one paraphrase generator removed (−NMT/−PPDB/−RULE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>We first discuss the performance of PARA4QA on GRAPHQUESTIONS and WEBQUESTIONS. The first block in <ref type="table" target="#tab_5">Table 4</ref> shows a variety of systems previously described in the literature using aver- age F1 as the evaluation metric <ref type="bibr" target="#b2">(Berant et al., 2013</ref>). Among these, PARASEMP, SUBGRAPH, MCCNN, and BILAYERED utilize paraphrasing resources. The second block compares PARA4QA against various related baselines (see Section 3.4). SIMPLEGRAPH results on WEBQUESTIONS and GRAPHQUESTIONS are taken from  and <ref type="bibr" target="#b33">Reddy et al. (2017)</ref>, respectively.</p><p>Overall, we observe that PARA4QA outper- forms baselines which either do not employ para- phrases (SIMPLEGRAPH) or paraphrase scoring (AVGPARA, DATAAUGMENT), or are not jointly trained (SEPPARA). On GRAPHQUESTIONS, our model PARA4QA outperforms the previous state of the art by a wide margin. Ablation experiments with one of the paraphrase generators removed Method MAP MRR BIGRAMCNN ( <ref type="bibr" target="#b49">Yu et al., 2014)</ref> 0.6190 0.6281 BIGRAMCNN+CNT ( <ref type="bibr" target="#b49">Yu et al., 2014)</ref> 0.6520 0.6652 PARAVEC ( <ref type="bibr" target="#b19">Le and Mikolov, 2014)</ref> 0.5110 0.5160 PARAVEC+CNT ( <ref type="bibr" target="#b19">Le and Mikolov, 2014</ref>) 0.5976 0.6058 LSTM ( <ref type="bibr" target="#b22">Miao et al., 2016)</ref> 0.6552 0.6747 LSTM+CNT ( <ref type="bibr" target="#b22">Miao et al., 2016)</ref> 0.6820 0.6988 NASM ( <ref type="bibr" target="#b22">Miao et al., 2016)</ref> 0.6705 0.6914 NASM+CNT ( <ref type="bibr" target="#b22">Miao et al., 2016)</ref> 0.6886 0.7069 KVMEMNET+CNT ( <ref type="bibr" target="#b23">Miller et al., 2016</ref>  show that performance drops most when the NMT paraphrases are not used on GRAPHQUESTIONS, whereas on WEBQUESTIONS removal of the rule- based generator hurts performance most. One rea- son is that the rule-based method has higher cov- erage on WEBQUESTIONS than on GRAPHQUES- TIONS (see <ref type="table" target="#tab_3">Table 3</ref>).</p><p>Results on WIKIQA are shown in <ref type="table" target="#tab_7">Table 5</ref>. We report MAP and MMR which evaluate the rela- tive ranks of correct answers among the candi- date sentences for a question. Again, we observe that PARA4QA outperforms related baselines (see BILSTM, DATAAUGMENT, AVGPARA, and SEP- PARA). Ablation experiments show that perfor- mance drops most when NMT paraphrases are re- moved. When word matching features are used (see +CNT in the third block), PARA4QA reaches state of the art performance.</p><p>Examples of paraphrases and their probabil- ities p θ (q |q) (see Equation (6)) learned by PARA4QA are shown in <ref type="table" target="#tab_8">Table 6</ref>. The two ex- amples are taken from the development set of GRAPHQUESTIONS and WEBQUESTIONS, re- spectively. We also show the Freebase relations used to query the correct answers. In the first ex- ample, the original question cannot yield the cor- rect answer because of the mismatch between the question and the knowledge base. The paraphrase contains "role" in place of "sort of part", increas- ing the chance of overlap between the question and Examples p θ (q |q) (music.concert performance.performance role) what sort of part do queen play in concert 0.0659 what role do queen play in concert 0.0847 what be the role play by the queen in concert 0.0687 what role do queen play during concert 0.0670 what part do queen play in concert 0.0664 which role do queen play in concert concert 0.0652 (sports.sports team roster.team) what team do shaq play 4</p><p>0.2687 what team do shaq play for 0.2783 which team do shaq play with 0.0671 which team do shaq play out 0.0655 which team have you play shaq 0.0650 what team have we play shaq 0.0497 the predicate words. The second question contains an informal expression "play 4", which confuses the QA model. The paraphrase model generates "play for" and predicts a high paraphrase score for it. More generally, we observe that the model tends to give higher probabilities p θ (q |q) to para- phrases biased towards delivering appropriate an- swers.</p><p>We also analyzed which structures were mostly paraphrased within a question. We manually in- spected 50 (randomly sampled) questions from the development portion of each dataset, and their three top scoring paraphrases (Equation <ref type="formula" target="#formula_6">(5)</ref>). We grouped the most commonly paraphrased struc- tures into the following categories: a) question words, i.e., wh-words and and "how"; b) ques- tion focus structures, i.e., cue words or cue phrases for an answer with a specific entity type <ref type="bibr" target="#b45">(Yao and Van Durme, 2014</ref>); c) verbs or noun phrases in- dicating the relation between the question topic entity and the answer; and d) structures requir- ing aggregation or imposing additional constraints the answer must satisfy ( ). In the example "which year did Avatar release in UK", the question word is "which", the question focus is "year", the verb is "release", and "in UK" con- strains the answer to a specific location.  often rewritten in GRAPHQUESTIONS compared to the other datasets. Finally, we examined how our method fares on simple versus complex questions. We performed this analysis on GRAPHQUESTIONS as it contains a larger proportion of complex questions. We con- sider questions that contain a single relation as simple. Complex questions have multiple rela- tions or require aggregation. <ref type="table" target="#tab_9">Table 7</ref> shows how our model performs in each group. We observe improvements for both types of questions, with the impact on simple questions being more pro- nounced. This is not entirely surprising as it is eas- ier to generate paraphrases and predict the para- phrase scores for simpler questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work we proposed a general framework for learning paraphrases for question answering. Paraphrase scoring and QA models are trained end-to-end on question-answer pairs, which re- sults in learning paraphrases with a purpose. The framework is not tied to a specific paraphrase gen- erator or QA system. In fact it allows to in- corporate several paraphrasing modules, and can serve as a testbed for exploring their coverage and rewriting capabilities. Experimental results on three datasets show that our method improves performance across tasks. There are several direc- tions for future work. The framework can be used for other natural language processing tasks which are sensitive to the variation of input (e.g., tex- tual entailment or summarization). We would also like to explore more advanced paraphrase scoring models ( <ref type="bibr" target="#b27">Parikh et al., 2016;</ref><ref type="bibr" target="#b41">Wang and Jiang, 2016)</ref> as well as additional paraphrase generators since improvements in the diversity and the quality of paraphrases could also enhance QA performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Question</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 showsFigure 3 :</head><label>33</label><figDesc>Figure 3 shows the degree to which different types of structures are paraphrased. As can be seen, most rewrites affect Relation Verb, especially on WEBQUESTIONS. Question Focus, Relation NP, and Constraint &amp; Aggregation are more</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>who created microsoft? Paraphrases q 1 : who founded microsoft? q 2 : who is the founder of microsoft? q 3 : who is the creator of microsoft? q m : who designed microsoft?</head><label></label><figDesc></figDesc><table>Answering 
Model 

Score 
Normalization 

p ψ (a|q 1 ) 

p ψ (a|q m ) 
... 

Question 
Encoder 

q 1 

q m 
... 

q 

... 

p ψ (a|q) 
q 

q 

s(q 1 |q) 

s(q m |q) 
... 

s(q|q) 

p θ (q 1 |q) 

p θ (q m |q) 
... 

p θ (q|q) 

Paraphrase 
Scoring Model 

Bi-LSTM 
Bi-LSTM 

... 
... 

Question 
Vectors 
Scores 

Answer 
Question 

q: microsoft 
org_ 
founder 
Paul Allen 
Bill Gates 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 presents</head><label>3</label><figDesc></figDesc><table>descriptive statistics on the para-
phrases generated by the various systems across 
datasets (training set). As can be seen, the av-
erage paraphrase length is similar to the average 
length of the original questions. The NMT method 
generates more paraphrases and has wider cover-
age, while the average number and coverage of the 
other two methods varies per dataset. As a way 
of quantifying the extent to which rewriting takes 
place, we report BLEU (Papineni et al., 2002) and 
TER (Snover et al., 2006) scores between the orig-
inal questions and their paraphrases. The NMT </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model performance on GRAPHQUES-
TIONS and WEBQUESTIONS. Results with addi-
tional task-specific resources are shown in paren-
theses. The base QA model is SIMPLEGRAPH. 
Best results in each group are shown in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Model performance on WIKIQA. +CNT: 
word matching features introduced in Yang et al. 
(2015). The base QA model is BILSTM. Best re-
sults in each group are shown in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Questions and their top-five paraphrases 
with probabilities learned by the model. The Free-
base relations used to query the correct answers 
are shown in brackets. The original question is 
underlined. Questions with incorrect predictions 
are in red. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>We group GRAPHQUESTIONS into sim-
ple and complex questions and report model per-
formance in each split. Best results in each group 
are shown in bold. The values in brackets are ab-
solute improvements of average F1 scores. 

</table></figure>

			<note place="foot" n="1"> wiki.answers.com</note>

			<note place="foot" n="3"> nlp.stanford.edu/projects/glove</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Paraphrasing with bilingual parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imitation learning of agenda-based semantic parsers. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">What is a paraphrase? Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<meeting>the European Conference on Machine Learning and Knowledge Discovery in Databases<address><addrLine>New York, NY, USA; New York, Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8724</biblScope>
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
	<note>ECML PKDD 2014</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Syntactic constraints on paraphrases extracted from parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentence rewriting for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="766" to="777" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Question answering over freebase with multicolumn convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Answering the question you wish they had asked: The impact of paraphrasing for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1168" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PPDB: the paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
	<note>AmirHossein Karimi, Antoine Bordes, and Jason Weston</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Applied morphological processing of english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Paraphrase generation from Latent-Variable PCFGs for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Natural Language Generation Conference</title>
		<meeting>the 9th International Natural Language Generation Conference<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transforming dependency structures to logical forms for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Universal semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On generating characteristic-rich question sets for qa evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Lecture 6.5RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Question answering on freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2326" to="2336" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lean question answering over freebase from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving semantic parsing via answer type inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional neural network for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep Learning for Answer Sentence Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
