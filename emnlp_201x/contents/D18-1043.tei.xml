<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Adversarial Unsupervised Word Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><forename type="middle">Hoshen</forename><surname>Facebook</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Adversarial Unsupervised Word Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="469" to="478"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>469</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised word translation from non-parallel inter-lingual corpora has attracted much research interest. Very recently, neu-ral network methods trained with adversarial loss functions achieved high accuracy on this task. Despite the impressive success of the recent techniques, they suffer from the typical drawbacks of generative adversarial models: sensitivity to hyper-parameters, long training time and lack of interpretability. In this paper, we make the observation that two sufficiently similar distributions can be aligned correctly with iterative matching methods. We present a novel method that first aligns the second moment of the word distributions of the two languages and then iteratively refines the alignment. Extensive experiments on word translation of European and Non-European languages show that our method achieves better performance than recent state-of-the-art deep adversarial approaches and is competitive with the supervised baseline. It is also efficient, easy to parallelize on CPU and interpretable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inferring word translations between languages is a long-standing research task. Earliest efforts con- centrated on finding parallel corpora in a pair of languages and inferring a dictionary by force alignment of words between the two languages. An early example of this approach is the transla- tion achieved using the Rosetta stone.</p><p>However, if most languages share the same ex- pressive power and are used to describe similar human experiences across cultures, they should share similar statistical properties. Exploiting sta- tistical properties of letters has been successfully employed by substitution crypto-analysis since at least the 9th century. It seems likely that one can learn to map between languages statistically, by considering the word distributions. As one spe- cific example, it is likely that the set of elements described by the most common words in one lan- guage would greatly overlap with those described in a second language.</p><p>Another support for the plausibility of unsuper- vised word translation came with the realization that when words are represented as vectors that encode co-occurrences, the mapping between two languages is well captured by an affine transfor- mation ( <ref type="bibr" target="#b17">Mikolov et al., 2013b</ref>). In other words, not only that one can expect the most frequent words to be shared, one can also expect the rep- resentations of these words to be similar up to a linear transformation.</p><p>A major recent trend in unsupervised learning is the use of Generative Adversarial Networks (GANs) presented by <ref type="bibr" target="#b8">Goodfellow et al. (2014)</ref>, in which two networks provide mutual training sig- nals to each other: the generator and the discrimi- nator. The discriminator plays an adversarial role to a generative model and is trained to distinguish between two distributions. Typically, these dis- tributions are labeled as "real" and "fake", where "fake" denotes the generated samples.</p><p>In the context of unsupervised translation <ref type="bibr" target="#b4">(Conneau et al., 2017;</ref><ref type="bibr">Zhang et al., 2017a,b)</ref>, when learning from a source language to a target lan- guage, the "real" distribution is the distribution of the target language and the "fake" one is the map- ping of the source distribution using the learned mapping. Such approaches have been shown re- cently to be very effective when employed on top of modern vector representations of words.</p><p>In this work, we ask whether GANs are nec- essary for achieving the level of success recently demonstrated for unsupervised word translation. Given that the learned mapping is simple and that the concepts described by the two languages are similar, we suggest to directly map every word in one language to the closest word in the other. While one cannot expect that all words would match correctly for a random initialization, some would match and may help refine the affine trans- formation. Once an improved affine transforma- tion is recovered, the matching process can repeat.</p><p>Naturally, such an iterative approach relies on a good initialization. For this purpose we employ two methods. First, an initial mapping is obtained by matching the means and covariances of the two distributions. Second, multiple solutions, which are obtained stochastically, are employed.</p><p>Using multiple stochastic solutions is crucial for languages that are more distant, e.g., more stochastic solutions are required for learning to translate between English and Arabic, in compari- son to English and French. Evaluating multiple so- lutions relies on the ability to automatically iden- tify the true matching without supervision and we present an unsupervised reconstruction-based cri- terion for determining the best stochastic solution.</p><p>Our presented approach is simple, has very few hyper-parameters, and is trivial to parallelize. It is also easily interpretable, since every step of the method has a clear goal and a clear success metric, which can also be evaluated without the ground truth bilingual lexicon. An extensive set of exper- iments shows that our much simpler and more ef- ficient method is more effective than the state-of- the-art GAN based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The earlier contributions in the field of word trans- lation without parallel corpora were limited to finding matches between a small set of carefully selected words and translations, and relied on co- occurrence statistics <ref type="bibr" target="#b18">(Rapp, 1995)</ref> or on similar- ity in the variability of the context before and af- ter the word <ref type="bibr" target="#b6">(Fung, 1995)</ref>. Finding translations of larger sets of words was made possible in follow- up work by incorporating a seed set of matching words that is either given explicitly or inferred based on words that appear in both languages or are similar in edit distance due to a shared etymol- ogy ( <ref type="bibr" target="#b7">Fung and Yee, 1998;</ref><ref type="bibr" target="#b19">Rapp, 1999;</ref><ref type="bibr" target="#b20">Schafer and Yarowsky, 2002;</ref><ref type="bibr" target="#b14">Koehn and Knight, 2002;</ref><ref type="bibr" target="#b9">Haghighi et al., 2008;</ref><ref type="bibr" target="#b11">Irvine and Callison-Burch, 2013;</ref><ref type="bibr" target="#b22">Xia et al., 2016;</ref><ref type="bibr" target="#b0">Artetxe et al., 2017)</ref>.</p><p>For example, <ref type="bibr" target="#b14">Koehn and Knight (2002)</ref> matched English with German. Multiple heuris- tics were suggested based on hand crafted rules, including similarity in spelling and word fre- quency. A weighted linear combination is em- ployed to combine the heuristics and the matching words are identified in a greedy manner. <ref type="bibr" target="#b9">Haghighi et al. (2008)</ref> modeled the problem of matching words across independent corpora as a genera- tive model, in which cross-lingual links are rep- resented by latent variables, and employed an iter- ative EM method.</p><p>Another example that employs iterations was presented by <ref type="bibr" target="#b0">Artetxe et al. (2017)</ref>. Similarly to our method, this method relies on word vector embed- dings, in their case the word2vec method ( <ref type="bibr" target="#b16">Mikolov et al., 2013a</ref>). Unlike our method, their method is initialized using seed matches.</p><p>Our core method incorporates a circularity term, which is also used in ( <ref type="bibr" target="#b22">Xia et al., 2016)</ref> for the task of NMT and later on in multiple contributions in the field of image synthesis <ref type="bibr" target="#b13">(Kim et al., 2017;</ref><ref type="bibr" target="#b26">Zhu et al., 2017)</ref>. This term is employed when learning bidirectional transformations to encour- ages samples from either domain to be mapped back to exactly the same sample when translated to the other domain and back. Since our transfor- mations are linear, this is highly related to employ- ing orthogonality as done in <ref type="bibr" target="#b23">(Xing et al., 2015;</ref><ref type="bibr" target="#b21">Smith et al., 2017;</ref><ref type="bibr" target="#b4">Conneau et al., 2017</ref>) for the task of weakly or unsupervised word vector space alignment. <ref type="bibr" target="#b4">Conneau et al. (2017)</ref> also employ a circularity term, but unlike our use of it as part of the optimization's energy term, there it is used for validating the solution and selecting hyperparam- eters.</p><p>Very recently, <ref type="bibr">Zhang et al. (2017a,b)</ref>; <ref type="bibr" target="#b4">Conneau et al. (2017)</ref> have proposed completely unsuper- vised solutions. All three solutions are based on GANs. The methods differ in the details of the ad- versarial training, in the way that model selection is employed to select the best configuration and in the way in which matching is done after the distri- butions are aligned by the learned transformation.</p><p>Due to the min-max property of GANs, meth- ods which rely on GANs are harder to interpret, since, for example, the discriminator D could fo- cus on a combination of local differences between the distributions. The reliance on a discriminator also means that complex weight dependent met- rics are implicitly used, and that these metrics evolve dynamically during training.</p><p>Our method does not employ GANs. Alter- natives to GANs are also emerging in other do-mains. For example, generative methods were trained by iteratively fitting random ("noise") vec- tors by <ref type="bibr" target="#b2">Bojanowski et al. (2017)</ref>; In the recent im- age translation work of <ref type="bibr" target="#b3">Chen and Koltun (2017)</ref>, distinguishability between distribution of images was measured using activations of pretrained net- works, a practice that is referred to as the "percep- tual loss" <ref type="bibr" target="#b12">(Johnson et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-Adversarial Word Translation</head><p>We present an approach for unsupervised word translation consisting of multiple parts: (i) Trans- forming the word vectors into a space in which the two languages are more closely aligned, (ii) Mini- Batch Cycle iterative alignment. There is an op- tional final stage of batch-based finetuning.</p><p>Let us define two languages X and Y, each con- taining a set of N X and N Y words represented by the feature vectors x 1 ..x N X and y 1 ..y N Y respec- tively. Our objective is to find the correspondence function f (n) such that for every x n , f (n) yields the index of the Y word that corresponds to the word x n . If a set of possible correspondences is available for a given word, our objective is to pre- dict one member of this set. In this unsupervised setting, no training examples of f (n) are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approximate Alignment with PCA</head><p>Each language consists of a set of words each pa- rameterized by a word vector. A popular exam- ple of a word embedding method is FastText ( <ref type="bibr" target="#b1">Bojanowski et al., 2016)</ref>, which uses the internal word co-occurrence statistics for each language. These word vectors are typically not expected to be aligned between languages and since the align- ment method we employ is iterative, a good ini- tialization is key.</p><p>Let us motivate our approach by a method com- monly used in 3D point cloud matching. Let A be a set of 3D points and T A be the same set of points with a rotated coordinate system. Assuming non- isotropic distributions of points, transforming each set of points to its principle axes of variations (us- ing PCA) will align the two point clouds. As noted by <ref type="bibr" target="#b5">Daras et al. (2012)</ref>, PCA-based alignment is common in the literature of point cloud matching.</p><p>Word distributions are quite different from 3D point clouds: They are much higher dimensional, and it is not obvious a priori that different lan- guages present different "views" of the same "ob- ject" and share exactly the same axes of variation.</p><p>The success of previous results, e.g. ( <ref type="bibr" target="#b4">Conneau et al., 2017)</ref>, to align word vectors between lan- guages using an orthonormal transformation does give credence to this approach. Our method re- lies on the assumption that many language pairs share some principle axes of variation. The em- pirical success of PCA initialization in this work supports this assumption.</p><p>For each language <ref type="bibr">[X , Y]</ref>, we first select the N most frequent word vectors. In our implementa- tion, we use N = 5000 and employ FastText vec- tors of dimension D = 300. We project the word vectors, after centering, to the top p principle com- ponents (we use p = 50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mini-Batch Cycle Iterative Closest Point</head><p>Although projecting to the top principle axes of variation would align a rotated non-isotropic point cloud, it does not do so in the general case. This is due to languages having different word distribu- tions and components of variation.</p><p>We therefore attempt to find a transformation T that will align every word x i from language X to a word y f (i) in language Y. The objective is there- fore to minimize:</p><formula xml:id="formula_0">argmin T i min f (i) |y i − T x f (i) |<label>(1)</label></formula><p>Eq. 1 is difficult to optimize directly and var- ious techniques have been proposed for its opti- mization. One popular method used in 3D point cloud alignment is Iterative Closest Point (ICP). ICP solves Eq. 1 iteratively in two steps.</p><p>1. For each y j , find the nearest T x i . We denote its index by f y (j) = i</p><formula xml:id="formula_1">2. Optimize for T in j y j − T x fy(j)</formula><p>In this work, we use a modified version of ICP which we call Mini-Batch Cycle ICP (MBC-ICP). MBC-ICP learns transformations T xy for X → Y and T yx for Y → X . We include cycle-constraints ensuring that a word x transformed to the Y do- main and back is unchanged (and similarly for ev- ery Y → X → Y transformation). The strength of the cycle constraints is parameterized by λ (we have λ = 0.1). We compute the nearest neigh- bor matches at the beginning of each epoch, and then optimize transformations T yx and T xy using mini-batch SGD with mini-batch size 128. Mini- batch rather than full-batch optimization greatly increases the success of the method. Experimen- tal comparisons can be seen in the results section. Note we only compute the nearest neighbors at the beginning of each epoch, rather than for each mini-batch due to the computational expense.</p><p>Every iteration of the final MBC-ICP algorithm therefore becomes:</p><p>1. For each y j , find the nearest T xy x i . We de- note its index by f y (j)</p><p>2. For each x i , find the nearest T yx y j . We de- note its index by f x (i)</p><p>3. Optimize T xy and T yx using mini-batch SGD for a single epoch of {x i } and {y j } on:</p><formula xml:id="formula_2">j y j − T xy x fy(j) + i x i − T yx y fx(i) + λ i x i − T yx T xy x i + λ j y j − T xy T yx y j</formula><p>A good initialization is important for ICP-type methods. We therefore begin with the projected data in which the transformation is assumed to be relatively small and initialize transformations T xy and T yx with the identity matrix. We denote this step PCA-MBC-ICP.</p><p>Once PCA-MBC-ICP has generated the corre- spondences functions f x (i) and f y (j), we run a MBC-ICP on the original 300D word vectors (no PCA). We denote this step: RAW-MBC-ICP. We initialize the optimization using f x (i) and f y (j) learned before, and proceed with MBC-ICP. At the end of this stage, we recover transformations ¯ T xy and ¯ T yx that transform the 300D word vec- tors from X → Y and Y → X respectively.</p><p>Reciprocal pairs: After several iterations of MBC-ICP, the estimated transformations become quite reliable. We can therefore use this transfor- mation to identify the pairs that are likely to be correct matches. We use the reciprocity heuristic: For every word y ∈ Y we find the nearest trans- formed word from the set {T xy x|x ∈ X}. We also find the nearest neighbors for the Y → X trans- formation. If a pair of words is matched in both X → Y and Y → X directions, the pair is de- noted reciprocal. During RAW-MBC-ICP, we use only reciprocal pairs, after the 50th epoch (this pa- rameter is not sensitive).</p><p>In summary: we run PCA-MBC-ICP on the 5k most frequent words after transformation to prin- ciple components. The resulting correspondences f x (i) and f y (j) are used to initialize a RAW- MBC-ICP on the original 300D data (rather than PCA), using reciprocal pairs. The output of the method are transformation matrices T xy and T yx .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-tuning</head><p>MBC-ICP is able to achieve very competitive per- formance without any further finetuning or use of large corpora. GAN-based methods on the other hand require iterative finetuning ( <ref type="bibr" target="#b4">Conneau et al., 2017;</ref><ref type="bibr" target="#b10">Hoshen and Wolf, 2018</ref>) to achieve competitive performance. To facilitate compari- son with such methods, we also add a variant of our method with identical finetuning to ( <ref type="bibr" target="#b4">Conneau et al., 2017</ref>). As we show in the results section, fine-tuning European languages typically results in small improvements in accuracy (1-2%) for our method, in comparison to 10-15% for the previ- ous work. Following <ref type="bibr" target="#b23">(Xing et al., 2015;</ref><ref type="bibr" target="#b4">Conneau et al., 2017)</ref>, fine-tuning is performed by running the Procrustes method iteratively on the full vo- cabulary of 200k words, initialized with the final transformation matrix from MBC-ICP. The Pro- crustes method uses SVD to find the optimal or- thonormal matrix between X and Y given approx- imate matches. The new transformation is used to finetune the approximate matches. We run 5 iter- ations of successive transformation and matching estimation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Matching Metrics</head><p>Although we optimize the nearest neighbor metric, we found that in accordance with ( <ref type="bibr" target="#b4">Conneau et al., 2017)</ref>, neighborhood retrieval methods such as In- verted Soft-Max (ISF) ( <ref type="bibr" target="#b21">Smith et al., 2017)</ref> and Cross-domain Similarity Local Scaling (CSLS) improve final retrieval performance. We there- fore evaluate using CSLS. The similarity between a word x ∈ X and a word y ∈ Y is computed as 2 cos(T xy x, y) − r(T xy x) − r(y), where r(.) is the average cosine similarity between the word and its 10-NN in the other domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multiple Stochastic Solutions</head><p>Our approach utilizes multiple stochastic solu- tions, to provide a good initialization for the MBC- ICP algorithm. There are two sources of stochas- ticity in our system: (i) The randomized nature of the PCA algorithm (it uses random matrices ( <ref type="bibr" target="#b15">Liberty et al., 2007)</ref>) (ii) The order of the train- ing samples (the mini-batches) when training the transformations. The main issue faced by unsupervised learn- ing in the case of multiple solutions, is either (i) choosing the best solution in case of a fixed par- allel run budget, or (ii) finding a good stopping criterion if attempting to minimize the number of runs serially.</p><p>We use the reconstruction cost as an unsuper- vised metrics for measuring convergence of MBC- ICP. Specifically, we measure how closely every x ∈ X and y ∈ Y is reconstructed by a trans- formed word from the other domain.</p><formula xml:id="formula_3">j y j − T xy x fy(j) + i x i − T yx y fx(i) (2)</formula><p>Although for isotropic distributions this has many degenerate solutions, empirically we find that val- ues that are significantly lower than the median al- most always correspond to a good solution.</p><p>The optimization profile of MBC-ICP is pre- dictable and easily lends itself for choosing effec- tive termination criteria. The optimization profile of a successfully converged and non-converging runs are presented in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The reconstruction loss clearly distinguish between the converged and non-converging runs. <ref type="figure" target="#fig_0">Fig. 1(b,c)</ref> presents the dis- tribution of final reconstruction costs for 500 dif- ferent runs for En-F r and En-Ar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluated our method extensively to ensure that it is indeed able to effectively and efficiently perform unsupervised word translation. As a strong baseline, we used the code and datasets from the MUSE repository by <ref type="bibr" target="#b4">Conneau et al. (2017)</ref>  <ref type="bibr">1</ref> . Care was taken in order to make sure that we report these results as fairly as possible: <ref type="formula" target="#formula_0">(1)</ref> the results from the previous work were copied as is, except for En-It, where our runs indicated better baseline results. (2) For languages not reported, we ran the code with multiple options and report the best results obtained. One crucial option for GAN was whether to center the data or not. From communication with the authors we learned that, in nearly all non-European languages, centering the data is crucial. For European languages, not centering gave better results. For Arabic, center- ing helps in one direction but is very detrimental in the other. In all such cases, we report the best baseline result per direction. (3) For the super- vised baseline, we report both the results from the original paper (in Tab. 1) and the results post Pro- crustes finetuning, which are better (Tab. 2). (4) Esperanto is not available in the MUSE repository at this time. We asked the authors for the data and will update the paper once available. Currently we are able to say (without the supervision data) that our method converges on En-Eo and Eo-En.</p><p>The evaluation concentrated on two aspects of the translation: (i) Word Translation Accuracy measured by the fraction of words translated to a correct meaning in the target language, and (ii) Runtime of the method.</p><p>We evaluated our method against the best meth- ods from ( <ref type="bibr" target="#b4">Conneau et al., 2017)</ref>. The supervised baseline method learns an alignment from 5k su- pervised matches using the Procrustes method. The mapping is then refined using the Procrustes method and CSLS matching on 200k unsupervised word vectors in the source and target languages. The unsupervised method proposed by ( <ref type="bibr" target="#b4">Conneau et al., 2017)</ref>, uses generative adversarial domain mapping between the word vectors of the 50k most common words in each language. The mapping is then refined using the same procedure that is used in the supervised baseline.</p><p>A comparison of the word translation accura-cies before finetuning can be seen in Tab. 1. Our method significantly outperforms the method of ( <ref type="bibr" target="#b4">Conneau et al., 2017</ref>) on all of the evaluated Eu- ropean language pairs. Additionally, for these lan- guages, our method performs comparably to the supervised baseline on all pairs except En-Ru for which supervision seems particularly useful. The same trends are apparent for simple nearest neigh- bors and CSLS although CSLS always performs better. For non-European languages, none of the unsupervised methods succeeds on all languages. We found that the GAN baseline fails on Farsi, Hindu, Bengali, Vietnamese and one direction of Japanese and Indonesian while our method does not succeed on Chinese, Japanese and Vietnamese. We conclude that the methods have complemen- tary strengths, our method doing better on more languages. On languages where both methods suc- ceed, MBC-ICP tends to do much better.</p><p>We present a comparison between the meth- ods after finetuning and using the CSLS metric in Tab. 2. All methods underwent the same finetun- ing procedure. We can see that our method still outperforms the GAN method and is comparable to the supervised baseline on European languages. Another observation is that on most European lan- guage pairs, finetuning only makes a small differ- ence for our method (1-2%). An unaligned vocab- ulary of 7.5k is sufficient to achieve most of the ac- curacy. This is in contrast with the GAN, that ben- efits greatly from finetuning on 200k words. Non- European language and English pairs are typically more challenging, finetuning helps much more for all unsupervised methods.</p><p>It is interesting to examine the languages on which each method could not converge. They typ- ically fall into geographical and semantic clusters. The GAN method failed on Arabic and Hebrew, Hindu, Farsi and Bengali. Whereas our method failed on Japanese and Chinese. We suspect that different statistical properties favor each method.</p><p>We also compare the different methods in terms of training time required by the method. We em- phasize that our method is trivially parallelizable, simply by splitting the random initializations be- tween workers. The run time of each solution of MBC-ICP is 47 seconds on CPU. The run time of all solutions can therefore be as low as a single run, at linear increase in compute resources. As it runs on CPU, parallelization is not expensive. The average number of runs required for conver- gence depends on the language pair (see below, <ref type="figure">Fig. 2</ref>). We note that our method learns transla- tions for both languages at the same time.</p><p>The current state-of-the-art baseline by (Con- neau et al., 2017) requires around 3000 seconds on the GPU. It is not obvious how to parallelize such a method efficiently. It requires about 30 times longer to train than our method (with par- allelization) and is not practical on a multi-CPU platform. The optional refinement step requires about 10 minutes. The performance increase of re- finement for our method are typically quite modest and can be be skipped at the cost of 1-2% in accu- racy, the GAN however requires finetuning to ob- tain competitive results. Another obvious advan- tage is that our method does not require a GPU.</p><p>Implementation: We used 100 iterations for the PCA-MBC-ICP stage on 50 PCA word vectors. This was run in parallel over 500 stochastic so- lutions. We selected the solution with the small- est unsupervised reconstitution criterion. This solution was used to initialize RAW-MBC-PCA, which we run for 100 iterations on the raw word vectors. The latter 50 iterations of RAW-MBC- ICP were carried out with only reciprocal pairs contributing to the optimization. Results were typ- ically not sensitive to hyper-parameter choice, al- though longer optimization generally resulted in better performance.</p><p>Ablation Analyses There are three important steps for the convergence of the ICP step: (i)  PCA, (ii) Dimensionality reduction, (iii) Multiple stochastic solutions. In Tab. 3 we present the ab- lation results on the En-Es pair with PCA and no dimensionality reduction, with only the top 50 PCs and without PCA at all (best run out of 500 cho- sen using the unsupervised reconstruction loss). We can observe that the convergence rate with- out PCA and with PCA but without dimension- ality reduction is much lower than with PCA, the best run without PCA has not succeeded in obtain- ing a good translation. This provides evidence that both PCA and dimensionality reduction are essen- tial for the success of the method.</p><p>We experimented with the different factors of randomness between runs, to understand the causes of diversity allowing for convergence in the more challenging language pairs (such as En-Ar). We performed the following four experiments: i. Fixing PCA and Batch Ordering. ii. Fixing all data batches to have the same ordering in all runs, iii. Fix the PCA bases of all runs, iv. Let both PCA and batch ordering vary between runs.</p><p>Tab. 4 compares the results on En-Es and En- Ar for the experiments described above. It can be seen that using both sources of stochasticity is usually better. Although there is some probability the PCA will result in aligned principle compo- nents between the two languages, this usually does not happen and therefore using stochastic PCA is highly beneficial.</p><p>Convergence Statistics In <ref type="figure">Fig. 2</ref> we present the statistics for all language pairs with Procrustes- ICP (P-ICP) vs MBC-ICP. In P-ICP, we first cal- culate the matches for the vocabulary, and then perform a batch estimate of the transformation us- ing the P-ICP method (starting from PCA word</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Es</head><p>Fr De Ru It Ar <ref type="figure">Figure 2</ref>: Histograms of the Reconstruction metric across 500 ICP runs for MBC-ICP (Red) and P-ICP (Blue). The comparison is shown for En-Es, En-Fr, En-De, En-Ru, En-It, En-Ar. On average MBC-ICP converges to much better minima. We can observe that MBC-ICP has many more converging runs than P-ICP. In fact for En-It and En-Ar, P-ICP does not converge even once in 500 runs.</p><p>vectors and T xy initialized at identity). The only source of stochasticity in P-ICP is the PCA where in MBC-ICP the order of mini-batches provides further stochasticity. Adding random noise to the mapping initialization was not found to help. Each plot shows the histogram in log space for the num- ber of runs that achieved unsupervised reconstruc- tion loss within the range of the bin. The con- verged runs with lower reconstruction values typ- ically form a peak which is quite distinct from the non-converged runs allowing for easy detection of converged runs. The rate of convergence gener- ally correlates with our intuition for distance be- tween languages (En-Ar much lower than En-Fr), although there are exceptions.</p><p>MBC-ICP converges much better than P-ICP: For the language pairs with a wide convergence range (En-Es, En-Fr, En-Ru) we can see that MBC-ICP converged on many more runs than P- ICP. For the languages with a narrow convergence range (En-Ar, En-It), P-ICP was not able to con- verge at all. We therefore conclude that the mini- batch update and batch-ordering stochasticity in- crease the convergence range and is important for effective unsupervised matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented an effective technique for un- supervised word-to-word translation. Our method is simple and non-adversarial. We showed empir- ically that our method outperforms current state- of-the-art GAN methods in terms of pre and post finetuning word translation accuracy. Our method runs on CPU and is much faster than current meth- ods when using parallelization. This will enable researchers from labs that do not possess graphi- cal computing resources to participate in this ex- citing field. The proposed method is interpretable, i.e. every stage has an intuitive loss function with an easy to understand objective.</p><p>It is interesting to consider the relative perfor- mance between language pairs. Typically more related languages yielded better performance than more distant languages (but note that Indonesian performed better than Russian when translated to English). Even more interesting is contrasting the better performance of our method on West and South Asian languages, and GAN's better perfor- mance on Chinese.</p><p>Overall, our work highlights the potential bene- fits of considering alternatives to adversarial meth- ods in unsupervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Evolution of reconstruction loss as a function of epoch number for successful (Blue) and unsuccessful runs (Red). (b) The final reconstruction loss distribution for En-Fr. (c) A similar histogram for En-Ar.</figDesc><graphic url="image-1.png" coords="5,77.98,62.81,139.24,104.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Comparison of word translation accuracy (%) -without finetuning. Bold: best unsupervised method.</head><label>1</label><figDesc></figDesc><table>Pair 
Supervised 
Unsupervised 

Baseline 
GAN 
Ours 

nn 
csls 
nn 
csls 
nn 
csls 

European Languages 

En-Es 77.4 81.4 69.8 75.7 75.9 81.1 
Es-En 77.3 82.9 71.3 79.7 76.0 82.1 

En-Fr 
74.9 81.1 70.4 77.8 74.8 81.5 
Fr-En 
76.1 82.4 61.9 71.2 75.0 81.3 

En-De 68.4 73.5 63.1 70.1 66.9 73.7 
De-En 67.7 72.4 59.6 66.4 67.1 72.7 

En-Ru 47.0 51.7 29.1 37.2 36.8 44.4 
Ru-En 58.2 63.7 41.5 48.1 48.4 55.6 

En-It 
75.7 77.3 54.3 65.1 71.1 77.0 
It-En 
73.9 76.9 55.0 64.0 70.4 76.6 

Non-European Languages 

En-Fa 
25.7 33.1 
* 
* 
19.6 29.0 
Fa-En 
33.5 38.6 
* 
* 
28.3 28.3 

En-Hi 23.8 33.3 
* 
* 
19.4 30.3 
Hi-En 34.6 42.8 
* 
* 
30.5 38.9 

En-Bn 10.3 15.8 
* 
* 
9.7 
13.5 
Bn-En 21.5 24.6 
* 
* 
7.1 
14.5 

En-Ar 31.3 36.5 18.9 23.5 26.9 33.3 
Ar-En 45.0 49.5 28.6 
31 
39.8 45.5 

En-He 10.3 15.8 17.9 22.7 31.3 38.9 
He-En 21.5 24.6 37.3 39.1 43.4 50.8 

En-Zh 40.6 42.7 12.7 16.0 
* 
* 
Zh-En 30.2 36.7 18.7 25.1 
* 
* 

En-Ja 
2.4 
1.7 
* 
* 
* 
* 
Ja-En 
0.0 
0.0 
3.1 
3.6 
* 
* 

En-Vi 
25.0 41.3 
* 
* 
* 
* 
Vi-En 
40.6 55.3 
* 
* 
* 
* 

En-Id 
55.3 65.6 18.9 23.5 39.4 57.1 
Id-En 
58.3 65.0 
* 
* 
37.1 58.1 
*Failed to converge 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Word translation accuracy (%) -after finetun- ing and using CSLS. Bold: best unsupervised methods.</head><label>2</label><figDesc></figDesc><table>Pair 
Supervised Unsupervised 

Baseline 
GAN Ours 

European Languages 

En-Es 
82.4 
81.7 
82.1 
Es-En 
83.9 
83.3 
84.1 

En-Fr 
82.3 
82.3 
82.3 
Fr-En 
83.2 
82.1 
82.9 

En-De 
75.3 
74.0 
74.7 
De-En 
72.7 
72.2 
73.0 

En-Ru 
50.7 
44.0 
47.5 
Ru-En 
63.5 
59.1 
61.8 

En-It 
78.1 
76.9 
77.9 
It-En 
78.1 
76.7 
77.5 

Non-European Languages 

En-Fa 
32.6 
* 
34.6 
Fa-En 
40.2 
* 
41.5 

En-Hi 
34.5 
* 
34.6 
Hi-En 
44.8 
* 
44.5 

En-Bn 
16.6 
* 
14.7 
Bn-En 
24.1 
* 
21.9 

En-Ar 
34.5 
35.3 
35.1 
Ar-En 
49.7 
49.7 
50.6 

En-He 
41.1 
41.6 
40.5 
He-En 
54.9 
52.6 
52.9 

En-Zh 
42.7 
32.5 
* 
Zh-En 
36.7 
31.4 
* 

En-Ja 
1.7 
* 
* 
Ja-En 
0.0 
4.2 
* 

En-Vi 
44.6 
* 
* 
Vi-En 
56.9 
* 
* 

En-Id 
68.0 
67.8 
68.0 
Id-En 
68.0 
66.6 
68.0 
*Failed to converge 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : En-Es accuracy with and without PCA</head><label>3</label><figDesc></figDesc><table>Method 
En-Es 
Es-En 

No PCA 
0.0% 
0.0% 
With 300 PCs 
0.0% 
0.0% 
With 50 PCs 
82.2% 83.8% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Fraction of converging runs per stochasticity. 

Method 
En-Es En-Ar 

No randomization 
0.0% 
0.0% 
Randomized Ordering 
0.0% 
0.0% 
Randomized PCA 
9.8% 
0.0% 
Randomized Ordering + PCA 16.8% 1.2% 

</table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/MUSE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05776</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Investigating the effects of multiple factors towards more accurate 3-d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apostolos</forename><surname>Axenopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Litos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="388" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compiling bilingual lexicon entries from a non-parallel english-chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Very Large Corpora. Massachusetts Institute of Technology Cambridge</title>
		<meeting>the Third Workshop on Very Large Corpora. Massachusetts Institute of Technology Cambridge</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="173" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An IR approach for translating new words from nonparallel, comparable texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Lo Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Computational linguistics</title>
		<meeting>the 17th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="414" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLHLT</title>
		<meeting>ACLHLT</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying analogies across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised bilingual lexicon induction with multiple monolingual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a translation lexicon from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition</title>
		<meeting>the ACL-02 workshop on Unsupervised lexical acquisition</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Randomized algorithms for the low-rank approximation of matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Liberty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Woolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
	<note>Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying word translations in non-parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 33rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="320" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic identification of word translations from unrelated english and german corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inducing translation lexicons via diverse similarity measures and bridge languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 6th conference on Natural language learning</title>
		<meeting>the 6th conference on Natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03859</idno>
		<title level="m">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00179</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s distance minimization for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
