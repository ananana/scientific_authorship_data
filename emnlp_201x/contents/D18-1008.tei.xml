<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Textual Analogy Parsing: What&apos;s Shared and What&apos;s Compared among Analogous Facts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">Tejasvi</forename><surname>Chaganty</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Stanford Computer Science</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Stanford Computer Science</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<email>{mlamm,jurafsky}@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Stanford Computer Science</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Stanford Computer Science</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Textual Analogy Parsing: What&apos;s Shared and What&apos;s Compared among Analogous Facts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="82" to="92"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>82</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To understand a sentence like &quot;whereas only 10% of White Americans live at or below the poverty line, 28% of African Americans do&quot; it is important not only to identify individual facts, e.g., poverty rates of distinct demographic groups, but also the higher-order relations between them, e.g., the disparity between them. In this paper, we propose the task of Textual Analogy Parsing (TAP) to model this higher-order meaning. The output of TAP is a frame-style meaning representation which explicitly specifies what is shared (e.g., poverty rates) and what is compared (e.g., White Americans vs. African Americans, 10% vs. 28%) between its component facts. Such a meaning representation can enable new applications that rely on discourse understanding such as automated chart generation from quantitative text. We present a new dataset for TAP, baselines, and a model that successfully uses an ILP to enforce the structural constraints of the problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of information extraction by and large seeks to populate a knowledge base with individ- uated facts extracted from text <ref type="bibr">(Sarawagi, 2008)</ref>. For example, given the sentence:  In textual analogy parsing (TAP), one maps analogous facts to semantic role represen- tations and identifies analogical relations between them. Automated chart generation from text is a motivating application of TAP. and C2 are juxtaposed <ref type="bibr" target="#b13">(Kehler, 2002</ref>). Thus the author intends that we consider them in relation to each other, inviting us to note, for example, a dis- parity of wealth distribution between demographic groups. To fail to capture this is to miss out on an important aspect of text understanding. We propose the task of Textual Analogy Parsing (TAP) to explicitly capture such relational mean- ing between analogous facts in text. Concretely, TAP first maps a set of analogous facts to semantic role (SRL) representations, and then identifies the roles along which they are similar (the shared con- tent) and along which they are distinct (the com- pared content)-see <ref type="figure" target="#fig_0">Figure 1</ref>. The resulting rep- resentation, the TAP frame, is a deeper represen- tation than the one output by shallow discourse parsers ( <ref type="bibr">Taboada and Mann, 2006;</ref><ref type="bibr" target="#b34">Prasad et al., 2007;</ref><ref type="bibr" target="#b32">Pitler et al., 2009;</ref><ref type="bibr" target="#b33">Prasad et al., 2010;</ref><ref type="bibr">Surdeanu et al., 2015)</ref>. Given (E1) above, a shallow discourse parser would classify the relation of con- trast between C1 and C2-indicating that some salient differences exist in the meanings of the jux- taposed phrases-but without identifying the na-According to the U.S. Census S12 , whereas only 10% V1 of White Americans W1 live at or below the poverty line <ref type="bibr">Q1</ref> to- day Tm12 , 28% V2 of African Ameri- cans <ref type="bibr">W2</ref> do <ref type="bibr">Q2</ref> . 10% V1 28%</p><formula xml:id="formula_0">(E1) [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>White Americans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W1</head><p>African Americans W2 U.S. Census S12 today Tm12 live at or below the poverty line</p><formula xml:id="formula_1">Q1 do Q2                   source U.S.</formula><p>Census quant live at or below the poverty line ≡ do</p><formula xml:id="formula_2">time today                   </formula><p>whole White Americans</p><formula xml:id="formula_3">value 10%     whole African Americans value 28%                                      mention analogy graph analogy frame</formula><p>Figure 2: The mapping from utterance to TAP frame. Vertices in the graph are labeled with abbreviated semantic roles. Single lines represent edges between a VALUE and other roles in its associated fact. Dou- ble lines represent coreference and synonymy. Springs represent analogy. Note that vertices connected by equivalence arcs, or any span which connects to both V1 and V2 via fact relations (i.e., scope), map to the shared content of the TAP frame. Analogous spans map to the compared content.</p><p>ture of those differences. We focus on applying TAP to quantitative facts, because TAP frames can be used to create graphi- cal plots from sentences with numbers, as in Fig- ure 1. This new application could help to sim- plify complex quantitative text on the web ( <ref type="bibr" target="#b1">Barrio et al., 2016;</ref><ref type="bibr" target="#b22">Leonhardt et al., 2017</ref>). We thus created an expert-annotated dataset of TAP frames over quantitative facts in the Wall Street Journal corpus <ref type="bibr" target="#b25">(Marcus et al., 1999</ref>).</p><p>We model TAP by jointly predicting SRL rep- resentations of facts in a sentence, and higher- order semantic relations between them. Our main findings are that a neural architecture outperforms a log-linear baseline, well-chosen linguistic fea- tures help performance, and so does the use of an integer-linear programming (ILP) decoder that en- forces the structural constraints of the task. Nev- ertheless, both quantitative and qualitative evalua- tion reveal room for improvement on TAP.</p><p>In sum, our main contributions are (1) a new task, Textual Analogy Parsing (TAP), that com- bines shallow semantic parsing with discourse meaning, (2) a dataset of TAP frames from quan- titative newswire, and (3) a preliminary study of a new application, automated chart generation from text. All data and code, including standardized evaluation scripts, are made freely available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Semantic Representation of Analogy</head><p>Let us revisit the example sentence from the previ- ous section (E1), where a pair of analogous quan- titative facts about poverty rates of different demo- graphic groups are presented in contrast. Individ- ually, these can be represented using the semantic role structures in <ref type="figure">Figure 3</ref>, but representing them separately in this way fails to capture the fact that Figure 3:</p><formula xml:id="formula_4">          source U.S.</formula><p>Two analogous quantitative facts represented independently, using the QSRL schema <ref type="bibr" target="#b18">(Lamm et al., 2018</ref>). they are analogous, i.e., structurally and semanti- cally similar but distinct.</p><p>Instead, we can explicitly show points of sim- ilarity and difference between them in the two- tiered frame structure in <ref type="figure">Figure 2</ref>, which we call a TAP frame. The outer tier of the TAP frame con- tains shared content, or information pertinent to all of the facts in question, and the inner tier con- tains compared content, the information that varies across the set of facts.</p><p>Mapping from an utterance to a TAP frame re- quires three types of relational reasoning. Firstly, one must decompose the utterance into a set of facts, where a fact is represented as a set of se- mantic roles. Then, one must identify the shared content across facts by aligning roles that are se- mantically equivalent, in the sense that they are ei- ther the same span, are coreferent, or are synony- mous. For example, in <ref type="figure">Figure 2</ref> the phrase 'U.S. Census' occurs as the SOURCE of both facts be- cause it scopes over the entire sentence in which they appear. Additionally, one must identify the compared content by aligning roles that are analo- gous, in the sense that they are semantically sim- ilar but nevertheless distinct. For example, the phrases 'White Americans' and 'African Ameri- cans' are analogous in our running sentence, play- ing the same role in their respective facts, while signifying distinct demographic groups.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Quantitative TAP Dataset</head><p>Motivated by the application of automated graph- ical plot generation from text, we annotated a dataset of quantitative TAP frames from the Penn Treebank WSJ corpus ( <ref type="bibr" target="#b25">Marcus et al., 1999</ref>).</p><p>As our SRL representation of quantitative facts, we employ the Quantitative Semantic Role Label- ing (QSRL) framework we previously defined in <ref type="bibr" target="#b18">Lamm et al. (2018)</ref>. Having identified a numerical VALUE in text (e.g., 10%), QSRL asks, "what does this number measure?" to determine its associ- ated QUANTITY (e.g., a poverty rate). It might also identify, for example, the WHOLE out of which this percentage is measured (e.g., the set of African Americans), and the TIME at which the quantity took on the value (e.g., today), etc. We employ all fifteen QSRL roles in our annotations.</p><p>Our annotations not only capture the relation between a quantitative predicate and its argu- ments, but also the higher-order analogy relations between them. The distinction is reflected in the sentences in <ref type="table">Table 1</ref> from the dataset: Colored spans are co-indexed when they participate in the same quantitative fact; spans with like roles sur- rounded by parentheses are shared content, mean- ing that they are either synonymous or co-referent; spans with like roles surrounded by brackets are compared content, meaning that they are analo- gous but semantically distinct.</p><p>To identify instances of quantitative analogy in the WSJ corpus, we first prune out any sen- tence having fewer than three numerical mentions, where a numerical mention is defined as a con- tiguous sequence of CD POS tags. Of those left, we manually identify those containing one or more quantitative analogies, i.e., ones in which numeri- cal values are compared content. We estimate the incidence of these to be around 20%. A linguist then annotated 1,100 of these for analogy relation- ships. See <ref type="table" target="#tab_3">Table 2</ref> for a summary.</p><p>Using an independent set of expert annotations on 100 of these sentences, we measured a signifi- cant per-token label agreement of 0.882 and edge label agreement of 0.991 using Krippendorf's α. <ref type="table" target="#tab_3">2  Table 1</ref> highlights some of the challenging lin- guistic phenomena in the data. With respect to identifying the shared content of a TAP frame, these can be coarsely divided into two sets. Firstly, in scope, ellipsis, and gapping, a single syntac- tic element serves as a role in multiple QSRL frames. This is exemplified by the phrase 'PS of New Hampshire' in <ref type="table">Table 1</ref>(a): It is mentioned ex- plicitly as a THEME of the first fact, and only im- plied in the second two. Based on a random sam- ple of 100 train sentences, we estimate that 86% of frames in the data exhibit these phenomena. Sec- ondly, in synonymy and coreference, multiple ele- ments appear in a sentence but contribute the same role to the shared content, e.g., 'offered' and 'bid' in <ref type="table">Table 1</ref>(a). We estimate that 31% of frames in the data exhibit these phenomena.</p><p>One must learn to identify analogy relationships over a diverse set of compared content roles, with distinct semantic properties: in <ref type="table">Table 1</ref>(a), AGENT is a compared content role, whereas in <ref type="table">Table 1</ref>(b), CAUSE is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling TAP in the Quantitative Setting</head><p>We model TAP by generating a typed analogy graph over spans of an input text that is isomor- phic to the set of TAP frames in that text, e.g., <ref type="figure">Figure 2</ref>. Each vertex in the graph corresponds to a role-labeled span, and edges represent semantic relations between them.</p><p>In this graph, each fact is uniquely identified by a VALUE vertex, which is connected via a FACT edge to all of its associated roles. Any two shared content vertices across facts are connected by an EQUIVALENCE edge, indicating that they are coreferent or synonymous. A single vertex can also be shared across facts by linking via a FACT edge to more than one VALUE vertex, sug- gesting a scopal relationship. Finally, any two ver- tices which are compared content in the graph are linked via an ANALOGY edge.</p><p>More formally, given an utterance x with to- kens x 1 , . . . , x n , let G be a graph with vertices V and edges E. For a vertex v = (i, j, l) ∈ V , 1 ≤ i &lt; j ≤ n are the start and end token indices of a span in x with role l ∈ L Q def = {VALUE, . . . , QUANT}, the set of QSRL roles. For</p><formula xml:id="formula_5">an edge e = (v, v , l) ∈ E, v, v ∈ V and l ∈ L R def = {FACT, EQUIVALENCE, ANALOGY}.</formula><p>For G so defined to encode a set of valid TAP frames, it must satisfy certain constraints:</p><p>1. Well-formedness constraints. For any two vertices v, v ∈ V , their associated spans must not overlap. Furthermore, every vertex must participate in at least one FACT edge, i.e., no disconnected vertices.</p><p>2. Typing constraints. FACT relations are al- ways drawn from a VALUE vertex to a non- VALUE vertex. ANALOGY and EQUIVA- LENCE are only ever drawn between two ver- tices of the same role.</p><p>3. Unique facts. If a VALUE vertex v is con- nected to two distinct vertices v and v of the same role via a FACT edge, then EQUIVALENCE(v , v ) exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transitivity constraints.</head><p>ANALOGY and EQUIVALENCE edges are transi- tive: if EQUIVALENCE(v, v ) ∈ E and EQUIVALENCE(v , v ) ∈ E then EQUIVALENCE(v, v ) ∈ E also. This also holds for ANALOGY edges, but only when v, v and v are VALUE vertices.</p><p>5. Analogy. There must be at least one pair of analogous VALUE vertices, and for each such pair, there must be a pair of analo- gous facts connected to them: if v, v are two VALUE vertices with ANALOGY(v, v ) ∈ E, then there must also exist w, w as two non-VALUE vertices with FACT(v, w) ∈ E,</p><formula xml:id="formula_6">FACT(v , w ) ∈ E, ANALOGY(w, w ) ∈ E.</formula><p>Note that while these constraints rely on the choice of VALUE as the role that grounds quantitative facts, they reflect the general idea that analogy is a structured mapping between meaning represen- tations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Neural and ILP Model for TAP</head><p>We now present a neural and ILP model that predicts analogy graphs as defined in Section 4. Given a sentence, the neural model predicts a dis- tribution over role-labeled spans with edges denot- ing semantic relations between them. Then, we use an ILP to decode while enforcing the TAP con- straints defined in Section 4. <ref type="figure" target="#fig_3">Figure 4</ref> presents an overview of the architecture.</p><p>Context-sensitive word embeddings. We first encode the words in a sentence by embedding each token using fixed word embeddings. We also concatenate a few linguistic features to the word embeddings, such as named entity tags and dependency relations. These features are gen- erated using CoreNLP ( ) and represented by randomly-initialized, learned embeddings for symbols together with the fixed word embedding of each token's dependency head and the dependency path length between adja- cent tokens. The token embeddings are then passed through several stacked convolutional lay- ers <ref type="bibr" target="#b15">(Kim, 2014)</ref>. While the first convolutional layer can only capture local information, subse- quent layers allow for longer-distance reasoning.</p><p>Span prediction. Next, we feed the outputs of a single fully-connected hidden layer to a condi- tional random field (CRF) ( <ref type="bibr" target="#b17">Lafferty et al., 2001</ref>),  The sentence embedding represents fea- tures across the entire sentence using multiple con- volutional layers. We then use a conditional ran- dom field (CRF) layer to predict labeled spans p m and to generate span and edge embeddings. We use a feedforward (FF) layer on the edge embed- dings to predict edge labels p mn . Together, p m and p mn form a distribution over edges and labels that we decode into TAP frames.</p><p>which defines a joint distribution over per-token role labels. We thus obtain spans from this distri- bution corresponding to vertices of the graph de- scribed in Section 4 by merging contiguous role- labels in the maximum likelihood label sequence predicted by the CRF.</p><p>Edge prediction with PATHMAX features. For edge prediction, we use the spans identified above to construct span and edge embeddings: for every span (i, j) that was predicted, we construct a span vector s m = j k=î x k . We also construct a role- label score vector for the span, p m by summing the role-label probability vectors of its constituent to- kens. Then, for every vertex pair (m, n), we con- struct an edge representation e mn . The basis of this representation is simply the concatenation of the span representations, the sum of the span rep- resentations, their respective role-label score vec- tors p m and p n , and relative token distances.</p><p>To capture long-distance phenomena like scope, we also incorporate features into e mn from the de- pendency paths between the two spans by max- pooling the (learned) dependency relation embed- dings along the path between the tokens. 3 When computing the representation between two spans, we take the average of the path embedding be- tween each pair of tokens within them. We call this extension PATHMAX.</p><p>The resulting edge representation e mn is passed through a single fully-connected hidden layer and an output layer to predict a distribution over edge labels p mn , for each pair of spans.</p><p>Training. The supervised data described in Sec- tion 3 provides gold spans and edges between them. Thus we define a loss function with two terms: one for the log-likelihood of the span labels output by the CRF model, and one for the cross- entropy loss on the edge labels. We train the span and edge components of the model jointly.</p><p>Decoding. We consider two methods for decod- ing the span-level and edge-level label distribu- tions p m and p m,n into a labeled graph respecting the constraints described in Section 4.</p><p>As a simple greedy method to enforce these constraints, we begin by picking the most likely role for each span and edge and then discard- ing any edges and spans that violate the well- formedness (1) and typing constraints (2). We then enforce transitivity constraints (4) by incremen- tally building a cluster of analogous and equivalent spans. We then resolve the unique facts constraint (3) by keeping only the span with highest FACT edge score. Finally, for every cluster of analogous VALUE spans, we check that the analogy constraint (5) holds and if not, discard the cluster.</p><p>We also implement an optimal decoder that en- codes the TAP constraints as an ILP ( <ref type="bibr" target="#b36">Roth and Yih, 2004;</ref><ref type="bibr" target="#b4">Do et al., 2012</ref>). The ILP tries to find an optimal decoding according to the model, subject to hard constraints imposed on the solution space. For example, we require that solutions satisfy the 'connected spans' constraint:</p><formula xml:id="formula_7">∀s∃s : e(s, s , FACT)</formula><p>In plain English, this says that every span s in a so- lution must be connected via a FACT edge to some other span s . See the supplementary material for the full list of constraints we employ. We solve the ILPs with Gurobi (Gurobi Optimization, Inc., 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We now describe the experimental setup of our neural model (Section 5) on the dataset of TAP frames we created (Section 3). Results and dis- cussion are reported in Section 7.</p><p>Evaluation metrics. The primary metric we use to measure the accuracy of a system on frame pre- diction is the precision, recall and F 1 between the labeled vertex-edge-vertex triples predicted by the model and those in the gold parse. If there are multiple predicted spans that overlap with a sin- gle gold span or vice versa, we find a matching of predicted and gold spans that maximizes overlap.</p><p>In addition to the primary metric, we also report precision, recall and F 1 when predicting labeled (non-VALUE) spans and predicting labeled edges before performing any decoding. <ref type="bibr">4</ref> We also use the matching process described above for both these sets of metrics. Standardized evaluation code is provided with the dataset.</p><p>Experimental setup. We compare the neural models presented in Section 5 in addition to a log- linear baseline. The log-linear baseline uses the same fixed word embeddings as the neural model in addition to the named entity and dependency parse features described in Section 5. The key difference is that instead of learning a sentence embedding or hidden layers, the log-linear model simply uses a CRF to predict span labels directly from fixed input features, and then uses a single sigmoid layer to predict edge labels from deter- ministic edge embeddings, e mn .</p><p>For the neural models, we used three convolu- tional layers for sentence embedding with a fil- ter size of 3. Every layer other than the in- put layer used a hidden dimension of 50 with ReLU nonlinearities. We introduced a single dropout layer (p = 0.5) between every two layers in the network (including at the input). We used 50-dimensional GloVe embeddings <ref type="bibr" target="#b31">(Pennington et al., 2014</ref>) learned from Wikipedia 2014 and Gigaword 5 as pre-trained word embeddings, and initialized the embeddings for the features randomly. We chose relatively low input-and hidden-vector dimension because of the size of our data. The network was trained for 15 epochs using ADADELTA <ref type="bibr">(Zeiler, 2012</ref>) with a learn- ing rate of 1.0. All models were implemented in PyTorch (Paszke et al., 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Discussion</head><p>Frame prediction results on the test set are sum- marized in <ref type="table" target="#tab_5">Table 3</ref>. Our three main findings are that (i) the neural network model far outperforms <ref type="bibr">4</ref> We exclude VALUE spans from span scores because they are easy to predict and thus inflate model performance.    <ref type="table">Table 4</ref>: Performance of models on labeled (non- VALUE) span prediction during cross-validation prior to decoding. We found using a CRF to be the most important aspect: simply using fixed word vectors with a CRF (i.e., the log-linear model) was sufficient to predict spans.</p><p>the log-linear model on our frame metric, (ii) in- cluding linguistic features further increases perfor- mance, and (iii) so does using an optimal decoder over a greedy method.</p><p>Quantitative error analysis. To better under- stand which aspects of our model contribute to the task, we perform an ablation study on the span and edge predictions of our model prior to decoding. With respect to span prediction <ref type="table">(Table 4)</ref>, we found that the fixed word vectors, along with a CRF, were able to capture the information needed to identify QSRL role-spans. Indeed, the log- linear baseline, which directly uses these word vectors as features for a CRF, did the best at span prediction. We believe that the drop in perfor- mance from introducing hidden layers with the neural models is a result of the model updating its span representations to do better edge prediction. <ref type="bibr">5</ref> Edge prediction  <ref type="table">Table 5</ref>: Performance of models on labeled edge prediction during cross-validation prior to decod- ing. We found that both dependency label (dep.) and path features (PATHMAX) help significantly.</p><p>While the log-linear model did well at predict- ing spans, it did a poor job predicting edges, in- dicating that learning to extract higher-order fea- tures from learned span embeddings is necessary for identifying semantic relations between them <ref type="table">(Table 5</ref>). We also found that linguistic fea- tures were important: in particular, we found that syntactic features -the dependency path features (PATHMAX) and dependency labels -played a big role in edge prediction, followed by type informa- tion from NER tags.</p><p>Qualitative error analysis. Our model is tasked with jointly identifying QSRL parses of analogous facts in a sentence, and ANALOGY and EQUIV- ALENCE relations among them. As described in Section 4, these pieces interact in mutually con- straining ways, and thus it is possible for local er- rors to have global effects on predicted frames.</p><p>In <ref type="figure">Figure 5</ref>, for example, the model correctly identifies the gold TIME spans as part of a TAP frame, but mistakenly predicts that they are linked by EQUIVALENCE, and thus modify the same VALUE span. In the gold parse, they are linked by ANALOGY, and modify distinct VALUE spans. As a result of this misclassification, the model leaves out an entire QSRL fact from the resulting parse.</p><p>In many cases, the model successfully identi- fies compared content roles between QSRL facts. In <ref type="figure" target="#fig_5">Figure 6</ref>, we show an example where it does not manage to do so. Here, unable to identify the ANALOGY relation between the phrases 'Those with a bullish view' and 'the dollar bears', the model instead chooses two identical sequences 'the dollar' as the non-VALUE compared content. Inspecting edge probability scores from the model before decoding reveals that the neural model thinks that the first instance of 'the dollar' in the </p><formula xml:id="formula_8">          theme daily contracts traded              time this year ≡ a year earlier value 4,645 time 1984 value 917                                        quant daily contracts traded                         </formula><formula xml:id="formula_9">                                         gold predicted</formula><p>Figure 5: TAP frames for the sentence, 'This year . . . daily contracts traded totaled 9,118, up from 4,645 a year earlier and from 917 in 1984.' The model not only misclassifies the QSRL role of 'daily contracts traded', but also mistakenly iden- tifies an EQUIVALENCE between 'this year' and 'the year earlier'. As a result, the VALUE 9,118 is left without a compared content role, and is dropped.</p><formula xml:id="formula_10">                     quant the dollar1</formula><p>value 1.9000 marks quant the dollar2</p><formula xml:id="formula_11">value 1.7600 marks                                 quant the dollar1 ≡ the U.S. currency              source</formula><p>Those with a bullish view value 1.9000 marks source the dollar2 bears sentence is semantically analogous to the second; it can be confused by surface similarity into clas- sifying ANALOGY relations.</p><formula xml:id="formula_12">value 1.7600 marks                         gold predicted</formula><p>Application to plot generation. As we have seen, textual analogy is frequently used to com- pare quantities along some axis of differentiation. For example, one might compare the stock prices of different companies, or describe the change in some quantity's value over time. Such analogy relationships can alternately be expressed in the form of a plot.</p><p>Indeed, there is a natural correspondence be- tween charts and TAP frames over quantitative facts: VALUES of a quantitative TAP frame are plotted against other compared content roles, and elements of the shared content correspond with scopal chart elements, such as titles. This mapping is well-defined provided analogous values share units. We present some initial results exploring this direction.</p><p>In <ref type="figure" target="#fig_6">Figure 7</ref>, we deterministically plot TAP frames generated by our system both before and after the imposition of global analogy constraints, for two sentences in the data. In the first sentence, VALUE spans are plotted against the TIME spans the model associates with their respective facts. In the second sentence, two analogy frames are plotted together, one reflecting the absolute val- ues of the stock prices mentioned (blue) and the other reflecting the changes in prices mentioned (red). Units are extracted from VALUE spans using simple pattern matching. Chart titles are only il- lustrative and were generated by stitching together shared content identified by our system. Note that with the imposition of global con- straints reflecting the structure of analogy, the sys- tem yields well-formed charts. Without these con- straints, generated charts either have multiple y- axis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Analogy. In the cognitive science literature, analogy is a general form of relational reason- ing unique to human cognition ( <ref type="bibr">Tversky and Gati, 1978;</ref><ref type="bibr" target="#b11">Holyoak and Thagard, 1996;</ref><ref type="bibr" target="#b8">Goldstone and Son, 2005;</ref><ref type="bibr" target="#b30">Penn et al., 2008;</ref><ref type="bibr" target="#b12">Holyoak, 2012)</ref>. Our model of textual analogy is particularly influenced by Structure Mapping Theory <ref type="bibr" target="#b5">(Falkenhainer et al., 1989;</ref><ref type="bibr" target="#b6">Gentner and Markman, 1997)</ref>, an influen- tial cognitive model of analogy as a structure- preserving map between concepts.</p><p>Within the NLP community, there has been much work focused on inferring lexical analogies between generic concepts, e.g., tennis:racket:: baseball:bat ( <ref type="bibr" target="#b26">Mikolov et al., 2013;</ref><ref type="bibr">Turney, 2013)</ref>, from global distributional statistics. Such analo- gies are generic, type-level patterns whose struc- ture exists in the nature of the language; here, we are interested in specific analogies whose structure is conveyed by a particular sentence.</p><p>Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text ( <ref type="bibr" target="#b0">Baker et al., 1998;</ref><ref type="bibr" target="#b7">Gildea and Jurafsky, 2002;</ref><ref type="bibr" target="#b28">Palmer et al., 2005</ref>); the lat- ter identifies discourse relations between syntactic clauses ( <ref type="bibr">Taboada and Mann, 2006;</ref><ref type="bibr" target="#b34">Prasad et al., 2007;</ref><ref type="bibr" target="#b32">Pitler et al., 2009;</ref><ref type="bibr" target="#b33">Prasad et al., 2010;</ref><ref type="bibr">Surdeanu et al., 2015)</ref>. TAP first maps from syntax to a set of SRL-style representations, and then identifies structurally- constrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by <ref type="bibr" target="#b2">Berant et al. (2014)</ref>.</p><p>Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning ( <ref type="bibr" target="#b16">Kushman et al., 2014;</ref><ref type="bibr">Roy et al., 2015)</ref>, numerical information extraction ( <ref type="bibr" target="#b23">Madaan et al., 2016)</ref>, and techniques for making numbers more easily interpretable in text <ref type="bibr" target="#b3">(Chaganty and Liang, 2016;</ref><ref type="bibr" target="#b14">Kim et al., 2016)</ref>.</p><p>If pursued further, the application of plotting quantitative text that we discuss in this paper could help to clarify quantitative text on the web <ref type="bibr" target="#b20">(Larkin and Simon, 1987;</ref><ref type="bibr" target="#b1">Barrio et al., 2016</ref>).</p><p>Neural modeling. Recent work has shown the promise of sophisticated neural models on seman- tic role labeling ( . Similar to other such sequence prediction models, e.g., those for named entity recognition ( <ref type="bibr" target="#b19">Lample et al., 2016)</ref> or semantic role labeling ( <ref type="bibr">Zhou and Xu, 2015)</ref>, our span prediction utilizes a neural CRF. Our model also has an edge-prediction component, which benefits from a simplified version of the PathLSTM model of <ref type="bibr">Roth and Lapata (2016)</ref>. Our edge-prediction model also uses an embedding concatenation component, which was inspired by recent work on neural coreference resolution ( .  also impose seman- tic constraints during prediction, but use A * search instead of an ILP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper we have presented a new task, textual analogy parsing, or TAP. Given a sentence about a set of analogous facts, TAP outputs a frame rep- resentation that expresses the points of similarity and difference in their meanings. We note that in the particular case of quantita- tive text, TAP frames correspond with charts. We develop a new dataset of TAP frames from quan- titative newswire, and compare a variety models for TAP. Our best model employs a globally opti- mal decoder to enforce the structural constraints of analogy; its outputs can be mapped to well-formed charts of quantitative information extracted from text.</p><p>We view this work to be an exciting step in the direction of deeper discourse modeling. Future work might further extend the recovery of anal- ogy as part of information extraction. This might include TAP outside of the quantitative domain, or TAP at the paragraph level. <ref type="bibr">Michael Roth and Mirella Lapata. 2016</ref>. Neural Se- mantic Role Labeling with Dependency Path Em- beddings. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1192- 1202. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: In textual analogy parsing (TAP), one maps analogous facts to semantic role representations and identifies analogical relations between them. Automated chart generation from text is a motivating application of TAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An overview of the proposed neural model: The sentence embedding represents features across the entire sentence using multiple convolutional layers. We then use a conditional random field (CRF) layer to predict labeled spans p m and to generate span and edge embeddings. We use a feedforward (FF) layer on the edge embeddings to predict edge labels p mn. Together, p m and p mn form a distribution over edges and labels that we decode into TAP frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: TAP frames for the sentence 'Those with a bullish view see [the dollar] 1 trading up near 1.900 marks.. . while [the dollar] 2 bears see the U.S. currency trading around 1.7600 marks'. Among other errors, the model failed to identify analogous SOURCE spans and instead predicts that the two instances of the phrase 'the dollar' (indicated with indexing) in the sentence contribute non-VALUE compared content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Charts generated from TAP frames. Charts (a) and (b) are generated from the sentence 'Vicker's PLC. .. raised its stake in the company Friday to 15.02% from about 14.6% Thursday and from 13.6% the previous week.' Before imposing constraints, the neural model assigns multiple values to the TIME arguments 'Thursday' and 'Friday', over-extending their scope. Imposing structural constraints ensures the correct assignment of TIMES to VALUES. Charts (c) and (d) are generated from the sentence 'In the auto sector, Bayerische Motoren Werke plunged 14.5 marks to 529 marks, Daimler-Benz dropped 10.5 to 700, and Volkswagen slumped 9 to 435.5.' Here, the model fails to associate an absolute (blue) and relative (red) VALUE pair with a THEME role. The imposition of global constraints corrects this, linking them to the THEME 'Diamler-Benz'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset statistics (average per sentence, 
max per sentence, and total over the dataset) for 
the number of analogy frames (Count) and the 
number of values compared within each frame 
(Length). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of models on the test data. 
Combining the neural model with linguistic fea-
tures and using an optimal decoder to enforce se-
mantic constraints led to the best performance. 

Span prediction 

Model 
P 
R 
F 1 

Log-linear (all feats.) 42.8 82.3 56.3 
Neural (no feats.) 
41.7 79.1 54.6 
Neural (all feats.) 
41.5 79.2 54.4 
w/o NER 
41.6 79.1 54.5 
w/o dep. 
41.2 77.5 53.8 
w/o CRF 
36.1 73.1 48.3 

</table></figure>

			<note place="foot" n="2"> High edge agreement should be expected because edges are type-constrained and thus easy to identify. Additionally, we computed agreement after matching overlapping spans.</note>

			<note place="foot" n="3"> The dependency paths are directed but unlexicalized.</note>

			<note place="foot" n="5"> In a separate experiment, the neural model outperformed the log-linear model when they were trained only to do span prediction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the members of the Stan-ford NLP Group for reviewing early versions of the paper, and would also like to thank the anony-mous reviewers for their thoughtful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Computational Linguistics</title>
		<meeting>the 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving Comprehension of Numbers in the News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">M</forename><surname>Daniel G Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2729" to="2739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling Biological Processes for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Vander Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brittany</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1499" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint Inference for Event Timeline Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Quang Xuan Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Structure-Mapping Engine: Algorithm and Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Falkenhainer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dedre</forename><surname>Kenneth D Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure Mapping in Analogy and Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dedre</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur B Markman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic Labeling of Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Transfer of Scientific Principles Using Concrete and Idealized Simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Y</forename><surname>Goldstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Learning Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="110" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gurobi Optimizer Reference Manual</title>
		<ptr target="http://www.gurobi.com" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>Gurobi Optimization, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Semantic Role Labeling: What Works and What&apos;s Next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="473" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Thagard</surname></persName>
		</author>
		<title level="m">Mental Leaps: Analogy in Creative Thought</title>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Analogy and Relational Reasoning. The Oxford Handbook of Thinking and Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holyoak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Coherence, Reference, and the Theory of Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>CSLI Publications</publisher>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating Personalized Spatial Analogies for Distances and Areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yea-Seul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to Automatically Solve Algebra Word Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">QSRL: A Semantic Role-Labeling Schema for Quantitative Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Financial Narrative Processing Workshop at LREC 2018</title>
		<meeting><address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why a Diagram is (Sometimes) Worth Ten Thousand Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert A</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="100" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end Neural Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leonhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jodi</forename><surname>Rudoren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Galinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karron</forename><surname>Skog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Giratikanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyson</forename><surname>Evans</surname></persName>
		</author>
		<title level="m">Journalism That Stands Apart: The Report of the 2020 Group</title>
		<meeting><address><addrLine>The New York Times</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Numerical Relation Extraction with Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>G Ramakrishnan Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2764" to="2771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The Stanford CoreNLP Natural Language Processing Toolkit. In Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Treebank</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="99" to="141" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monique</forename><forename type="middle">W</forename><surname>Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Black Stats. The New Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An Annotated Corpus of Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Autodifferentiation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Darwin&apos;s mistake: Explaining the discontinuity between human and nonhuman minds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><forename type="middle">J</forename><surname>Derek C Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Povinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="130" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting Scope for Shallow Discourse Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
		<idno>2.0</idno>
		<title level="m">The Penn Discourse Treebank</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Institute for Research in Cognitive Sciences</title>
		<imprint/>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Annotation Manual</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Linear Programming Formulation for Global Inference in Natural Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Illinois University at Urbana-Champaign, Dept of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
