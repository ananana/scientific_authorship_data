<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RELLY: Inferring Hypernym Relationships Between Relational Phrases</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Grycner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max-Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Campus E1.4, Santa Cruz Santa Cruz</addrLine>
									<postCode>66123, 95064</postCode>
									<settlement>Saarbrücken</settlement>
									<region>CA</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max-Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Campus E1.4, Santa Cruz Santa Cruz</addrLine>
									<postCode>66123, 95064</postCode>
									<settlement>Saarbrücken</settlement>
									<region>CA</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Pujara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max-Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Campus E1.4, Santa Cruz Santa Cruz</addrLine>
									<postCode>66123, 95064</postCode>
									<settlement>Saarbrücken</settlement>
									<region>CA</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foulds</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max-Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Campus E1.4, Santa Cruz Santa Cruz</addrLine>
									<postCode>66123, 95064</postCode>
									<settlement>Saarbrücken</settlement>
									<region>CA</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max-Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Campus E1.4, Santa Cruz Santa Cruz</addrLine>
									<postCode>66123, 95064</postCode>
									<settlement>Saarbrücken</settlement>
									<region>CA</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RELLY: Inferring Hypernym Relationships Between Relational Phrases</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Relational phrases (e.g., &quot;got married to&quot;) and their hypernyms (e.g., &quot;is a relative of&quot;) are central for many tasks including question answering, open information extraction , paraphrasing, and entailment detection. This has motivated the development of several linguistic resources (e.g. DIRT, PATTY, and WiseNet) which systematically collect and organize relational phrases. These resources have demonstra-ble practical benefits, but are each limited due to noise, sparsity, or size. We present a new general-purpose method, RELLY, for constructing a large hypernymy graph of relational phrases with high-quality subsumptions using collective probabilistic programming techniques. Our graph induction approach integrates small high-precision knowledge bases together with large automatically curated resources, and reasons collectively to combine these resources into a consistent graph. Using RELLY, we construct a high-coverage, high-precision hypernymy graph consisting of 20K relational phrases and 35K hy-pernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the many challenges in natural lan- guage understanding is interpreting the multi- word phrases that denote relationships between entities. Semantically organizing the complex re- lationships between diverse phrases is crucial to applications including question answering, open information extraction, paraphrasing, and entail- ment detection ( <ref type="bibr" target="#b33">Yahya et al., 2012;</ref><ref type="bibr" target="#b14">Fader et al., 2011;</ref><ref type="bibr" target="#b24">Madnani et al., 2012;</ref><ref type="bibr" target="#b11">Dagan et al., 2005</ref>). For example, a corpus containing the phrase "George Burns was married to Gracie Allen" al- lows us to answer the query "Who was the spouse of George Burns?" However, "Jay Z is in a re- lationship with Beyoncé" provides insufficient in- formation to determine whether the couple is mar- ried. To capture the knowledge found in text, rela- tional phrases need to be systematically organized with lexical links like synonymy ("married to" and "spouse of") and hypernymy ("in a relationship" generalizing "married to").</p><p>Many projects address the challenge of under- standing relational phrases, but existing linguis- tic resources are often limited to synonymy, suffer from low precision, or have low coverage. Sys- tems such as DIRT ( <ref type="bibr" target="#b23">Lin and Pantel, 2001</ref>), RE- SOLVER ( <ref type="bibr" target="#b34">Yates and Etzioni, 2009)</ref>, and WiseNet <ref type="bibr" target="#b27">(Moro and Navigli, 2012</ref>) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption informa- tion. The PATTY ( <ref type="bibr" target="#b28">Nakashole et al., 2012</ref>) project goes beyond clustering and introduces a subsump- tion hierarchy, but suffers from sparsity and con- tains few hypernymy links. The HARPY <ref type="bibr" target="#b17">(Grycner and Weikum, 2014</ref>) project extended PATTY, generating 600K hypernymy links, but with low precision. <ref type="bibr" target="#b4">Berant et al. (2011)</ref> introduced en- tailment graphs that provided a high-quality sub- sumption hierarchy. This method required parti- tioning the graph and the largest component con- sisted of 120 relations. A number of manually- curated relational taxonomies such as WordNet <ref type="bibr">(Fellbaum, 1998)</ref>, <ref type="bibr">VerbNet (Kipper et al., 2008)</ref>, and FrameNet ( <ref type="bibr" target="#b2">Baker et al., 1998</ref>) also offer high- precision hierarchies with limited coverage.</p><p>In this paper, we introduce RELLY, a method for producing a hypernymy graph that has both high coverage and precision. We build on pre- vious work, integrating the high-precision knowl- edge in resources such as <ref type="bibr">YAGO (Suchanek et al., 2007)</ref> and WordNet with noisy statistical informa- tion from OpenIE projects PATTY and HARPY. RELLY maintains a consistent graph by includ- ing collective global constraints such as transitiv- ity, asymmetry, and acyclicity. Scalability is of- ten a concern when employing collective reason- ing over large corpora, but our system can pro- duce graphs with over 100K edges on conven- tional hardware. As a result, we produce a large, complete, and high-precision hypernym graph that includes alignments and type information.</p><p>RELLY leverages probabilistic soft logic (PSL) ( <ref type="bibr" target="#b0">Bach et al., 2015</ref>), a popular probabilistic mod- eling framework, to collectively infer hypernymy links at scale. PSL uses continuously-valued vari- ables and evidence, allowing easy integration of uncertain statistical information while encoding dependencies between variables using a first-order logic syntax. We define a PSL model with rules that combine statistical features, semantic infor- mation, and structural constraints. Statistical fea- tures, such as argument overlap and alignments to WordNet verbs senses, allow RELLY to learn from large text collections. Semantic informa- tion, such as type information for relation argu- ments, improves precision of the resulting infer- ences. Structural constraints, such as transitivity and acyclicity, enforce a complete and consistent set of edges. Using this PSL model, we learn rule weights with a small amount of training data and then perform joint inference over all hypernymy links in the graph.</p><p>We highlight three major contributions of our work. First, we introduce RELLY, a scalable method for integrating statistical and semantic sig- nals to produce a hypernymy graph. RELLY is ex- tensible and can easily incorporate additional in- formation sources and features. Second, we gener- ate a complete and precise hypernymy graph over 20K relational phrases and 35K hypernymy links. We have publicly released this hypernymy graph as a resource for the NLP community. Third, we present a thorough empirical evaluation to mea- sure the precision of the hypernymy graph as well as demonstrate its usefulness in a real-world docu- ment ranking task. Our results show a high preci- sion (0.78) and superior performance in document ranking compared to state-of-the-art models such as word2vec <ref type="bibr" target="#b26">(Mikolov et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Before describing the details of RELLY, we be- gin with necessary background information on the task of semantically organizing relational phrases, as well as the probabilistic soft logic modeling lan- guage which we use to develop our hypernymy graph construction method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relational Phrases</head><p>Relational phrases are textual representations of relations which occur between named entities (e.g., "Terry Pratchett") or noun phrases (e.g., "the great writer"). <ref type="bibr" target="#b28">Nakashole et al. (2012)</ref> identify re- lational phrases with the semantic type signature of the relation, i.e. the fine-grained lexical types of left-and right-hand side arguments. For ex- ample, "Terry Pratchett published his new novel The Colour of Magic" is an instance of the re- lational phrase "&lt;person&gt; published his * ADJ novel &lt;book&gt;." In this case, the left-hand ar- gument (the domain of the relation) has the type &lt;person&gt; and the right-hand argument (the range of the relation) has the type &lt;book&gt;.</p><p>Several projects from the Open Information Extraction community have addressed the task of finding synonyms of relational phrases us- ing clustering algorithms. The biggest collec- tion of relational phrases and their synonyms is currently the PATTY project ( <ref type="bibr" target="#b28">Nakashole et al., 2012)</ref>, with around 350,000 semantically typed relational phrases. Prominent alternatives are WiseNet ( <ref type="bibr" target="#b27">Moro and Navigli, 2012)</ref>, which offers 40,000 synsets of relational phrases, PPDB <ref type="bibr" target="#b16">(Ganitkevitch et al., 2013)</ref>, which contains over 220 million paraphrase pairs, as well as DIRT and Ver- bOcean ( <ref type="bibr" target="#b23">Lin and Pantel, 2001;</ref><ref type="bibr" target="#b10">Chklovski and Pantel, 2004</ref>) which inspired the approach and results pursued here.</p><p>Relational phrases can be further organized into a hierarchical structure according to their hyper- nymy (subsumption) relationships. For example, "&lt;person&gt; moves to &lt;country&gt;" is a hypernym of the relational phrase "&lt;musician&gt; emigrates to &lt;country&gt;." Of the aforementioned collections, only PATTY attempts to automatically create a subsumption hierarchy for the extracted relational phrases. The authors of the HARPY system ar- gue that the sparseness of PATTY's graph comes from the lack of general phrases in the source corpus. As a solution, they propose using the WordNet verb hierarchy (which contains general verb senses) to construct a similar hierarchy with PATTY's relational phrases. The graph obtained by HARPY consists of around 600,000 hyper- nymy links for around 20,000 relational phrases. However, the final graph was not evaluated for pre- cision; rather, the evaluation was instead concen- trated on the alignment between verb senses and relations.</p><p>In this paper we will make use of several con- cepts that are closely related to hypernymy, which we define below. Note that although the following definitions concern verbs, we also apply them to relational phrases:</p><p>• hypernym: the verb Y is a hypernym of the verb X if Y is more general than X. To per- ceive is a hypernym of to listen <ref type="bibr" target="#b1">(Bai et al., 2010</ref>).</p><p>• troponym: the verb Y is a troponym of the verb X if doing Y is doing X, in some manner. To lisp is a troponym of to talk <ref type="bibr" target="#b1">(Bai et al., 2010)</ref>. Troponym is a verb counterpart for hyponym, which applies to nouns. In this work we use these two terms interchangeably.</p><p>• entailment: the verb Y is entailed by X if, by doing X, you must be doing Y . To sleep is entailed by to snore <ref type="bibr" target="#b1">(Bai et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Probabilistic Soft Logic</head><p>Our approach is based on probabilistic soft logic (PSL), a popular statistical relational learning sys- tem which we briefly describe here. PSL is a tem- plating language for a class of graphical models known as hinge-loss Markov random fields. PSL models are specified using rules in first-order logic syntax, expressing dependencies between interre- lated variables. For example, the PSL rule</p><formula xml:id="formula_0">w : HYPERNYM(P 1 , P 2 ) ∧ HYPERNYM(P 2 , P 3 ) ⇒ HYPERNYM(P 1 , P 3 )</formula><p>expresses the transitivity of hypernyms: if phrase P 1 is a hypernym of phrase P 2 and P 2 is a hyper- nym of P 3 , then P 1 is a hypernym of P 3 . Rules are weighted (w) to indicate their importance in the model, and weight learning in PSL allows these weights to be learned from training data. Each rule is ground by substituting the variables in the rule with constants, e.g. "married to" and "relative of" for P 1 and P 2 . However, unlike pre- vious approaches such as Markov logic networks, the atoms in each logical rule take values in the <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> continuous domain. In addition to provid- ing a natural way of incorporating uncertainty and similarity into models, continuous-valued vari- ables allow the inference objective to be formu- lated as convex optimization making MAP infer- ence extremely efficient, with empirical perfor- mance that scales linearly with the number of ground rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hypernymy Graph Construction</head><p>In this section we detail RELLY, our system for constructing a hypernymy graph. RELLY incor- porates semantic and statistical information from sources such as YAGO, WordNet, PATTY, and HARPY, and uses PSL to combine and reason over these sources. For each source, we in- troduce a PSL predicate ( <ref type="table">Table 1</ref>). The predi- cates are divided into three categories: statisti- cal (continuous-valued features arising from sta- tistical methods), semantic (binary predicates ac- quired from knowledge bases) and output (the tar- get variables). We relate these predicates with a series of rules which combine alignment links, ar- gument similarity, and hierarchical information. The collection of rules defines the PSL model, which we describe in Section 3.1 and <ref type="table">Table 2</ref>.</p><p>In the resulting hypernymy graph, an edge from a relational phrase R1 to a relational phrase R2 de- notes that R1 is more specific than R2, i.e. R2 is a hypernym of R1. For example, there is an edge from R1 ="&lt;musician&gt; emigrates to &lt;coun- try&gt;" to R2 = "&lt;person&gt; moves to &lt;country&gt;." In the PSL model the strength of this edge is rep- resented by the confidence score of the predicate hyponym(R1, R2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PSL Rules</head><p>The PSL rules that define the model are shown in <ref type="table">Table 2</ref>. Each of the rules is additionally supplied with a weight which describes its importance in the model. The weights are learned from a small hand-crafted hierarchy of relational phrases. The full PSL model combines multiple statistical and semantic signals into the hypernymy graph.</p><p>Our model includes rules to encode signals that provide evidence for hypernymy, as well as rules to encode consistency in the graph. One statistical signal for phrase subsumption is argument over- lap. If the arguments to a relational phrase R1 are also found as arguments to another relational phrase R2, R1 and R2 may be synonymous or <ref type="table">Table 1</ref>: PSL predicates; R1, R2 are relational phrases, V b1, V b2 WordNet verb senses and T L1, T R1, T 1, T 2 YAGO types PSL predicate Type Description weedsInclusion(R1, R2) statistical degree of inclusion of sets of argument pairs of re- lations defined as |ArgsR1| |ArgsR1∩ArgsR2| <ref type="bibr" target="#b32">(Weeds and Weir, 2003)</ref> pattySubsumption(R1, R2) statistical PATTY subsumption (Nakashole et al., 2012) harpy(R1, V b1) statistical alignment links between relational phrases and Word- Net verb senses ( <ref type="bibr" target="#b17">Grycner and Weikum, 2014</ref>) wordnetHyponym(V b1, V b2) semantic hyponymy link between WordNet verb senses lT ype(R1, T L1) semantic left (domain) type of arguments of a relational phrase rT ype(R1, T R1) semantic right (range) type of arguments of a relational phrase yagoHyponym(T 1, T 2) semantic T 1 is a subtype of T 2 in YAGO hierarchy candidateHyponym(R1, R2) output relational phrase R1 is more specific than R2 (without enforcing consistent argument types) hyponym(R1, R2) output relational phrase R1 is more specific than R2</p><p>R2 may be a hypernym of R1. We use two mea- sures of argument overlap, weedsInclusion and pattySubsumption, in rules 1 and 2, respectively, to capture the relationship between argument over- lap and subsumption. Another signal, used in rule 3, is the alignment between relational phrases and WordNet verb senses. If relational phrases R1 and R2 are aligned to WordNet verb senses V b1 and V b2 which are in a hyponymy relationship, then this is evidence that R1 is more specific than R2. An example of using HARPY alignment links and WordNet hierarchy is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We encode local consistency requirements us- ing Rules 4-6. Rule 4 (types compatibility) is a constraint to restrict hypernymy links to be be- tween relations whose types are compatible, i.e they are identical or the types of the more specific relation are subtypes of the types of the more gen- eral relation. Rules 5 and 6 create a transitive clo- sure of both WordNet and YAGO hierarchies. As a result of these rules, we can use indirect hyponyms (in rule 3) or indirect subtypes (in rule 4).</p><p>Finally, rules 7, 8 and 9 shape the structure of the output graph with collective global constraints. Rule 7 (asymmetry) removes bidirectional links, rule 8 (transitivity) creates a transitive closure of the graph and rule 9 (acyclicity) prevents the cre- ation of small cycles in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RELLY Overview</head><p>RELLY has four stages: data pre-processing, rule weight learning, inference, and thresholding. First, in the data pre-processing stage, we as- sign confidence scores of 0 or 1 for the binary- valued semantic predicates in the PSL model. For example, the wordnetHyponym(V b1, V b2) confidence score is set to 1 if there is a hy- ponymy link between verb senses V b1 and V b2 and 0 otherwise. In other cases, the confidence is set to a similarity score of a feature which is represented by a predicate. For example, the weedsInclusion(R1, R2) confidence is equal to the Weeds inclusion score between relations R1 and R2.</p><p>In the next stage the weights of the PSL rules described in <ref type="table">Table 2</ref> are learned from a small handcrafted graph of relational phrases. The weight learning is performed using an EM al- gorithm. Later, the most-probable explanation (MPE) state of the output predicates is inferred.</p><p>Finally, we export the inferred confidence scores of the predicate hyponym and perform ad- ditional cleaning. Whenever two links contradict each other (e.g. we have both hyponym(R1, R2) and hyponym(R2, R1)) we remove the link with <ref type="table">Table 2: PSL rules   Id Feature  PSL rule  1</ref> Weeds inclusion weedsInclusion(R1, R2) ⇒ candidateHyponym(R1, R2) 2</p><p>Patty subsumption pattySubsumption(R1, R2) ⇒ candidateHyponym(R1, R2) 3</p><p>Harpy alignment wordnetHyponym(V b1, V b2) ∧ harpy(R1, V b1) ∧ harpy(R2, V b2) ⇒ candidateHyponym(R1, R2) 4</p><p>Types candidateHyponym(R1, R2) ∧ lT ype(R1, T L1) ∧ rT ype(R1, T R1) compatibility ∧lT ype(R2, T L2) ∧ rT ype(R2, T R2) ∧ yagoHyponym(T L1, T L2) ∧yagoHyponym(T R1, T R2) ⇒ hyponym(R1, R2) 5</p><p>WordNet wordnetHyponym(V b1, V b2) ∧ wordnetHyponym(V b2, V b3) hierarchy ⇒ wordnetHyponym(V b1, V b3) 6</p><p>Yago hierarchy</p><formula xml:id="formula_1">yagoHyponym(T 1, T 2) ∧ yagoHyponym(T 2, T 3) ⇒ yagoHyponym(T 1, T 3) 7 Asymmetry hyponym(R1, R2) ⇒ ¬hyponym(R2, R1) 8 Transitivity hyponym(R1, R2) ∧ hyponym(R2, R3) ⇒ hyponym(R1, R3) 9 Acyclicity hyponym(R1, R2) ∧ hyponym(R2, R3) ⇒ ¬hyponym(R3, R1)</formula><p>the lower confidence score. If both predicates have the same confidence score we exclude them both from the final graph. Additionally, we only con- sider links with a confidence score above an em- pirically chosen threshold of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In our experiments, we use a large corpus of rela- tional phrases to construct a hypernymy graph us- ing RELLY. We evaluate RELLY using both intrin- sic and extrinsic evaluation. In the intrinsic evalua- tion, we asked human annotators to judge the rela- tionship between two relational phrases and com- pared results from several hypernymy graphs. In the extrinsic evaluation, we used the hypernymy graph for a real-world document ranking task and measured the mean reciprocal rank (MRR) for a number of methods. In both evaluations, the hy- pernymy graph constructed by RELLY demon- strates significantly better performance than com- peting algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use RELLY to build a hypernymy graph with data from the PATTY and HARPY projects. The input to our system consists of 20,812 relational phrases and the associated argument types ex- tracted from the English-language Wikipedia web- site using the PATTY system. For simplicity, we only include relational phrases that contain exactly one verb (e.g. "took the throne"), excluding noun phrases (e.g. "member of") and phrases contain- ing multiple verbs (e.g. "hit and run"). The verb "to be" and modal verbs were not considered in the dataset. We also include HARPY alignments to the corresponding verb senses in WordNet for each phrase in the corpus. Additionally, we use a subset of the type-subsumption hierarchy from YAGO consisting of 144 types and 323 subsump- tion relationships. During graph inference, RELLY evaluated 7.9M possible hypernymy links using 9.7M ground logical rules and constraints. Ultimately, RELLY produced 35,613 hypernymy links be- tween relational phrases with confidence scores above 0.2. The hypernymy graph consisted of 3,730 roots. Running RELLY on a multi-core 2.27GHz server with 64GB of RAM required ap- proximately 20 hours. For comparison, PATTY produced 8,162 subsumption links out of 350,569 phrases with approximately 2,300 roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intrinsic Evaluation</head><p>In our intrinsic evaluation, we assess the precision of hypernymy links inferred by RELLY and com- pare with the precision of hypernymy graphs of PATTY and HARPY. In this evaluation, we mea- sure precision for both the most confident hyper- nymy links in the system (precision@100) and the precision of a random sample of 100 hypernymy links. Each set of hypernymy links were presented to several human annotators for labeling.</p><p>To measure precision@100, we choose the top 100 hypernymy links using the confidence scores reported by PSL. We similarly choose the top 100 links from PATTY using the PATTY subsumption score. Since HARPY does not provide confidence scores, we were unable to compute precision@100 for HARPY.</p><p>For each of the three systems, we used the full set of hypernymy links they produce, which con- sisted of 8K links from PATTY, 600K links from HARPY and 35K links from RELLY. We ran- domly sampled 100 hypernymy links from each of these systems.</p><p>We presented the selected hypernymy links to several human annotators. The labeling task re- quired the annotator to judge the relationship be- tween two relational phrases in a hypernymy link. For each relational phrase, we provided annota- tors with type information about the phrase argu- ments (domain and range) and examples of sen- tences that use the relational phrase. Based on this information, annotators could make one of four judgments: (1) the phrases are unrelated; (2) the phrases are synonymous; (3) the first phrase is more specific than the second phrase; (4) the second phrase is more specific than the first phrase. This evaluation task had good inter- annotator agreement, with a Cohen's Kappa of 0.624. Separately, the precision@100 dataset had Cohen's Kappa of 0.708 and the randomly sam- pled dataset had Cohen's Kappa of 0.521.</p><p>We show the results of the intrinsic evaluation in <ref type="table" target="#tab_0">Table 3</ref> with 0.9-confidence Wilson score interval ( <ref type="bibr" target="#b8">Brown et al., 2001</ref>). In comparison to HARPY and PATTY, RELLY has higher precision for both precision@100 and random evaluations. Precision in RELLY is comparable to PATTY, but RELLY has more than four times as many hypernym links. HARPY has far more hypernymy links, but with a precision of 0.43, we find that many of these links are incorrect. <ref type="table" target="#tab_1">Table 4</ref> includes example hypernymy links from RELLY. There are examples where PATTY's sub- sumption is a dominant signal ("&lt;person&gt; pub- licly accused &lt;person&gt;" ⇒ "&lt;person&gt; accused &lt;person&gt;"). We also observe YAGO type hier- archy influence ("&lt;athlete&gt; played for &lt;team&gt;" ⇒ "&lt;person&gt; played for &lt;organization&gt;"), as well as the influence of combined WordNet hierar- chy with HARPY alignments ("&lt;person&gt; marry daughter &lt;person&gt;" ⇒ "&lt;person&gt; joins &lt;per- son&gt;"). The advantage of RELLY is that it com- putes the final graph jointly and incorporates tran- sitivity, asymmetry and acyclicity rules. It leads to less semantic drift in longer hypernymy chains  <ref type="figure" target="#fig_2">Figure 2</ref>) compared with PATTY where "&lt;organization&gt; merged &lt;organization&gt;" can lead to "&lt;team&gt; beat &lt;team&gt;".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Two advantages of RELLY that we have high- lighted are easily incorporating new information sources and collectively enforcing global con- straints. To analyze the influence of these sys- tem components, we performed an ablation study where we omitted PSL rules corresponding to specific model features. Using this approach, we quantify the importance of these features to RELLY's performance.</p><p>First, we demonstrate the value of type informa- tion in determining hypernymy. The YAGO type hierarchy allows RELLY to detect hypernymy links between relational phrases where types do not match exactly, but are compatible through type subsumption. When the YAGO type hierar- chy rules are omitted from the model, coverage is reduced dramatically; the resulting hypernymy graph contains only 12,000 hypernymy links in contrast to the 35,000 links in the original model. Additionally, removing YAGO type information harms precision, with a precision of 0.75 ± 0.09  <ref type="table">Hyponym relational phrase  Hypernym relational phrase  Domain  Text pattern  Range  Domain  Text pattern  Range  head of state abdicated in favor of  sovereign  person resigns as  person  person  publicly accused  person  person accused  person  person  marry daughter  person  person joins  person  person  had paid  person  person interacted with  person  athlete</ref> played for team person played for organization <ref type="table">Table 5</ref>: Results for Entailment graphs induction</p><p>Prec. Rec. Next, we show how global constraints on the hypernymy graph such as anti-symmetry and acyclicity improve the quality of the hypernymy graph. Since the relational phrases generated by PATTY are clustered to find synonymous re- lations, these global constraints prevent RELLY from merging clusters. When the anti-symmetry and acyclicity rules were removed from the model, the resulting hypernymy graph included approxi- mately 500 additional hypernymy links, while 10 existing links were removed. We manually evalu- ated the newly introduced links, and found that the majority of links were false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Entailment Graph Induction</head><p>We compared the performance of PSL against the Integer Linear Programming (ILP) formulation by <ref type="bibr" target="#b4">(Berant et al., 2011</ref>). The comparison was per- formed on the task of creating entailment graphs as described in <ref type="bibr" target="#b4">(Berant et al., 2011</ref>). This task is strongly related to finding hypernyms of relational phrases. The experiments were executed on the dataset of 10 manually annotated graphs. In to- tal this dataset contains 3,427 positive and 35,585 negative examples. Our model uses the transi- tivity rule (entails(A, B) ∧ entails(B, C) ⇒ entails(A, C)). We also include the local en- tailment scores (score(A, B) ⇒ entails(A, B)) which were released by <ref type="bibr" target="#b4">(Berant et al., 2011</ref>). Ta- ble 5 presents micro-averaged precision, recall and F1 scores for this comparison.</p><p>PSL was much faster than the other exact meth- ods used for this problem. To compare efficiency we measured the run-time of our method. With- out any graph decomposition it took on average 232 seconds. The experiments were performed on a multi-core 2.67GHz server with 32GB of RAM. The methods reported in <ref type="bibr" target="#b5">(Berant et al., 2012</ref>), which did not utilize graph decomposition method, had run-time above 5000 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extrinsic Evaluation</head><p>The ultimate goal of producing a high-quality hy- pernymy graph is to deepen our understanding of natural language and improve performance on the many NLP applications. One such application is document retrieval, where billions of queries are performed each day through search engines. In our extrinsic evaluation, we demonstrate how a hyper- nymy graph can improve performance on a docu- ment ranking and retrieval task. We consider a task where an input query document is compared to a corpus of docu- ments with the aim of finding the most relevant related documents. To isolate the evaluation to relational phrases, we anonymize the doc- uments, by replacing all named entities and noun phrases with placeholders. For example, the sentence "The villain has already fled to the Republica de Isthmus" is anonymized to " * has already fled to * ." Anonymized retrieval has potential appli- cations in security and for sensitive documents.</p><p>We collected a dataset consisting of movie plot summaries from two different websites, Wikipedia and the Internet Movie Database (IMDB). We chose plot synopses from 25 James Bond movies and 23 movies based on the Marvel Comics char- acters. For each plot synopsis, we have two plot descriptions: one from Wikipedia and another from IMDB. Given a query in the form of an anonymized plot description from one website, the task is to rank the anonymized plot descriptions from the other dataset using relational phrase sim- ilarity. For example, given a query plot description of "Iron Man" from Wikipedia, rank plot descrip- tions from IMDB with the goal of maximizing the ranking of the corresponding "Iron Man" plot summary. We evaluate the quality of these rank- ings using the mean reciprocal rank (MRR) score,</p><formula xml:id="formula_2">MRR = 1 |Q| |Q| i=1 1 rank i</formula><p>. Here, Q is the number of documents in the collection (i.e. 2*48 = 96) and rank i is the position of the counterpart document in the ranking of document i.</p><p>As baseline algorithms, we use a unigram word2vec model and a bigram model. In the uni- gram word2vec model documents are represented by the average of the 300-dimensional word vec- tors trained on part of Google News dataset (about 100 billion words) ( <ref type="bibr" target="#b26">Mikolov et al., 2013</ref>). We could not use the bigram word2vec model because of the frequent occurrence of the placeholder sym- bol. In the bigram model, documents are rep- resented by vectors in the bag-of-bigrams model with bigram frequency weights. The similarity measure in both cases is the cosine similarity mea- sure.</p><p>As the first of our approaches we proposed a solution purely based on relational phrases. In the relational phrases model we extract relational phrases from a text and we map them to their synsets from PATTY (clusters of synonyms). A phrase is mapped to a synset if the Jaccard simi- larity between tokens of extracted relation and to- kens of one of the phrases in the synset is above a threshold. Next we represent the document as a vector of the relational phrase synsets weighted by the frequency of the synset in the document (bag-of-relational phrases). The similarity score between two documents is the cosine similarity between two vectors representing two documents. The ranking is created based on the similarity scores. In the relational phrases + hypernyms model we add hypernyms of the extracted rela- tional phrases to the document vector (based on the hypernymy graph). Hypernyms are addition- ally weighted by the confidence score produced by the algorithm described in the Section 3. In the second approach we combine relational phrases models with the best of the baselines. The similar- ity score is then equal to λsim 1 +(1−λ)sim 2 . The λ parameter is trained on a different dataset (2*8 plot descriptions of Harry Potter movies). Train- ing was performed by maximization of the MRR The results of the experiment are presented in <ref type="table" target="#tab_2">Table 6</ref>. The best MRR score was obtained by re- lational phrases + hypernyms + bigrams model. The number of samples, 96, was large enough for statistical significance. We performed a paired t- test for M RR between each of these methods. The obtained p-values were below 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The biggest sources of hypernyms, subsumptions, and hierarchical structure can be found in exist- ing knowledge bases. Examples of these are Free- base ( <ref type="bibr" target="#b6">Bollacker et al., 2008)</ref>, YAGO, DBPedia ( <ref type="bibr" target="#b21">Lehmann et al., 2014)</ref>, and Google Knowledge Vault ( <ref type="bibr" target="#b13">Dong et al., 2014</ref>). However, these knowl- edge bases are mainly concentrated on named enti- ties and noun phrases, and the variety of relations between entities is much smaller. Relations and information about them are underrepresented.</p><p>Open Information Extraction systems try to solve this problem by extracting new relations from natural text. These new relations do not necessarily follow the standard schema of knowl- edge bases. Additionally, these systems often or- ganize the newly extracted relations by clustering or hierarchy construction. A first attempt to ex- tract and cluster similar relations was presented in DIRT. This work was followed by projects such as ReVerb, PATTY, WiseNet, NELL <ref type="bibr" target="#b9">(Carlson et al., 2010)</ref>, and RESOLVER ( <ref type="bibr" target="#b34">Yates and Etzioni, 2009)</ref>. PATTY and WiseNet also introduced se- mantic types to their concept of relational phrases. All of these systems rely on the co-occurrence of arguments of clustered relations. A different ap- proach was presented in PPDB, where the authors cluster phrases based on the similarity of transla- tions to other languages.</p><p>Of these systems, only PATTY attempted to cre- ate a hierarchy of relations and the result was very sparse. HARPY aimed to overcome this problem by disambiguating and aligning relational phrases with WordNet, and performing a simple recon- struction of the WordNet hierarchy on top of rela- tional phrases from PATTY. A very similar prob- lem was addressed in the entailment graph project ( <ref type="bibr" target="#b22">Levy et al., 2014</ref>). The authors automatically created graphs of entailments between proposi- tions, using Integer Linear Programing as one of the main components. Propositions can be en- coded as triples of form (subject, relation, ob- ject). Edges in the entailment graph occur between these triples, whereas edges connect typed rela- tions in PATTY and HARPY. Moreover, the rela- tions in the propositions were mainly limited to single verbs, whereas in our case we also consider longer relational phrases. Relations with semantic types were also used in typed entailment graphs <ref type="bibr" target="#b4">(Berant et al., 2011</ref>). However, the type hierarchy was not considered there, which prevented from creating links between two relations with different semantic types. The input dataset was also smaller -the biggest graph consisted of 118 relations.</p><p>Although there is a scarcity of automatically created taxonomies of relations, there exist several manually curated taxonomies. Manually crafted verb or relation hierarchies are available in Word- Net, VerbNet and FrameNet. WordNet has 13,767 verb synsets, which are organized into a hierarchy with 13,239 hypernymy links.</p><p>Automatic construction of taxonomies of named entities or noun phrases has received much more attention than organization of verbs or rela- tions. In ( <ref type="bibr" target="#b30">Snow et al., 2006</ref>), the WordNet taxon- omy was extended by 10,000 novel noun synsets with hypernym-hyponym links. In ( <ref type="bibr" target="#b3">Bansal et al., 2014</ref>), the authors reconstructed WordNet's noun hypernymy/hyponymy hierarchy from scratch us- ing a probabilistic graphical model formulation. Another method of organizing noun phrases was proposed in ( <ref type="bibr" target="#b25">Mehdad et al., 2013)</ref>, where an en- tailment graph of noun phrases was constructed.</p><p>Building a hypernymy graph for relational phrases is strongly related with the textual entail- ment task <ref type="bibr" target="#b12">(Dagan et al., 2010)</ref>. This concept was introduced in the Recognizing Textual Entailment (RTE) shared task ( <ref type="bibr" target="#b11">Dagan et al., 2005</ref>). Instead of short typed relational phrases, the input data are two texts -the entailing text T and the hypothesis text H. According to <ref type="bibr" target="#b11">(Dagan et al., 2005</ref>)'s defi- nition, "T entails H if, typically, a human reading T would infer that H is most probably true."</p><p>In RELLY, we use probabilistic soft logic (PSL) as the main ingredient of our approach. PSL was successfully used for numerous other applications including knowledge graph construction ( <ref type="bibr" target="#b29">Pujara et al., 2013)</ref>, trust in social networks <ref type="bibr" target="#b19">(Huang et al., 2012b</ref>), ontology alignment <ref type="bibr" target="#b7">(Broecheler and Getoor, 2009)</ref>, and social group modeling <ref type="bibr" target="#b18">(Huang et al., 2012a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents RELLY, a scalable method for integrating statistical and semantic signals to pro- duce a hypernymy graph of relational phrases. We used RELLY to create a hypernymy graph that has both high coverage and precision, as shown in our evaluation. RELLY is extensible and can easily in- corporate additional information sources and fea- tures. The hypernymy graph of relational phrases could potentially be useful for many problems of natural language processing and information re- trieval. For example, we applied the hypernymy graph to a document-relevance task, which we used to evaluate RELLY extrinsically. As a future work, RELLY can incorporate more information sources and statistical signals and be expanded to infer multi-verb or noun relational phrases. The RELLY resource is publicly available at www.mpi-inf.mpg.de/yago-naga/patty/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: HARPY alignment usage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>&lt;organization&gt; acquires &lt;organization&gt; &lt;organization&gt; purchased share &lt;organization&gt; &lt;organization&gt; bought half of &lt;company&gt; &lt;company&gt; bought half of &lt;company&gt; &lt;company&gt; later bought half of &lt;company&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Chain of hypernymy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>F1 Berant et al. (2011) 0.422 0.434 0.428 PSL 0.461 0.435 0.447 with 0.9-confidence Wilson score interval for a random sample of 100 examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 3 : Intrinsic evaluation</head><label>3</label><figDesc></figDesc><table>Prec. 
Range 
Cvg. 

precision@100 

RELLY 
0.87 0.81 -0.92 35K 
PATTY 
0.83 0.76 -0.90 
8K 

random sample 

RELLY 
0.78 0.71 -0.84 35K 
PATTY 
0.75 0.68 -0.82 
8K 
HARPY 0.43 0.35 -0.52 600K 

(e.g. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 : Example RELLY hypernymy links</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 6 : Extrinsic evaluation (Bond &amp; Marvel)</head><label>6</label><figDesc></figDesc><table>MRR score 

word2vec 
0.26 

bigram 
0.55 

relational phrases 
0.28 
+ hypernyms 
0.25 
+ bigrams 
0.58 
+ hypernyms + bigrams 
0.60 

score using grid search. We consider the combina-
tion of the bigram model with relational phrases, 
as well as the combination of the bigram model 
with relational phrases + hypernyms. 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04406</idno>
		<title level="m">Hinge-loss Markov random fields and probabilistic soft logic</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extract semantic information from wordnet to improve text classification performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Science and Information Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics and Conference on Computational Linguistics (COLING/ACL)</title>
		<meeting>Association for Computational Linguistics and Conference on Computational Linguistics (COLING/ACL)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured learning for taxonomy induction with belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
		<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
		<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="610" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient tree-based approximation for entailment graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meni</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
		<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic similarity logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Statistical Relational Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interval estimation for a binomial proportion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Tony</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="101" to="133" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<meeting>Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VerbOcean: Mining the web for fine-grained semantic verb relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: Rational, evaluation and approaches erratum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="105" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge Vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">WordNet An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</title>
		<meeting>North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HARPY: Hypernyms and alignment of relational paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Grycner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Computational Linguistics (COLING)</title>
		<meeting>Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2195" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social group modeling with probabilistic soft logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Social Network and Social Media Analysis: Methods, Models, and Applications</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic soft logic for trust analysis in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Golbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Statistical Relational Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A large-scale classification of english verbs. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neville</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="21" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">DBpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bizer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Semantic Web Journal</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focused entailment graphs for Open IE propositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DIRT @SBT@discovery of inference rules from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</title>
		<meeting>North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards topic labeling with phrase entailment and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</title>
		<meeting>North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="179" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">WiseNet: building a wikipedia-based semantic network with ontologized relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1672" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PATTY: A taxonomy of relational patterns with semantic types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL</title>
		<meeting>Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference (ISWC)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics and International Conference on Computational Linguistics (COLING/ACL)</title>
		<meeting>Association for Computational Linguistics and International Conference on Computational Linguistics (COLING/ACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on World Wide Web (WWW)</title>
		<meeting>International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural language questions for the web of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL</title>
		<meeting>Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="379" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised methods for determining object and relation synonyms on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="255" to="296" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
