<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Activity Recognition with Verb Attribute Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Engineering University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Engineering University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Engineering University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Activity Recognition with Verb Attribute Induction</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="946" to="958"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb &quot;salute&quot; has several properties, such as being a light movement, a social act, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We study the problem of inferring action verb at- tributes based on how the word is defined and used in context. For example, given a verb such as "swig" shown in <ref type="figure">Figure 1</ref>, we want to infer var- ious properties of actions such as motion dynam- ics (moderate movement), social dynamics (soli- tary act), body parts involved (face, arms, hands), and duration (less than 1 minute) that are generally true for the range of actions that can be denoted by the verb "swig".</p><p>Our ultimate goal is to improve zero-shot learn- ing of activities in computer vision: predicting a previously unseen activity by integrating back- ground knowledge about the conceptual properties of actions. For example, a computer vision system may have seen images of "drink" activities during Figure 1: An overview of our task. Our goal is twofold. A: we seek to use use distributed word embeddings in tandem with dictionary definitions to obtain a high level understanding of verbs. B: we seek to use these predicted attributes to allow a classifier to recognize a broader set of activities than what was seen in training time.</p><p>training, but not "swig". Ideally, the system should infer the likely visual characteristics of "swig" us- ing world knowledge implicitly available in dictio- nary definitions and word embeddings.</p><p>However, most existing literature on zero-shot learning has focused on object recognition with only a few notable exceptions (see Related Work in Section 8). There are two critical reasons: ob- ject attributes, such as color, shape, and texture, are conceptually straightforward to enumerate. In addition, they have distinct visual patterns which are robust for current vision systems to recognize. In contrast, activity attributes are more difficult to conceptualize as they involve varying levels of abstractness, which are also more challenging for computer vision as they have less distinct visual patterns. Noting this difficulty, <ref type="bibr" target="#b2">Antol et al. (2014)</ref> instead employ cartoon illustrations as intermedi- ate mappings for zero-shot dyadic activity recog- nition. We present a complementary approach: that of tackling the abstractness of verb attributes directly. We develop and use a corpus of verb at- tributes, using linguistic theories on verb seman- tics (e.g., aspectual verb classes of <ref type="bibr" target="#b36">Vendler (1957)</ref>) and also drawing inspiration from studies on lin- guistic categorization of verbs and their properties <ref type="bibr" target="#b11">(Friedrich and Palmer, 2014;</ref><ref type="bibr" target="#b34">Siegel and McKeown, 2000</ref>).</p><p>In sum, we present the first study aiming to re- cover general action attributes for a diverse collec- tion of verbs, and probe their predictive power for zero-shot activity recognition on the recently in- troduced imSitu dataset ( <ref type="bibr" target="#b40">Yatskar et al., 2016)</ref>. Em- pirical results show that action attributes inferred from language can help classifying previously un- seen activities and suggest several avenues for fu- ture research on this challenging task. We publicly share our dataset and code for future research. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Action Verb Attributes</head><p>We consider seven different groups of action verb attributes. They are motivated in part by poten- tial relevance for visual zero-shot inference, and in part by classical literature on linguistic theories on verb semantics. The attribute groups are sum- marized below. <ref type="bibr">2</ref> Each attribute group consists of a set of attributes, which sums to K = 24 distinct attributes annotated over the verbs. <ref type="bibr">3</ref> [1] Aspectual Classes We include the aspectual verb classes of Vendler (1957):</p><p>• state: a verb that does not describe a chang- ing situation (e.g. "have", "be")</p><p>• achievement: a verb that can be completed in a short period of time (e.g. "open", "jump")</p><p>• accomplishment: a verb with a sense of com- pletion over a longer period of time (e.g. "climb")</p><p>• activity: a verb without a clear sense of com- pletion (e.g. "swim", "walk", "talk")</p><p>[2] Temporal Duration This attribute group re- lates to the aspectual classes above, but provides additional estimation of typical time duration with four categories. We categorize verbs by best- matching temporal units: seconds, minutes, hours, or days, with an additional option for verbs with unclear duration (e.g., "provide").</p><p>[3] Motion Dynamics This attribute group fo- cuses on the energy level of motion dynamics in four categories: no motion ("sleep"), low motion ("smile"), medium ("walk"), or high ("run"). We add an additional option for verbs whose motion level depends highly on context, such as 'finish.'</p><p>[4] Social Dynamics This attribute group fo- cuses on the likely social dynamics, in particular, whether the action is usually performed as a soli- tary act, a social act, or either. This is graded on a 5-part scale from least social (−2) to either (+0) to most social (+2)</p><p>[5] Transitivity This attribute group focuses on whether the verb can take an object, or be used without. This gives the model a sense of the im- plied action dynamics of the verb between the agent and the world. We record three variables: whether or not the verb is naturally transitive on a person ("I hug her" is natural), on a thing ("I eat it"), and whether the verb is intransitive ("I run").</p><p>[6] Effects on Arguments This attribute group focuses on the effects of actions on agents and other arguments. For each of the possible tran- sitivities of the verb, we annotate whether or not it involves location change ("travel"), world change ("spill"), agent or object change ("cry") , or no visible change ("ponder").</p><p>[7] Body Involvements This attribute group specifies prominent body parts involved in carry- ing out the action. For example, "open" typically involves "hands" and "arms" when used in a phys- ical sense. We use five categories: head, arms, torso, legs, and other body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Attributes and Contextual Variations</head><p>In general, contextual variations of action at- tributes are common, especially for frequently used verbs that describe everyday physical activi- ties. For example, while "open" typically involves "hands", there are exceptions, e.g. "open one's eyes". In this work, we focus on stereotypical or prominent characteristics across a range of actions that can be denoted using the same verb. Thus, three investigation points of our work include: (1) crowd-sourcing experiments to estimate the dis- tribution of human judgments on the prominent characteristics of everyday physical action verbs, (2) the feasibility of learning models for inferring the prominent characteristics of the everyday ac- tion verbs despite the potential noise in the human annotation, and (3) their predictive power in zero- shot action recognition despite the potential noise from contextual variations of action attributes. As we will see in Section 7, our study confirms the usefulness of studying action attributes and moti- vates the future study in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance to Linguistic Theories</head><p>The key idea in our work that action verbs project certain expec- tations about their influence on their arguments, their pre-and post-conditions, and their implica- tions on social dynamics, etc., relates to the orig- inal Frame theories of <ref type="bibr" target="#b4">Baker et al. (1998a)</ref>. The study of action verb attributes are also closely related to formal studies on verb categorization based on the characteristics of the actions or states that a verb typically associates to <ref type="bibr" target="#b21">(Levin, 1993)</ref>, and cognitive linguistics literature that focus on causal structure and force dynamics of verb mean- ings (Croft, 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Verb Attributes from Language</head><p>In this section we present our models for learn- ing verb attributes from language. We consider two complementary types of language-based in- put: dictionary definitions and word embeddings.</p><p>The approach based on dictionary definitions re- sembles how people acquire the meaning of a new word from a dictionary lookup, while the approach based on word embeddings resembles how people acquire the meaning of words in context.</p><p>Overview This task follows the standard super- vised learning approach where the goal is to pre- dict K attributes per word in the vocabulary V.</p><p>Let x v ∈ X represent the input representation of a word v ∈ V. For instance, x v could denote a word embedding, or a definition looked up from a dic- tionary (modeled as a list of tokens). Our goal is to produce a model f : X → R d that maps the input to a representation of dimension d. Modeling op- tions include using pretrained word embeddings, as in Section 3.1, or using a sequential model to encode a dictionary, as in Section 3.2. Then, the estimated probability distribution over attribute k is given by:</p><formula xml:id="formula_0">ˆ y v,k = σ(W (k) f (x v )).<label>(1)</label></formula><p>If the attribute is binary, then W (k) is a vector of dimension d and σ is the sigmoid function. Other- wise, W (k) is of shape d k × d, where d k is the di- mension of attribute k, and σ is the softmax func- tion. Let the vocabulary V be partitioned into sets V train and V test ; then, we train by minimizing the cross-entropy loss over V train and report attribute- level accuracy over words in V test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection to Learning Object Attributes</head><p>This problem has been studied before for zero-shot object recognition, but there are several key dif- ferences. Al-Halah et al. <ref type="formula" target="#formula_0">(2016)</ref> build the 'Class- Attribute Association Prediction' model (CAAP) that classifies the attributes of an object class from its name. They apply it on the Animals with At- tributes dataset, a dataset containing 50 animal classes, each described by 85 attributes <ref type="bibr" target="#b20">(Lampert et al., 2014</ref>). Importantly, these attributes are con- crete details with semantically meaningful names such as "has horns" and "is furry". The CAAP model takes advantage of this, consisting of a ten- sor factorization model initialized by the word embeddings of the object class names as well as the attribute names. On the other hand, verb at- tributes such as the ones we outline in Section 2, are technical linguistic terms. Since word embed- dings principally capture common word senses, they are unsuited for verb attributes. Thus, we evaluate two versions of CAAP as a baseline: one where the model is preinitialized with GloVe embeddings ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>) for the at- tribute names <ref type="bibr">(CAAP-pretrained)</ref>, and one where the model is learned from random initialization (CAAP-learned).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning from Distributed Embeddings</head><p>One way of producing attributes is from dis- tributed word embeddings such as word2vec ). Intuitively, we expect sim- ilar verbs to have similar distributions of nearby nouns and adverbs, which can greatly help us in zero-shot prediction. In our experiments, we use 300-dimensional GloVe vectors trained on 840B tokens of web data ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>). We use logistic regression to predict each attribute, as we found that extra hidden layers did not improve performance. Thus, we let f emb (x v ) = w v , the GloVe embedding of v, and use Equation 1 to get the distribution over labels. We additionally experiment with retrofitted em- beddings, in which embeddings are mapped in accordance with a lexical resource. Follow- ing the approach of <ref type="bibr" target="#b10">Faruqui et al. (2015)</ref>, we retrofit embeddings using WordNet <ref type="bibr" target="#b26">(Miller, 1995)</ref>, Paraphrase-DB ( <ref type="bibr" target="#b13">Ganitkevitch et al., 2013)</ref>, and FrameNet ( <ref type="bibr" target="#b5">Baker et al., 1998b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning from Dictionary Definitions</head><p>We additionally propose a model that learns the attribute-grounded meaning of verbs through dic- tionary definitions. This is similar in spirit to the task of using a dictionary to predict word embed- dings ( <ref type="bibr" target="#b16">Hill et al., 2016)</ref>.</p><p>BGRU encoder Our first model involves a Bidi- rectional Gated Recurrent Unit (BGRU) encoder ( <ref type="bibr" target="#b6">Cho et al., 2014)</ref>. Let x v,1:T be a definition for verb v, with T tokens. To encode the input, we pass it through the GRU equation:</p><formula xml:id="formula_1">h t = GRU(x v,t , h t−1 ).<label>(2)</label></formula><p>Let h 1 denote the output of a GRU applied on the reversed input x v,T :1 . Then, the BGRU encoder is the concatenation f bgru = h T h 1 .</p><p>Bag-of-words encoder Additionally, we try two common flavors of a Bag-of-Words model. In the standard case, we first construct a vocabulary of 5000 words by frequency on the dictionary def- initions. Then,</p><formula xml:id="formula_2">f bow (x v ) represents the one-hot encoding f bow (x v ) i = [i ∈ x v ]</formula><p>, in other words, whether word i appears in definiton x v for verb v.</p><p>Additionally, we try out a Neural Bag-of-Words model where the word embeddings in a defini- tion are averaged ( <ref type="bibr" target="#b18">Iyyer et al., 2015)</ref>. This is</p><formula xml:id="formula_3">f nbow (x v,1:T ) = 1 |T | T t=1 f emb (x v,t ).</formula><p>Dealing with multiple definitions per verb One potential pitfall with using dictionary defini- tions is that there are often many defnitions asso- ciated with each verb. This creates a dataset bias </p><formula xml:id="formula_4">f emb ~ h 1 ~ h 2 ~ h 3 ~ h 4 ~ h 5 ~ h 1 ~ h 2 ~ h 3 ~ h 4 ~ h 5</formula><p>Figure 2: Overview of our combined dictionary + embedding to attribute model. Our encoding is the concatenation of a Bidirectional GRU of a defini- tion and the word embedding for that word. The encoding is then mapped to the space of attributes using parameters W (k) .</p><p>since polysemic verbs are seen more often. Ad- ditionally, dictionary definitions tend to be sorted by relevance, thus lowering the quality of the data if all definitions are weighted equally during train- ing. To counteract this, we randomly oversample the definitions at training time so that each verb has the same number of definitions. <ref type="bibr">4</ref> At test time, we use the first-occurring (and thus generally most relevant) definition per verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Dictionary and Embedding representations</head><p>We hypothesize that the two modalities of the dic- tionary and distributional embeddings are comple- mentary. Therefore, we propose an early fusion (concatenation) of both categories. <ref type="figure">Figure 2</ref> de- scribes the GRU + embedding model -in other words, f BGRU +emb = f BGRU f emb . This can likewise be done with any choice of definition en- coder and word embedding.</p><p>Overview A formal description of the task is at follows. Let the space of labels be V, partitioned into V train and V test . Let z v ∈ Z represent an image with label v ∈ V; our goal is to correctly predict this label amongst verbs v ∈ V test at test time, despite never seeing any images with labels in V test during training. Generalization will be done through a lookup table A, with known attributes for each v ∈ V. Formally, for each attribute k we define it as:</p><formula xml:id="formula_5">A (k) v ,i = 1 if attribute k for verb v is i −1 otherwise (3)</formula><p>For binary attributes, we need only one entry per verb, making A (k) a single column vector. Let our image encoder be represented by the map g : Z → R d . We then use the linear map in Equation 1 to produce the log-probability distribution over each attribute k. The distribution over labels is then:</p><formula xml:id="formula_6">P (·|z v ) = softmax v k A (k) W (k) g(z v )<label>(4)</label></formula><p>where W (k) is a learned parameter that maps the image encoder to the attribute representation. We then train our model by minimizing the cross- entropy loss over the training verbs V train .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Neural Network (CNN) Encoder</head><p>Our image encoder is a CNN with the Resnet-152 architecture ( <ref type="bibr" target="#b15">He et al., 2016)</ref>. We use weights pre- trained on ImageNet ( <ref type="bibr" target="#b8">Deng et al., 2009</ref>) and per- form additional pretraining on ImSitu using the classes V train . After this, we remove the top layer and set g(x v ) to be the 2048-dimensional image representation from the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Connection to other attribute models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint prediction from Attributes and Embeddings</head><p>To combine the representation power of the at- tribute and embedding models, we build an en- semble combining both models. This is done by adding the logits before the softmax is applied:</p><formula xml:id="formula_7">softmax v k A (k) W (k) g(z v ) + A emb W emb g(z v )</formula><p>A diagram is shown in <ref type="figure">Figure 3</ref>. We find that during optimization, this model can easily over- fit, presumably by excessive coadaption of the em- bedding and attribute components. To solve this, we train the model to minimize the cross entropy of three sources independently: the attributes only, the embeddings only, and the sum, weighting each equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating predicted and gold attributes</head><p>We additionally experiment with an ensemble of our model, combining predicted and gold at- tributes of V test . This allows the model to hedge against cases where a verb attribute might have several possible correct answers. A single model is trained; at test time, we multiply the class level probabilities P (·|z v ) of each together to get the fi- nal predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Actions and Attributes Dataset</head><p>To evaluate our hypotheses on action attributes and zero-shot learning, we constructed a dataset using crowd-sourcing experiments. The Actions and At- tributes dataset consists of annotations for 1710 verb templates, each consisting of a verb and an optional particle (e.g. "put" or "put up").</p><p>We selected all verbs from the ImSitu cor- pus, consisting of images representing verbs from many categories ( <ref type="bibr" target="#b40">Yatskar et al., 2016)</ref>, then extended the set using the MPII movie vi- sual description dataset and ScriptBase datasets, ( <ref type="bibr">Rohrbach et al., 2015;</ref><ref type="bibr" target="#b14">Gorinski and Lapata, 2015)</ref>. We used the spaCy dependency parser ( <ref type="bibr" target="#b17">Honnibal and Johnson, 2015)</ref> to extract the verb template for each sentence, and collected annota- tions on Mechanical Turk to filter out nonliteral and abstract verbs. Turkers annotated this filtered set of templates using the attributes described in Section 2. In total, 1203 distinct verbs are in- cluded. The templates are split randomly by verb; out of 1710 total templates, we save 1313 for train- ing, 81 for validation, and 316 for testing.</p><p>To provide signal for classifying these verbs, we collected dictionary definitions for each verb us- ing the Wordnik API, <ref type="bibr">6</ref> including only senses that are explicitly labeled "verb." This leaves us with 23,636 definitions, an average of 13.8 per verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>BGRU pretaining We pretrain the BGRU model on the Dictionary Challenge, a collection of 800,000 word-definition pairs obtained from Wordnik and Wikipedia articles ( <ref type="bibr" target="#b16">Hill et al., 2016)</ref>; the objective is to obtain a word's embedding given one of its definitions. For the BGRU model, we use an internal dimension of 300, and embed the words to a size 300 representation. The vocab- ulary size is set to 30,000 (including all verbs for which we have definitions). During pretraining, we keep the architecture the same, except a differ- ent 300-dimensional final layer is used to predict the GloVe embeddings.</p><p>Following <ref type="bibr" target="#b16">Hill et al. (2016)</ref>, we use a ranking loss. LetˆwLetˆ Letˆw = W emb f (x) be the predicted word embeddings for each definition x of a word in the dictionary (not necessarily a verb). Let w be the word's embedding, and w be the embedding of a random dictionary word. The loss is then given by:</p><formula xml:id="formula_8">L = max{0, .1 − cos(w, ˆ w) + cos(w, w)}</formula><p>After pretraining the model on the Dictionary Challenge, we fine-tune the attribute weights W (k) using the cross-entropy over Equation 1.</p><p>Zero-shot with the imSitu dataset We build our image-to-verb model on the newly introduced imSitu dataset, which contains a diverse collection of images depicting one of 504 verbs. The images represent a variety of different semantic role labels ( <ref type="bibr" target="#b40">Yatskar et al., 2016</ref>). <ref type="figure" target="#fig_2">Figure 4</ref> shows examples from the dataset. We apply our attribute split to the dataset and are left with 379 training classes, 29 validation classes, and 96 test classes.</p><p>Zero-shot activity recognition baselines We compare against several additional baseline mod- els for learning from attributes and embeddings. Romera-Paredes and Torr <ref type="formula" target="#formula_0">(2015)</ref>   <ref type="table">Table 1</ref>: Results on the text-to-attributes task. All values reported are accuracies (in %). For attributes where multiple labels can be selected, the accuracy is averaged over all instances (e.g., the accuracy of "Body" is given by the average of accuracies from correctly predicting Head, Torso, etc.). As such, we report two ways of averaging the results: microaveraging (where the accuracy is the average of accuracies on the underlying labels) and macroaveraging (where the accuracy is averaged together from the groups).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Predicting Action Attributes from Text</head><p>Our results for action attribute prediction from text are given in <ref type="table">Table 1</ref>. Several examples are given in the supplemental section in <ref type="table">Table 3</ref>. Our results on the text-to-attributes challenge confirm that it is a challenging task for two reasons. First, there is noise associated with the attributes: many verb at- tributes are hard to annotate given that verb mean- ings can change in context. <ref type="bibr">7</ref> Second, there is a lack of training data inherent to the problem: there are not many common verbs in English, and it can be difficult to crowdsource annotations for rare ones. Third, any system must compete with strong frequency-based baselines, as attributes are gener- ally sparse. Moreover, we suspect that were more attributes collected (so as to cover more obscure patterns), the sparsity would only increase. Despite this, we report strong baseline results on this problem, particularly with our embedding based models. The performance gap between embedding-and definition-based models can pos- sibly be explained by the fact that the word em- beddings are trained on a very large corpus of real- world examples of the verb, while the definition is only a single high-level representation meant to be understood by someone who already speaks that language. For instance, it is likely difficult for the definition-based model to infer whether a verb is transitive or not (Transi.), since definitions might assume commonsense knowledge about the under- <ref type="bibr">7</ref> As such, our attributes have a median Krippendorff Al- pha of α = .359. lying concepts the verb represents. The strong performance of embedding models is further en- hanced by using retrofitted word embeddings, sug- gesting an avenue for improvement on language grounding through better representation of linguis- tic corpora.</p><p>We additionally see that both joint dictionary- embedding models outperform the dictionary-only models overall. In particular, the BGRU+GloVe model performs especially well at determining the aspect and motion attributes of verbs, particularly relative to the baseline. The strong performance of the BGRU+GloVe model indicates that there is some signal that is missing from the distributional embeddings that can be recovered from the dictio- nary definition. We thus use the predictions of this model for zero-shot image recognition.</p><p>Based on error analysis, we found that one com- mon mode of failure is where contextual knowl- edge is required. To give an example, the embed- ding based model labels "shop" as a likely soli- tary action. This is possibly because there are a lack of similar verbs in V train ; by random chance, "buy" is also in the test set. We see that this can be partially mitigated by the dictionary, as evidenced by the fact that the dictionary-based models label "shop" as in between social and solitary. Still, it is a difficult task to infer that people like to "visit stores in search of merchandise" together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Zero-shot Action Recognition</head><p>Our results for verb prediction from images are given in  <ref type="table" target="#tab_3">Table 2</ref>: Results on the image-to-verb task. atts(P) refers to attributes predicted from the BGRU+GloVe model described in Section 3, atts(G) to gold attributes, and GloVe to GloVe vec- tors. The accuracies reported are amongst the 96 unseen labels of V test .</p><p>our models show predictive power. Although our attribute models do not outperform our embed- ding models and DeVISE alone, we note that our joint attribute and embedding model scores the best overall, reaching 18.10% in top-1 and 41.46% in top-5 accuracy when using gold attribute anno- tations for the zero-shot verbs. This result is possi- bly surprising given the small number of attributes (K = 24) in total, of which most tend to be sparse (as can be seen from the baseline performance in <ref type="table">Table 1</ref>). We thus hypothesize that collecting more activity attributes would further improve perfor- mance.</p><p>We also note the success in performing zero- shot learning with predicted attributes. Perhaps paradoxically, our attribute-only models (along with DAP) perform better in both accuracy met- rics when given predicted attributes at test time, as opposed to gold attributes. Further, we get an ex- tra boost by ensembling predictions of our model when given two sets of attributes at test time, giv- ing us the best results overall at 18.15% top-1 ac- curacy and 42.17% top-5. Interestingly, better per- formance with predicted attributes is also reported by <ref type="bibr" target="#b1">Al-Halah et al. (2016)</ref>: predicting the attributes with their CAAP model and then running the DAP model on these predicted attributes outperforms the use of gold attributes at test time. It is some- what unclear why this is the case -possibly, there is some bias in the attribute labeling, which the at- tribute predictor can correct for.</p><p>In addition to quantitative results, we show some zero-shot examples in <ref type="figure" target="#fig_2">Figure 4</ref>. The ex- amples show inherent difficulty of zero-shot ac- tion recognition. Incorrect predictions are of- ten reasonably related to the situation ("rub" vs "dye") but picking the correct target verb based on attribute-based inference is still a challenging task.</p><p>Although our results appear promising, we ar- gue that our model still fails to represent much of the semantic information about each image class.</p><p>In particular, our model is prone to hubness: the overprediction of a limited set of labels at test time: those that closely match signatures of ex- amples in the training set. This problem has pre- viously been observed with the use of word em- beddings for zero-shot learning <ref type="bibr" target="#b23">(Marco and Georgiana, 2015)</ref> and can be seen in our examples (for instance, the over-prediction of "buy"). Unfortu- nately, we were unable to mitigate this problem in a way that also led to better quantitative results (for instance, by using a ranking loss as in DeVISE ( <ref type="bibr" target="#b12">Frome et al., 2013)</ref>). We thus leave resolving the hubness problem in zero-shot activity recognition as a question for future work.  <ref type="formula" target="#formula_0">(2016)</ref> predicts the object attributes of concrete nouns for use in zero-shot learning. In contrast, we predict verb attributes. A related task is that of improving word embeddings using multimodal data and linguistic resources <ref type="bibr" target="#b10">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b35">Silberer et al., 2013;</ref><ref type="bibr" target="#b37">Vendrov et al., 2016)</ref>. Our work runs orthogonal to this, as we focus on word attributes as a tool for a zero-shot activity recognition pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Zero-shot learning with objects Though dis- tinct, our work is related to zero-shot learn- ing of objects in computer vision. There are several datasets <ref type="bibr" target="#b28">(Nilsback and Zisserman, 2008;</ref><ref type="bibr" target="#b38">Welinder et al., 2010)</ref> and models developed on this task <ref type="bibr" target="#b32">(Romera-Paredes and Torr (2015)</ref>; <ref type="bibr" target="#b20">Lampert et al. (2014)</ref>; <ref type="bibr" target="#b27">Mukherjee and Hospedales (2016)</ref>; <ref type="bibr" target="#b9">Farhadi et al. (2010)</ref>).</p><p>In addition, Zero-shot activity recognition In prior work, zero-shot activity recognition has been studied on video datasets, each containing a selection of con- crete physical actions. The MIXED action dataset, itself a combination of three action recognition datasets, has 2910 labeled videos with 21 actions, each described by 34 action attributes ( <ref type="bibr" target="#b22">Liu et al., 2011</ref>). These action attributes are concrete bi- nary attributes corresponding to low-level physical movements, for instance, "arm only motion," "leg: up-forward motion." By using word embeddings instead of attributes, <ref type="bibr" target="#b39">Xu et al. (2017)</ref> study video activity recognition on a variety of action datasets, albeit in the transductive setting wherein access to the test labels is provided during training. In com- parison with our work on imSitu ( <ref type="bibr" target="#b40">Yatskar et al., 2016)</ref>, these video datasets lack broad coverage of verb-level classes (and for some, sufficient data points per class). The abstractness of broad-coverage activity la- bels makes the problem much more difficult to study with attributes. To get around this, <ref type="bibr" target="#b2">Antol et al. (2014)</ref> present a synthetic dataset of car- toon characters performing dyadic actions, and use these cartoon illustrations as internal mappings for zero-shot recognition of dyadic actions in real- world images. We investigate an alternative ap- proach by using linguistically informed verb at- tributes for activity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Future work / Conclusion</head><p>Several possibilities remain open for future work. First, more attributes could be collected and evalu- ated, possibly integrating other linguistic theories about verbs, with more accurate modeling of con- text. Second, while our experiments use attributes as a pivot between language and vision domains, the effects of this could be explored more in future work. In particular, since our experiments show that unsupervised word embeddings significantly help performance, it might be desirable to learn data-driven attributes in an end-to-end fashion di- rectly from a large corpus or from dictionary defi- nitions. Third, future research on action attributes should ideally include videos to better capture at- tributes that require temporal signals.</p><p>Overall, however, our work presents a strong early step towards zero-shot activity recognition, a relatively less studied task that poses several unique challenges over zero-shot object recogni- tion. We introduce new action attributes motivated by linguistic theories and demonstrate their empir- ical use for reasoning about previously unseen ac- tivities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Our CNN and BGRU models are built in Py- Torch <ref type="bibr">8</ref> . All of our one-layer neural network mod- els are built in Scikit-learn <ref type="bibr" target="#b29">(Pedregosa et al., 2011</ref>) using the provided LogisticRegression class (us- ing one-versus-rest if appropriate). Our neural models use the Adam optimizer ( <ref type="bibr" target="#b19">Kingma and Ba, 2014</ref>), though we weak the default hyperparame- ters somewhat.</p><p>Recall that our dictionary definition model is a bidirectional GRU with a hidden size of 300, with a vocabulary size of 30,000. After pretraining on the Dictionary Challenge, we freeze the word embeddings and apply a dropout rate of 50% be- fore the final hidden layer. We found that such an aggressive dropout rate was necessary due to the small size of the training set. During pretraining, we used a learning rate of 10 −4 , a batch size of 64, and set the Adam parameter to the default 1e −8 . During finetuning, we set = 1.0 and the batch size to 32. In general, we found that setting too low of an during finetuning caused our zero- shot models to update parameters too aggressively during the first couple of updates, leading to poor results.</p><p>For our CNN models, we pretrained the Resnet 152 (initialized with imagenet weights) on the training classes of the imSitu dataset, using a learning rate of 10 −4 and = 10 −8 . During fine- tuning, we dropped the learning rate to 10 −5 and set = 10 −1 . We also froze all parameters except for the final resnet block, and the linear attribute and embedding weights. We also found L2 regu- larization quite important in reducing overfitting, and we applied regularization at a weight of 10 −4 to all trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full list of attributes</head><p>The following is a full list of the attributes. In addition to the attributes presented here, we also crowdsourced attributes for the emotion content of each verb (e.g., happiness, sadness, anger, and sur- prise). However, we found these annotations to be skewed towards "no emotion", since most verbs do not strongly associate with a specific emotion. Thus, we omit them in our experiments. (a) Intransitive 1: 1 if the verb is intransitive and the subject moves somewhere (b) Intransitive 2: 1 if the verb is intransitive and the external world changes (c) Intransitive 3: 1 if the verb is intransitive, and the subject's state changes (d) Intransitive 4: 1 if the verb is intransitive, and noth- ing changes (e) Transitive (obj) 1: 1 if the verb is transitive for ob- jects and the object moves somewhere (f) Transitive (obj) 2: 1 if the verb is transitive for ob- jects and the external world changes (g) Transitive (obj) 3: 1 if the verb is transitive for ob- jects and the object's state changes (h) Transitive (obj) 4: 1 if the verb is transitive for ob- jects and nothing changes (i) Transitive (person) 1: 1 if the verb is transitive for people and the object is a person that moves some- where (j) 'Transitive (person) 2: 1 if the verb is transitive for people and the external world changes (k) Transitive (person) 3: 1 if the verb is transitive for people and if the object is a person whose state changes (l) Transitive (person) 4: 1 if the verb is transitive for people and nothing changes </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Rubinstein et al. (2015) seek to predict McRae et al. (2005)'s feature norms from word embed- dings of concrete nouns. Likewise, the CAAP model of Al-Halah et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Predictions on unseen classes from our attribute+embedding model with gold attributes. The top and bottom rows show successful and failure cases respectively. The bars to the right of each image represent a probability distribution, showing the ground truth class and the top 5 scoring incorrect classes.</figDesc><graphic url="image-12.png" coords="9,184.89,127.51,60.48,60.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>On the order of seconds (c) On the order of hours (d) On the order of days (3) Motion Dynamics: One attribute with 5 values: (a) unclear without context (b) No motion (c) Low motion (d) Medium motion (e) High motion (4) Social Dynamics: One attribute with 5 values:Transitive (person): 1 if the verb can be used in the form "&lt;someone&gt;", 0 otherwise (c) Transitive (object): 1 if the verb can be used in the form "&lt;verb&gt; something", 0 otherwise (6) Effects on Arguments: 12 binary attributes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 7 )</head><label>7</label><figDesc>Body Involements: 5 binary attributes (a) Arms: 1 if arms are used (b) Head: 1 if head is used (c) Legs: 1 if legs are used (d) Torso: 1 if torso is used (e) Other: 1 if another body part is used</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Despite the difficulty of pre-
dicting the correct label over 96 unseen choices, </table></figure>

			<note place="foot" n="1"> Available at http://github.com/rowanz/verb-attributes 2 The full list is available in the supplemental section. 3 Several of our attributes are categorical; if converted to binary attributes, we would have 40 attributes in total.</note>

			<note place="foot" n="4"> Zero-Shot Activity Recognition 4.1 Verb Attributes as Latent Mappings Given learned attributes for a collection of activities, we would like to evaluate their performance at describing these activities from real world images in a zero-shot setting. Thus, we consider several models that classify an image&apos;s label by pivoting through an attribute representation. 4 For the (neural) bag of words models, we also tried concatenating the definitions together per verb and then doing the encoding. However, we found that this gave worse results.</note>

			<note place="foot" n="6"> Available at http://developer.wordnik.com/ with access to American Heriatge Dictionary, the Century Dictionary, the GNU Collaborative International Dictionary, Wordnet, and Wiktionary.</note>

			<note place="foot" n="8"> pytorch.org</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5975" to="5984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Zero-Shot Learning via Visual Abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In ECCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL &apos;98: Proceedings of the Conference</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Verbs: Aspect and causal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OUP Oxford. Pg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attribute-centric recognition for cross-category generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2352" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Retrofitting Word Vectors to Semantic Lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic prediction of aspectual class of verbs in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annemarie</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="517" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeViSE: A Deep VisualSemantic Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><forename type="middle">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ppdb: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Movie script summarization as graph-based scene extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Gorinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1066" to="1076" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to Understand Phrases by Embedding the Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An improved non-monotonic transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1373" to="1378" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boydgraber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">English verb classes and alternations : a preliminary investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3337" to="3344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baroni</forename><surname>Angeliki Lazaridou Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinu</forename><surname>Georgiana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic feature production norms for a large set of living and nonliving things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George S Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Seidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnorgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gaussian visual-linguistic embedding for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02530[cs].ArXiv:1501.02530</idno>
		<title level="m">Niket Tandon, and Bernt Schiele. 2015. A Dataset for Movie Description</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How well do distributional models capture different types of semantic knowledge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effi</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning methods to combine linguistic indicators: Improving aspectual classification and revealing linguistic insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="595" to="628" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Models of semantic representation with visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Verbs and Times. The Philosophical Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Vendler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="143" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<title level="m">Caltech-ucsd birds</title>
		<imprint>
			<date type="published" when="0200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transductive zero-shot action recognition by word-vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
