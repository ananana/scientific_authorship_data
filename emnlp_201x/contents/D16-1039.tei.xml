<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
							<email>at.luu@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Infocomm Research</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Infocomm Research</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Infocomm Research</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Infocomm Research</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See</forename><forename type="middle">Kiong</forename><surname>Ng</surname></persName>
							<email>skng@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Infocomm Research</orgName>
								<orgName type="department" key="dep2">Institute for Infocomm Research</orgName>
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="403" to="413"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Taxonomic relation identification aims to recognize the &apos;is-a&apos; relation between two terms. Previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches, but the accuracy of these approaches is far from satisfactory. In this paper , we propose a novel supervised learning approach for identifying taxonomic relations using term embeddings. For this purpose, we first design a dynamic weighting neural network to learn term embeddings based on not only the hypernym and hyponym terms, but also the contextual information between them. We then apply such embeddings as features to identify taxonomic relations using a supervised method. The experimental results show that our proposed approach significantly out-performs other state-of-the-art methods by 9% to 13% in terms of accuracy for both general and specific domain datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Taxonomies which serve as the backbone of struc- tured knowledge are useful for many NLP applica- tions such as question answering ( <ref type="bibr" target="#b8">Harabagiu et al., 2003</ref>) and document clustering <ref type="bibr" target="#b6">(Fodeh et al., 2011</ref>). However, the hand-crafted, well-structured tax- onomies including WordNet <ref type="bibr" target="#b18">(Miller, 1995)</ref>, Open- Cyc ( <ref type="bibr" target="#b16">Matuszek et al., 2006</ref>) and Freebase <ref type="bibr" target="#b4">(Bollacker et al., 2008</ref>) that are publicly available may not be complete for new or specialized domains. It is also time-consuming and error prone to identify taxonomic relations manually. As such, methods for automatic identification of taxonomic relations is highly desirable.</p><p>The previous methods for identifying taxonomic relations can be generally classified into two cate- gories: statistical and linguistic approaches. The sta- tistical approaches rely on the idea that frequently co-occurring terms are likely to have taxonomic re- lationships. While such approaches can result in taxonomies with relatively high coverage, they are usually heavily dependent on the choice of feature types, and suffer from low accuracy. The linguis- tic approaches which are based on lexical-syntactic patterns (e.g. 'A such as B') are simple and efficient. However, they usually suffer from low precision and coverage because the identified patterns are unable to cover the wide range of complex linguistic struc- tures, and the ambiguity of natural language com- pounded by data sparsity makes these approaches less robust.</p><p>Word embedding ( <ref type="bibr" target="#b2">Bengio et al., 2001</ref>), also known as distributed word representation, which represents words with high-dimensional and real- valued vectors, has been shown to be effective in exploring both linguistic and semantic relations be- tween words. In recent years, word embedding has been used quite extensively in NLP research, rang- ing from syntactic parsing <ref type="bibr" target="#b24">(Socher et al., 2013a</ref>), machine translation ( <ref type="bibr" target="#b36">Zou et al., 2013)</ref> to senti- ment analysis <ref type="bibr" target="#b25">(Socher et al., 2013b</ref>). The cur- rent methods for learning word embeddings have focused on learning the representations from word co-occurrence so that similar words will have simi- lar embeddings. However, using the co-occurrence based similarity learning alone is not effective for the purpose of identifying taxonomic relations. vised method to learn term embeddings based on pre-extracted taxonomic relation data. However, this method is heavily dependent on the training data to discover all taxonomic relations, i.e. if a pair of terms is not in the training set, it may become a negative example in the learning process, and will be classified as a non-taxonomic relation. The de- pendency on training data is a huge drawback of the method as no source can guarantee that it can cover all possible taxonomic relations for learning. More- over, the recent studies ( <ref type="bibr" target="#b29">Velardi et al., 2013;</ref><ref type="bibr" target="#b13">Levy et al., 2014;</ref><ref type="bibr" target="#b28">Tuan et al., 2015)</ref> showed that contex- tual information between hypernym and hyponym is an important indicator to detect taxonomic relations. However, the term embedding learning method pro- posed in ( <ref type="bibr" target="#b34">Yu et al., 2015</ref>) only learns through the pairwise relations of terms without considering the contextual information between them. Therefore, the resultant quality is not good in some specific do- main areas.</p><p>In this paper, we propose a novel approach to learn term embeddings based on dynamic weight- ing neural network to encode not only the informa- tion of hypernym and hyponym, but also the con- textual information between them for the purpose of taxonomic relation identification. We then ap- ply the identified embeddings as features to find the positive taxonomic relations using the supervised method SVM. The experimental results show that our proposed term embedding learning approach outperforms other state-of-the-art embedding learn- ing methods for identifying taxonomic relations with much higher accuracy for both general and spe- cific domains. In addition, another advantage of our proposed approach is that it is able to general- ize from the training dataset the taxonomic relation properties for unseen pairs. Thus, it can recognize some true taxonomic relations which are not even defined in dictionary and training data. For the rest of this paper, we will discuss the proposed term em- bedding learning approach and its performance re- sults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Previous works on taxonomic relation identification can be roughly divided into two main approaches of statistical learning and linguistic pattern matching.</p><p>Statistical learning methods include co-occurrence analysis <ref type="bibr" target="#b12">(Lawrie and Croft, 2003)</ref>, hierarchical la- tent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b3">Blei et al., 2004;</ref><ref type="bibr" target="#b20">Petinot et al., 2011</ref>), clustering ( <ref type="bibr" target="#b14">Li et al., 2013)</ref>, lin- guistic feature-based semantic distance learning ( <ref type="bibr" target="#b33">Yu et al., 2011</ref>), distributional representation ( <ref type="bibr" target="#b22">Roller et al., 2014;</ref><ref type="bibr" target="#b31">Weeds et al., 2014;</ref><ref type="bibr" target="#b11">Kruszewski et al., 2015)</ref> and co-occurrence subnetwork mining ( <ref type="bibr" target="#b30">Wang et al., 2013)</ref>. Supervised statistical methods <ref type="bibr" target="#b20">(Petinot et al., 2011</ref>) rely on hierarchical labels to learn the corresponding terms for each label. These methods require labeled training data which is costly and not always available in practice. Unsupervised statis- tical methods <ref type="bibr" target="#b21">(Pons-Porrata et al., 2007;</ref><ref type="bibr" target="#b14">Li et al., 2013;</ref><ref type="bibr" target="#b30">Wang et al., 2013</ref>) are based on the idea that terms that frequently co-occur may have taxonomic relationships. However, these methods generally achieve low accuracies.</p><p>Linguistic approaches rely on lexical-syntactic patterns <ref type="bibr" target="#b9">(Hearst, 1992</ref>) (e.g. 'A such as B') to cap- ture textual expressions of taxonomic relations, and match them with the given documents or Web in- formation to identify the relations between a term and its hypernyms ( <ref type="bibr" target="#b10">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b19">Navigli et al., 2011;</ref><ref type="bibr" target="#b32">Wentao et al., 2012</ref>). These pat- terns can be manually created ( <ref type="bibr" target="#b10">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b32">Wentao et al., 2012</ref>) or automatically identi- fied ( <ref type="bibr" target="#b23">Snow et al., 2004;</ref><ref type="bibr" target="#b19">Navigli et al., 2011</ref>). Such liguistic pattern matching methods can generally achieve higher precision than the statistical methods, but they suffer from lower coverage. To balance the precision and recall, <ref type="bibr" target="#b35">Zhu et al. (2013)</ref> and <ref type="bibr" target="#b27">Tuan et al. (2014)</ref> have combined both unsupervised statis- tical and linguistic methods for finding taxonomic relations.</p><p>In recent years, there are a few studies on tax- onomic relation identification using word embed- dings such as the work of <ref type="bibr" target="#b26">Tan et al. (2015)</ref> and <ref type="bibr" target="#b7">Fu et al. (2014)</ref>. These studies are based on word em- beddings from the Word2Vec model ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>), which is mainly optimized for the purpose of analogy detection using co-occurrence based sim- ilarity learning. As such, these studies suffer from poor performance on low accuracy for taxonomic re- lation identification.</p><p>The approach that is closest to our work is the one proposed by <ref type="bibr" target="#b34">Yu et al. (2015)</ref>, which also learns term embeddings for the purpose of taxonomic relation identification. In the approach, a distance-margin neural network is proposed to learn term embed- dings based on the pre-extracted taxonomic relations from the Probase database ( <ref type="bibr" target="#b32">Wentao et al., 2012</ref>). However, the neural network is trained using only the information of the term pairs (i.e. hypernym and hyponym) without considering the contextual infor- mation between them, which has been shown to be an important indicator for identifying taxonomic re- lations from previous studies <ref type="bibr" target="#b29">(Velardi et al., 2013;</ref><ref type="bibr" target="#b13">Levy et al., 2014;</ref><ref type="bibr" target="#b27">Tuan et al., 2014</ref>). Moreover, if a pair of terms is not contained in the training set, there is high possibility that it will become a nega- tive example in the learning process, and will likely be recognized as a non-taxonomic relation. The key assumption behind the design of this approach is not always true as no available dataset can possibly con- tain all taxonomic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first propose an approach for learning term embeddings based on hypernym, hy- ponym and the contextual information between them. We then discuss a supervised method for iden- tifying taxonomic relations based on the term em- beddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning term embeddings</head><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, there are three steps for learn- ing term embeddings: (i) extracting taxonomic rela- tions; (ii) extracting training triples; and (iii) training neural network. First, we extract from WordNet all taxonomic relations as training data. Then, we ex- tract from Wikipedia all sentences which contain at least one pair of terms involved in a taxonomic rela- tion in the training data, and from that we identify the triples of hypernym, hyponym and contextual words between them. Finally, using the extracted triples as input, we propose a dynamic weighting neural network to learn term embeddings based on the information of these triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Extracting taxonomic relations</head><p>This step aims to extract a set of taxonomic re- lations for training. For this purpose, we use Word- Net hierarchies for extracting all (direct and indirect) taxonomic relations between noun terms in Word- Net. However, based on our experience, the rela-  tions involving with top-level terms such as 'object', 'entity' or 'whole' are usually ambiguous and be- come noise for the learning purpose. Therefore, we exclude from the training set all relations which in- volve with those top-level terms. Note that we also exclude from training set all taxonomic relations that are happened in the datasets used for testing in Sec- tion 4.1. As a result, the total number of extracted taxonomic relations is 236,058.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Extracting training triples</head><p>This step aims to extract the triples of hypernym, hyponym and the contextual words between them. These triples will serve as the inputs to the neural network for training. In this research, we define contextual words as all words located between the hypernym and hyponym in a sentence. We use the latest English Wikipedia corpus as the source for ex- tracting such triples.</p><p>Using the set of taxonomic relations extracted from the first step as reference, we extract from the Wikipedia corpus all sentences which contain at least two terms involved in a taxonomic relation. Specifically, for each sentence, we use the Stanford parser ( <ref type="bibr" target="#b15">Manning et al., 2014</ref>) to parse it, and check whether there is any pair of terms which are nouns or noun phrases in the sentence having a taxonomic relationship. If yes, we extract the hypernym, hy- ponym and all words between them from the sen-tence as a training triple. In total, we have extracted 15,499,173 training triples from Wikipedia.</p><p>Here, we apply the Stanford parser rather than matching the terms directly in the sentence in order to avoid term ambiguity as a term can serve for dif- ferent grammatical functions such as noun or verb. For example, consider the following sentence:</p><p>• Many supporters book tickets for the premiere of his new publication.</p><p>The triple ('publication', 'book', 'tickets for the pre- miere of his new') may be incorrectly added to the training set due to the occurrence of the taxonomic pair ('publication', 'book'), even though the mean- ing of 'book' in this sentence is not about the 'pub- lication'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Training neural network</head><p>Contextual information is an important indicator for detecting taxonomic relations. For example, in the following two sentences:</p><p>• Dog is a type of animal which you can have as a pet.</p><p>• Animal such as dog is more sensitive to sound than human.</p><p>The occurrence of contextual words 'is a type of' and 'such as' can be used to identify the taxo- nomic relation between 'dog' and 'animal' in the sentences. Many works in the literature ( <ref type="bibr" target="#b10">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b19">Navigli et al., 2011;</ref><ref type="bibr" target="#b32">Wentao et al., 2012</ref>) attempted to manually find these contextual patterns, or automatically learn them. However, due to the wide range of complex linguistic structures, it is difficult to discover all possible contextual pat- terns between hypernyms and hyponyms in order to detect taxonomic relations effectively. In this paper, instead of explicitly discovering the contextual patterns of taxonomic relations, we pro- pose a dynamic weighting neural network to encode this information, together with the hypernym and hyponym, for learning term embeddings. Specifi- cally, the target of the neural network is to predict the hypernym term from the given hyponym term and contextual words. The architecture of the pro- posed neural network is shown in <ref type="figure">Figure 2</ref>, which consists of three layers: input layer, hidden layer and output layer.</p><p>In our setting, the vocabulary size is V , and the hidden layer size is N . The nodes on adjacent lay- ers are fully connected. Given a term/word t in the vocabulary, the input vector of t is encoded as a one-hot V -dimensional vector x t , i.e. x t consists of 0s in all elements except the element used to uniquely identify t which is set as 1. The weights between the input layer and output</p><note type="other">layer are repre- sented by a V ×N matrix W . Each row of W is a N -dimensional vector representation v t of the asso- ciated word/term t of the input layer. Given a hyponym term hypo and k context words c 1 , c 2 , .., c k in the training triple, the output of hid- den layer h is calculated as:</note><formula xml:id="formula_0">h = W · 1 2k (k × x hypo + x c 1 + x c 2 + ... + x c k ) = 1 2k (k × v hypo + v c 1 + v c 2 + ... + v c k )<label>(1)</label></formula><p>where v t is the vector representation of the input word/term t.</p><p>The weight of h in Equation <ref type="formula" target="#formula_0">(1)</ref> is calculated as the average of the vector representation of hyponym term and contextual words. Therefore, this weight is not based on a fixed number of inputs. Instead, it is dynamically updated based on the number of contextual words k in the current training triple, and the hyponym term. This model is called dynamic weighting neural network to reflect its dynamic na- ture. Note that to calculate h, we also multiply the vector representation of hyponym by k to reduce the bias problem of high number of contextual words, so that the weight of the input vector of hyponym is balanced with the total weight of contextual words.</p><p>From the hidden layer to the output layer, there is another weight N × V for the output matrix W . Each column of W is a N -dimensional vector v t representing the output vector of t. Using these weights, we can compute an output score u t for each term/word t in the vocabulary:</p><formula xml:id="formula_1">u t = v t · h<label>(2)</label></formula><p>where v t is the output vector of t. We then use soft-max, a log-linear classification model, to obtain the posterior distribution of hyper- nym terms as follows:</p><formula xml:id="formula_2">W V x N W V x N W V x N W V x N W' N x V Output layer</formula><p>Hidden layer</p><p>Input layer</p><formula xml:id="formula_3">V-dimension N-dimension V-dimension x hypo x c1 x c2 x ck h Figure 2:</formula><p>The architecture of the proposed dynamic weighting neural network model.</p><formula xml:id="formula_4">p(hype|hypo, c 1 , c 2 , .., c k ) = e u hype V i=1 e u i = e v hype · 1 2k (k×v hypo + k j=1 vc j ) V i=1 e v i · 1 2k (k×v hypo + k j=1 vc j )<label>(3)</label></formula><p>The objective function is then defined as:</p><formula xml:id="formula_5">O = 1 T T t=1 log(p(hype t |hypo t , c 1t , c 2t , .., c kt ))<label>(4)</label></formula><p>where T is the number of training triples; hype t , hypo t and c it are hypernym term, hyponym term and contextual words respectively in the training triple t.</p><p>After maximizing the log-likelihood objective function in Equation (4) over the entire training set using stochastic gradient descent, the term embed- dings are learned accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised taxonomic relation identification</head><p>To decide whether a term x is a hypernym of term y, we build a classifier that uses embedding vec- tors as features for taxonomic relation identification.</p><p>Specifically, we use Support Vector Machine (SVM) <ref type="bibr" target="#b5">(Cortes and Vapnik, 1995)</ref> for this purpose. Given an ordered pair (x, y), the input feature is the con- catenation of embedding vectors (v x ,v y ) of x and y. In addition, our term embedding learning approach has the property that the embedding of hypernym is encoded based on not only the information of hy- ponym but also the information of contextual words. Therefore, we add one more feature to the input of SVM, i.e. the offset vector (v x − v y ), to contain the information of all contextual words between x and y. In summary, the feature vector is a 3d dimensional vector v x , v y , v x − v y , where d is the dimension of term embeddings. As will be shown later in the experimental results, the offset vector plays an im- portant role in the task of taxonomic relation identi- fication of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments to evaluate the perfor- mance of our term embedding learning approach on the general domain areas as well as the specific do- main areas. In performance evaluation, we compare our approach with two other state-of-the-art super- vised term embedding learning methods in <ref type="bibr" target="#b34">Yu et al. (2015)</ref> and the Word2Vec model ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>There are five datasets used in the experiments. Two datasets, namely BLESS and ENTAILMENT, are general domain datasets. The other three datasets, namely Animal, Plant and Vehicle, are specific do- main datasets.</p><p>• BLESS ( <ref type="bibr" target="#b0">Baroni and Lenci, 2011)</ref>  • Animal, Plant and Vehicle datasets (Velardi et al., 2013): They are taxonomies constructed based on the dictionaries and data crawled from the Web for the corresponding domains. The positive examples are created by extracting all possible (direct and indirect) taxonomic rela- tions from the taxonomies. The negative ex- amples are generated by randomly pairing two terms which are not involved in any taxonomic relation.</p><p>The number of terms, positive examples and neg- ative examples extracted from the five datasets are summarized in <ref type="table" target="#tab_2">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison models</head><p>In the experiments, we use the following supervised models for comparison:</p><p>• SVM+Our: This model uses SVM and the term embeddings obtained by our learning approach. The input is a 3d-dimensional vector v x , v y , v x − v y , where d is the dimension of term em- beddings, x and y are two terms used to check whether x is a hypernym of y or not, and v x , v y are the term embeddings of x and y respec- tively.</p><p>• SVM+Word2Vec: This model uses SVM and the term embeddings obtained by applying the Skip-gram model ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>) on the entire English Wikipedia corpus. The in- put is also a 3d-dimensional vector as in the SVM+Our model. Note that the results of the Skip-gram model are word embeddings. So if a term is a multiword term, its embedding is cal- culated as the average of all words in the term.</p><p>• SVM+Yu: This model uses SVM and the term embeddings obtained by using <ref type="bibr">Yu et al.'s method (2015)</ref>. According to the best setting stated in ( <ref type="bibr" target="#b34">Yu et al., 2015)</ref>, the input is a 2d+1 dimensional vector O(x), E(y), O(x)-E(y) 1 , where O(x), E(y) and O(x)-E(y) 1 are hy- ponym embedding of x, hypernym embedding of y and 1-norm distance of the vector (O(x)- E(y)) respectively.</p><p>Parameter settings. The SVM in the three models is trained using a RBF kernel with λ= 0.03125 and penalty term C = 8.0. For term embedding learning, the vector's dimension is set to 100. The tuning of the dimension will be discussed in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance on general domain datasets</head><p>For the general domain datasets, we have conducted two experiments to evaluate the performance of our proposed approach. Experiment 1. For the BLESS dataset, we hold out one concept for testing and train on the remaining 199 concepts. The hold-out concept and its rela- tum constitute the testing set, while the remaining 199 concepts and their relatum constitute the train- ing set. To further separate the training and test- ing sets, we exclude from the training set any pair of terms that has one term appearing in the testing set. We report the average accuracy across all con- cepts. For the ENTAILMENT dataset, we use the same evaluation method: hold out one hypernym for testing and train on the remaining hypernyms, and we also report the average accuracy across all hy- pernyms. Furthermore, to evaluate the effect of the offset vector to taxonomic relation identification, we deploy a setting that removes the offset vector in the feature vectors of SVM. Specifically, for SVM+Our and SVM+Word2Vec, the input vector is changed from v x , v y , v x − v y to v x , v y . We use the sub- script short to denote this setting.   <ref type="table" target="#tab_4">Table 2</ref> shows the performance of the three su- pervised models in Experiment 1. Our approach achieves significantly better performance than Yu's method and Word2Vec method in terms of accu- racy (t-test, p-value &lt; 0.05) for both BLESS and ENTAILMENT datasets. Specifically, our approach improves the average accuracy by 4% compared to Yu's method, and by 9% compared to the Word2Vec method. The Word2Vec embeddings have the worst result because it is based only on co-occurrence based similarity, which is not effective for the clas- sifier to accurately recognize all the taxonomic re- lations. Our approach performs better than Yu's method and it shows that our approach can learn em- beddings more effectively. Our approach encodes not only hypernym and hyponym terms but also the contextual information between them, while Yu's method ignores the contextual information for tax- onomic relation identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Moreover, from the experimental results of SVM+Our and SVM+Our short , we can observe that the offset vector between hypernym and hyponym, which captures the contextual information, plays an important role in our approach as it helps to improve the performance in both datasets. However, the off- set feature is not so important for the Word2Vec model. The reason is that the Word2Vec model is targeted for the analogy task rather than taxonomic relation identification.</p><p>Experiment 2. This experiment aims to evaluate the generalization capability of our extracted term em- beddings. In the experiment, we train the classifier on the BLESS dataset, test it on the ENTAILMENT dataset and vice versa. Similarly, we exclude from the training set any pair of terms that has one term appearing in the testing set. The experimental results in <ref type="table" target="#tab_6">Table 3</ref> show that our term embedding learning approach performs better than other methods in ac- curacy. It also shows that the taxonomic properties identified by our term embedding learning approach have great generalization capability (i.e. less depen- dent on the training set), and can be used generically for representing taxonomic relations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance on specific domain datasets</head><p>Similarly, for the specific domain datasets, we have conducted two experiments to evaluate the perfor- mance of our proposed approach.</p><p>Experiment 3. For each of the Animal, Plant and Vehicle datasets, we also hold out one term for test- ing and train on the remaining terms. The posi- tive and negative examples which contain the hold- out term constitute the testing set, while other pos- itive and negative examples constitute the training set. We also exclude from the training set any pair of terms that has one term appearing in the test- ing set. The experimental results are given in <ref type="table" target="#tab_8">Ta- ble 4</ref>. We can observe that not only for general do- main datasets but also for specific domain datasets, our term embedding learning approach has achieved significantly better performance than Yu's method and the Word2Vec method in terms of accuracy (t- test, p-value &lt; 0.05). Specifically, our approach im- proves the average accuracy by 22% compared to Yu's method, and by 9% compared to the Word2Vec method.  Another interesting point to observe is that the ac- curacy of Yu's method drops significantly in spe- cific domain datasets (as shown in <ref type="table" target="#tab_8">Table 4</ref>) when compared to the general domain datasets (as shown in <ref type="table" target="#tab_4">Table 2</ref>). One possible explanation is the accu- racy of Yu's method depends on the training data. As Yu's method learns the embeddings using pre- extracted taxonomic relations from Probase, and if a relation does not exist in Probase, there is high pos- sibility that it becomes a negative example and be recognized as a non-taxonomic relation by the clas- sifier. Therefore, the training data extracted from Probase plays an important role in Yu's method. For general domain datasets (BLESS and ENTAIL- MENT), there are about 75%-85% of taxonomic re- lations in these datasets found in Probase, while there are only about 25%-45% of relations in the specific domains (i.e. Animal, Plant and Vehicle) found in Probase. Therefore, Yu's method achieves better performance in general domain datasets than the specific ones. Our approach, in contrast, less de- pends on the training relations. Therefore, it can achieve high accuracy in both the general and spe- cific domain datasets. Experiment 4. Similar to experiment 2, this ex- periment aims to evaluate the generalization capa- bility of our term embeddings. In this experiment, for each of the Animal, Plant and Vehicle domains, we train the classifier using the positive and nega- tive examples in each domain and test the classifier in other domains. The experimental results in <ref type="table" target="#tab_10">Table  5</ref> show that our approach achieves the best perfor- mance compared to other state-of-the-art methods for all the datasets. As also shown in <ref type="table" target="#tab_6">Table 3</ref>, our ap- proach has achieved high accuracy for both general and specific domain datasets, while in Yu's method, there is a huge difference in accuracy between these domain datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Training  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Empirical comparison with WordNet</head><p>By error analysis, we found that our results may complement WordNet. For example, in the Animal domain, our approach identifies 'wild sheep' as a hyponym of 'sheep', but in WordNet, they are sib- lings. However, many references 1, 2 consider 'wild sheep' as a species of 'sheep'. Another such ex- ample is shown in the Plant domain, where our ap-proach recognizes 'lily' as a hyponym of 'flowering plant', but WordNet places them in different sub- trees incorrectly <ref type="bibr">3</ref> . Therefore, our results may help restructure and even extend WordNet. Note that these taxonomic relations are not in our training set. They are also not recognized by the term embeddings obtained from the Word2Vec method and Yu et al.'s method. It again shows that our term embedding learning approach has the capa- bility to identify taxonomic relations which are not even defined in dictionary or training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Tuning vector dimensions</head><p>We also conduct experiments to learn term embed- dings from the general domain datasets with differ- ent dimensions (i.e. 50, 100, 150 and 300) using our proposed approach. We then use these embeddings to evaluate the performance of taxonomic relation identification based on training time and accuracy, and show the results in  In general, when increasing the vector dimension, the accuracy of our term embedding learning ap- proach will be increased gradually. More specifi- cally, the accuracy improves slightly when the di- mension is increased from 50 to 150. But after that, increasing the dimension has very little effect on the accuracy. We observe that the vector dimension for learning term embeddings can be set between 100 to 150 to achieve the best performance, based on the trade-off between accuracy and training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel approach to learn term embeddings using dynamic weighting neural network. This model encodes not only the hyper- nym and hyponym terms, but also the contextual in- formation between them. Therefore, the extracted term embeddings have good generalization capabil- ity to identify unseen taxonomic relations which are not even defined in dictionary and training data. The experimental results show that our approach signifi- cantly outperforms other state-of-the-art methods in terms of accuracy in identifying taxonomic relation identification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Proposed approach for learning term embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Dataset 
# terms # positive # negative 
BLESS 
5229 
1337 
13210 
ENTAILMENT 
2392 
1385 
1385 
Animal 
659 
4164 
8471 
Plant 
520 
2266 
4520 
Vehicle 
117 
283 
586 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Datasets used in the experiments.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance results for the BLESS and ENTAIL-

MENT datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance results for the general domain datasets 

when using one domain for training and another domain for 

testing. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance results for the Animal, Plant and Vehicle 

datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance results for the specific domain datasets 

when using one domain for training and another domain for 

testing. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>The experiments 
are carried out on a PC with Intel(R) Xeon(R) CPU 
at 3.7GHz and 16GB RAM. 

Dimension Dataset Training time Accuracy 
50 
BLESS 
1825s 
87.7% 
100 
BLESS 
2991s 
89.4% 
150 
BLESS 
4025s 
89.9% 
300 
BLESS 
7113s 
90.0% 
50 
ENTAIL 
1825s 
88.5% 
100 
ENTAIL 
2991s 
90.6% 
150 
ENTAIL 
4025s 
90.9% 
300 
ENTAIL 
7113s 
90.9% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance results based on training time and accu-

racy of the SVM+Our model using different vector dimensions. 

</table></figure>

			<note place="foot" n="1"> http://en.wikipedia.org/wiki/Ovis 2 http://www.bjornefabrikken.no/side/norwegian-sheep/</note>

			<note place="foot" n="3"> https://en.wikipedia.org/wiki/Lilium</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<title level="m">A Neural Probabilistic Language Model. Proceedings of the NIPS conference</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="932" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supportvector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On ontology-driven document clustering using core semantic features. Knowledge and information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samah</forename><surname>Fodeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Punch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><forename type="middle">N</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="395" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the ACL</title>
		<meeting>the 52nd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Open-domain textual question an411 swering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Maiorano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><forename type="middle">A</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="267" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference on Computational Linguistics</title>
		<meeting>the 14th Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Semi-supervised Method to Learn and Construct Taxonomies Using the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deriving boolean structures from distributional vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating hierarchical summaries for web searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dawn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGIR conference</title>
		<meeting>the 26th ACM SIGIR conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="457" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL conference</title>
		<meeting>the NAACL conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1390" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Hierarchical Entity-based Approach to Structuralize User Generated Content in Social Media: A Case of Yahoo! Answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1521" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the ACL</title>
		<meeting>the 52nd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An introduction to the syntax and content of cyc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Deoliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Spring Symposium</title>
		<meeting>the AAAI Spring Symposium</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WordNet: a Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Graph-based Algorithm for Inducing Lexical Taxonomies from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 20th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1872" to="1877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hierarchical model of web summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Petinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the ACL</title>
		<meeting>the 49th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="670" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Topic discovery based on text mining techniques. Information processing &amp; management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurora</forename><surname>Pons-Porrata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Berlanga-Llavori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Ruiz-Shulcloper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="752" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING conference</title>
		<meeting>the COLING conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the ACL</title>
		<meeting>the 51st Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="932" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Usaar-wlv: Hypernym generation with deep neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liling</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SemEval</title>
		<meeting>the SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="932" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Taxonomy Construction using Syntactic Contextual Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">J</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="810" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Incorporating Trustiness and Collective Synonym/Contrastive Evidence into Taxonomy Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">J</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1013" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ontolearn reloaded: A graph-based algorithm for taxonomy induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="707" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A phrase mining framework for recursive construction of a topical hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nihit</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thrivikrama</forename><surname>Taula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD conference</title>
		<meeting>the 19th ACM SIGKDD conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="437" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING conference</title>
		<meeting>the COLING conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Probase: A probabilistic taxonomy for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Wentao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hongsong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Haixun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD conference</title>
		<meeting>the ACM SIGMOD conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain-assisted product aspect hierarchy generation: towards hierarchical organization of unstructured consumer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning term embeddings for hypernymy identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1390" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Topic hierarchy construction for the organization of multi-source user generated contents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th ACM SIGIR conference</title>
		<meeting>the 36th ACM SIGIR conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
