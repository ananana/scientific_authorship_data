<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Neural Solver for Math Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Neural Solver for Math Word Problems</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="845" to="854"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a deep neural solver to automatically solve math word problems. In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model, without sophisticated feature engineering. We further design a hybrid model that combines the RNN model and a similarity-based retrieval model to achieve additional performance improvement. Experiments conducted on a large dataset show that the RNN model and the hybrid model significantly outperform state-of-the-art statistical learning methods for math word problem solving.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Developing computer models to automatically solve math word problems has been an interest of NLP researchers <ref type="bibr">since 1963</ref><ref type="bibr">since Feigenbaum et al. (1963</ref>; <ref type="bibr" target="#b3">Bobrow (1964)</ref>; <ref type="bibr" target="#b4">Briars and Larkin (1984)</ref>; <ref type="bibr" target="#b11">Fletcher (1985)</ref>. Recently, machine learning tech- niques ; <ref type="bibr" target="#b0">Amnueypornsakul and Bhat (2014)</ref>; <ref type="bibr" target="#b30">Zhou et al. (2015)</ref>; <ref type="bibr" target="#b20">Mitra and Baral (2016)</ref> and semantic parsing methods <ref type="bibr" target="#b24">Shi et al. (2015)</ref>; <ref type="bibr" target="#b16">Koncel-Kedziorski et al. (2015)</ref> are proposed to tackle this problem and promising re- sults are reported on some datasets. Although progress has been made in this task, performance of state-of-the-art techniques is still quite low on large datasets having diverse problem types <ref type="bibr" target="#b14">Huang et al. (2016)</ref>.</p><p>A typical math word problems are shown in Ta- ble 1. The reader is asked to infer how many pens Dan and Jessica have, based on constraints pro- vided. Given the success of deep neural network- s (DNN) on many NLP tasks (like POS tagging, Problem: Dan have 2 pens, Jessica have 4 pens. How many pens do they have in total ? Equation: x = 4+2 Solution: 6 <ref type="table">Table 1</ref>: A math word problem syntactic parsing, and machine translation), it may be interesting to study whether DNN could also help math word problem solving. In this paper, we propose a recurrent neural network (RNN) model for automatic math word problem solving. It is a sequence to sequence (seq2seq) model that trans- forms natural language sentences in math word problems to mathematical equations. Experiments conducted on a large dataset show that the RNN model significantly outperforms state-of-the-art s- tatistical learning approaches.</p><p>Since it has been demonstrated <ref type="bibr" target="#b14">Huang et al. (2016)</ref> that a simple similarity based method per- forms as well as more sophisticated statistical learning approaches on large datasets, we imple- ment a similarity-based retrieval model and com- pare with our seq2seq model. We observe that al- though seq2seq performs better on average, the re- trieval model is able to correctly solve many prob- lems for which RNN generates wrong results. We also find that the accuracy of the retrieval model positively correlate with the maximal similarity s- core between the target problem and the problems in training data: the larger the similarity score, the higher the average accuracy is.</p><p>Inspired by these observations, we design a hy- brid model which combines the seq2seq model and the retrieval model. In the hybrid model, the retrieval model is chosen if the maximal similar- ity score returned by the retrieval model is larger than a threshold, otherwise the seq2seq model is selected to solve the problem. Experiments on our dataset show that, by introducing the hybrid mod- el, the accuracy increases from 58.1% to 64.7%.</p><p>Our contributions are as follows: 1) To the best of our knowledge, this is the first work of using DNN technology for automatic math word problem solving.</p><p>2) We propose a hybrid model where a se- q2seq model and a similarity-based retrieval mod- el are combined to achieve further performance improvement.</p><p>3) A large dataset is constructed for facilitating the study of automatic math problem solving. <ref type="bibr">1</ref> The remaining part of this paper is organized as follows: After analyzing related work in Sec- tion 2, we formalize the problem and introduce our dataset in Section 3. We present our RNN-based seq2seq model in Section 4, and the hybrid model in Section 5. Then experimental results are shown and analyzed in Section 6. Finally we conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Math Word Problems Solving</head><p>Previous work on automatic math word problem solving falls into two categories: symbolic ap- proaches and statistical learning approaches. <ref type="bibr" target="#b3">In 1964</ref><ref type="bibr">, STUDENT Bobrow (1964</ref> handles al- gebraic problems by two steps: first, they trans- form natural language sentences into kernel sen- tences using a small set of transformation pat- terns. Then the kernel sentences are transformed to mathematical expressions by pattern match- ing. A similar approach is also used to solve En- glish rate problems Charniak <ref type="bibr">(1968,</ref><ref type="bibr">1969)</ref>. Ligu- da and Pfeiffer <ref type="bibr" target="#b18">Liguda and Pfeiffer (2012)</ref> pro- pose modeling math word problems with aug- mented semantic networks. In addition, Addi- tion/subtraction problems are studied most <ref type="bibr" target="#b4">Briars and Larkin (1984)</ref>; <ref type="bibr" target="#b9">Dellarosa (1986)</ref>; <ref type="bibr" target="#b2">Bakman (2007)</ref>; <ref type="bibr" target="#b29">Yuhui et al. (2010)</ref>; <ref type="bibr" target="#b22">Roy et al. (2015)</ref>.</p><p>In 2015, Shi et.al <ref type="bibr" target="#b24">Shi et al. (2015)</ref> propose a system SigmaDolphin which automatically solves math word problems by semantic parsing and rea- soning. In the same year, Koncel et.al <ref type="bibr">KoncelKedziorski et al. (2015)</ref> also formalizes the prob- lem of solving multi-sentence algebraic word problems as that of generating and scoring equa- tion trees.</p><p>Since 2014, statistical learning based approach- es are proposed to solve the math word problems. Hosseini et al. <ref type="bibr" target="#b13">Hosseini et al. (2014)</ref> deal with the open-domain aspect of algebraic word problems by learning verb categorization from training data.  proposed a equation template system to solve a wide range of algebra word problems. Zhou et al. <ref type="bibr" target="#b30">Zhou et al. (2015)</ref> further extends this method by adopting the max-margin objective, which results in higher ac- curacy and lower time cost. In addition, Roy and Roth <ref type="bibr" target="#b22">Roy et al. (2015)</ref>; <ref type="bibr" target="#b21">Roy and Roth (2016)</ref> tries to handle arithmetic problems with multiple step- s and operations without depending on additional annotations or predefined templates. Mitra et al. <ref type="bibr" target="#b20">Mitra and Baral (2016)</ref> presents a novel method to learn to use formulas to solve simple addition- subtraction arithmetic problems.</p><p>As reported in 2016 <ref type="bibr" target="#b14">Huang et al. (2016)</ref>, state- of-the-art approaches have extremely low per- formance on a big and highly diverse data set <ref type="bibr">(18,000+ problems)</ref>. In contrast to these ap- proaches, we study the feasibility of applying deep learning to the task of math word problem solving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequence to Sequence (seq2seq) Learning</head><p>With the framework of seq2seq learning ; <ref type="bibr" target="#b28">Wiseman and Rush (2016)</ref>, re- cent advances in neural machine translation (N- MT) ;  and neural responding machine (NRM) <ref type="bibr" target="#b23">Shang et al. (2015)</ref> have demonstrated the power of recurren- t neural networks (RNNs) at capturing and trans- lating natural language semantics. The NMT and NRM models are purely data-driven and directly learn to converse from end-to-end conversational corpora.</p><p>Recently, the task of translating natural lan- guage queries into regular expressions is explored by using a seq2seq model <ref type="bibr" target="#b19">Locascio et al. (2016)</ref>, which achieves a performance gain of 19.6% over previous state-of-the-art models. To our knowl- edge, we are the first to apply seq2seq model to the task of math word problem solving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation and Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>A math word problem P is a word sequence W p and contains a set of variables V p = {v 1 , . . . , v m , x 1 , . . . , x k } where v 1 , . . . , v m are known numbers in P and x 1 , . . . , x k are variables Problem: Dan have 5 pens and 3 pencils, Jessica have 4 more pens and 2 less pencils than him. How many pens and pencils do Jessica have in total? Equation: x = 5 + 4 +3 -2 Solution: 10 <ref type="table">Table 2</ref>: A math word problem whose values are unknown. A problem P can be solved by a mathematical equation E p formed by V p and mathematical operators.</p><p>In math word problems, different equations may belong to a same equation template. For exam- ple, equation x = (9 * 3) + 7 and equation x = (4 * 5) + 2 share the same equation template x = (n 1 * n 2 ) + n 3 . To decrease the diversity of equations, we map each equation to an equation template T p through a number mapping M p . The number mapping process can be defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 Number mapping: For a problem</head><p>P with m known numbers, a number mapping M p maps the numbers in problem P to a list of number tokens {n 1 , . . . , n m } by their order in the problem text.</p><p>Definition 2 Equation template: A general for- m of equations. For a problem P with equation E p and number mapping M p , its equation template is obtained by mapping numbers in E p to a list of number tokens {n 1 , . . . , n m } according to M p .</p><p>Take the problem in <ref type="table">Table 2</ref> as an example, first we can obtain a number mapping from the prob- lem:</p><p>M : {n 1 = 5; n 2 = 3; n 3 = 4; n 4 = 2; } and then the given equation can be expressed as an equation template:</p><formula xml:id="formula_0">x = n 1 + n 3 + n 2 − n 4</formula><p>After number mapping, the problem in <ref type="table">Table 2</ref> can be mapped to:</p><p>"Dan have n 1 pens and n 2 pencils, Jessica have n 3 more pens and n 4 less pencils than him. How many pens and pencils do Jessica have in total?"</p><p>We solve math word problems by generating e- quation templates through a seq2seq model. The input of the seq2seq model is the sequence W P af- ter number mapping, and the output is an equation template T P . The equation E P can be obtained by applying the corresponding number mapping M P to T P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing a Large Dataset</head><p>Most public datasets for automatic math word problem solving are quite small and contains lim- ited types of problems. The most frequently used Alg514 ( ) dataset contains only 514 linear algebra problems with 28 equa- tion templates. There are 1,000 problems in the newly constructed DRAW-1K <ref type="bibr">(Shyam and MingWei, 2017</ref>) dataset. Dophin1878 ( <ref type="bibr" target="#b24">Shi et al., 2015</ref>) includes 1,878 number word problems. An ex- ception is the Dolphin18K dataset ( <ref type="bibr" target="#b14">Huang et al., 2016</ref>) which contains 18,000+ problems. Howev- er, this dataset has not been made publicly avail- able so far.</p><p>Since DNN-based approaches typically need large training data, we have to build a large dataset of labeled math word problems. We crawl over 60,000 Chinese math word problems from a cou- ple of online education web sites. All of them are real math word problems for elementary school s- tudents. We focus on one-unknown-variable lin- ear math word problems in this paper. For oth- er problem types, we would like to leave as fu- ture work. Please pay attention that the solutions to the problems are in natural language, and we have to extract equation systems and structured answers from the solution text. We implemen- t a rule-based extraction method for this purpose, which achieves very high precision and medium recall. That is, most equations and structured an- swers extracted by our method are correct, and many problems are dropped from the dataset. As a result, we get dataset Math23k which contains 23,161 problems labeled with structured equation- s and answers. Please refer to <ref type="table">Table 3</ref> for some s- tatistics of the dataset and a comparison with other public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep Neural Solver</head><p>In this section, we propose a RNN-based seq2seq model to translate problem text to math equations.</p><p>Since not all numbers in problem text may be use- ful for solving the problem, we propose, in Section 4.2, a significant number identification model to distinguish whether a number in a problem should appear in the corresponding equations. <ref type="figure" target="#fig_0">Figure 1</ref> shows our RNN-based seq2seq model for transforming problem text to a math equation, us- ing the problem in <ref type="table">Table 2</ref> as an example. The in-dataset # problems # templates # sentences # words problem <ref type="table" target="#tab_0">types  Alg514  514  28  1.62k  19.3k  algebra, linear  Dolphin1878  1,878  1,183  3.30k  41.4k  number word problems  DRAW-1K</ref> 1,000 Unknown 6.23k 81.5k algebra, linear, one-VAR <ref type="table">Math23K  23,161  2,187  70.1k  822k</ref> algebra, linear, one-VAR <ref type="table">Table 3</ref>: Statistics of our dataset and several publicly available datasets "Dan have n 1 pens and n 2 pencils, Jessica have n 3 more pens and n 4 less pencils than him. How many pens and pencils do Jessica have in total?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RNN based Seq2seq Model</head><p>The output sequence R = {r 1 , . . . , r s } is the equation template:</p><formula xml:id="formula_1">x = n 1 + n 3 + n 2 − n 4</formula><p>The gated recurrent units (GRU) ( <ref type="bibr" target="#b8">Chung et al., 2014</ref>) and long short-memory (LSTM) (Hochreit- er and Schmidhuber, 1997) cells are used for en- coding and decoding, respectively. The reason why we use GRU as the encoder instead of LSTM is that the GRU has less parameters and less likely to be overfitted on small dataset. Four fundamen- tal operational stages of GRU are as follows:</p><formula xml:id="formula_2">z t =σ(W (z) x t + U z h t−1 ) (Update gate) r t =σ(W (r) x t + U r h t−1 ) (Reset gate) ˆ h t =tanh(r t U h t−1 + W x t ) (New memory) h t =(1 − z t ) ˆ h t + z t h t−1 (Hidden state)<label>(1)</label></formula><p>where σ represents the sigmoid function and is an element-wise multiplication. The input x t is a word w t along with previously generated character r t−1 . The variables U and W are weight matrices for each gate.</p><p>The fundamental operational stages of LSTM are as follows:</p><formula xml:id="formula_3">i t = σ(W (i) x t + U i h t−1 ) (Input gate) f t = σ(W (f ) x t + U f h t−1 ) (Forget gate) o t = σ(W (o) x t + U o h t−1 ) (Output gate) ˜ c t = tanh(W (c) x t + U (c) h t−1 ) (New memory) c t = f t ˜ c t−1 + i t ˜ c t (Final memory) h t = o t tanh(c t ) (Hidden state)<label>(2)</label></formula><p>where the input x t is a word w t along with previ- ously generated character r t−1 . Then, we redesigned the activation function of the seq2seq model, which is different from vanil- la seq2seq models. If we directly generate equa- tion templates by a softmax function, some incor- rect equations may be generated, such as: "x = n 1 + + * n 2 " and "x = (n 1 * n 2 ". To ensure that the output equations are mathematically cor- rect, we need to find out which characters are ille- gal according to previously generated characters. This is done by five predefined rules like:</p><p>• Rule 1: If r t−1 in {+, −, * , /}, then r t will not in {+, −, * , /, ), =};</p><p>• Rule 2: If r t−1 is a number, then r t will not be a number and not in {(, =};</p><p>• Rule 3: If r t−1 is "=", then r t will not in {+, −, * , /, =, )};</p><p>• Rule 4: If r t−1 is "(", then r t will not in {(, ), +, −, * , /, =};</p><p>• Rule 5: If r t−1 is ")", then r t will not be a number and not in {(, )};</p><p>A binary vector ρ t can be generated depends on r t−1 and these rules. Each position in ρ t is corre- sponding to a character in the output vocabulary, where "1" represents that the character is mathe- matically correct, and "0" indicates mathematical- ly incorrect. Thus, the output probability distribu- tion at each time-step t can be calculated as:</p><formula xml:id="formula_4">P ( ˆ r t |h t ) = ρ t e h T t W s ρ t e h T t W s<label>(3)</label></formula><p>where h t is the output of LSTM decoder, and W s is the weight matrix. The probability of mathemat- ically incorrect characters will be 0. Our model is five layers deep, with a word em- bedding layer, a two-layer GRU as encoder and a two-layer LSTM as decoder. Both the encoder and decoder contain 512 nodes. We perform standard dropout during training ( <ref type="bibr" target="#b26">Srivastava et al., 2014</ref>) af- ter GRU and LSTM layer with dropout probability equal to 0.5. We train for 80 epochs, utilizing a mini-batch size of 256 and a learning-rate of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Significant Number Identification (SNI)</head><p>In a math word problem, not all numbers appear in the equation for solving the problem. An ex- ample is shown in <ref type="table" target="#tab_0">Table 4</ref>, where the number "1" in "1 day, 1 girl" and number "2" in "She has 2 types of" should not be used in equation construc- tion. We say a number is significant if the number should be included in the equation to the problem; otherwise it is insignificant. For the problem in Ta- ble 4, significant numbers are 9, 3, and 5, while 1 and 2 are insignificant numbers. Identifying sig- nificant and insignificant numbers are important for constructing correct equations. For this pur- pose, we build a LSTM-based binary classification model to determine whether a number in a piece of problem text is significant.</p><p>The training data for SNI model are extract- ed from the math word problems. Each number and its context in problems is a training instance of SNI. An instance will be labelled"True" if the number is significant, otherwise it will be labelled "False". The structure of SNI model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. By using single layer LSTMs with 128 nodes and a symmetric window of length 3, our model achieves 99.1% accuracy. <ref type="table" target="#tab_0">Table 4</ref> is an ex- ample of number mapping with and without SNI.</p><p>Problem: 1 day, 1 girl was organizing her book case making sure each of the shelves had exactly 9 books on it. She has 2 types of books -mystery books and picture books. If she had 3 shelves of mystery books and 5 shelves of picture books, how many books did she have in total? Number mapping: n 1 = 1; n 2 = 1; n 3 = 9; n 4 = 2; n 5 = 3; n 6 = 5 Equation template: x = n 5 * n 3 + n 6 * n 3 Number mapping with SNI: n 1 = 9; n 2 = 3; n 3 = 5 Equation template with SNI: x = n 2 * n 1 + n 3 * n 1 Problem after number mapping and SNI: 1 day, 1 girl was organizing her book case making sure each of the shelves had exactly n 1 books on it. She has 2 types of books -mystery books and picture books. If she had n 2 shelves of mystery books and n 3 shelves of picture books, how many books did she have in total?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hybrid Model</head><p>To compare the performance of our deep neural solver and traditional statistical learning methods, we implement a similarity-based retrieval model (refer to Section 5.1 for more details).</p><p>The Venn diagram in <ref type="figure" target="#fig_2">Figure 3</ref> shows the rela- tionship between the problems solved by the re- White area: problems that both models fail to solve trieval model and those solved by the seq2seq model. We can see that although seq2seq perform- s better on average, the retrieval model is able to correctly solve many problems that seq2seq can- not solve. If we can combine the two models prop- erly to build a hybrid model, more problems may get solved.</p><p>In this section, we first give some details about the retrieval model in Section 5.1, then the hybrid model is introduced in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Retrieval Model</head><p>The retrieval model solves problems by calculat- ing the lexical similarity between the testing prob- lem and each problem in the training data, and then the equation template of the most similar problem is applied to the testing problem. Each problem is modeled as a vector of word TF-IDF scores W = [w 1,d , w 2,d , . . . , w N,d ] T , where</p><formula xml:id="formula_5">w t,d = tf t,d * |D| |d ∈ D|t ∈ d|<label>(4)</label></formula><p>and tf t,d is the word frequency of word t in prob- lem d; |D| is the total number of problems in dataset; |d ∈ D|t ∈ d| is the number of documents containing the word t.</p><p>The similarity between the testing problem P T and another problem Q can be calculated by the Jaccard similarity between their corresponding vectors:</p><formula xml:id="formula_6">J(P T , Q) = |P T ∩ Q| |P T ∪ Q| = |P T ∩ Q| |P T | + |Q| − |P T ∩ Q|<label>(5)</label></formula><p>The retrieval model will choose training prob- lem Q 1 that have the maximal similarity with P T and use the equation template T of Q 1 as the tem- plate of problem P T .</p><p>An important and interesting observation about the retrieval model is the relation between the maximal similarity and solution accuracy. <ref type="figure" target="#fig_3">Figure  4</ref> shows the results of only considering the prob- lems for those the maximal similarity returned by retrieval model is above a threshold θ (in oth- er words, we skip a problem if its corresponding maximal similarity is below the threshold). It is clear that the larger the similarity score, the higher the average accuracy is. In our hybrid model, we make use of this property to combine the seq2seq model and the retrieval model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hybrid Model</head><p>Our hybrid model combines the retrieval mod- el and the seq2seq model by setting a hyper- parameter θ as the threshold of similarity. In algo- rithm 1, if the Jaccard similarity between testing problem P T and the retrieved problem Q 1 is high- er than θ, the model will choose the equation tem- plate T of Q 1 as the equation template of problem P T . Otherwise an equation template will be gen- erated by a seq2seq model. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the retrieval model has a higher precision than the seq2seq model when we set a high threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we conduct experiments on two datasets to examine the performance of the pro- posed models. Our main experimental result is to show a significant improvement over the baseline Algorithm 1 Hybrid model Input: Q: problems in training data; P T : testing problem; θ: pre-defined threshold of similarity Output: Problem solution 1: Get equation templates and number mappings for training problems Q and testing problem P T . 2: Number identification: identify significan- t numbers 3: Retrieval:</p><p>choose problem Q 1 from Q that has the max- imal Jaccard similarity with P T 4: if J(P T , Q 1 ) &gt; θ then 5:</p><p>Apply the retrieval model: select equation template T of Q 1 6: else</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Apply the seq2seq model: T = seq2seq(P T ) 8: end if 9: Applying number mappings of P T to T and calculating final solution method on the proposed Math23K dataset. We fur- ther show that the baseline method cannot solve problems with new equation templates. In con- trast, the proposed seq2seq model is quite robust on problems with new equation templates (refer to <ref type="table">Table 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Datasets: As introduced in Section 3.2, we col- lected a dataset called Math23K which contain- s 23161 math word problems labeled with equa- tion templates and answers. All these problems are linear algebra questions with only one variable. There are 2187 equation templates in the dataset. In addition, we also evaluate our method on a pub- lic dataset Alg514 ( ).</p><p>Baseline: We compare our proposed methods with two baselines. The first baseline is the re- trieval model introduced in Section 5.1. The sec- ond one is ZDC ( <ref type="bibr" target="#b30">Zhou et al., 2015)</ref>, which is an improved version of KAZB (    <ref type="table">Table 6</ref>: Result of significance test. The meaning of abbreviations in this table is as follows: R: re- trieval model w/o SNI; R(S): retrieval model w/ S- NI; Seq: seq2seq model w/o SNI; Seq(S): seq2seq model w/ SNI; H: hybrid model w/o SNI; H(S): hybrid model w/ SNI to Stanford coreNLP output formats. <ref type="bibr">2</ref> </p><formula xml:id="formula_7">ZDC R R(S) Seq Seq(S) H R(S) &gt; Seq Seq(S) H H(S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Results</head><p>Each approach is evaluated on each dataset via 5- fold cross-validation: In each run, 4 folds are used for training and 1 fold is used for testing. Evalu- ation results are summarized in <ref type="table" target="#tab_2">Table 5</ref>. First, to test the effectiveness of significant number identi- fication (SNI), model performance before and af- ter the application of SNI are compared. Then, the performance of the hybrid model, seq2seq model, and retrieval model are examined on two datasets respectively.</p><p>To check whether the performance improve- ments are significant enough, we conduct statisti- cal significance study upon pairs of methods. Ta- ble 6 shows the results of sign test, where the symbol &gt; indicates that the method in the row significantly (with p value &lt; 0.05) improves the performance of the method in the column, and the symbol indicates that the performance im- provement is extremely significant (with p value &lt; 0.01).</p><p>Several observations can be made from the re-sults. First, the seq2seq model significantly out- performs state-of-the-art statistical learning meth- ods (ZDC and the retrieval model). Second, by combining the retrieval model and the seq2seq model using a simple mechanism, our hybrid mod- el achieves significant performance gain with re- spect to the seq2seq model. Third, the SNI mod- ule can effectively improve model accuracy. The accuracy of the hybrid model and seq2seq mod- el gains approximately 4% increase after number identification. Please pay attention that on the s- mall dataset of Alg514, the seq2seq model be- haves much worse than others. This is not surpris- ing, because deep neural networks typically need large training data. <ref type="figure" target="#fig_4">Figure 5</ref> shows the performance of differen- t models on various scales of training data. As expected, the seq2seq model performs very well on big datasets, but poorly on small datasets. Ability to Generate New Equation Templates: please note that many problems in Math23K can be solved using the same equation template. For example, a problem which corresponds to the e- quation x = (9 * 3) + 7 and a different problem that maps to x = (4 * 5) + 2 share the same equa- tion template.</p><p>One nice property of the seq2seq model is its a- bility of generating new equation templates. Most previous statistical learning methods (with a few exceptions) for math word problem solving are on- ly able to select an equation template from those in the training data. In other words, they cannot generate new templates. To test the performance of the seq2seq model in generating new templates,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Math23K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZDC 15.1% Retrieval model w/o SNI 26.1% Retrieval model w/ NI 29.2% Seq2seq model w/o SNI 40.3% Seq2seq model w/ SNI</head><p>47.5% Hybrid model w/o SNI 40.3% Hybrid model w/ SNI 47.7% <ref type="table">Table 7</ref>: Experimental results of non-overlapping templates between training data and test data we make a new split of our dataset between train- ing data and test data, to ensure that the training data and the test data do not share overlapped tem- plates. As a result, we get a training set with 19, 024 problems and 1, 802 equation templates, and a testing set with 4, 137 problems and 315 equation templates.</p><p>Experimental results on the new training set and test set are shown is shown in <ref type="table">Table 7</ref>. By com- paring <ref type="table" target="#tab_2">Table 5 and Table 7</ref>, it is clear that the gap between the seq2seq model and the baselines be- comes larger in the new settings. It is because the seq2seq model can effectively generate new equa- tion templates for new problems, instead of select- ing equation templates from the training set.</p><p>Although ZDC and the retrieval model cannot generate new templates, their accuracy is not zero in the new settings. That is because one problem can be solved by multiple equation templates: Al- though one problem is labeled with template T 1 in the test set, it may also be solved by another tem- plate T 2 in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Compare to most previous statistical learning methods for math problem solving, our proposed seq2seq model and hybrid model have the follow- ing advantages: 1) They have higher accuracy on large training data. On the Math23K dataset, the hybrid model achieves at least 22% higher accura- cy than the baselines. 2) They have the ability of generating new templates (i.e., templates that are not in the training data. 3) They do not rely on sophisticated feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed an RNN-based seq2seq model to automatically solve math word problems. This model directly transforms problem text to a math equation template. This is the first work of ap- plying deep learning technologies to math word problem solving. In addition, we have designed a hybrid model which combines the seq2seq mod- el and a retrieval model to further improve perfor- mance. A large dataset has been constructed for model training and empirical evaluation. Exper- imental results show that both the seq2seq mod- el and the hybrid model significantly outperfor- m state-of-the-art statistical learning methods in math word problem solving.</p><p>The output of our seq2seq model is a single e- quation containing one unknown variable. There- fore our approach is only applicable to the prob- lems whose solution involves one linear equation of one unknown variable. As future work, we plan to extend our model to be able to generate equation systems and nonlinear equations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The seq2seq model</figDesc><graphic url="image-1.png" coords="4,73.14,171.64,216.00,229.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The significant number identification model</figDesc><graphic url="image-2.png" coords="5,314.25,488.66,201.60,118.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Green area: problems correctly solved by the retrieval model; Blue area: problems correctly solved by the seq2seq model; Overlapped area: problems correctly solved by both models; White area: problems that both models fail to solve</figDesc><graphic url="image-3.png" coords="6,89.77,62.81,180.00,108.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision and recall of the retrieval model, and the precision of the seq2seq model w.r.t. different similarity threshold (θ) values</figDesc><graphic url="image-4.png" coords="6,314.25,288.60,201.60,150.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of different models versus the size of training set</figDesc><graphic url="image-5.png" coords="8,73.14,334.11,216.00,160.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 4 : Significant number identification (SNI) example</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Model comparison (average accuracy of 
5-fold cross validation) 

</table></figure>

			<note place="foot" n="1"> We plan to make the dataset publicly available when the paper is published</note>

			<note place="foot" n="2"> We also try to run KAZB on our dataset, but fail on our workstation (2 12-core E5-2650 CPU, 128G RAM, 4 K80 GPUs) due to large memory consumption.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine-guided solution to mathematical word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bussaba</forename><surname>Amnueypornsakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arX- iv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Robust understanding of word problems with extraneous information. arXiv preprint math/0701393</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefim</forename><surname>Bakman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An integrated model of skill in solving elementary word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Briars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and instruction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="296" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CALCULUS WORD PROBLEMS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computer solution of calculus word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international joint conference on Artificial intelligence</title>
		<meeting>the 1st international joint conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A computer simulation of childrens arithmetic word-problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denise</forename><surname>Dellarosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="154" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Computers and thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Edward A Feigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding and solving arithmetic word problems: A computer simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles R Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How well do computers solve math word probl ems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">North American Chapter of the ACL (NAACL HLT)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena Dumas</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to automatically solve algebra word problems. Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling math word problems with augmented semantic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Liguda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thies</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Application of Natural Language to Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="247" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural generation of regular expressions from natural language with minimal domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Locascio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Deleon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03000</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to use formulas to solve simple arithmetic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno>arX- iv:1608.01413</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<title level="m">Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02364</idno>
		<title level="m">Neural responding machine for short-text conversation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatically solving number word problems by semantic parsing and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1132" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Annotating derivations: A new evaluation strategy and dataset for algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upadhyay</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang Ming-Wei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="494" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02960</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frame-based calculus of solving arithmetic multi-step addition and subtraction word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Guangzuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Ronghuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Education Technology and Computer Science (ETCS), 2010 Second International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="476" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learn to solve algebra word problems using quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaixiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="817" to="822" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
