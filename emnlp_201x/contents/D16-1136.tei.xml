<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Crosslingual Word Embeddings without Bilingual Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research -Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research -Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Victoria Research Laboratory</orgName>
								<orgName type="institution">National ICT Australia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">International Computer Science Institute</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Crosslingual Word Embeddings without Bilingual Corpora</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1285" to="1295"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the mono-lingual word similarity and cross-lingual document classification task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monolingual word embeddings have had widespread success in many NLP tasks includ- ing sentiment analysis , dependency parsing ( <ref type="bibr" target="#b6">Dyer et al., 2015)</ref>, machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages <ref type="bibr" target="#b30">(Yarowsky and Ngai, 2001;</ref><ref type="bibr" target="#b4">Das and Petrov, 2011;</ref><ref type="bibr" target="#b27">Täckström et al., 2012;</ref><ref type="bibr" target="#b5">Duong et al., 2015)</ref>. A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space ( <ref type="bibr" target="#b13">Klementiev et al., 2012</ref>).</p><p>Most previous work has focused on down-stream crosslingual applications such as document classi- fication and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through mono- lingual word similarity and bilingual lexicon induc- tion tasks. Moreover, many prior work ( <ref type="bibr" target="#b2">Chandar A P et al., 2014;</ref><ref type="bibr" target="#b14">Kočisk´Kočisk´y et al., 2014</ref>) used bilingual or comparable corpus which is also expensive for many low-resource languages. <ref type="bibr">Søgaard et al. (2015)</ref> im- pose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods. To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora ( <ref type="bibr" target="#b18">Luong et al., 2015)</ref>. How- ever, many previous approaches are not capable of scaling up either because of the complicated objec- tive functions or the nature of the algorithm. Other methods use a dictionary as the bridge between lan- guages ( <ref type="bibr" target="#b19">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b29">Xiao and Guo, 2014</ref>), however they do not adequately handle translation ambiguity.</p><p>Our model uses a bilingual dictionary from Pan- lex ( <ref type="bibr" target="#b12">Kamholz et al., 2014</ref>) as the source of bilin- gual signal. Panlex covers more than a thousand lan- guages and therefore our approach applies to many languages, including low-resource languages. Our method selects the translation based on the context in an Expectation-Maximization style training algo- rithm which explicitly handles polysemy through in- corporating multiple dictionary translations (word sense and translation are closely linked <ref type="bibr" target="#b22">(Resnik and Yarowsky, 1999)</ref>). In addition to the dictionary, our method only requires monolingual data. Our approach is an extension of the continuous bag-of- words (CBOW) model <ref type="bibr" target="#b20">(Mikolov et al., 2013b</ref>) to inject multilingual training signal based on dictio- nary translations. We experiment with several vari- ations of our model, whereby we predict only the translation or both word and its translation and con- sider different ways of using the different learned center-word versus context embeddings in applica- tion tasks. We also propose a regularisation method to combine the two embedding matrices during training. Together, these modifications substantially improve the performance across several tasks. Our final model achieves state-of-the-art performance on bilingual lexicon induction task, large improvement over word similarity task compared with previous published crosslingual word embeddings, and com- petitive result on cross-lingual document classifica- tion task. Notably, our embedding combining tech- niques are general, yielding improvements also for monolingual word embedding.</p><p>This paper makes the following contributions:</p><p>• Proposing a new crosslingual training method for learning vector embeddings, based only on monolingual corpora and a bilingual dictio- nary;</p><p>• Evaluating several methods for combining em- beddings, which are shown to help in both crosslingual and monolingual evaluations; and</p><p>• Achieving consistent results which are compet- itive in monolingual, bilingual and crosslingual transfer settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource. This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target lan- guages, such that translations are assigned similar embedding vectors ( <ref type="bibr" target="#b18">Luong et al., 2015;</ref><ref type="bibr" target="#b13">Klementiev et al., 2012</ref>). These approaches are affected by errors from automatic word alignments, motivat- ing other approaches which operate at the sentence level <ref type="bibr" target="#b2">(Chandar A P et al., 2014;</ref><ref type="bibr" target="#b14">Hermann and Blunsom, 2014;</ref>) through learning compositional vector representations of sentences, in order that sentences and their translations rep- resentations closely match. The word embeddings learned this way capture translational equivalence, despite not using explicit word alignments. Nev- ertheless, these approaches demand large parallel corpora, which are not available for many language pairs.</p><p>Vuli´c <ref type="bibr" target="#b28">Vuli´c and Moens (2015)</ref> use bilingual compara- ble text, sourced from Wikipedia. Their approach creates a psuedo-document by forming a bag-of- words from the lemmatized nouns in each compa- rable document concatenated over both languages. These pseudo-documents are then used for learning vector representations using Word2Vec. Their sys- tem, despite its simplicity, performed surprisingly well on a bilingual lexicon induction task (we com- pare our method with theirs on this task.) Their ap- proach is compelling due to its lesser resource re- quirements, although comparable bilingual data is scarce for many languages. Related, <ref type="bibr">Søgaard et al. (2015)</ref> exploit the comparable part of Wikipedia. They represent word using Wikipedia entries which are shared for many languages.</p><p>A bilingual dictionary is an alternative source of bilingual information. <ref type="bibr" target="#b9">Gouws and Søgaard (2015)</ref> randomly replace the text in a monolingual cor- pus with a random translation, using this corpus for learning word embeddings. Their approach doesn't handle polysemy, as very few of the translations for each word will be valid in context. For this reason a high coverage or noisy dictionary with many trans- lations might lead to poor outcomes. <ref type="bibr" target="#b19">Mikolov et al. (2013a)</ref>, <ref type="bibr" target="#b29">Xiao and Guo (2014)</ref> and <ref type="bibr" target="#b7">Faruqui and Dyer (2014)</ref> filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, how- ever discarding much of the information in the dic- tionary. Our approach also uses a dictionary, how- ever we use all the translations and explicitly disam- biguate translations during training.</p><p>Another distinguishing feature on the above-cited research is the method for training embeddings. <ref type="bibr" target="#b19">Mikolov et al. (2013a)</ref> and <ref type="bibr" target="#b7">Faruqui and Dyer (2014)</ref> use a cascade style of training where the word em- beddings in both source and target language are trained separately and then combined later using the dictionary. Most of the other works train multlingual models jointly, which appears to have better perfor- mance over cascade training ( ).</p><p>For this reason we also use a form of joint training in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word2Vec</head><p>Our model is an extension of the contextual bag of words (CBOW) model of <ref type="bibr" target="#b20">Mikolov et al. (2013b)</ref>, a method for learning vector representations of words based on their distributional contexts. Specifically, their model describes the probability of a token w i at position i using logistic regression with a factored parameterisation,</p><formula xml:id="formula_0">p(w i |w i±k\i ) = exp(u w i h i ) w∈W exp(u w h i ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">h i = 1 2k k j=−k;j =0 v w i+j</formula><p>is a vector en- coding the context over a window of size k centred around position i, W is the vocabulary and the pa- rameters V and U ∈ R |W |×d are matrices referred to as the context and word embeddings. The model is trained to maximise the log-pseudo likelihood of a training corpus, however due to the high complex- ity of computing the denominator of equation <ref type="formula" target="#formula_0">(1)</ref>, <ref type="bibr" target="#b20">Mikolov et al. (2013b)</ref> propose negative sampling as an approximation, by instead learning to differenti- ate data from noise (negative examples). This gives rise to the following optimisation objective i∈D log σ(u</p><formula xml:id="formula_2">w i h i )+ p j=1 E w j ∼Pn(w) log σ(−u w j h i ) ,<label>(2)</label></formula><p>where D is the training data and p is the number of negative examples randomly drawn from a noise distribution P n (w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>Our approach extends CBOW to model bilingual text, using two monolingual corpora and a bilin- gual dictionary. We believe this data condition to be less stringent than requiring parallel or compa- rable texts as the source of the bilingual signal. It is common for field linguists to construct a bilin- gual dictionary when studying a new language, as one of the first steps in the language documentation process. Translation dictionaries are a rich informa- tion source, capturing much of the lexical ambigu- ity in a language through translation. For example, the word bank in English might mean the river bank Algorithm 1 EM algorithm for selecting translation during training, where θ = (U, V) are the model parameters and η is the learning rate.</p><p>1: randomly initialize V, U 2: for i &lt; Iter do 3:</p><formula xml:id="formula_3">for i ∈ D e ∪ D f do 4: s ← v w i + h i 5: ¯ w i = argmax w∈dict(w i ) cos(s, v w ) 6: θ ← θ + η ∂O( ¯ w i ,w i ,h i ) ∂θ {see (3) or (5)} 7:</formula><p>end for 8: end for or financial bank which corresponds to two differ- ent translations sponda and banca in Italian. If we are able to learn to select good translations, then this implicitly resolves much of the semantic ambiguity in the language, and accordingly we seek to use this idea to learn better semantic vector representations of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dictionary replacement</head><p>To learn bilingual relations, we use the context in one language to predict the translation of the centre word in another language. This is motivated by the fact that the context is an excellent means of disam- biguating the translation for a word. Our method is closely related to <ref type="bibr" target="#b9">Gouws and Søgaard (2015)</ref>, how- ever we only replace the middle word w i with a translation ¯ w i while keeping the context fixed. We replace each centre word with a translation on the fly during training, predicting instead p( ¯ w i |w i±k\i ) but using the same formulation as equation <ref type="formula" target="#formula_0">(1)</ref> albeit with an augmented U matrix to cover word types in both languages.</p><p>The translation ¯ w i is selected from the possible translations of w i listed in the dictionary. The prob- lem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that cross- lingual word embeddings will allow for accurate translation, however to learn these embeddings we need to know the translations. We propose an EM- inspired algorithm, as shown in Algorithm 1, which operates over both monolingual corpora, D e and D f . The vector s is the semantic representation combining both the centre word, w i , and the con-text, 1 which is used to choose the best translation into the other language from the bilingual dictionary dict(w i ). <ref type="bibr">2</ref> After selecting the translation, we use ¯ w i together with the context vector h to make a stochas- tic gradient update of the CBOW log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Joint Training</head><p>Words and their translations should appear in very similar contexts. One way to enforce this is to jointly learn to predict both the word and its translation from its monolingual context. This gives rise to the following joint objective function,</p><formula xml:id="formula_4">O = i∈De∪D f α log σ(u w i h i )+(1−α) log σ(u ¯ w i h i ) + p j=1 E w j ∼Pn(w) log σ(−u w j h i ) ,<label>(3)</label></formula><p>where α controls the contribution of the two terms.</p><p>For our experiments, we set α = 0.5. The nega- tive examples are drawn from combined vocabulary unigram distribution calculated from combined data</p><formula xml:id="formula_5">D e ∪ D f .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combining Embeddings</head><p>Many vector learning methods learn two embedding spaces V and U. Usually only V is used in appli- cation. The use of U, on the other hand, is under- studied ( <ref type="bibr" target="#b15">Levy and Goldberg, 2014</ref>) with the excep- tion of <ref type="bibr" target="#b21">Pennington et al. (2014)</ref> who use a linear combination U + V, with minor improvement over V alone. We argue that with our model, V is better at cap- turing the monolingual regularities and U is better at capturing bilingual signal. The intuition for this is as follows. Assuming that we are predicting the word finance and its Italian translation finanze from the context (money, loan, bank, debt, credit) as shown in <ref type="figure" target="#fig_0">figure 1</ref>. In V only the context word representa- tions are updated and in U only the representations of finance, finanze and negative samples such as tree and dog are updated. CBOW learns good embed- dings because each time it updates the parameters, the words in the contexts are pushed closer to each other in the V space. Similarly, the target word w i and the translation ¯ w i are also pushed closer in the U space. This is directly related to poitwise mutual information values of each pair of word and context explained in <ref type="bibr" target="#b15">Levy and Goldberg (2014)</ref>. Thus, U is bound to better at bilingual lexicon induction task and V is better at monolingual word similarity task.</p><p>The simple question is, how to combine both V and U to produce a better representation. We exper- iment with several ways to combine V and U. First, we can follow <ref type="bibr" target="#b21">Pennington et al. (2014)</ref> to interpolate V and U in the post-processing step. i.e.</p><formula xml:id="formula_6">γV + (1 − γ)U<label>(4)</label></formula><p>where γ controls the contribution of each embed- ding space. Second, we can also concatenate V and U instead of interpolation such that C = [V : U] where C ∈ R |W |×2d and W is the combined vocab- ulary from D e ∪ D f . Moreover, we can also fuse V and U during training. For each word in the combined dictionary V e ∪ V f , we encourage the model to learn similar representation in both V and U by adding a regular- ization term to the objective function in equation <ref type="formula" target="#formula_4">(3)</ref> during training.</p><formula xml:id="formula_7">O = O + δ w∈Ve∪V f u w − v w 2 2 (5)</formula><p>where δ controls to what degree we should bind two spaces together. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Our experimental evaluation seeks to determine how well lexical distances in the learned embedding spaces match with known lexical similarity judge- ments from bilingual and monolingual lexical re- sources. To this end, in §6 we test crosslingual distances using a bilingual lexicon induction task in which we evaluate the embeddings in terms of how well nearby pairs of words from two lan- guages in the embedding space match with human judgements. Next, to evaluate the monolingual em- beddings we evaluate word similarities in a single language against standard similarity datasets ( §7). Lastly, to demonstrate the usefulness of our em- beddings in a task-based setting, we evaluate on crosslingual document classification ( §9).</p><p>Monolingual Data The monolingual data is taken from the pre-processed Wikipedia dump from Al- <ref type="bibr" target="#b0">Rfou et al. (2013)</ref>. The data is already cleaned and tokenized. We additionally lower-case all words. Normally monolingual word embeddings are trained on billions of words. However, obtaining that much monolingual data for a low-resource language is in- feasible. Therefore, we only select the first 5 million sentences (around 100 million words) for each lan- guage.</p><p>Dictionary A bilingual dictionary is the only source of bilingual correspondence in our tech- nique. We prefer a dictionary that covers many languages, such that our approach can be applied widely to many low-resource languages. We use Panlex, a dictionary which currently covers around 1300 language varieties with about 12 million ex- pressions. The translations in PanLex come from various sources such as glossaries, dictionaries, au- tomatic inference from other languages, etc. Ac- cordingly, Panlex has high language coverage but often noisy translations. 4 <ref type="table">Table 1</ref> summarizes the sizes of monolingual corpora and dictionaries for each pair of language in our experiments. <ref type="bibr">4</ref> We also experimented with a crowd-sourced dictionary from Wiktionary. Our initial observation was that the transla- tion quality was better but with a lower-coverage. For example, for en-it dictionary, Panlex and Wiktionary have a coverage of 42.1% and 16.8% respectively for the top 100k most frequent English words from Wikipedia. The average number of trans- lations are 5.2 and 1.9 respectively. We observed similar trend using Panlex and Wiktionary dictionary in our model. How- ever, using Panlex results in much better performance. We can run the model on the combined dictionary from both Panlex and Wiktionary but we leave it for future work.</p><formula xml:id="formula_8">Source (M) Target (M) Dict (k)</formula><p>en-es 120.1 (73.9%) 126.8 (74.4%) 712.0 en-it 120.1 (74.7%) 114.6 (67.4%) 560.1 en-nl 120.1 (69.1%) 80.2 (63.4%) 406.6 en-de 120.1 (77.8%) 90.8 (68.3%) 964.4 en-sr 120.1 (28.0%) 7.5 (17.5%) 35.1 <ref type="table">Table 1</ref>: Number of tokens in millions for the source and target languages in each language pair. Also shown is the number of entries in the bilingual dic- tionary in thousands. The number in the parenthesis shows the token coverage in the dictionary on each monolingual corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Bilingual Lexicon Induction</head><p>Given a word in a source language, the bilingual lexicon induction (BLI) task is to predict its transla- tion in the target language. Vuli´c <ref type="bibr" target="#b28">Vuli´c and Moens (2015)</ref> proposed this task to test crosslingual word embed- dings. The difficulty of this is that it is evaluated using the recall of the top ranked word. The model must be very discriminative in order to score well. We build the CLWE for 3 language pairs: it-en, es-en and nl-en, using similar parameters set- ting with Vuli´c <ref type="bibr" target="#b28">Vuli´c and Moens (2015)</ref>. <ref type="bibr">5</ref> The remaining tunable parameters in our system are δ from Equa- tion (5), and the choice of algorithm for combining embeddings. We use the regularization technique from §4.3 for combining context and word embed- dings with δ = 0.01, and word embeddings U are used as the output for all experiments (but see com- parative experiments in §8.)</p><p>Qualitative evaluation We jointly train the model to predict both w i and the translation ¯ w i , combine V and U during training for each language pair. Ta- ble 2 shows the top 10 closest words in both source and target languages according to cosine similarity. Note that the model correctly identifies the transla- tion in en as the top candidate, and the top 10 words in both source and target languages are highly re- lated. This qualitative evaluation initially demon- strates the ability of our CLWE to capture both the bilingual and monolingual relationship.  <ref type="table">Table 2</ref>: Top 10 closest words in both source and target language corresponding to es word gravedad (left) and it word tassazione (right). They have 15 and 4 dictionary translations respectively. The en words in the dictionary translations are marked with ( * ). The correct translation is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative evaluation</head><p>ment <ref type="bibr" target="#b9">Gouws and Søgaard (2015)</ref> using Panlex and Wiktionary dictionaries. The result with Panlex is substantially worse than with Wiktionary. This con- firms our hypothesis in §2. That is the context might be corrupted if we just randomly replace the training data with the translation from noisy dictionary such as Panlex.</p><p>Our model when randomly picking the translation is similar to <ref type="bibr" target="#b9">Gouws and Søgaard (2015)</ref>, using the Panlex dictionary. The biggest difference is that they replace the training data (both context and middle word) while we fix the context and only replace the middle word. For a high coverage yet noisy dictio- nary such as Panlex, our approach gives better av- erage score. Comparing our two most basic mod- els (EM selection and random selection), it is clear that the model using EM to select the translation out- performs random selection by a significant margin.</p><p>Our joint model, as described in equation (3) which predicts both target word and the transla- tion, further improves the performance, especially for nl-en. We use equation <ref type="formula">(5)</ref> to combine both context embeddings V and word embeddings U for all three language pairs. This modification dur- ing training substantially improves the performance. More importantly, all our improvements are consis- tent for all three language pairs and both evaluation metrics, showing the robustness of our models.</p><p>Our combined model out-performed previous ap- proaches by a large margin. Vuli´c <ref type="bibr" target="#b28">Vuli´c and Moens (2015)</ref> used bilingual comparable data, but this might be hard to obtain for some language pairs. Their perfor- mance on nl-en is poor because their comparable data between en and nl is small. Besides, they also use POS tagger and lemmatizer to filter only Noun and reduce the morphology complexity during train- ing. These tools might not be available for many languages. For a fairer comparison to their work, we also use the same Treetagger <ref type="bibr" target="#b23">(Schmid, 1995)</ref> to lemmatize the output of our combined model before evaluation. <ref type="table" target="#tab_0">Table 3</ref> (+lemmatization) shows some improvements but minor. It demonstrates that our model is already good at disambiguating morphol- ogy. For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Monolingual Word Similarity</head><p>Now we consider the efficacy of our CLWE on monolingual word similarity. We evaluate on En- glish monolingual similarity on WordSim353 (WS- en), RareWord (RW-en) and German version of WordSim353 (WS-de) ( <ref type="bibr" target="#b8">Finkelstein et al., 2001;</ref><ref type="bibr" target="#b17">Luong et al., 2013;</ref><ref type="bibr" target="#b18">Luong et al., 2015)</ref>. Each of those datasets contain many tuples (w 1 , w 2 , s) where s is a scalar denoting the semantic similarity between w 1 and w 2 given by human annotators. Good sys- tem should produce the score correlated with human judgement.</p><p>We train the model as described in §4, which is the combine embeddings setting from <ref type="table" target="#tab_0">Table 3</ref>. Since the evaluation involves de and en word similar- ity, we train the CLWE for en-de pair. <ref type="table">Table 4</ref> shows the performance of our combined model com- pared with several baselines. Our combined model out-performed both <ref type="bibr" target="#b18">Luong et al. (2015)</ref> and <ref type="bibr" target="#b9">Gouws and Søgaard (2015)</ref>  <ref type="bibr">6</ref> which represent the best pub- lished crosslingual embeddings trained on bitext and monolingual data respectively.</p><p>We also compare our system with the monolin- gual CBOW model trained on the monolingual data for each language, using the same parameter settings from earlier ( §6). Surprisingly, our combined model performs better than the monolingual CBOW base- line which makes our result close to the monolin- gual state-of-the-art on each different dataset. How- ever, the best monolingual methods use much larger <ref type="bibr">6</ref> trained using the Panlex dictionary </p><note type="other">es-en it-en nl-en Average rec1 rec5 rec1 rec5 rec1 rec5 rec1 rec5</note><p>Gouws and Søgaard (2015) + Panlex 37.6 63.6 26.6 56.3 49.8 76.0 38.0 65.3 Gouws and Søgaard (2015) + Wikt 61.6 78.9 62.6 81.1 65.6 79.7 63.3 79.9 BilBOWA:  51.6 - 55.7 - 57.5 - 54.9 - Vuli´c <ref type="bibr" target="#b28">Vuli´c and Moens (2015)</ref> 68    Next we explain the gain of our combined model compared with the monolingual CBOW model. First, we compare the combined model with the joint-model with respect to monolingual CBOW model <ref type="table">(Table 4)</ref>. It shows that the improvement seems mostly come from combining V and U. If we apply the combining algorithm to the monolin- gual CBOW model (CBOW + combine), we also ob- serve an improvement. Clearly most of the improve- ment is from combining V and U, however our V and U are more complementary as the gain is more marked. Other improvements can be explained by the observation that a dictionary can improve mono- lingual accuracy through linking synonyms <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014</ref>). For example, since plane, airplane and aircraft have the same Italian translation aereo, the model will encourage those words to be closer in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Model selection</head><p>Combining context embeddings and word embed- dings results in an improvement in both monolin- gual similarity and bilingual lexicon induction. In §4.3, we introduce several combination methods in- cluding post-processing (interpolation and concate- nation) and during training (regularization). In this section, we justify our parameter and model choices.</p><p>We use en-it pair for tuning purposes, consid- ering the value of γ in equation 4. <ref type="figure" target="#fig_3">Figure 2</ref> shows the performances using different values of γ. The two extremes where γ = 0 and γ = 1 corresponds to no interpolation where we just use U or V re- spectively. As γ increases, the performance on WS- en increases yet BLI decreases. These results con- firm our hypothesis in §4.3 that U is better at cap- turing bilingual relations and V is better at captur- ing monolingual relations. As a compromise, we choose γ = 0.5 in our experiments. Similarly, we tune the regularization sensitivity δ in equation <ref type="formula">(5)</ref> which combines embeddings space during training. We test δ = 10 −n with n = {0, 1, 2, 3, 4} and us-   <ref type="table">Table 5</ref>: Performance on en-it BLI and en mono- lingual similarity WordSim353 (WS-en) for various combining algorithms mentioned in §4.3 w.r.t just using U or V alone (after joint-training). We use γ = 0.5 for interpolation and δ = 0.01 for regular- ization with the choice of V, U or interpolation of both V+U 2 for the output. The best scores are bold.</p><p>ing V, U or the interpolation of both V+U 2 as the learned embeddings, evaluated on the same BLI and WS-en. We select δ = 0.01. <ref type="table">Table 5</ref> shows the performance with and with- out using combining algorithms mentioned in §4.3. As the compromise between both monolingual and crosslingual tasks, we choose regularization + U as the combination algorithm. All in all, we apply the regularization algorithm for combining V and U with δ = 0.01 and U as the output for all language pairs without further tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Crosslingual Document Classification</head><p>In this section, we evaluate our CLWE on a down- stream crosslingual document classification (CLDC) Model en → de de → en MT baseline 68.1 67.4 <ref type="bibr" target="#b13">Klementiev et al. (2012)</ref> 77.6 71.1  86.5 75.0 Kočisk´ <ref type="bibr" target="#b14">Kočisk´y et al. (2014)</ref> 83.1 75.4 <ref type="bibr" target="#b2">Chandar A P et al. (2014)</ref> 91.8 74.2 <ref type="bibr" target="#b14">Hermann and Blunsom (2014)</ref> 86.4 74.7 <ref type="bibr" target="#b18">Luong et al. (2015)</ref> 88.4 80.3 Our model 86.3 76.8 <ref type="table">Table 6</ref>: CLDC performance for both en → de and de → en direction for many CLWE. The MT base- line uses phrase-based statistical machine translation to translate the source language to target language ( <ref type="bibr" target="#b13">Klementiev et al., 2012</ref>). The best scores are bold. task. In this task, the document classifier is trained on a source language and then applied directly to classify a document in the target language. This is convenient for a target low-resource language where we do not have document annotations. The experi- mental setup is the same as <ref type="bibr" target="#b13">Klementiev et al. (2012)</ref>  <ref type="bibr">7</ref> with the training and testing data sourced from Reuter RCV1/RCV2 corpus ( <ref type="bibr" target="#b16">Lewis et al., 2004</ref>). The documents are represented as the bag of word embeddings weighted by tf.idf. A multi-class classifier is trained using the average perceptron al- gorithm on 1000 documents in the source language and tested on 5000 documents in the target language. We use the CLWE, such that the document repre- sentation in the target language embeddings is in the same space with the source language.</p><p>We build the en-de CLWE using combined models as described in section §4. Following prior work, we also use monolingual data 8 from the RCV1/RCV2 corpus ( <ref type="bibr" target="#b13">Klementiev et al., 2012;</ref><ref type="bibr" target="#b2">Chandar A P et al., 2014)</ref>. <ref type="table">Table 6</ref> shows the CLDC results for various CLWE. Despite its simplicity, our model achieves competitive performance. Note that aside from our model, all other models in <ref type="table">Table 6</ref> use a large bi- text (Europarl) which may not exist for many low- resource languages, limiting their applicability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Low-resource languages</head><p>Our model exploits dictionaries, which are more widely available than parallel corpora. However the question remains as to how well this performs of a real low-resource language, rather than a simulated condition like above, whereupon the quality of the dictionary is likely to be worse. To test this, we eval- uation on Serbian, a language with few annotated language resources. <ref type="table">Table 1</ref> shows the relative size of monolingual data and dictionary for en-sr com- pared with other language pairs. Both the Serbian monolingual data and the dictionary size is more than 10 times smaller than other language pairs. We build the en-sr CLWE using our best model (joint + combine) and evaluate on the bilingual word in- duction task using 939 gold translation pairs. <ref type="bibr">9</ref> We achieved recall score of 35.8% and 45.5% at 1 and 5 respectively. Although worse than the earlier results, these numbers are still well above chance.</p><p>We can also simulate low-resource setting using our earlier datasets. For estimating the performance loss on all three tasks, we down sample the dictio- nary for en-it and en-de based on en word fre- quency. <ref type="figure" target="#fig_4">Figure 3</ref> shows the performance with dif- ferent dictionary sizes for all three tasks. The mono- lingual similarity performance is very similar across various sizes. For BLI and CLDC, dictionary size is more important, although performance levels off at around 80k dictionary pairs. We conclude that this size is sufficient for decent performance. <ref type="bibr">9</ref> The sr→en translations are sourced from Google Trans- late by translating one word at a time, followed by manually verification, after which 61 translation pairs were ruled out as being bad or questionable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion</head><p>Previous CLWE methods often impose high re- source requirements yet have low accuracy. We in- troduce a simple framework based on a large noisy dictionary. We model polysemy using EM transla- tion selection during training to learn bilingual cor- respondences from monolingual corpora. Our algo- rithm allows to train on massive amount of mono- lingual data efficiently, representing monolingual and bilingual properties of language. This allows us to achieve state-of-the-art performance on bilin- gual lexicon induction task, competitive result on monolingual word similarity and crosslingual doc- ument classification task. Our combination tech- niques during training, especially using regulariza- tion, are highly effective and could be used to im- prove monolingual word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of V and U space during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 :</head><label>4</label><figDesc>Spearman's rank correlation for monolin- gual similarity measurement on 3 datasets WS-de (353 pairs), WS-en (353 pairs) and RW-en (2034 pairs). We compare against 5 baseline crosslingual word embeddings. The best CLWE performance is bold. For reference, we add the monolingual CBOW with and without embeddings combination, Yih and Qazvinian (2012) and Shazeer et al. (2016) which represents the monolingual state-of-the-art results for WS-en and RW-en. monolingual corpora (Shazeer et al., 2016), Word- Net or the output of commercial search engines (Yih and Qazvinian, 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of word embeddings interpolated using different values of γ evaluated using BLI (Recall@1, Recall@5) and English monolingual WordSim353 (WS-en).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curve showing how task scores increase with increasing dictionary size; showing bilingual lexicon induction (BLI) task (left), monolingual similarity (center) and crosslingual document classification (right). BLI is trained on en-it, and monolingual similarity and CLDC are trained on en-de.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 shows</head><label>3</label><figDesc>our re- sults compared with prior work. We reimple-</figDesc><table>gravedades 

tassazioneit 
es 
en 
it 
en 

gravitacional gravity  *  
tasse 
taxation  *  
gravitatoria 
gravitation  *  
fiscale 
taxes 
aceleracin 
acceleration 
tassa 
tax  *  
gravitacin 
non-gravitational imposte 
levied 
inercia 
inertia 
imposta 
fiscal 
gravity 
centrifugal 
fiscali 
low-tax 
msugra 
free-falling 
l'imposta revenue 
centrífuga 
gravitational 
tonnage 
levy 
curvatura 
free-fall 
tax 
annates 
masa 
newton 
accise 
evasion 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Bilingual Lexicon Induction performance from es, it, nl to en. Gouws and Søgaard (2015) 
+ Panlex/Wikt is our reimplementation using Panlex/Wiktionary dictionary. All our models use Panlex as 
the dictionary. We reported the recall at 1 and 5. The best performance is bold. 

Model 
WS-de WS-en RW-en 

Baselines 
Klementiev et al. (2012) 
23.8 
13.2 
7.3 
Chandar A P et al. (2014) 
34.6 
39.8 
20.5 
Hermann and Blunsom (2014) 28.3 
19.8 
13.6 
Luong et al. (2015) 
47.4 
49.3 
25.3 
Gouws and Søgaard (2015) 
67.4 
71.8 
31.0 

Mono 
CBOW 
62.2 
70.3 
42.7 
+ combine 
65.8 
74.1 
43.1 
Yih and Qazvinian (2012) 
-
81.0 
-
Shazeer et al. (2016) 
-
74.8 
48.3 

Ours 
Our joint-model 
59.3 
68.6 
38.1 
+ combine 
71.1 
76.2 
44.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Model 
BLI 
Mono 
rec1 rec5 WS-en 

Alone 
Joint-model + V 
67.6 82.8 
70.5 
Joint-model + U 
76.2 84.7 
48.4 

Combine 
Interpolation 
V+U 

2 


75.0 85.9 
72.7 
Concatenation 
72.7 85.2 
71.2 
Regularization + V 
80.3 89.8 
45.9 
Regularization + U 
80.8 90.4 
74.8 
Regularization + V+U 

2 

80.9 91.1 
72.3 

</table></figure>

			<note place="foot" n="1"> Using both embeddings gives a small improvement compared to just using context vector h alone. 2 We also experimented with using expectations over translations, as per standard EM, with slight degredation in results.</note>

			<note place="foot" n="3"> In the stochastic gradient update for a given word in context, we only compute the gradient of the regularisation term in (5) with respect to the words in the set of positive and negative examples.</note>

			<note place="foot" n="5"> Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1e −4 , embedding dimension d = 200, window size cs = 48 and run for 15 epochs.</note>

			<note place="foot" n="7"> The data split and code are kindly provided by the authors. 8 We randomly sample documents in RCV1 and RCV2 corpora and selected around 85k documents to form 400k monolingual sentences for both en and de. For each document, we perform basic pre-processing including: lower-casing, remove html tags and tokenization. These monolingual data are then concatenated with the monolingual data from Wikipedia to form the final training data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was conducted during Duong's internship at IBM Research-Tokyo and partially supported by the University of Melbourne and National ICT Australia (NICTA). We are grateful for support from NSF Award 1464553 and the DARPA/I2O, Contract No. HR0011-15-C-0114. We thank Yuta Tsuboi and Alvin Grissom II for helpful discussions, JaňJaň Snajder for helping with sr-en evaluation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised partof-speech tagging with bilingual graph-based projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on World Wide Web, WWW &apos;01</title>
		<meeting>the 10th International Conference on World Wide Web, WWW &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple taskspecific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1386" to="1390" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>David Blei and Francis Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Panlex: Building a resource for panlingual lexical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kamholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Colowick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3145" to="50" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning bilingual word representations by marginalizing alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural word embedding as a factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>De- cember 8-13</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-08" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Vector Space Modeling for NLP</title>
		<meeting><address><addrLine>Denver, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="133" />
			<date type="published" when="1999-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improvements in part-of-speech tagging with an application to german</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL SIGDAT-Workshop</title>
		<meeting>the ACL SIGDAT-Workshop</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="47" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Swivel: Improving embeddings by noticing what&apos;s missing. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Waterson</surname></persName>
		</author>
		<idno>abs/1602.02215</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inverted indexing for cross-lingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor Martínez</forename><surname>Zeljko Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johannsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1713" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="477" to="487" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="719" to="725" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning, chapter Distributed Word Representation Learning for Cross-Lingual Dependency Parsing</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning, chapter Distributed Word Representation Learning for Cross-Lingual Dependency Parsing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="119" to="129" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL &apos;01</title>
		<meeting>the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL &apos;01<address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Measuring word relatedness using heterogeneous vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahed</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qazvinian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="616" to="620" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
