<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics and Philology</orgName>
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miryam</forename><surname>De Lhoneux</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics and Philology</orgName>
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Stymne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics and Philology</orgName>
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics and Philology</orgName>
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2711" to="2720"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2711</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We provide a comprehensive analysis of the interactions between pre-trained word embed-dings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces large improvements over a baseline system using randomly initialised word embeddings only, but combining them quickly leads to diminishing returns. We categorise words by frequency, POS tag and language in order to systematically investigate how each of the techniques affects parsing quality. For many word categories , applying any two of the three techniques is almost as good as the full combined system. Character models tend to be more important for low-frequency open-class words, especially in morphologically rich languages, while POS tags can help disambiguate high-frequency function words. We also show that large character embedding sizes help even for languages with small character sets, especially in morphologically rich languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The last few years of research in natural language processing (NLP) have witnessed an explosion in the application of neural networks and word em- beddings. In tasks ranging from POS tagging to reading comprehension to machine translation, a unique dense vector is learned for each word type in the training data. These word embeddings have been shown to capture essential semantic and morphological relationships between words ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>, and have precipitated the enormous success of neural network-based archi- tectures across a wide variety of NLP tasks <ref type="bibr" target="#b18">(Plank et al., 2016;</ref><ref type="bibr" target="#b3">Dhingra et al., 2017b;</ref><ref type="bibr" target="#b27">Vaswani et al., 2017</ref>).</p><p>When task-specific training data is scarce or the morphological complexity of a language leads to sparsity at the word-type level, word embeddings often need to be augmented with sub-word or part- of-speech (POS) tag information in order to re- lease their full power ( <ref type="bibr" target="#b8">Kim et al., 2016;</ref><ref type="bibr" target="#b20">Sennrich et al., 2016;</ref><ref type="bibr" target="#b1">Chen and Manning, 2014</ref>). Initialis- ing vectors with embeddings trained for a different task, typically language modelling, on huge un- labelled corpora has also been shown to improve results significantly ( <ref type="bibr" target="#b2">Dhingra et al., 2017a)</ref>. In de- pendency parsing, the use of character ( ) and POS ( ) models is widespread, and the majority of parsers make use of pre-trained word embeddings ( <ref type="bibr" target="#b28">Zeman et al., 2017</ref>).</p><p>While previous research has examined in de- tail the benefits of character and POS models in dependency parsing and their interactions <ref type="bibr" target="#b4">Dozat et al., 2017)</ref>, there has been no systematic investigation into the way these techniques combine with the use of pre- trained embeddings. Our results suggest a large amount of redundancy between all three tech- niques: in isolation, each gives large improve- ments over a simple baseline model, but these im- provements are not additive. In fact combining any two of the three methods gives similar results, close to the performance of the fully combined system.</p><p>We set out to systematically investigate the ways in which pre-trained embeddings, char- acter and POS models contribute to improving parser quality. We break down results along three dimensions-word frequency, POS tag, and language-in order to tease out the complex inter- actions between the three techniques. Our main findings can be summarized as follows:</p><p>• For all techniques, improvements are largest for low-frequency and open-class words and for morphologically rich languages.</p><p>• These improvements are largely redundant when the techniques are used together.</p><p>• Character-based models are the most effec- tive technique for low-frequency words.</p><p>• Part-of-speech tags are potentially very effec- tive for high-frequency function words, but current state-of-the-art taggers are not accu- rate enough to take full advantage of this.</p><p>• Large character embeddings are helpful for morphologically rich languages, regardless of character set size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Chen and Manning (2014) introduced POS tag embeddings: a learned dense representation of each tag designed to exploit semantic similari- ties between tags. In their greedy transition-based parser, the inclusion of these POS tag embeddings improved labelled attachment score (LAS) by 1.7 on the English Penn Treebank (ETB) and almost 10 on the Chinese Penn Treebank (CTB). They also tested the use of pre-trained word embeddings for initialisation of word vectors, finding gains of 0.7 for PTB and 1.7 for CTB.  in their Stack Long Short- Term Memory (LSTM) dependency parser, show that POS tag embeddings in their architecture im- prove LAS by 0.6 for English and 6.6 for Chi- nese. Unlike <ref type="bibr" target="#b1">Chen and Manning (2014)</ref>, they do not use pre-trained word embeddings for initialisa- tion, instead concatenating them as a fixed vector representation to a separate randomly-initialised learned representation. This leads to improve- ments in LAS of 0.9 and 1.6 of English and Chi- nese, respectively.</p><p>Following on from the work of ,  introduced the first character-based parsing model. They found that a model based purely on character informa- tion performed at the same level as a model using a combination of word embeddings and POS tags. Combining character and POS models produced even better results, but they conclude that POS tags are less important for character-based parsers. They also showed that character models are par- ticularly effective for morphologically rich lan- guages, but that performance remains good in lan- guages with little morphology, and that character models help substantially with out-of-vocabulary (OOV) words, but that this does not fully explain the improvements they bring. The use of pre- trained embeddings was not considered in their work. <ref type="bibr" target="#b9">Kiperwasser and Goldberg (2016)</ref>, in the transition-based version of their parser based on BiLSTM feature extractors, found that POS tags improved performance by 0.3 LAS for English and 4.4 LAS for Chinese. Like , they concatenate a randomly-initialised word embed- dings to a pre-trained word vector; however in this case the pre-trained vector is also updated during training. They find that this helps LAS by 0.5-0.7 for English and 0.9-1.2 for Chinese, depending on the specific architecture of their system. <ref type="bibr" target="#b4">Dozat et al. (2017)</ref>, building on the graph-based version of <ref type="bibr" target="#b9">Kiperwasser and Goldberg (2016)</ref>, con- firmed the relationship between character models and morphological complexity, both for POS tag- ging and parsing. They also examined the im- portance of the quality of POS tags on parsing, showing that their own tagger led to better parsing results than a baseline provided by UDPipe v1.1 ( <ref type="bibr" target="#b24">Straka et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Parser</head><p>We use and extend UUParser 1 (de <ref type="bibr">Lhoneux et al., 2017a;</ref><ref type="bibr" target="#b22">Smith et al., 2018)</ref>, a variation of the transition-based parser of Kiperwasser and Gold- berg (2016) (K&amp;G). The K&amp;G architecture can be adapted to both transition-and graph-based depen- dency parsing, and has quickly become a de facto standard in the field ( <ref type="bibr" target="#b28">Zeman et al., 2017)</ref>. In a K&amp;G parser, BiLSTMs <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Graves, 2008)</ref> are employed to learn useful representations of tokens in context. A multi-layer perceptron (MLP) is trained to predict transitions and possible arc labels, taking as in- put the BiLSTM vectors of a few tokens at a time. Crucially, the BiLSTMs and MLP are trained to- gether, enabling the parser to learn very effective token representations for parsing. For further de- tails we refer the reader to <ref type="bibr" target="#b15">Nivre (2008)</ref> and <ref type="bibr" target="#b9">Kiperwasser and Goldberg (2016)</ref>, for transition-based parsing and BiLSTM feature extractors, respec- tively.</p><p>Our version of the K&amp;G parser is extended with a SWAP transition to facilitate the construction of non-projective dependency trees <ref type="bibr" target="#b16">(Nivre, 2009)</ref>. We use a static-dynamic oracle to allow the parser to learn from non-optimal configurations at train- ing time in order to recover better from mistakes at test time, as described in de <ref type="bibr" target="#b12">Lhoneux et al. (2017b)</ref>.</p><p>In this paper we experiment with a total of eight variations of the parser, where the difference be- tween each version resides in the vector represen- tations x i of word types w i before they are passed to the BiLSTM feature extractors (see Section 3 of <ref type="bibr" target="#b9">Kiperwasser and Goldberg (2016)</ref>). In the sim- plest case, we set x i equal to the word embedding e r (w i ):</p><formula xml:id="formula_0">x i = e r (w i )</formula><p>The superscript r refers to the fact that the word embeddings are initialised randomly at training time. This is the setup in our BASELINE system. For our +CHAR system, the word embedding e r (w i ) is concatenated to a character-based vector, obtained by running a BiLSTM over the characters ch 1:m of w i :</p><formula xml:id="formula_1">x i = e r (w i ) • BiLSTM(ch 1:m )</formula><p>In the +POS setting, the word embedding is in- stead concatenated to an embedding p(w i ) of the word's universal POS tag ( <ref type="bibr" target="#b17">Nivre et al., 2016</ref>):</p><formula xml:id="formula_2">x i = e r (w i ) • p(w i )</formula><p>This scenario necessitates knowledge of the POS tag of w i ; at test time, we therefore need a POS tagger to provide predicted tags.</p><p>In another version of our parser (+EXT), pre- trained embeddings are used to initialise the word embeddings. <ref type="bibr">2</ref> We use the superscript t to distin- guish these from randomly initialised vectors:</p><formula xml:id="formula_3">x i = e t (w i )</formula><p>We use the embeddings that were released as part of the 2017 CoNLL Shared Task on Universal De- pendency Parsing (CoNLL-ST-17) ( <ref type="bibr" target="#b28">Zeman et al., 2017)</ref>. Words in the training data that do not have pre-trained embeddings are initialised randomly. At test time, we look up the updated embeddings for all words seen in the training data; OOV words are assigned their un-updated pre-trained embed- ding where it exists, otherwise a learnt OOV vec- tor.</p><p>In our COMBINED setup, we include pre-trained embeddings along with the character vector and POS tag embedding:</p><formula xml:id="formula_4">x i = e t (w i ) • BiLSTM(ch 1:m ) • p(w i )</formula><p>The three remaining versions of the vector x i con- stitute all possible combinations of two techniques of pre-trained embeddings, the character model and POS tags. We refer to these versions of the parser as −EXT, −CHAR, and −POS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup 4.1 Data</head><p>We ran our experiments on nine treebanks from Universal Dependencies (Nivre et al., 2016) (v2.0): Ancient Greek PROIEL, Arabic, Chinese, English, Finnish, Hebrew, Korean, Russian and Swedish. Inspired partially by de <ref type="bibr" target="#b13">Lhoneux et al. (2017c)</ref>, these treebanks were chosen to reflect a diversity of writing systems, character set sizes, and morphological complexity. As error analysis is carried out on the results, we perform all exper- iments on the dev data sets. <ref type="table">Table 1</ref> shows some statistics of each treebank. Of particular note are the large character set sizes in Chinese and Korean, an order of magnitude big- ger than those of all other treebanks. The high type-token ratio for Finnish, Russian and Korean also stands out; this is likely due to the high mor- phological complexity of these languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Treebank</head><p>Sentences </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parser settings</head><p>The parser is trained three times for each language with different random seeds for 30 epochs each.</p><p>At the end of each epoch we parse the dev data Word embedding size 100 Character embedding size 500 Character BiLSTM output size 100 POS tag embedding size 20 and calculate LAS. For each training run, results are averaged over the five best epochs for each lan- guage. In this way, we attempt to make our results more robust to variance due to randomness in the training procedure. 3 Our macro-averaged scores are based on a total of 135 different epochs (3 ran- dom seeds × 5 best epochs × 9 languages). <ref type="table" target="#tab_1">Table 2</ref> shows the embedding sizes we found to produce best results in preliminary work and which we use in all experiments in Section 5. Note our unusually large character embedding size; we will discuss this in more detail in Section 6. We use predicted UPOS tags from the system of <ref type="bibr" target="#b4">Dozat et al. (2017)</ref> for experiments with POS tags, 4 other than in Section 7 where we compare results with different taggers and gold POS tags, in order to set a ceiling on the potential gains from a perfect POS tagger. For all other hyperparameters we use default values (Smith et al., 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>The hypothesis underlying our choice of analysis is that the three techniques under study here-pre- trained embeddings, character vectors and POS tag embeddings-affect words differently depend- ing on their frequencies, POS tags, and the lan- guage of the sentence. We do not claim this to be an exhaustive list; many other dimensions of analysis are clearly possible (dependency relation would be another obvious choice for example), but we believe that these are likely to be three of the most informative factors. In the frequency and POS tag cases, we want to examine the overall contribution to LAS of words from each category. We expect changing the representation of a token to affect how likely it is to be assigned the correct head in the dependency tree, but also how likely it is to be assigned correctly as the head of other words. We thus introduce a new metric for this part of the analysis: the head and dependents la- belled attachment score, which we refer to as HD-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAS.</head><p>When calculating HDLAS, the dependency analysis for a given token is only considered cor- rect if the token has the correct labelled head and the complete set of correctly labelled dependents. This is a harsher metric than LAS, which only considers whether a token has the correct labelled head. Note that when calculating HDLAS for all tokens in a sentence, each dependency relation is counted twice, once for the head word and once for the dependent. It only makes sense to use this metric when analysing individual tokens in a sen- tence, or when grouping tokens into different cat- egories across multiple sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Frequency</head><p>In this analysis, we first label each token in the dev data for each language by its relative frequency in the train data, with add-one smoothing. <ref type="bibr">5</ref> Fre- quency categories are created by rounding the log relative frequency down to the nearest integer. We calculate the HDLAS for each frequency category for each language, before macro-averaging the re- sults across the nine languages to produce a final score for each frequency class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">POS tag</head><p>In this case, we label each word from the dev data by its gold POS tag, before calculating HDLAS for each category and taking the macro average across languages. Here the total number of to- kens in each category varies across several orders of magnitude: the most common category NOUNs make up 26.0% of all words, while the smallest class SYM represents just 0.1%. For this reason, and to make our graphs more readable, we do not show results for the six smallest categories: INTJ, NUM, PART, SCONJ, SYM, and X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Language</head><p>Here we consider LAS directly for each language; the HDLAS metric used in the previous two sec- tions is not relevant as all tokens in a given sen- tence are assigned to the same category deter- mined by the language of the sentence.  <ref type="table" target="#tab_2">Table 3</ref>: Mean LAS across nine languages for a baseline system employing randomly-initialised word embeddings only, compared to three sepa- rate systems using pre-trained word embeddings (+EXT), a character model (+CHAR), and POS tags (+POS). Scores are also shown for a com- bined system that utilises all three techniques and corresponding systems where one of the three techniques is ablated (−EXT, −CHAR and −POS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>trained embeddings (+8.4), the character model (+10.6) and POS tags (+8.2) all give large im- provements in LAS over the baseline system. The combined system is the best overall, but the im- provement of 13.3 LAS is far from the sum of its components. Employing two of the techniques at a time reduces LAS by only 0.7-1.8 compared to the combined system. <ref type="figure" target="#fig_0">Fig. 1</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref> compare systems by word fre- quency. As expected, accuracy improves with fre- quency for all systems: the parser does better with words it sees more often during training. There is a levelling off for the highest frequency words, probably due to the fact that these categories con- tain a small number of highly polysemous word types. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates a clear trend in the im- provement achieved by each of the individual techniques over the baseline, with larger gains  for lower frequency words. This confirms a re- sult from , who found that character models help substantially with OOV words. We can generalise this to say that charac- ter models improve parsing quality most for low frequency words (including OOV words), and that this is also true, albeit to a slightly lesser effect, of POS tags and pre-trained word embeddings. It is notable however that HDLAS increases univer- sally across all frequency classes: even the highest frequency words benefit from enhancements to the basic word representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Frequency</head><p>What immediately stands out in <ref type="figure" target="#fig_1">Fig. 2</ref> is that for mid-and high frequency words, there is little difference in HDLAS between different combina- tions of two of the three techniques, and for the highest frequency words this is at a level almost in- distinguishable from the full COMBINED system. The slight improvements we see for COMBINED in <ref type="table" target="#tab_2">Table 3</ref> compared to the three ablated systems thus principally also come from the low-frequency range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">POS tags</head><p>In <ref type="figure" target="#fig_2">Fig. 3</ref> systems are compared by POS tag. We observe a universal improvement across all POS tags for each of the three variations of the system compared to the baseline. However, it is notable that the biggest gains in HDLAS are for open word classes: NOUNs, VERBs and ADJs. As these make up a large overall proportion of words, these differences have an overall relatively large impact on LAS.</p><p>For the most frequent POS categories NOUN and VERB we again see a clear victory for the character model (note that while these POS cat-egories are frequent, they contain a large num- ber of low-frequency words). Overall the charac- ter model succeeds best for the open-class POS categories, while having the right POS tag is marginally better for closed-class categories such as DET, CCONJ, and AUX. It is interesting that the character model is not as strong for PROPN, despite the fact that these are open-class low- frequency words; for these words pre-trained em- beddings are the best single technique. This may be due to the fact that the rules governing the com- position of names at the character level are differ- ent from other words in the language.</p><p>It is perhaps surprising that the advantage of POS tag embeddings is not greater when it comes to auxiliary verbs, for example, where the distinc- tion from main verbs can be difficult and crucial for a correct syntactic analysis. The reason prob- ably lies in the fact that this distinction is equally difficult for the POS tagger. We will investigate this further in Section 7.   stantial differences between languages. The three biggest overall improvements are for Finnish, Ko- rean and Russian, with a particularly notable in- crease in the Korean case. This suggests that the baseline model struggles to learn adequate repre- sentations for each word type in these languages. These are the three languages we identified in Sec- tion 4.1 as having high type-token ratios in their training data. It is also notable that the character model becomes more important compared to other methods for these three languages. In fact, despite the overall superiority of the character model (see <ref type="table" target="#tab_2">Table 3</ref>), it is only the best single technique for 4 of the 9 languages, the three already mentioned plus Ancient Greek.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Character Embedding Size</head><p>All results with character models observed thus far make use of a character embedding of dimen- sion 500. This value is large compared to typ- ical sizes used for character models <ref type="bibr" target="#b8">(Kim et al., 2016;</ref>. A common belief is that larger character embedding sizes are justi- fied for languages with larger character set sizes such as Chinese: in other words, the embedding size should be related to the number of entities be- ing embedded <ref type="bibr" target="#b21">(Shao, 2018)</ref>.</p><p>In <ref type="table">Table 4</ref>, we show how LAS varies with a few values of this hyperparameter when averaged across our nine-language sample. We see a steady BASELINE 67.7 −CHAR 79.2 +CH-24 76.8 +CH-24 80.5 +CH-100 77.7 +CH-100 80.6 +CH-500 78.3 +CH-500 81.0 <ref type="table">Table 4</ref>: Mean LAS across nine languages for BASELINE system compared to systems with char- acter vectors of different sizes. Comparison also shown for systems employing pre-trained word vectors and POS tag embeddings. improvement in LAS as the character embedding size increases, both when compared to a baseline with randomly initialised word embeddings only and when compared to a system that also em- ploys pre-trained word vectors and POS tag em- beddings. <ref type="bibr">6</ref> It is particularly interesting to break down the effects here by language. In <ref type="table" target="#tab_4">Table 5</ref> we show re- sults for Chinese, Finnish, Korean and Russian. It is particularly striking that the larger character em- beddings do not help for Chinese; the score for the largest character embedding size is actually marginally lower than a baseline without a char- acter model at all. This is despite the fact that a small character embedding improves LAS, albeit marginally, suggesting that there is some useful in- formation in the characters even when pre-trained embeddings and POS tags are present. Conversely, the large character models are very effective for Finnish, a treebank with a character set less than a tenth of the size of Chinese (see <ref type="table">Table 1</ref>).  We claim therefore that character set size is not in fact a good metric to use in determining char- acter embedding sizes. Our tentative explanation is that while languages like Finnish have relatively small character sets, those characters interact with each other in much more complex ways, thus re- quiring larger embeddings to store all the neces- sary information. While there are many characters in Chinese, the entropy in the interactions between characters appears to be smaller, enabling smaller character embeddings to do just as good a job.</p><p>It is also worth noting <ref type="table" target="#tab_4">from Tables 4 and 5</ref> that, in the presence of POS tags and pre-trained em- beddings, the improvement gained from increas- ing the character embedding size from 24 to 100 is small (0.1 LAS for Finnish, 0.2 for Korean, 0.1 for Russian; 0.1 on average across the nine tree- banks). This perhaps gives the impression of di- minishing returns; that going even larger is likely to lead to ever smaller improvements. This may be the reason that smaller character embeddings have generally been preferred previously. However, we in fact observe a much greater gain when increas- ing from 100 to 500 (0.9 for Finnish, 1.2 for Ko- rean, 1.0 for Russian; 0.4 on average across the nine treebanks), suggesting that very large charac- ter embeddings are effective, and particularly use- ful for morphologically rich languages. UDPipe 73.    from UDPipe to Stanford and then from Stanford to gold tags over the baseline system. This par- tially confirms results from <ref type="bibr" target="#b4">Dozat et al. (2017)</ref>, where the Stanford tagger was found to improve parsing results significantly over the UDPipe base- line. More surprising perhaps is the result when comparing to the −POS system, which also makes use of pre-trained word embeddings and a char- acter model. Here, results do not improve at all by adding predicted tags from UDPipe. Stanford tags do give an improvement of 0.7 LAS over −POS, but this is a long way from the improve- ment of 8.2 LAS we see when adding them on top of BASELINE. Gold tags do however still give a big improvement over −POS (3.5 LAS), suggest- ing strongly that both UDPipe and Stanford strug- gle with the decisions that would be most benefi- cial to parsing accuracy.</p><p>In <ref type="figure" target="#fig_5">Fig. 5</ref> we present the parsing results bro- ken down by POS tag for the various POS tag- gers. It is particularly notable that results when tagging with UDPipe are no better than for −POS, which does not use POS tags at all, across most categories, and particularly for the closed-classes ADP, PRON, DET, CCONJ and AUX. Stanford tags do marginally better, but access to gold tags is particularly important in these cases; we see a particularly striking improvement when ADPs and AUXs are correctly tagged over an already strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Parser speed</head><p>It should be noted that increasing the character embedding size and character BiLSTM output di- mension as in Section 6 slows down the parser during training and at test time. We found no noticeable difference in speed between the base- line system and versions of the parser with smaller character embedding sizes (24/100), with approx- imately 20 sentences per second being processed on average during training and 65 sentences per second parsed at test time on the Taito super clus- ter. <ref type="bibr">7</ref> There was however a discernible difference when the character embedding size was increased to 500, with only 12 sentences processed per sec- ond during training and 44 during testing.</p><p>Adding a POS tag embedding makes no appre- ciable difference to parser speed, 8 but necessitates a pipeline system that first predicts POS tags (as- suming gold tags are unavailable). The applica- tion of pre-trained embeddings, meanwhile, re- quires expensive pre-training on large unlabelled corpora. Loading these embeddings into the parser takes time and can occupy large amounts of mem- ory, but does not directly impact the time it takes to process a sentence during training or parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions and Future Work</head><p>In this article we examined the complex interac- tions between pre-trained word vectors, character models and POS tags in neural transition-based dependency parsing. While previous work had shown that POS tags are not as important in the presence of character models, we extend that con- clusion to say that in the presence of two of the three techniques, the third is never as important. The best system, however, is always a combina- tion of all three techniques.</p><p>We introduced the HDLAS metric to capture the overall effect on parsing quality of changes to the representation of a particular word. We found that all three techniques produce substantial improve- ments across a range of frequency classes, POS tags, and languages, but the biggest improvements for all techniques were for low-frequency, open- class words. We suggest that this goes some way <ref type="bibr">7</ref> https://research.csc.fi/ taito-supercluster 8 Note that the POS tag embedding we use is small relative to the other components of the word type representation (see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>to explaining the redundancy between the three techniques: they target the same weaknesses in the baseline word-type level embedding.</p><p>We confirmed a previous result that the char- acter model is particularly important for morpho- logically rich languages with high type-token ra- tios, and went on to show that these languages also benefit from larger character embedding sizes, whereas morphologically simpler languages make do with small character embeddings, even if the character set size is large.</p><p>POS tag embeddings can improve results for difficult closed-class categories, but our current best POS taggers are not capable of making the distinctions necessary to really take advantage of this. The strength of pre-trained embeddings is that they are trained on much larger corpora than the task-specific data; the use of character mod- els and POS tag embeddings however seems to al- low us to generalise much better from smaller data sets, as each character and each POS tag is nor- mally seen many times, even if each word type is rare.</p><p>We saw that increasing the character embedding size slows the parser down; whether this trade- off is worthwhile will depend on the application in question. If accuracy is all that matters, we recommend using a fully combined system with large character embeddings in tandem with POS tags and pre-trained embeddings. Where speed is more important, it may be worth considering a sys- tem that employs a smaller character embedding and does without POS tags, using just pre-trained embeddings.</p><p>In future work it would be interesting to investi- gate whether the patterns observed here also hold true for other types of models in dependency pars- ing; possible variations to examine include alter- native character models such as convolutional neu- ral networks, joint tagging-parsing models, and graph-based parsers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: BASELINE system compared to pretrained embeddings (+EXT), character model (+CHAR) and POS tags (+POS).</figDesc><graphic url="image-1.png" coords="5,72.00,570.13,218.26,142.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: COMBINED system compared to ablated systems where pre-trained embeddings (−EXT), character models (−CHAR) and POS tags (−POS) are removed.</figDesc><graphic url="image-2.png" coords="5,307.28,62.81,218.26,142.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison by POS tag of BASELINE system to +EXT, +CHAR, and +POS. Tags are sorted by frequency.</figDesc><graphic url="image-3.png" coords="6,72.00,365.03,218.27,274.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 compares the systems by language. Once again improvement is universal for each system compared to the baseline. There are however sub</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison by language of BASELINE system to +EXT, +CHAR, and +POS.</figDesc><graphic url="image-4.png" coords="6,307.28,62.81,218.27,243.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison by POS tag of POS taggers.</figDesc><graphic url="image-5.png" coords="8,72.00,62.81,218.27,274.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Embedding sizes.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 gives the LAS for each of the eight sys</head><label>3</label><figDesc></figDesc><table>-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison by language of different char-
acter embedding sizes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Mean LAS across nine languages for 
BASELINE system compared to systems with POS 
tags predicted by different systems. Compari-
son also shown for systems employing pre-trained 
word vectors and a character vector. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 shows</head><label>6</label><figDesc>how LAS varies with the differ- ent POS taggers when averaged across the nine- language sample. We see a clear improvement</figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/UppsalaNLP/ uuparser</note>

			<note place="foot" n="2"> This strategy proved more successful in preliminary experiments than others for incorporating pre-trained embeddings discussed in Section 2.</note>

			<note place="foot" n="3"> Changing the random seed has been shown to produce results that appear statistically significant different in neural systems (Reimers and Gurevych, 2017). 4 Available at https://web.stanford.edu/ ˜ tdozat/.</note>

			<note place="foot" n="5"> The smoothing ensures that OOV tokens, those that appear in dev but not train, are not assigned zero frequency; this alleviates the problem of taking log(0) in the subsequent conversion to log relative frequency.</note>

			<note place="foot" n="6"> Note that for character embeddings of dimension 24, we use an output size for the character BiLSTM of 50, for character embeddings of dimension 100, we use an output size of 75, and for character embeddings of dimension 500, we use an output size of 100. We checked in separate experiments that the improvements are not simply due to the increase in output size.</note>

			<note place="foot" n="7"> POS tagger In this section we apply our POS tag analysis to the effect of the POS tagger used to produce tags at test time. We compare three setups: firstly using tags predicted by UDPipe (Straka and Straková, 2017), which was the baseline model for CoNLLST-2017, secondly using tags predicted by the winning Stanford system (Dozat et al., 2017), and thirdly using gold tags. Note that for the Stanford system, we train on gold tags and use predicted tags at test time, while for UDPipe we train on a jackknifed version of the train data with predicted tags that was released as part of CoNLL-ST-2017. BASELINE 67.7 −POS 80.3</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the computational resources pro-vided by CSC in Helsinki and Sigma2 in Oslo through NeIC-NLPL (www.nlpl.eu). Aaron Smith was supported by the Swedish Research Council. We would like to thank Sujoung Baeck for a valu-able discussion regarding Korean morphology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Comparative Study of Word Embeddings for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00993</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GatedAttention Readers for Text Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stanford&apos;s Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TransitionBased Dependency Parsing with Stack Long ShortTerm Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Technical University Munich</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Character-aware Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Basirat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiperwasser</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sara Stymne, Yoav Goldberg, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From Raw Text to Universal Dependencies-Look, No Tags!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Arc-Hybrid Non-Projective Dependency Parsing with a Static-Dynamic Oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Parsing Technologies</title>
		<meeting>the 15th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Old School vs. New School: Comparing Transition-Based Parsers with and without Neural Network Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Treebanks and Linguistic Theories Workshop</title>
		<meeting>the 15th Treebanks and Linguistic Theories Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms for Deterministic Incremental Dependency Parsing. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-Projective Dependency Parsing in Expected Linear Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal Dependencies v1: A Multilingual Treebank Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Natalia Silveira, Reut Tsarfaty, and Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Segmenting and Tagging Text with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Uppsala University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stymne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Models: Universal Dependency Parsing with Multi-Treebank Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Treebanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">UDPipe: Trainable Pipeline for Processing CoNLLU Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL</title>
		<meeting>the CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<imprint>
			<biblScope unit="page" from="88" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
