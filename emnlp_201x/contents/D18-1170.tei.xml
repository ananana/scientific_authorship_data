<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexue</forename><surname>He</surname></persName>
							<email>zexueh@mail.bnu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">College of IST</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1402" to="1411"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1402</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The goal of Word Sense Disambiguation (WSD) is to identify the correct meaning of a word in the particular context. Traditional supervised methods only use labeled data (con-text), while missing rich lexical knowledge such as the gloss which defines the meaning of a word sense. Recent studies have shown that incorporating glosses into neural networks for WSD has made significant improvement. However, the previous models usually build the context representation and gloss representation separately. In this paper, we find that the learning for the context and gloss representation can benefit from each other. Gloss can help to highlight the important words in the context, thus building a better context representation. Context can also help to locate the key words in the gloss of the correct word sense. Therefore, we introduce a co-attention mechanism to generate co-dependent representations for the context and gloss. Furthermore , in order to capture both word-level and sentence-level information, we extend the attention mechanism in a hierarchical fashion. Experimental results show that our model achieves the state-of-the-art results on several standard English all-words WSD test datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD) is a cru- cial task and long-standing problem in Natu- ral Language Processing (NLP). Previous re- searches mainly exploit two kinds of resources. Knowledge-based methods <ref type="bibr" target="#b26">(Lesk, 1986;</ref>) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, super- vised feature-based methods (  and neural-based meth- ods ) usually use labeled data to train one or more classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>As they often play football together, they know each other quite well Glosses g1: participate in games or sports g2: perform music on an instrument g3: behave in a certain way <ref type="table">Table 1</ref>: An example of the context and three glosses of different senses according to the target word "play". It shows that the words "games/sports" in the gloss g 1 can help to highlight the important words "football" in the context and ignore the words "know each other" which are useless for distinguishing the sense of word "play". Meanwhile, the context can potentially help to stress on the words "games/sports" of the gloss g 1 which is actually the correct sense for the target word.</p><p>Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the inte- gration of knowledge into consideration. To the best of our knowledge,  are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as dis- tributed vectors and later calculates their similarity in a memory network. However, we find that the learning of the representations of the context and gloss can contribute to each other. We use an ex- ample to illustrate our ideas. <ref type="table">Table 1</ref> shows that the red words are more important than the blue words when distinguishing the sense of the tar- get word. In other words, we should pay more at- tention to the words which can "overlap" between the context and the gloss when generating the rep- resentations of context and gloss. Therefore, we introduce a co-attention mechanism to model the mutual influence between the representations of context and gloss.</p><p>Moreover, we find that both word-level and sentence-level information are crucial to WSD. As shown in <ref type="table">Table 1</ref>, the local word "football" is cru- cial for distinguishing the sense of word "play". However, in more complex sentences such as "In- vestors played it carefully for maximum advan- tage" 1 , sentence-level information is necessary. Therefore, we extend the co-attention model in a hierarchical fashion to capture both the word-level and sentence-level semantic information.</p><p>The main contributions are listed as follows.</p><p>• We propose a novel way to integrate gloss knowledge into a neural network for WSD via a co-attention mechanism in order to build better representations of context and gloss. In this way, our model can benefit from both labeled data and lexical knowledge.</p><p>• We further extend the attention mechanism into a hierarchical architecture, since both word-level and sentence-level information are crucial to disambiguating the word sense.</p><p>• We conduct a series of experiments, which show that our models outperform the state-of- the-art systems on several standard English all-words WSD test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Lexical knowledge is a fundamental component of Word Sense Disambiguation and provides rich resources which are essential to associate senses with words <ref type="bibr" target="#b31">(Navigli, 2009)</ref>. Unsupervised knowledge-based methods have shown the effec- tiveness of textual knowledge such as gloss <ref type="bibr" target="#b26">(Lesk, 1986;</ref>) and the structural knowl- edge ( <ref type="bibr" target="#b17">Agirre et al., 2014</ref>) of the lexical databases. However, the prime short- coming of knowledge-based methods is that they have worse performance than supervised methods, but they have wider coverage for the polysemous words, thanks to the use of large-scale knowledge resources <ref type="bibr" target="#b31">(Navigli, 2009)</ref>. There are many other tasks such as Chinese Word Segmentation ( <ref type="bibr" target="#b43">Zhang et al., 2018)</ref>, Lan- guage Modeling ( <ref type="bibr" target="#b18">Ahn et al., 2016)</ref>, and LSTMs ( <ref type="bibr" target="#b41">Xu et al., 2016;</ref><ref type="bibr" target="#b42">Yang and Mitchell, 2017)</ref> show that integrating knowledge and labeled data into a unified system can achieve better performance than other methods which only learn from large scale labeled data. Therefore, it's a promising and 1 Play in the sentence means behave in a certain way. challenging study to integrate labeled data and lex- ical knowledge into a unified system.</p><p>A few recent studies of WSD have exploited several ways to incorporate lexical resources into supervised systems. In the field of traditional feature-based methods <ref type="bibr" target="#b20">(Chen et al., 2015;</ref><ref type="bibr" target="#b37">Rothe and Schütze, 2015)</ref>, they usually utilize knowl- edge (to train word sense embeddings) as features of the classifier like the support vector machine (SVM). In the field of neural-based methods, Ra- ganato et al. (2017a) regard lexical resource LEX which is extracted from the WordNet as an aux- iliary classification task, and propose a multi-task learning framework for WSD and LEX.  integrate the context and glosses of the tar- get word into a unified framework via a memory network. It encodes the context and glosses of the target word separately, and then models the se- mantic relationship between the context vector and gloss vector in the memory module. What's more,  utilize much more knowledge about gloss via its semantic relations such as hy- pernymy and hyponymy in WordNet. All studies listed above show that integrating lexical resources especially gloss into supervised systems of WSD can significantly improve the performance. There- fore, we follow this direction and seek a new way of better integrating gloss knowledge.</p><p>Instead of building representations for context and gloss separately, we use the inner connection between the gloss and the context to promote the representation of each other. The interaction pro- cess can be modeled by a co-attention mechanism which has made great progress in the question an- swering task <ref type="bibr" target="#b40">(Xiong et al., 2016;</ref><ref type="bibr" target="#b38">Seo et al., 2016;</ref><ref type="bibr" target="#b22">Hao et al., 2017;</ref><ref type="bibr" target="#b27">Lu et al., 2016)</ref>. We are enlight- ened by this iterative procedure and introduce it into WSD. We then make some adaptations to the output of the original co-attention model to get the score of each word sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Co-Attention Model for WSD</head><p>In this section, we first give an overview of the CAN: co-attention neural network for WSD <ref type="figure" target="#fig_0">(Fig- ure 1</ref>). And then, we extend it into a hierarchical architecture HCAN <ref type="figure" target="#fig_2">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The overall architecture of the proposed non- hierarchical co-attention model is shown in <ref type="figure" target="#fig_0">Figure  1</ref>  • Input Embedding Layer: First of all, we en- code the input context and each gloss 2 into distributed representations C and G, which are also called embeddings in the paper. In <ref type="figure" target="#fig_0">Figure 1</ref>, if C and G are word embeddings, we call the model CAN w in the paper. If C and G are sentence embeddings, we call the model CAN s .</p><p>• Co-Attention Layer: Then, each co- attention mechanism in this layer generates a context vector and a gloss vector according to the corresponding gloss and context rep- resentations. The outputs of the co-attention layer are N pairs of context vector and gloss vector.</p><p>• Output Layer: Finally, the output layer takes the N pairs of context vector and gloss vector as inputs and calculates the score of each word sense. <ref type="figure" target="#fig_0">Figure 1</ref> shows the non-hierarchical co- attention model which generates either word-level representations (CAN w ) or sentence-level repre- sentations (CAN s ). Since both the word-level and sentence-level representations can help to disam- biguate the word sense, we extend CAN into a hierarchical model, named as HCAN ( <ref type="figure" target="#fig_2">Figure 2</ref>). The extensions of each layer are listed as follows:</p><p>1. The input embedding layer is extended to two sub-layers in the hierarchical architecture which encodes both word-level and sentence- level representations.</p><p>2. The co-attention layer is also extended to two attention layers for capturing two different levels' attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input Embedding Layer</head><p>We denote each input sentence (context or gloss) as a sequence of words</p><formula xml:id="formula_0">[x 1 , x 2 , . . . , x Tx ],</formula><p>where T x is the length of the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Word Embedding</head><p>After looking up a pre-trained word embedding matrix E w ∈ R dw×V , we transfer a one-hot vec- tor </p><formula xml:id="formula_1">x i into a d w -dimensional vector e i . We treat [e 1 , e 2 , . . . , e</formula><formula xml:id="formula_2">[e g i 1 , e g i 2 , . . . , e g i m ]</formula><p>, where n and m represent the max length of context and gloss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sentence Embedding</head><p>We utilize a bi-directional long short-term mem- ory network <ref type="bibr">(Bi-LSTM)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Co-Attention Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Co-Attention Mechanism</head><p>The right part of <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the co- attention mechanism which is the most crucial part of the model. The inputs are context representa- tions C ∈ R d×n and gloss representations G ∈ R d×m , where d is the dimension of the input rep- resentation vector. The outputs are the gloss-aware context vector c ∈ R d and the context-aware gloss vector g ∈ R d . Therefore, we can define the co- attention mechanism as a function  Next, we give the detailed definition of the co- attention mechanism function CoAt. We begin to compute a similarity matrix A, in which each element A ij indicates the similarity between i-th context word and j-th gloss word. The similarity matrix A is computed by</p><formula xml:id="formula_3">(c, g) = CoAt(C, G)<label>(1</label></formula><formula xml:id="formula_4">A = C U G ∈ R n×m (2)</formula><p>where U ∈ R d×d is a trainable parameter. Based on the similarity matrix A, we can com- pute the gloss-to-context attention matrix A c and context-to-gloss attention matrix A g .</p><p>Gloss-to-Context Attention. Since each gloss word may focus on different context words, we can generate a context representation which is aware of a particular gloss word. Note that each element of A in the j-th column indicates the similarity between j-th gloss word and each con- text word. Thus, we can get the attention weight for each context word through a softmax function across the column of A:</p><formula xml:id="formula_5">A c :j = sof tmax(A :j )<label>(3)</label></formula><p>where A :j denotes j-th column of A and A c :j de- notes j-th column of A c ∈ R n×m .</p><p>Hence we can get the gloss-aware context rep- resentationsˆCresentationsˆ resentationsˆC by a product of the initial context representations C and attention weight matrix A c :</p><formula xml:id="formula_6">ˆ C = CA c ∈ R d×m<label>(4)</label></formula><p>Note that j-th column inˆCinˆ inˆC means the context representation according to the j-th gloss word. Therefore, we can get the final context vector c by summing across the column ofˆCofˆ ofˆC:</p><formula xml:id="formula_7">c = j ˆ C :j ∈ R d<label>(5)</label></formula><p>Context-to-Gloss Attention. Conversely, each context word may focus on different gloss words, we can generate a gloss representation which is aware of a particular context word. Since each el- ement of A in the i-th row indicates the similarity between i-th context word and each gloss word, we can get the attention weight of each gloss word through a softmax function across the row of A (or across the column of A )</p><formula xml:id="formula_8">A g :j = sof tmax(B :j )<label>(6)</label></formula><p>where B = A , B :j denotes j-th column of B (also j-th row of A) and A g :j denotes j-th column of A g ∈ R m×n . Now we can get the context-aware gloss repre- sentations in the same way as Equation 4:</p><formula xml:id="formula_9">ˆ G = GA g ∈ R d×n<label>(7)</label></formula><p>Note that j-th column inˆGinˆ inˆG denotes the gloss representation according to j-th context word. Therefore, we can get the final gloss vector g by summing across the column ofˆGofˆ ofˆG:</p><formula xml:id="formula_10">g = j ˆ G :j ∈ R d<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Word-Level Co-Attention Layer</head><p>Since there are N glosses according to N different word senses, we use N independent co-attention mechanisms in both word-level and sentence-level co-attention layers. And each layer shares a same parameter U in Equation 2. For i-th word-level co-attention mechanism, the inputs are word em- beddings of the context and i-th gloss (in Sec</p><note type="other">- tion 3.2.1). Define C w = [e c 1 , e c 2 , . . . , e c n ] and G w i = [e g i 1 , e g i 2 , . . . , e g i m ], thus the outputs of i-th word-level co-attention mechanism are computed as (c w i , g w</note><formula xml:id="formula_11">i ) = CoAt(C w , G w i )<label>(9)</label></formula><p>Inspired by the well-known Lesk algorithm <ref type="bibr" target="#b26">(Lesk, 1986)</ref> and its variants ( ), the score of the i-th word sense can be computed as the similarity of the context vector c w i and the gloss vector g w i :</p><formula xml:id="formula_12">β w i = c w i · g w i<label>(10)</label></formula><p>The word-level context embedding vectorˆcvectorˆ vectorˆc w can be computed as the average of the N gloss- aware context vectors c w i :</p><formula xml:id="formula_13">ˆ c w = 1 N N i=1 c w i<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Sentence-Level Co-Attention Layer</head><p>Same to word-level co-attention layer, for the i- th co-attention mechanism, the inputs of sentence- level co-attention layer are Bi-LSTM hidden states of context and the i-th gloss (in Sec- tion 3.2.2). Define</p><formula xml:id="formula_14">C s = [h c 1 , h c 2 , . . . , h c n ] and G s i = [h g i 1 , h g i 2 , . . . , h g i m ]</formula><p>, thus the outputs of i-th sentence-level co-attention mechanism are com- puted as</p><formula xml:id="formula_15">(c s i , g s i ) = CoAt(C s , G s i )<label>(12)</label></formula><p>Like Equation 10, we can also calculate a sentence-level score for the i-th word sense by a dot product of the context vector c s i and the gloss vector g s i :</p><formula xml:id="formula_16">β s i = c s i · g s i<label>(13)</label></formula><p>The sentence-level context embedding vectorˆcvectorˆ vectorˆc s is also computed as the average of N gloss-aware context vectors c s i :</p><formula xml:id="formula_17">ˆ c s = 1 N N i=1 c s i<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Output Layer</head><p>The output layer aims to calculate the scores of N senses of the target word x t and finally outputs a sense probability distribution over the N senses. The final score of each sense is a weighted sum of two values: µ and ν. µ is the similarity score of gloss and context, which reveals the influence of knowledge. ν is generated by the context vector through a linear projection layer, which reveals the influence of labeled data. Finally, the probability distributionˆydistributionˆ distributionˆy over all the senses of the target word is computed asˆy</p><formula xml:id="formula_18">asˆ asˆy = sof tmax(λ xt µ + (1 − λ xt )ν))</formula><p>where λ xt ∈ [0, 1] is the parameter for word x t . For the non-hierarchical model CAN in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, the final score µ and ν are generated only by the outputs of the one level co-attention layer. Specifically, for the word-level co-attention model</p><formula xml:id="formula_19">CAN w : µ = [β w 1 , β w 2 , . . . , β w N ]<label>(15)</label></formula><formula xml:id="formula_20">ν = W xtˆcxtˆ xtˆc w + b xt<label>(16)</label></formula><p>For the sentence-level co-attention model</p><formula xml:id="formula_21">CAN s : µ = [β s 1 , β s 2 , . . . , β s N ]<label>(17)</label></formula><formula xml:id="formula_22">ν = W xtˆcxtˆ xtˆc s + b xt<label>(18)</label></formula><p>For the hierarchical co-attention model HCAN in <ref type="figure" target="#fig_2">Figure 2</ref>, the outputs of the word and sen- tence level layer are merged together to generate the final results. Therefore, the final similarity score between i-th gloss and context is computed as the weighted sum of word-level score β w i and sentence-level score β s i :</p><formula xml:id="formula_23">β i = αβ w i + (1 − α)β s i<label>(19)</label></formula><p>Meanwhile, the final context embedding vector is also generated by a combination of two lev- els' context embedding vector: ˆ c w andˆcandˆ andˆc s . In order to transfer from word-level encoding space to sentence-level encoding space, we introduce a non-linear projection layer on top of the word- level context vectorˆcvectorˆ vectorˆc w . Therefore, the final con- text embedding vectorˆcvectorˆ vectorˆc is generated byˆc</p><formula xml:id="formula_24">byˆ byˆc = tanh(W ˆ c w + b) + ˆ c s<label>(20)</label></formula><p>In total, for the hierarchical co-attention model HCAN:</p><formula xml:id="formula_25">µ = [β 1 , β 2 , . . . , β N ] (21) ν = W xtˆcxtˆ xtˆc + b xt<label>(22)</label></formula><p>It's noteworthy that in Equation 16, 18 and 22, each ambiguous word x t has its corresponding weight matrix W xt and bias b xt .</p><p>During training, all model parameters θ are jointly learned by minimizing a cross-entropy loss betweenˆybetweenˆ betweenˆy and the true label y.</p><formula xml:id="formula_26">L(θ) = − 1 M M i=1 N i j=1 y ij logˆylogˆ logˆy ij (23)</formula><p>where M is the number of examples in the dataset, N i is the word sense number of i-th example, y ij andˆyandˆ andˆy ij are the true and predict probability of the i-th example belongs to j-th label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Validation and Evaluation Datasets: We evalu- ate our model on several English all-words WSD datasets. For a fair comparison, we use the bench- mark datasets proposed by <ref type="bibr" target="#b36">Raganato et al. (2017b)</ref> which include five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions:</p><p>• Senseval-2 ( <ref type="bibr">Edmonds and Cotton, 2001, SE2)</ref>: It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjec- tives.</p><p>• Senseval-3 task 1 ( <ref type="bibr">Snyder and Palmer, 2004, SE3)</ref>: It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, ad- verbs and adjectives.</p><p>• • SemEval-13 task 12 ( <ref type="bibr">Navigli et al., 2013, SE13)</ref>: It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only.</p><p>• SemEval-15 task 13 (Moro and Navigli, 2015, SE15): It's the latest WSD dataset, which consists of 1022 sense annotations from three heterogeneous domains.</p><p>Training Dataset: SemCor 3.0 is the largest manually annotated corpus for WSD, which was also used by , , <ref type="bibr" target="#b36">Raganato et al. (2017b)</ref>, , , etc. It consists of 226,036 sense annotations from 352 documents, which includes nouns, verbs, adverbs and adjec- tives.</p><p>Knowledge Base: The original WordNet ver- sion of sense inventory for SemCor 3.0, SE2, SE3, SE7, SE13, SE15 <ref type="figure" target="#fig_0">are 1.4, 1.7, 1.7.1, 2.1, 3</ref>.0 and 3.0, respectively. <ref type="bibr" target="#b36">Raganato et al. (2017b)</ref> map all the sense annotations in the training and test datasets to WordNet 3.0 via a semi-automatic method. Therefore, We choose WordNet 3.0 as the sense inventory for extracting the gloss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Noun  .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>We use the validation set (SE7) to find the optimal hyper parameters of our models: the word embed- ding size d w , the hidden state size d s of LSTM, the optimizer, etc. However, since there are no ad- verbs and adjectives in SE7, we randomly sample some adverbs and adjectives from training dataset into SE7 for validation. We use the pre-trained word embeddings 4 . The hidden state size d s is 256. The mini-batch size is set to 32. The opti- mizer is Adam ( <ref type="bibr" target="#b25">Kingma and Ba, 2014</ref>) with 0.001 initial learning rate. In order to avoid over-fitting, we use dropout regularization on the outputs of LSTM and set drop rate to 0.5. Orthogonal ini- tialization is used for initialing weights in LSTM and random uniform initialization with range [- 0.1, 0.1] is used for others. Training runs for up to 50 epochs with early stopping if the validation loss doesn't improve within the last 5 epochs. <ref type="table">Table 3</ref>: F1-score (%) for fine-grained English all-words WSD on the test sets. Bold font indicates best systems. The * represents the systems which don't use any lexical knowledge. The five blocks list the baseline, 2 knowledge- based systems, 2 supervised feature-based systems, 7 neural-based systems and our models, respectively. . <ref type="table">Table 3</ref> shows the results on four test datasets and different parts of speech. Note that all the systems in <ref type="table">Table 3</ref> are trained on SemCor 3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">English all-words results</head><p>In the first block, we show the MFS baseline, which simply selects the most frequent sense in the training dataset.</p><p>In the second block, we show two lat- est knowledge-based (unsupervised) systems. Lesk ext+emb is a variant of the well-known Lesk algorithm <ref type="bibr" target="#b26">(Lesk, 1986)</ref> which computes the over- lap of gloss and context as the score of word sense. <ref type="bibr" target="#b2">Babelfy (Moro et al., 2014</ref>) is a graph- based system performed on BabelNet <ref type="bibr" target="#b33">(Navigli and Ponzetto, 2012</ref>). We can find that MFS is a strong baseline for knowledge-based systems.</p><p>In the third block, we show two traditional su- pervised systems which only learn from labeled data based on manual designed features. IMS ) is a flexible framework which trains K SVM classifiers for K polysemous words. Its variant IMS +emb ( ) adds word embedding features into IMS. Both of them train a dedicated classifier for each word individu- ally. In other words, each target word has its own parameters. Therefore, IMS +emb is a hard to beat system for many neural networks which also only uses labeled data but builds a unified system for all the polysemous words.</p><p>In the fourth block, we show four latest neu- ral networks. Except for Bi-LSTM , which is a baseline for neural models, the others all utilize not only labeled data but also lexical knowledge. Bi- LSTM +att.+LEX ( ) and its variant Bi-LSTM +att.+LEX+P OS are multi-task learning frameworks for WSD, POS tagging and LEX with context self-attention mechanism. GAS ( ) is a gloss-augmented neural net- work in an improved memory network paradigm. The best neural network is GAS ext which extends from GAS and uses more gloss knowledge via the semantic relations in WordNet. <ref type="bibr">5</ref> In the last block, we give the performance of our proposed co-attention models for WSD. We can see that our best model HCAN improves state-of- the-art result by 0.5% on the concatenation of four datasets. Even though we use less gloss knowl- edge than the previous best system GAS ext , our co-attention models can still get the best results on three test datasets. For non-hierarchical mod- els, CAN s performs much better than the CAN w , which reveals that global sentence-level informa- tion is much more useful than local word-level information. Integration of these two levels' in- formation (HCAN) can further boost the perfor- mance. What's more, we find that our best model HCAN performs best on all parts of speech, ex- cept for adverbs. However, there are only 346 ex- amples about adverbs which account for 5% of the four test datasets, thus 1% drop on adverbs means only 4 examples are wrongly classified which will make little influence on the overall score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this part, we further discuss the impacts of the components of our hierarchical model HCAN. In order to ablate the co-attention mechanism, we re- place the co-attention function CoAt in Equation 1 with a function Avg which simply calculates the average of input representation vectors. Specifi- cally, in function Avg, the outputs c = j C :j and g = j G :j . We re-train HCAN by ablating certain compo- nents:</p><p>• No Attention: We totally replace the co- attention function CoAt with Avg in both word-level and sentence-level co-attention layers. This is the baseline for comparison.</p><p>• W/O Word-level Attention: We replace the word-level co-attention function CoAt with Avg. Note that this ablation model is differ- ent from CAN s , for that the word-level rep- resentation vectorˆcvectorˆ vectorˆc w is used to calculate the final score in this ablation model.</p><p>• W/O Sentence-level Attention: We replace the sentence-level co-attention function CoAt with Avg. Note that this ablation model is not same as CAN w , for the sentence-level repre- sentation vectorˆcvectorˆ vectorˆc s is also used to calculate the final score in this ablation model.</p><p>• W/O Context2Gloss Attention: We remove the attention of generating the context vector, which means all elements in A g are set to 1.</p><p>• W/O Gloss2Context Attention: We remove the attention of generating the gloss vector, which means all elements in A c are set to 1. <ref type="table" target="#tab_5">Table 4</ref> indicates the effectiveness of differ- ent components in the proposed model HCAN. It shows that without any attention mechanism, the overall score declines 2.3%.</p><p>Ablated versions without word, sentence level co-attention decline 0.6% and 2.0%, respectively. It reveals that sentence-level co-attention mech- anism seems much more important to HCAN, which is consistent with the scores of CAN s and CAN w . However, we find that the re- sults of ablated versions without word-level and sentence-level co-attention are worse than CAN s and CAN w . We hypothesize that it is because that the context and gloss vector generated from the layer (or level) which doesn't use attention mech- anism may bring some noise to the final scores.</p><p>Without the context-to-gloss attention, the score declines 1.2% on concatenation of the four test datasets. Conversely, without the gloss-to-context attention, the score declines 0.4%. It is proba- bly due to that the context-to-gloss attention which generates the context-aware gloss vector is more direct to find out the correct word sense.</p><p>In conclusion, the results in <ref type="table" target="#tab_5">Table 4</ref> show that all components in the proposed hierarchical co- attention model HCAN can contribute to boosting the performance of WSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we investigate the problem of in- corporating gloss knowledge into neural network for Word Sense Disambiguation. We find that the gloss can highlight the important words in the con- text, and later contribute to the representation of the context. Meanwhile, context can also help to focus on the words in gloss of the right word sense. Therefore, we propose a co-attention mech- anism to model the gloss-to-context and context- to-gloss attention. Furthermore, in order to cap- ture not only local word-level features but also global sentence-level features, we extend the co- attention model into a hierarchical architecture. The experimental results show that our proposed models achieve the state-of-the-art results on sev- eral standard English all-words WSD datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed co-attention neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Tx ] as the word-level representations of the sentence. Specifically, context's word-level representations are denoted as [e c 1 , e c 2 , . . . , e c n ] and i-th gloss's word-level representations are denoted as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical co-attention model for WSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. It consists of three parts:</figDesc><table>Co-Attention 
Co-Attention 
Co-Attention 

μ 

cN 
ci 
c1 

W 

sum 

ν 

ɡ1 
ɡi 
ɡN 

1 st Gloss 
i th Gloss 
N th Gloss 
Context 

CAN w : 
is word embedding 

Input 
Embedding 
Layer 

Co-Attention 
Layer 

Output 
Layer 

CAN 
s : 
is sentence embedding 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the different parts of speech annotations in English all-words WSD train and test datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ablation study of the proposed model HCAN. 

. 

</table></figure>

			<note place="foot" n="2"> There are N glosses in total, where N is the sense number of the target word in the context. 3. The output layer merges the outputs of the two levels&apos; co-attention layers and generates a sense probability over all word senses. Since the non-hierarchical model CAN is a subset or a simplified version of the hierarchical model HCAN, the next sections are organized to illustrate the hierarchical co-attention model HCAN 3 shown in Figure 2.</note>

			<note place="foot" n="3"> CAN w and CAN s will also be expressed in Section 3.4.</note>

			<note place="foot">Co-Attention Gloss2Context Context2Gloss sum Co-Attention Co-Attention Co-Attention</note>

			<note place="foot" n="4"> We download the pre-trained word embeddings from https://github.com/stanfordnlp/GloVe</note>

			<note place="foot" n="5"> The released code can be found in https://github. com/luofuli/word-sense-disambiguation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Lei Sha, Jianmin Zhang and Junbing Liu for insightful comments and sug-gestions. This paper is supported by NSFC project 61772040 and 61751201. The contact authors are Baobao Chang and Zhifang Sui.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>66.0 63.8 67.1 67.7 49.8 73.1 80.5 65.5</idno>
		<title level="m">SE3 SE13 SE15 Noun Verb Adj Adv All MFS baseline* 65</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lesk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basile</surname></persName>
		</author>
		<idno>0 63.7 66.2 64.6 70.0 51.1 51.7 80.6 64.2</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babelfy (moro</surname></persName>
		</author>
		<idno>0 63.5 66.4 70.3 68.9 50.7 73.2 79.8 66.4</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ims (zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng ; Iacobacci</surname></persName>
		</author>
		<idno>72.2 70.4 65.9 71.5 71.9 56.6 75.9 84.7 70.1</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
	<note>9 69.3 65.3 69.5 70.5 55.8 75.6 82.9 68.9 IMS +emb</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Lstm (</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomonsson</forename><surname>Kågebäck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>* 71.1 68.4 64.8 68.3 69.5 55.9 76.2 82.4 68.4</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bi-Lstm+att</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>+lex (raganato</surname></persName>
		</author>
		<idno>72.0 69.4 66.4 72.4 71.6 57.1 75.6 83.2 69.9</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bi-Lstm+att</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>+lex+p Os (raganato</surname></persName>
		</author>
		<idno>72.0 69.1 66.9 71.5 71.5 57.5 75.0 83.8 69.9</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Luo</surname></persName>
		</author>
		<idno>72.0 70.0 66.7 71.6 71.7 57.4 76.5 83.5 70.1</idno>
		<title level="m">GAS (Linear)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Luo</surname></persName>
		</author>
		<idno>72.1 70.2 67.0 71.8 72.1 57.2 76.0 84.4 70.3</idno>
		<title level="m">GAS (Concatenation)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Luo</surname></persName>
		</author>
		<idno>72.4 70.1 67.1 72.1 71.9 58.1 76.4 84.7 70.4</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>GASext (Linear</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; (</forename><surname>Gasext</surname></persName>
			<affiliation>
				<orgName type="collaboration">Concatenation</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
			<affiliation>
				<orgName type="collaboration">Concatenation</orgName>
			</affiliation>
		</author>
		<idno>2018) 72.2 70.5 67.2 72.6 72.2 57.7 76.6 85.0 70.6 CAN w 72.3 69.8 65.5 71.1 71.1 57.3 76.5 84.7 69.8 CAN s 72.2 70.2 69.1 72.2 73.5 56.5 76.6 80.3 70.9 HCAN 72.8 70.3 68.5 72.8 72.7 58.2 77.4 84.1 71.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<idno>72.8 70.3 68.5 72.8 72.7 58.2 77.4 84.1 71.1</idno>
		<title level="m">Test Datasets Concatenation of Test Datasets System SE2 SE3 SE13 SE15 Noun Verb Adj Adv All Full Model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">No Attention 70.2 68.1 67.6 68</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>2 54.3 74.3 82.1 68.8</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W/O</forename><surname>Word</surname></persName>
		</author>
		<idno>-level Attention 71.5 70.4 68.2 71.7 72.7 57.1 75.3 81.8 70.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W/O</forename><surname>Sentence</surname></persName>
		</author>
		<idno>-level Attention 70.0 69.5 66.8 70.3 71.3 55.2 74.4 83.0 69.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W/O</forename><surname>Context2gloss</surname></persName>
		</author>
		<idno>Attention 70.7 69.7 68.2 71.0 72.2 55.2 75.5 82.7 69.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W/O</forename><surname>Gloss2context</surname></persName>
		</author>
		<idno>Attention 72.3 70.3 68.2 71.9 73.2 56.4 75.4 83.8 70.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random walks for knowledge-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References Eneko Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Oier Lpez De Lacalle, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A neural knowledge language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Sungjin Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1608.00318</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An enhanced lesk word sense disambiguation algorithm through a distributional semantic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalina</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Roceedings of COLING 2014, the International Conference on Computational Linguistics: Technical Papers</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving distributed representation of word sense via wordnet gloss composition and context clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmospheric Measurement Techniques</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5211" to="5251" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Senseval-2: overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An end-toend model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Embeddings for word sense disambiguation: An evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Word sense disambiguation using a bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Salomonsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03568</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries:how to tell a pine cone from an ice cream cone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Acm Special Interest Group for Design of Communication</title>
		<imprint>
			<biblScope unit="page" from="24" to="26" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Incorporating glosses into neural word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semeval2015 task 13: Multilingual all-words sense disambiguation and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="288" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word sense disambiguation:a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 12: Multilingual word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 17: English lexical sample, srl and all words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluations</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A unified evaluation framework and empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01127</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>abs/1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Incorporating loosestructured knowledge into LSTM with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1605.05110</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in lstms for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1436" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural networks incorporating dictionaries for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence</title>
		<meeting>the ThirtySecond AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden, System Demonstrations</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
