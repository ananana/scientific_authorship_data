<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Geometric, Textual and Visual Features for Predicting Prepositions in Image Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Institut de Robòtica i Informàtica Industrial (UPC-CSIC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIRIS</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
							<email>{ying.lu, emmanuel.dellandrea}@ec-lyon.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">LIRIS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Institut de Robòtica i Informàtica Industrial (UPC-CSIC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Ecole Centrale de Lyon</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Geometric, Textual and Visual Features for Predicting Prepositions in Image Descriptions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the role that geometric, tex-tual and visual features play in the task of predicting a preposition that links two visual entities depicted in an image. The task is an important part of the subsequent process of generating image descriptions. We explore the prediction of prepositions for a pair of entities, both in the case when the labels of such entities are known and unknown. In all situations we found clear evidence that all three features contribute to the prediction task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been an increased in- terest in the task of automatic generation of natu- ral language image descriptions at sentence level, compared to earlier work that annotates images with a laundry list of terms ( <ref type="bibr" target="#b2">Duygulu et al., 2002</ref>). The task is important in that such detailed anno- tations are more informative and discriminative compared to isolated textual labels, and are essen- tial for improved text and image retrieval.</p><p>The most standard approach to generating such descriptions involves first detecting instances of pre-defined concepts in the image, and then rea- soning about these concepts to generate image de- scriptions e.g. ( <ref type="bibr">Kulkarni et al., 2011;</ref><ref type="bibr" target="#b20">Yang et al., 2011</ref>). Our work is also based on this paradigm. However, we assume that object instances have already been pre-detected by visual recognisers, and concentrate on a specific subtask of descrip- tion generation. More specifically, given two vi- sual entity instances where one could potentially act as a modifier to the other, we address the prob- lem of identifying the appropriate preposition to connect these two entities <ref type="figure" target="#fig_0">(Figure 1</ref>). The inferred prepositional relations will subsequently act as an *A. Ramisa and J. Wang contributed equally to this work. The main contribution of this paper is therefore to learn to predict the most suitable preposition given its context, and to learn this jointly from im- ages and their descriptions. In particular, we con- centrate on learning from (i) geometric relations between two visual entities from image annota- tions; (ii) textual features from textual descrip- tions; (iii) visual features from images. Previous work exists <ref type="bibr" target="#b20">(Yang et al., 2011</ref>) that uses text cor- pora to 'guess' the prepositions given the context without considering the appropriate spatial rela- tions between the entities in the image, signifying a gap between visual content and its correspond- ing description. For example, although person on horse might commonly occur in text corpora, a particular image might actually depict a person standing beside a horse. On the other hand, work that does consider the image content for generat- ing prepositions <ref type="bibr">(Kulkarni et al., 2011;</ref><ref type="bibr" target="#b3">Elliott and Keller, 2013)</ref> map geometric relations to a limited set of prepositions using manually defined rules, not as humans would naturally use them with a richer vocabulary. We would like to have the best of both worlds, by considering image content as well as textual information to select the preposi- tion best used to express the relation between two entities. Our hypothesis is that the combination of geometric, textual and visual features can help with the task of predicting the most appropriate preposition, since incorporating geometric and vi- sual information should help generate a relation that is consistent with the image content, whilst incorporating textual information should help gen- erate a description that is consistent with natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Natural Language Processing Community has significant interest in different aspects of prepo- sitions. The Prepositions Project ( <ref type="bibr" target="#b14">Litkowski and Hargraves, 2005</ref>) analysed and produced a lex- icon of English prepositions and their senses, and subsequently used them in the Word Sense Disambiguation of Prepositions task in <ref type="bibr">SemEval2007 (Litkowski and</ref><ref type="bibr" target="#b15">Hargraves, 2007)</ref>.</p><p>In SemEval-2012, <ref type="bibr" target="#b8">Kordjamshidi et al. (2012)</ref> intro- duce the more fine-grained task of spatial role labelling to detect and classify spatial relations expressed by triples (trajector, landmark, spa- tial indicator). In the latest edition of SemEval- 2015, the SpaceEval task ( <ref type="bibr" target="#b19">Pustejovsky et al., 2015)</ref> introduce further tasks of identifying spatial and motion signals, as well as spatial configura- tions/orientation and motion relation.</p><p>In work that links prepositions more strongly to image content, <ref type="bibr" target="#b6">Gupta and Davis (2008)</ref> model prepositions implicitly to disambiguate image re- gions, rather than for predicting prepositions. Their work also require manual annotation of prepositional relations. In image description gen- eration work, <ref type="bibr">Kulkarni et al. (2011)</ref> manually map spatial relations to pre-defined prepositions, whilst <ref type="bibr" target="#b20">Yang et al. (2011)</ref> predict prepositions from large- scale text corpora solely based on the complement term, with the prepositions constrained to describ- ing scenes (on the street). <ref type="bibr" target="#b3">Elliott and Keller (2013)</ref> define a list of eight spatial relations and their cor- responding prepositional term for sentence gener- ation. Although they also present alternative mod- els that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. <ref type="bibr" target="#b12">Le et al. (2014)</ref> exam- ine prepositions modifying human actions (verbs), and conclude that these relate to positional infor- mation to a certain extent. Other related work in- clude training classifiers for prepositions with spa- tial relation features to improve image segmenta- tion and detection <ref type="bibr" target="#b5">(Fidler et al., 2013)</ref>; this work is however limited to four prepositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>We formally define the task of predicting prepo- sitions as follows: Let P be the set of possible prepositions. Let L be the set of possible land- mark entities acting as the complement of a prepo- sition, and let T be the set of possible trajector entities modified by the prepositional phrase com- prising a preposition and its landmark 1 . For exam- ple, for the phrase person on bicycle, on would be the preposition, bicycle the landmark, and person the trajector. For this paper, we constrain trajector and landmark to be entities that are visually iden- tifiable in an image since we are interested in dis- covering the role of visual features and geometric configurations between two entities in the prepo- sition prediction task.</p><p>Let</p><formula xml:id="formula_0">D = {d 1 , d 2 , ..., d N } be the set of N ob- servations, where each d i for i = 1, 2..., N is rep- resented by d i = (x i , y i , r i )</formula><p>, where x i and y i are the feature representations for the trajector and the landmark entities respectively, and r i the relative geometric feature between the two visual entities.</p><p>Given d i , the objective of the preposition pre- diction task is to produce a ranked list of preposi- tions (p 1 , p 2 , ...p |P | ) according to how likely they are to express the appropriate spatial relation be- tween the given trajector and landmark entities that are either known (Section 6.1) or only repre- sented by visual features (Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO ( <ref type="bibr" target="#b13">Lin et al., 2014</ref>) and Flickr30k ( <ref type="bibr" target="#b21">Young et al., 2014;</ref><ref type="bibr" target="#b18">Plummer et al., 2015)</ref>. To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based de- pendency parser of <ref type="bibr" target="#b0">Chen and Manning (2014)</ref> as implemented in Stanford CoreNLP ( ). Dependencies signifying prepositional Bounding Box feature (number of dimensions)</p><p>• Vector (x, y) from centroid of trajector to centroid of landmark, normalised by the size of the bounding box enclosing both objects (2)</p><p>• Area of trajector bounding box relative to landmark (1)</p><p>• Aspect ratio of each bounding box (2)</p><p>• Area of each bounding box w.r.t. enclosing box <ref type="formula">(2)</ref> • Intersection over union of the bounding boxes <ref type="formula">(1)</ref> • Euclidean distance between the trajector and landmark bounding boxes, normalised by the image size (1)</p><p>• Area of each bounding box w.r.t. the whole image (2) We consider two variants of trajector and land- mark terms in our experiments: (i) using the provided high level categories as terms (80 for MSCOCO and 8 for Flickr30k); (ii) using the terms occurring in the sentence directly, which constitute a bigger and more realistic challenge. For Flickr30k, the descriptive phrases may cause data sparseness (the furry, black and white dog). Thus, we extracted the lemmatised head word of each phrase, using a 'semantic head' variant of the head finding rules of <ref type="bibr" target="#b1">Collins (2003)</ref> in Stan- ford CoreNLP. Entities from the same coreference chain are denoted with a common head noun cho- sen by majority vote among the group, with ties broken by the most frequent head noun in the cor- pus, and further ties broken at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Features</head><p>Geometric Features: Geometric features be- tween a trajector and a landmark entity are derived from bounding box annotations. We defined an 11-dimensional vector of bounding box features, covering geometric relations such as distance, ori- entation, relative bounding box sizes and overlaps between bounding boxes <ref type="table" target="#tab_0">(Table 1)</ref>. We chose to use continuous features as we felt these may be more powerful and expressive compared to dis- crete, binned features. Despite some of these fea- tures being correlated, we left it to the classifier to determine the most useful features for discrimina- tion without having to withhold any unnecessarily.</p><p>Textual features: We consider two textual fea- tures to encode the trajector and landmark terms w t i and w l i . The first feature is a one-hot indica- tor vector x I i and y I i for the trajector and land- mark respectively, where x I i,t = 1 if index t cor- responds to the trajector term w t i and 0 elsewhere (and similarly for landmark). As data sparseness may be an issue, we also explore an alternative tex- tual feature which encodes the terms as word2vec embeddings ( <ref type="bibr" target="#b17">Mikolov et al., 2013)</ref>. This encodes each term as a vector such that semantically re- lated terms are close in the vector space. This al- lows information to be transferred across seman- tically related terms during training (e.g. infor- mation from person on boat can help predict the preposition that mediates man and boat).</p><p>Image Features: While it is ideal to have vi- sion systems produce a firm decision about the vi- sual entity instance detected in an image, in real- ity it may be beneficial to defer the decision by allowing several possible interpretations of the in- stance being detected. In such cases, we will not have a single concept label for the entity, but in- stead a high-level visual representation. For this scenario, we extracted visual representations from the final layer of a Convolutional Neural Network trained on ImageNet ( <ref type="bibr" target="#b9">Krizhevsky et al., 2012)</ref>, and used them as representations for entity instances in place of textual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Preposition Prediction</head><p>Here we highlight interesting findings from exper- iments performed for the task of predicting prepo- sitions for two different scenarios (Sections 6.1 and 6.2). Detailed results can be found in the sup- plementary material.</p><p>Evaluation metrics. As there may be more than one 'correct' preposition for a given context (per- son on horse and person atop horse), we pro- pose the mean rank of the correct preposition as the main evaluation metric, as it accommodates  <ref type="table">Table 2</ref>: Top: Mean rank of the correct preposition (lower is better). Bottom: Accuracy with different feature configurations. All results are with the original trajector/landmark terms from descriptions. IND stands for Indicator Vectors, W2V for Word2Vec, and GF for Geometric Features. As baseline we rank the prepositions by their relative frequencies in the training dataset. multiple possible prepositions that may be equally valid. For completeness we also report classifica- tion accuracy results.</p><p>Baseline. As baseline, we rank the prepositions by their relative frequencies in the training dataset. We found this to be a sufficiently strong baseline, as ubiquitous prepositions such as with and in tend to occur frequently in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ranking with known entity labels</head><p>In this section, we focus on predicting the best preposition given the geometric and textual fea- tures of the trajector and landmark entities. This simulates the scenario of a vision detector provid- ing a firm decision on the concept label for the detected entities. We use a multi-class logistic regression classifier <ref type="bibr" target="#b4">(Fan et al., 2008</ref>), and con- catenate multiple features into a single vector. We compare high-level categories and terms from de- scriptions as trajector/landmark labels. Preposi- tions are ranked in descending order of the clas- sifier output scores.</p><p>We found a few prepositions (e.g. with) dom- inating the datasets. Thus, we also evaluated our models on a balanced subset where each preposi- tion is limited to a maximum of 50 random test samples. The training samples are weighted ac- cording to their class frequency in order to train non-biased classifiers to predict this balanced test set. The results on both the original and balanced   <ref type="table">Table 3</ref>: Accuracy (acc) and mean rank (rank, with max rank in parenthesis) for each variable of the CRF model, trained using the high-level concept labels. Columns under Prep (known labels) refer to the results of predicting prepositions with the trajector and landmark labels fixed to the correct values.</p><p>test sets are compared. As shown in <ref type="table">Table 2</ref>, the system performed sig- nificantly better than the baseline in most cases. In general, geometric features perform better than the baseline, and when combined with text features further improve the results. In a per-preposition analysis, the geometric features show up to 14% improvement in the mean rank for Flickr30k.</p><p>In feature ablation tests on MSCOCO (bal- anced), we found the y component of the trajector to landmark vector to be important to most prepo- sitions, especially for under, above and on. Other important geometric features include the final two features in <ref type="table" target="#tab_0">Table 1</ref> (Euclidean distance and area).</p><p>The benefit of the word2vec text feature is clear when moving from high-level categories to origi- nal terms from descriptions, where it consistently improves the mean rank (up to 25%). In contrast, the indicator vectors resulted in a less significant improvement, if not worse performance, when us- ing the sparse original terms.</p><p>We also evaluated the relative importance of the trajector and the landmark, by withholding either from the textual feature vector. We found that the landmark plays a larger role in preposition predic- tion as omitting the trajector produces 10%-30% better results than omitting the landmark. <ref type="figure" target="#fig_1">Figure 2</ref> shows the confusion matrices of the best-performing systems. Note that many mis- takes arise from prepositions that are often equally valid (e.g. predicting near instead of next to).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ranking with unknown entity labels</head><p>Here, we investigate the task of jointly predicting prepositions with the entity labels given geomet- ric and visual features (without the trajector and landmark labels). This simulates the scenario of a vision detector output. For this structured pre- diction task, we use a 3-node chain CRF model 2 , with the centre node representing the preposition and the two end nodes representing the trajector and landmark. We use image features for the en- tity nodes, and geometric features for the preposi- tion node (Section 5). Due to computational con- straints only high-level category labels are used, but as seen in Section 6.1, this may actually be hurting the performance. <ref type="table">Table 3</ref> shows the results of the structured model used to predict the most likely (trajector, preposition, landmark) combination. To facili- tate comparison with Section 6.1, column Prep (known labels) shows the results with the trajec- tor and landmark labels as known conditions and fixed to the correct values, thus only needing to predict the preposition. The model achieved excel- lent performance considering the added difficulty of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We explored the role of geometric, textual and visual features in learning to predict a preposi- tion given two bounding box instances in an im- age, and found clear evidence that all three fea- tures play a part in the task. Our system per- forms well even with uncertainties surrounding the entity labels. Future work could include non- prepositional terms like verbs, having preposi- tions modify verbs, adding word2vec embeddings to the structured prediction model, and providing stronger features -whether textual, visual or geo- metric.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Given a subject boy and an object sled and their location in the image, what would the best preposition be to connect the two entities?</figDesc><graphic url="image-1.png" coords="1,314.54,221.96,203.49,152.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Normalised confusion matrices on the balanced test subsets for the two datasets (left: MSCOCO, right: Flickr30k), using geometric features and word2vec with the original terms.</figDesc><graphic url="image-2.png" coords="4,77.73,280.72,210.23,210.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Geometric features derived from bound-
ing boxes. 

relations are retained where both the governor and 
its dependent overlap with the entity mentions in 
the descriptions, and where both mentions have 
corresponding bounding boxes. The MSCOCO 
validation set is further annotated to remove er-
rors arising from dependency parsing (notably PP 
attachment errors), and is used as our clean test 
set. Our final dataset comprises 8,029 training 
and 3,431 test instances for MSCOCO, and 46,847 
training and 20,010 test instances for Flickr30k. 
Details on how the triples were extracted from 
captions and matched to instances in images are 
available in the supplementary material. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>IND W2V GF IND+GF W2V+GF Baseline</head><label></label><figDesc></figDesc><table>Mean rank 

MSCOCO (max rank 17) 
1.45 
1.43 
1.72 
1.44 
1.42 
2.14 
MSCOCO (balanced) 
3.20 
3.10 
4.60 
3.00 
2.90 
5.40 
Flickr30k (max rank 52) 
1.91 
1.87 
2.35 
1.88 
1.85 
2.54 
Flickr30k (balanced) 
11.10 
9.04 15.55 
10.23 
8.90 
15.13 

Accuracy 
MSCOCO 
79.7% 80.3% 68.4% 
79.8% 
80.4% 
40.2% 
MSCOCO (balanced) 
52.5% 54.2% 31.5% 
52.7% 
53.9% 
11.9% 
Flickr30k 
75.4% 75.2% 58.5% 
75.8% 
75.4% 
53.7% 
Flickr30k (balanced) 
24.6% 25.9% 
9.0% 
25.2% 
26.9% 
4.0% 

</table></figure>

			<note place="foot" n="1"> The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011)</note>

			<note place="foot" n="2"> We used the toolbox by Mark Schmidt: http://www. cs.ubc.ca/ ˜ schmidtm/Software/UGM.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by the ERA-Net CHIST-ERA D2K VisualSense project (Spanish MINECO PCIN-2013-047, UK EPSRC EP/K019082/1 and French ANR Grant ANR-12-CHRI-0002-04) and the Spanish MINECO RobInstruct project TIN2014-58178-R. Ying Lu was also supported by the China Scholarship Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image description using visual dependency representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A sentence is worth a thousand pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial role labeling: Towards extraction of spatial relations from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Martijn Van Otterlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="8" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TUHOI: Trento Universal Human Object Interaction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieu-Thu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dublin City University and the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
	<note>Proceedings of the Third Workshop on Vision and Language</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The preposition project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orin</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-SIGSEM Workshop on The Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 06: Word-sense disambiguation of prepositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orin</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hargraves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno>abs/1505.04870</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Dworman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Yocum</surname></persName>
		</author>
		<title level="m">Semeval-2015 task 8: Spaceeval. In Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting><address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="884" to="894" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corpus-guided sentence generation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014-02" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
