<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Semantic Representations using Bayesian Probabilistic Tensor Factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM T.J. Waston Research Yorktown Heights</orgName>
								<orgName type="institution">Columbia University Computer Science New York</orgName>
								<address>
									<postCode>10027, 10598</postCode>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Salwen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM T.J. Waston Research Yorktown Heights</orgName>
								<orgName type="institution">Columbia University Computer Science New York</orgName>
								<address>
									<postCode>10027, 10598</postCode>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM T.J. Waston Research Yorktown Heights</orgName>
								<orgName type="institution">Columbia University Computer Science New York</orgName>
								<address>
									<postCode>10027, 10598</postCode>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM T.J. Waston Research Yorktown Heights</orgName>
								<orgName type="institution">Columbia University Computer Science New York</orgName>
								<address>
									<postCode>10027, 10598</postCode>
									<region>NY, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Semantic Representations using Bayesian Probabilistic Tensor Factorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1522" to="1531"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many forms of word relatedness have been developed, providing different perspectives on word similarity. We introduce a Bayesian probabilistic tensor factoriza-tion model for synthesizing a single word vector representation and per-perspective linear transformations from any number of word similarity matrices. The resulting word vectors, when combined with the per-perspective linear transformation, approximately recreate while also regulariz-ing and generalizing, each word similarity perspective. Our method can combine manually created semantic resources with neural word embeddings to separate synonyms and antonyms, and is capable of generalizing to words outside the vocabulary of any particular perspective. We evaluated the word embeddings with GRE antonym questions, the result achieves the state-of-the-art performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, vector space models (VSMs) have been proved successful in solving various NLP tasks including named entity recognition, part-of-speech tagging, parsing, semantic role- labeling and answering synonym or analogy ques- tions ( <ref type="bibr" target="#b19">Turney et al., 2010;</ref><ref type="bibr" target="#b2">Collobert et al., 2011</ref>). Also, VSMs are reported performing well on tasks involving the measurement of word related- ness ( <ref type="bibr" target="#b19">Turney et al., 2010</ref>). Many existing works are distributional models, based on the Distribu- tional Hypothesis, that words occurring in simi- lar contexts tend to have similar meanings <ref type="bibr" target="#b5">(Harris, 1954</ref>). The limitation is that word vectors de- veloped from distributional models cannot reveal word relatedness if its information does not lie in word distributions. For instance, they are believed to have difficulty distinguishing antonyms from synonyms, because the distribution of antonymous words are close, since the context of antonymous words are always similar to each other <ref type="bibr" target="#b14">(Mohammad et al., 2013)</ref>. Although some research claims that in certain conditions there do exist differ- ences between the contexts of different antony- mous words ( <ref type="bibr" target="#b17">Scheible et al., 2013</ref>), the differences are subtle enough that it can hardly be detected by such language models, especially for rare words.</p><p>Another important class of lexical resource for word relatedness is a lexicon, such as Word- Net <ref type="bibr" target="#b11">(Miller, 1995)</ref> or Roget's Thesaurus <ref type="bibr" target="#b6">(Kipfer, 2009)</ref>. Manually producing or extending lexi- cons is much more labor intensive than generat- ing VSM word vectors using a corpus. Thus, lex- icons are sparse with missing words and multi- word terms as well as missing relationships be- tween words. Considering the synonym / antonym perspective as an example, WordNet answers less than 40% percent of the the GRE antonym ques- tions provided by <ref type="bibr" target="#b13">Mohammad et al. (2008)</ref> di- rectly. Moreover, binary entries in lexicons do not indicate the degree of relatedness, such as the de- gree of lexical contrast between happy and sad or happy and depressed. The lack of such informa- tion makes it less fruitful when adopted in NLP applications.</p><p>In this work, we propose a Bayesian tensor fac- torization model (BPTF) for synthesizing a com- posite word vector representation by combining multiple different sources of word relatedness. The input is a set of word by word matrices, which may be sparse, providing a number indicating the presence or degree of relatedness. We treat word relatedness matrices from different perspectives as slices, forming a word relatedness tensor. Then the composite word vectors can be efficiently obtained by performing BPTF. Furthermore, given any two words and any trained relatedness perspective, we can create or recreate the pair-wise word related- ness with regularization via per-perspective linear transformation.</p><p>This method allows one set of word vectors to represent word relatednesses from many different perspectives (e.g. LSA for topic relatedness / cor- pus occurrences, ISA relation and YAGO type) It is able to bring the advantages from both word re- latedness calculated by distributional models, and manually created lexicons, since the former have much more vocabulary coverage and many varia- tions, while the latter covers word relatedness that is hard to detect by distributional models. We can use information from distributional perspectives to create (if does not exist) or re-create (with regular- ization) word relatedness from the lexicon's per- spective.</p><p>We evaluate our model on distinguishing syn- onyms and antonyms. There are a number of re- lated works ( <ref type="bibr" target="#b7">Lin and Zhao, 2003;</ref><ref type="bibr" target="#b20">Turney, 2008;</ref><ref type="bibr" target="#b13">Mohammad et al., 2008;</ref><ref type="bibr" target="#b14">Mohammad et al., 2013;</ref><ref type="bibr" target="#b22">Yih et al., 2012;</ref><ref type="bibr" target="#b1">Chang et al., 2013)</ref>. A number of sophisticated methods have been applied, produc- ing competitive results using diverse approaches. We use the GRE antonym questions <ref type="bibr" target="#b13">(Mohammad et al., 2008</ref>) as a benchmark, and answer these questions by finding the most contrasting choice according to the created or recreated synonym / antonym word relatedness. The result achieves state-of-the-art performance.</p><p>The rest of this paper is organized as fol- lows. Section 2 describes the related work of word vector representations, the BPTF model and antonymy detection. Section 3 presents our BPTF model and the sampling method. Section 4 shows the experimental evaluation and results with Sec- tion 5 providing conclusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Vector Representations</head><p>Vector space models of semantics have a long his- tory as part of NLP technologies. One widely- used method is deriving word vectors using la- tent semantic analysis (LSA) <ref type="bibr">(Deerwester et al., 1990)</ref>, for measuring word similarities. This pro- vides a topic based perspective on word simi- larity. In recent years, neural word embeddings have proved very effective in improving various NLP tasks (e.g. part-of-speech tagging, chunking, named entity recognition and semantic role label- ing) <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>). The proposed neural models have a large number of variations, such as feed-forward networks ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>, hi- erarchical models <ref type="bibr" target="#b12">(Mnih and Hinton, 2008)</ref>, re- current neural networks <ref type="bibr" target="#b10">(Mikolov, 2012)</ref>, and re- cursive neural networks <ref type="bibr" target="#b18">(Socher et al., 2011</ref>). <ref type="bibr" target="#b9">Mikolov et al. (2013)</ref> reported their vector-space word representation is able to reveal linguistic regularities and composite semantics using sim- ple vector addition and subtraction. For example, "King−Man+Woman" results in a vector very close to "Queen". <ref type="bibr" target="#b8">Luong et al. (2013)</ref> proposed a recursive neural networks model incorporating morphological structure, and has better perfor- mance for rare words.</p><p>Some non-VSM models 1 also generate word vector representations. <ref type="bibr" target="#b22">Yih et al. (2012)</ref> apply po- larity inducing latent semantic analysis (PILSA) to a thesaurus to derive the embedding of words. They treat each entry of a thesaurus as a docu- ment giving synonyms positive term counts, and antonyms negative term counts, and preform LSA on the signed TF-IDF matrix In this way, syn- onyms will have cosine similarities close to one and antonyms close to minus one. <ref type="bibr" target="#b1">Chang et al. (2013)</ref> further introduced Multi- Relational LSA (MRLSA), as as extension of LSA, that performs Tucker decomposition over a three-way tensor consisting of multiple relations (document-term like matrix) between words as slices, to capture lexical semantics. The purposes of MRLSA and our model are similar, but the dif- ferent factorization techniques offer different ad- vantages. In MRLSA, the k-th slice of tensor W is approximated by</p><formula xml:id="formula_0">W :,:,k ≈ X :,:,k = US :,:,k V T ,</formula><p>where U and V are both for the same word list but are not guaranteed (or necessarily desired) to be the same. Thus, this model has the ability to capture asymmetric relations, but this flexibility is a detriment for symmetric relatedness. In order to expand word relatedness coverage, MRLSA needs to choose a pivot slice (e.g. the synonym slice), thus there always must existence such a slice, and the model performance depends on the quality of this pivot slice. Also, while non-completeness is a pervasive issue in manually created lexicons, MRLSA is not flexible enough to treat the un- known entries as missing. Instead it just sets them to zero at the beginning and uses the pivot slice to re-calculate them. In contrast, our method of BPTF is well suited to symmetric relations with many unknown relatedness entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BPTF Model</head><p>Salakhutdinov and Mnih (2008) introduced a Bayesian Probabilistic Matrix Factorization (BPMF) model as a collaborative filtering algo- rithm. <ref type="bibr" target="#b21">Xiong et al. (2010)</ref> proposed a Bayesian Probabilistic Tensor Factorization (BPTF) model which further extended the original model to incorporate temporal factors. They modeled latent feature vector for users and items, both can be trained efficiently using Markov chain Monte Carlo methods, and they obtained competitive results when applying their models on real-world recommendation data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Antonomy Detection</head><p>There are a number of previous works in detect- ing antonymy. <ref type="bibr" target="#b7">Lin and Zhao (2003)</ref> identifies antonyms by looking for pre-identified phrases in corpus datasets. <ref type="bibr" target="#b20">Turney (2008)</ref> proposed a su- pervised classification method for handling analo- gies, then apply it to antonyms by transforming antonym pairs into analogy relations. <ref type="bibr" target="#b13">Mohammad et al. (Mohammad et al., 2008;</ref><ref type="bibr" target="#b14">Mohammad et al., 2013)</ref> proposed empirical approaches consid- ering corpus co-occurrence statistics and the struc- ture of a published thesaurus. Based on the as- sumption that the strongly related words of two words in a contrasting pair are also often antony- mous, they use affix patterns (e.g. "un-", "in-" and "im-") and a thesaurus as seed sets to add con- trast links between word categories. Their best performance is achieved by further manually an- notating contrasting adjacent categories. This ap- proach relies on the Contrast Hypothesis, which will increase false positives even with a carefully designed methodology. Furthermore, while this approach can expand contrast relationships in a lexicon, out-of-vocabulary words still pose a sub- stancial challenge. achieves the state-of-the-art performance in answering GRE antonym questions. In addition to the word vectors generated from PILSA, they use morphology and k-nearest neighbors from dis- tributional word vector spaces to derive the em- beddings for out-of-vocabulary words. The latter is problematic since both synonyms and antonyms are distributionally similar. Their approach is two stage: polarity inducing LSA from a manually created thesaurus, then falling back to morphol- ogy and distributional similarity when the lexicon lacks coverage. In contrast, we focus on fusing the information from thesauruses and automati- cally induced word relatedness measures during the word vector space creation. Then prediction is done in a single stage, from the latent vectors capturing all word relatedness perspectives and the appropriate per-perspective transformation vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Bayesian Probabilistic Tensor Factorization Model</head><p>Our model is a variation of the BPMF model ( <ref type="bibr" target="#b15">Salakhutdinov and Mnih, 2008)</ref>, and is similar to the temporal BPTF model ( <ref type="bibr" target="#b21">Xiong et al., 2010)</ref>. To model word relatedness from multiple perspec- tives, we denote the relatedness between word i and word j from perspective k as R k ij . Then we can organize these similarities to form a three-way tensor R ∈ R N ×N ×K . <ref type="table">Table 1</ref> shows an example, the first slice of the tensor is a N × N matrix consists of 1/-1 corre- sponding to the synonym/antonym entries in the Roget's thesaurus, and the second slice is a N ×N matrix consists of the cosine similarity from neural word embeddings created by <ref type="bibr" target="#b8">Luong et al. (2013)</ref>, where N is the number of words in the vocabu- lary. Note that in our model the entries missing in <ref type="table">Table 1a</ref> do not necessarily need to be treated as zero. Here we use the indicator variable I k ij to denote if the entry R k ij exists (I k ij = 1) or not (I k ij = 0). If K = 1, the BPTF model becomes to BPMF. Hence the key difference between BPTF and BPMF is that the former combines multi- ple complementary word relatedness perspectives, while the later only smooths and generalizes over one.</p><p>We assume the relatedness R k ij to be Gaussian, and can be expressed as the inner-product of three D-dimensional latent vectors:</p><formula xml:id="formula_1">R k ij |V i , V j , P k ∼ N (&lt; V i , V j , P k &gt;, α −1 ),</formula><p>where &lt; ·, ·, · &gt; is a generalization of dot product: and α is the precision, the reciprocal of the vari- ance. V i and V j are the latent vectors of word i and word j, and P k is the latent vector for perspective k.</p><formula xml:id="formula_2">&lt; V i , V j , P k &gt;≡ D d=1 V (d) i V (d) j P (d) k , happy joyful lucky sad depressed happy 1 1 -1 -1 joyful 1 -1 lucky 1 -1 sad -1 -1 -1 1 depressed -1 1 (a)</formula><p>We follow a Bayesian approach, adding Gaus- sian priors to the variables: </p><formula xml:id="formula_3">V i ∼ N (µ V , Λ −1 V ), P i ∼ N (µ P , Λ −1 P ),</formula><formula xml:id="formula_4">p(α) = W(α|ˆWα|ˆ α|ˆW 0 , ν 0 ), p(µ V , Λ V ) = N (µ V |µ 0 , (β 0 Λ V ) −1 )W(Λ V |W 0 , ν 0 ), p(µ P , Λ P ) = N (µ P |µ 0 , (β 0 Λ P ) −1 )W(Λ P |W 0 , ν 0 ),</formula><p>where W(W 0 , ν 0 ) is the Wishart distribution of degree of freedom ν and a D-by-D scale matrix W , andˆWandˆ andˆW 0 is a 1-by-1 scale matrix for α. The graphical model is shown in <ref type="figure" target="#fig_1">Figure 1</ref> (with β 0 set to 1). After choosing the hyper-priors, the only re- maining parameter to tune is the dimension of the latent vectors.</p><p>Due to the existence of prior distributions, our model can capture the correlation between dif- ferent perspectives during the factorization stage, then create or re-create word relatedness using this correlation for regularization and generalization. This advantage is especially useful when such cor- relation is too subtle to be captured by other meth- ods. On the other hand, if perspectives (let's say k and l) are actually unrelated, our model can handle it as well by making P k and P l orthogonal to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>To avoid calculating intractable distributions, we use a numerical method to approximate the re- sults. Here we use the Gibbs sampling algorithm to perform the Markov chain Monte Carlo method. When sampling a block of parameters, all other parameters are fixed, and this procedure is re- peated many times until convergence. The sam- pling algorithm is shown in Algorithm 1.</p><formula xml:id="formula_5">R k ij P k µ P Λ P µ 0 W 0 , ν 0 α V i V j Λ V µ V W 0 , ν 0 µ 0 · · · · · · · · · k = 1, ..., K I k i,j = 1 i = j i, j = 1, ..., N</formula><p>With conjugate priors, and assuming I k i,i = 0, ∀i, k (we do not consider a word's relatedness to itself), the posterior distributions for each block of parameters are:</p><formula xml:id="formula_6">p(α|R, V, P) = W( ˆ W0 * , ˆ ν0 * )<label>(1)</label></formula><p>Where:</p><formula xml:id="formula_7">ˆ ν * 0 = ˆ ν0 + 2 k=1 N i,j=1 I k ij , ( ˆ W * 0 ) −1 = ˆ W −1 0 + 2 k=1 N i,j=1 I k ij (R k ij − &lt; Vi, Vj, P k &gt;) 2 p(µV , ΛV |V) = N (µV |µ * 0 , (β * 0 ΛV ) −1 )W(ΛV |W * 0 , ν * 0 )<label>(2)</label></formula><p>Where:</p><formula xml:id="formula_8">µ * 0 = β0µ0 + N ¯ V β0 + N , β * 0 = β0 + N, ν * 0 = ν0 + N, (W * 0 ) −1 = W −1 0 + N ¯ S + β0N β0 + N (µ0 − ¯ V )(µ0 − ¯ V ) T , ¯ V = 1 N N i=1 Vi, ¯ S = 1 N N i=1 (Vi − ¯ V )(Vi − ¯ V ) T p(µP , ΛP |P) = N (µP |µ * 0 , (β * 0 ΛP ) −1 )W(ΛP |W * 0 , ν * 0 )<label>(3)</label></formula><p>Which has the same form as p(µV , ΛV |V).</p><formula xml:id="formula_9">p(Vi|R, V¬i, P, µV , ΛV , α) = N (µ * i , (Λ * i ) −1 )<label>(4)</label></formula><p>Where:</p><formula xml:id="formula_10">µ * i = (Λ * i ) −1 (ΛV µV + α 2 k=1 N j=1 I k ij R k ij Q jk ), Λ * i = ΛV + α 2 k=1 N j=1 I k ij Q jk Q T jk , Q jk = Vj P k</formula><p>is the element-wise product.</p><formula xml:id="formula_11">p(Pi|R, V, P¬i, µP , ΛP , α) = N (µ * i , (Λ * i ) −1 )<label>(5)</label></formula><p>Where:</p><formula xml:id="formula_12">µ * k = (Λ * k ) −1 (ΛP µP + α N i,j=1 I k ij R k ij Xij), Λ * k = ΛP + α N i,j=1 I k ij XijX T ij , Xij = Vi Vj</formula><p>The influence each perspective k has on the la- tent word vectors is roughly propotional to the number of non-empty entries n k = i,j I k i,j . If one wants to adjust the weight of each slices, this can easily achieved by adjusting (e.g. down sam- pling) the number of entries of each slice sampled at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Out-of-Vocabulary words</head><p>It often occurs that some of the perspectives have greater word coverage than the others. For ex- ample, hand-labeled word relatedness usually has much less coverage than automatically acquired similarities. Of course, it is typically for the hand- labeled perspectives that the generalization is most</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Gibbs Sampling for BPTF</head><p>Initialize the parameters. repeat</p><p>Sample the hyper-parameters α, µ V , Λ V , µ P , Λ P (Equation 1, 2, 3) for i = 1 to N do Sample V i (Equation 4) end for for k = 1 to 2 do Sample P k (Equation 5) end for until convergence desired. In this situation, our model can generalize word relatedness for the sparse perspective. For example, assume perspective k has larger vocabu- lary coverage N k , while perspective l has a smaller coverage N l .</p><p>There are two options for using the high vocab- ulary word relation matrix to generalize over the perspective with lower coverage. The most direct way simply considers the larger vocabulary in the BPTF R ∈ R N k ×N k ×K directly. A more efficient method trains on a tensor using the smaller vocab- ulary R ∈ R N l ×N l ×K , then samples the N k − N l word vectors using Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Predictions</head><p>With MCMC method, we can approximate the word relatedness distribution easily by averaging over a number of samples (instead of calculating intractable marginal distribution):</p><formula xml:id="formula_13">p( ˆ R k ij |R) ≈ 1 M M m=1 p( ˆ R k ij |V m i , V m j , P m k , α m ),</formula><p>where m indicate parameters sampled from differ- ent sampling iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scalability</head><p>The time complexity of training our model is roughly O(n × D 2 ), where n is the number of ob- served entries in the tensor. If one is only inter- ested in creating and re-creating word relatedness of one single slice rather than synthesizing word vectors, then entries in other slices can be down- sampled at every iteration to reduce the training time. In our model, the vector length D is not sensitive and does not necessarily need to be very long. <ref type="bibr" target="#b21">Xiong et al. (2010)</ref> reported in their collab- orative filtering experiment D = 10 usually gives satisfactory performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>In this section, we evaluate our model by answer- ing antonym questions. This task is especially suitable for evaluating our model since the perfor- mance of straight-forward look-up from the the- sauruses we considered is poor. There are two ma- jor limitations:</p><p>1. The thesaurus usually only contains antonym information for word pairs with a strong con- trast.</p><p>2. The vocabulary of the antonym entries in the thesaurus is limited, and does not contain many words in the antonym questions.</p><p>On the other hand, distributional similarities can be trained from large corpora and hence have a large coverage for words. This implies that we can treat the thesaurus data as the first slice, and the distributional similarities as the second slice, then use our model to create / recreate word relatedness on the first slice to answer antonym questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The GRE Antonym Questions</head><p>There are several publicly available test datasets to measure the correctness of our word embed- dings. In order to be able to compare with pre- vious works, we follow the widely-used GRE test dataset provided by <ref type="bibr" target="#b13">(Mohammad et al., 2008)</ref>, which has a development set (consisting of 162 questions) and a test set (consisting of 950 ques- tions). The GRE test is a good benchmark because the words are relatively rare (19% of the words in Mohammad's test are not in the top 50,000 most frequent words from Google Books ( <ref type="bibr" target="#b4">Goldberg and Orwant, 2013)</ref>), thus it is hard to lookup answers from a thesaurus directly with high recall. Below is an example of the GRE antonym question:</p><p>adulterate: a. renounce b. forbid c. purify d. criticize e. correct The goal is to choose the most opposite word from the target, here the correct answer is purify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Resources</head><p>In our tensor model, the first slice (k = 1) con- sists of synonyms and antonyms from public the- sauruses, and the second slice (k = 2) consists of cosine similarities from neural word embeddings (example in Table 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Thesaurus</head><p>Two popular thesauruses used in other research are the Macquarie Thesaurus and the Encarta The- saurus. Unfortunately, their electronic versions are not publicly available. In this work we use two alternatives:</p><p>WordNet Words in WordNet (version 3.0) are grouped into sense-disambiguated synonym sets (synsets), and synsets have links between each other to express conceptual relations. Previ- ous works reported very different look-up perfor- mance using WordNet ( <ref type="bibr" target="#b13">Mohammad et al., 2008;</ref><ref type="bibr" target="#b22">Yih et al., 2012)</ref>, we consider this difference as different understanding of the WordNet struc- ture. By extending "indirect antonyms" defined in WordNet to nouns, verbs and adverbs that similar words share the antonyms,we achieve a look-up performance close to <ref type="bibr" target="#b22">Yih et al. (2012)</ref>. Using this interpretation of WordNet synonym and antonym structure we obtain a thesaurus containing 54,239 single-token words. Antonym entries are present for 21,319 of them with 16.5 words per entry on average, and 52,750 of them have synonym entries with 11.7 words per entry on average.</p><p>Roget's Only considering single-token words, the Roget's Thesaurus <ref type="bibr" target="#b6">(Kipfer, 2009</ref>) contains 47,282 words. Antonym entries are present for 8,802 of them with 4.2 words per entry on av- erage, and 22,575 of them have synonym entries with 20.7 words per entry on average. Although the Roget's Thesaurus has a less coverage on both vocabulary and antonym pairs, it has better look- up precision in the GRE antonym questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Distributional Similarities</head><p>We use cosine similarity of the morphRNN word representations 2 provided by <ref type="bibr" target="#b8">Luong et al. (2013)</ref> as a distributional word relatedness perspective. They used morphological structure in training re- cursive neural networks and the learned mod- els outperform previous works on word similarity tasks, especially a task focused on rare words. The vector space models were initialized from exist- ing word embeddings trained on Wikipedia. We use word embeddings adapted from <ref type="bibr" target="#b2">Collobert et al. (2011)</ref>. This advantage complements the weak- ness of the thesaurus perspective -that it has less coverage on rare words. The word vector data con- tains 138,218 words, and it covers 86.9% of the words in the GRE antonym questions. Combining the two perspectives, we can cover 99.8% of the Dev. Set Test Set Prec. Rec. F 1 Prec. Rec. F 1 WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42 WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60 WordNet MRLSA 0.66 0.65 0.65 0.61 0.59 0.60 Encarta lookup 0.65 0.61 0.63 0.61 0.56 0.59</p><note type="other">Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77 Encarta MRLSA 0.87 0.82 0.84 0.82 0.74 0.78 Encarta PILSA + S2Net + Emebed 0.88 0.87 0.87 0.81 0.80 0.81 W&amp;E MRLSA 0.88 0.85 0.87 0.81 0.77 0.79 WordNet lookup* 0.93 0.32 0.48 0.95 0.33 0.49 WordNet lookup 0.48 0.44 0.46 0.46 0.43 0.44 WordNet BPTF 0.63 0.63 0.63 0.63 0.62 0.62 Roget lookup* 1.00 0.35 0.52 0.99 0.31 0.47 Roget lookup 0.61 0.44 0.51 0.55 0.39 0.45 Roget BPTF</note><p>0.80 0.80 0.80 0.76 0.75 0.76 W&amp;R lookup* 1.00 0.48 0.64 0.98 0.45 0.62 W&amp;R lookup 0.62 0.54 0.58 0.59 0.51 0.55 W&amp;R BPMF 0.59 0.59 0.59 0.52 0.52 0.52 W&amp;R BPTF 0.88 0.88 0.88 0.82 0.82 0.82 GRE antonym question words. Further using mor- phology information from WordNet, the coverage achieves 99.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tests</head><p>To answer the GRE questions, we calculate R 1 ij for word pair (i, j), where i is the target word and j is one of the question's candidates. The candidate with the smallest similarity is then the predicted answer. If a target word is missing in the vocabu- lary, that question will not be answered, while if a choice is missing, that choice will be ignored.</p><p>We first train on a tensor from a subset consist- ing of words with antonym entries, then add all other words using the out-of-vocabulary method described in Section 3. During each iteration, ze- ros are randomly added into the first slice to keep the model from overfitting. In the meantime, the second slice entries is randomly downsampled to match the number of non-empty entries in the first slice. This ensures each perspective has approxi- mately equal influence on the latent word vectors.</p><p>We sample the parameters iteratively, and choose the burn-in period and vector length D ac- cording to the development set. We choose the vector length D = 40, the burn-in period starting from the 30 th iterations, then averaging the relat- edness over 200 runs. The hyper-priors used are µ 0 = 0, ν 0 = ˆ ν 0 = D, β 0 = 1 and W 0 = ˆ W 0 = I (not tuned). Note that <ref type="bibr" target="#b22">Yih et al. (2012)</ref> use a vec- tor length of 300, which means our embeddings save considerable storage space and running time. Our model usually takes less than 30 minutes to meet the convergence criteria (on a machine with an Intel Xeon E3-1230V2 @ 3.3GHz CPU ). In contrast, the MRLSA requires about 3 hours for tensor decomposition <ref type="bibr" target="#b1">(Chang et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>The results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. We list the results of previous works <ref type="bibr" target="#b22">(Yih et al., 2012;</ref><ref type="bibr" target="#b1">Chang et al., 2013</ref>) at the top of the table, where the best performance is achieved by PILSA on Encarta with further discriminative training and embed- ding. For comparison, we adopt the standard first used by <ref type="bibr" target="#b13">(Mohammad et al., 2008)</ref>, where preci- sion is the number of questions answered correctly divided by the number of questions answered. Re- call is the number of questions answered correctly divided by the total number of questions. BPMF (Bayesian Probabilistic Matrix Factorization) re- sult is derived by only keeping the synonym &amp; antonym slice in our BPTF model. By using Roget's and WordNet together, our method increases the baseline look-up recall from 51% to 82% on the test set, while Yih's method increases the recall of Encarta from 56% to 80%. This state-of-the-art performance is achieved with the help of a neural network for fine tuning and multiple schemes of out-of-vocabulary embed- ding, while our method has inherent and straight- forward "out-of-vocabulary embedding". While MRLSA, which has this character as well, only has a recall 77% when combining WordNet and Encarta together.</p><p>WordNet records less antonym relations for nouns, verbs and adverbs, while the GRE antonym questions has a large coverage of them. Al- though by extending these antonym relations us- ing the "indirect antonym" concept achieves better look-up performance than Roget's, in contrast, the BPTF performance is actually much lower. This implies Roget's has better recording of antonym relations. <ref type="bibr" target="#b13">Mohammad et al. (2008)</ref> reproted a 23% F-score look-up performance of WordNet which support this claim as well. Combining WordNet and Roget's together can improve the look-up per- formance further to 59% precision and 51% recall (still not as good as Encarta look-up).</p><p>Notably, if we strictly follow our BPTF ap- proach but only use the synonym &amp; antonym slice (i.e. a matrix factorization model instead of ten- sor factorization model), this single-slice model BPMF has performance that is only slightly bet- ter than look-up. Meanwhile <ref type="figure" target="#fig_1">Figure 1</ref> shows the convergence curves of BPMF and BPTF. BPMF actually has lower MAE after convergence. Such behavior is caused by overfitting of BPMF on the training data. While known entries were recreated well, empty entries were not filled correctly. On the other hand, note that although our BPTF model has a higher MAE, it has much better performance in answering the GRE antonym questions. We in- terpret this as the regularization and generalization effect from other slice(s). Instead of focusing on one-slice training data, our model fills the missing entries with the help of inter-slice relations.</p><p>We also experimented with a linear metric learning method over the generated word vectors (to learn a metric matrix A to measure the word relatedness via V T i AV j ) using L-BFGS. By op- timizing the mean square error on the synonym &amp; antonym slice, we can reduce 8% of the mean square error on a held out test set, and improve the F-score by roughly 0.5% (of a single iteration). Although this method doesn't give a significant improvement, it is general and has the potential to boost the performance in other scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a method to map words into a metric space automatically using thesaurus data, previous vector space models, or other word relatedness matrices as input, which is capable of handling out-of-vocabulary words of any par- ticular perspective. This allows us to derive the relatedness of any given word pair and any per- spective by the embedded word vectors with per- perspective linear transformation. We evaluated the word embeddings with GRE antonym ques- tions, and the result achieves the state-of-the-art performance.</p><p>For future works, we will extend the model and its applications in three main directions. First, in this model we only use a three-way tensor with two slices, while more relations may be able to add into it directly. Possible additional perspec- tive slices include LSA for topic relatedness, and corpus occurrences in engineered or induced se- mantic patterns.</p><p>Second, we will apply the method to other tasks that require completing a word relatedness matrix. We evaluated the performance of our model on creating / recreating one perspective of word re- latedness: antonymy. Perhaps using vectors gen- erated from many kinds of perspectives would im- prove the performance on other NLP tasks, such as term matching employed by textual entailment and machine translation metrics.</p><p>Third, if our model does learn the relation be- tween semantic similarities and distributional sim- ilarities, there may be fruitful information con- tained in the vectors V i and P k that can be ex- plored. One straight-forward idea is that the dot product of perspective vectors P k · P l should be a measurement of correlation between perspectives.</p><p>Also, a straightforward adaptation of our model has the potential ability to capture asymmet- ric word relatedness as well, by using a per- perspective matrix instead of vector for the asym- metric slices (i.e.</p><p>use</p><formula xml:id="formula_14">V T i A k V j instead of D d=1 V (d) i P (d) k V (d) j</formula><p>for calculating word related- ness, where A k is a square matrix).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Yih et al. (2012) and Chang et al. (2013) also applied their vectors on antonymy detection, and Yih et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The graphical model for BPTF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convergence curves of BPMF and BPTF in training the W&amp;R dataset. MAE is the mean absolute error over the synonym &amp; antonym slice in the training tensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Development and test results on the GRE antonym questions. *Note: to allow comparison, in 
look-up we follow the approach used by (Yih et al., 2012): randomly guess an answer if the target word 
is in the vocabulary while none of the choices are. Asterisk indicates the look-up results without random 
guessing. 

</table></figure>

			<note place="foot" n="1"> As defined by Turney et al. (2010), VSM must be derived from event frequencies.</note>

			<note place="foot" n="2"> http://www-nlp.stanford.edu/ lmthang/morphoNLM/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Christopher Kedzie for assisting the Semantic Technologies in IBM Watson seminar course in which this work has been carried out, and Kai-Wei Chang for giving detailed explana-tion of the evaluation method in his work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-relational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Harshman. 1990. Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dataset of syntactic-ngrams over time from a very large corpus of english books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Orwant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (* SEM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="241" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Roget&apos;s 21st Century Thesaurus, Third Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Ann</forename><surname>Kipfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Philip Lief Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying synonyms among distributionally similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-03</title>
		<meeting>IJCAI-03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">14921493</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
			<publisher>November</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing word-pair antonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing lexical contrast. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="590" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uncovering distributional differences between synonyms and antonyms in a word space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silke</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Springorum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A uniform approach to analogies, synonyms, antonyms, and associations. Coling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal collaborative filtering with bayesian probabilistic tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polarity inducing latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1212" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
