<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do Multi-Sense Embeddings Improve Natural Language Understanding?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiweil@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<email>jurafsky@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Do Multi-Sense Embeddings Improve Natural Language Understanding?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while &apos;multi-sense&apos; methods have been proposed and tested on artificial word-similarity tasks, we don&apos;t know if they improve real natural language understanding tasks. In this paper we introduce a multi-sense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language understanding. We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relat-edness, controlling for embedding dimen-sionality. We find that multi-sense embed-dings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition , various forms of sentiment analysis). We discuss how these differences may be caused by the different role of word sense information in each of the tasks. The results highlight the importance of testing embedding models in real applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enriching vector models of word meaning so they can represent multiple word senses per word type seems to offer the potential to improve many language understanding tasks. Most tra- ditional embedding models associate each word type with a single embedding (e.g., <ref type="bibr" target="#b0">Bengio et al. (2006)</ref>). Thus the embedding for homonymous words like bank (with senses including 'sloping land' and 'financial institution') is forced to rep- resent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding.</p><p>Early research pointed out that embeddings could model aspects of word sense <ref type="bibr" target="#b12">(Kintsch, 2001</ref>) and recent research has proposed a number of models that represent each word type by dif- ferent senses, each sense associated with a sense- specific embedding <ref type="bibr" target="#b12">(Kintsch, 2001;</ref><ref type="bibr" target="#b25">Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b18">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b10">Huang et al., 2012;</ref><ref type="bibr" target="#b2">Chen et al., 2014;</ref><ref type="bibr" target="#b22">Pina and Johansson, 2014;</ref><ref type="bibr" target="#b31">Wu and Giles, 2015;</ref><ref type="bibr" target="#b13">Liu et al., 2015)</ref>. Such sense-specific embeddings have shown improved performance on simple artificial tasks like match- ing human word similarity judgments-WS353 ( <ref type="bibr" target="#b26">Rubenstein and Goodenough, 1965</ref>) or MC30 ( <ref type="bibr" target="#b10">Huang et al., 2012)</ref>.</p><p>Incorporating multisense word embeddings into general NLP tasks requires a pipelined architec- ture that addresses three major steps:</p><p>1. Sense-specific representation learning:</p><p>learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet.</p><p>2. Sense induction: given a text unit (a phrase, sentence, document, etc.), infer word senses for its tokens and associate them with corre- sponding sense-specific embeddings.</p><p>3. Representation acquisition for phrases or sentences: learn representations for text units given sense-specific embeddings and pass them to machine learning classifiers.</p><p>Most existing work on multi-sense embeddings emphasizes the first step by learning sense spe-cific embeddings, but does not explore the next two steps. These are important steps, however, since it isn't clear how existing multi-sense em- beddings can be incorporated into and benefit real- world NLU tasks.</p><p>We propose a pipelined architecture to address all three steps and apply it to a variety of NLP tasks: part-of-speech tagging, named entity recog- nition, sentiment analysis, semantic relation iden- tification and semantic relatedness. We find:</p><p>• Multi-sense embeddings give improved per- formance in some tasks (e.g., semantic sim- ilarity for words and sentences, seman- tic relation identification part-of-speech tag- ging), but not others (e.g., sentiment analysis, named entity extraction). In our analysis we offer some suggested explanations for these differences.</p><p>• Some of the improvements for multi-sense embeddings are no longer visible when us- ing more sophisticated neural models like LSTMs which have more flexibility in fil- tering away the informational chaff from the wheat.</p><p>• It is important to carefully compare against embeddings of the same dimensionality.</p><p>• When doing so, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.</p><p>After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its perfor- mance for word similarity, and then various NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Neural embedding learning frameworks represent each token with a dense vector representation, op- timized through predicting neighboring words or decomposing co-occurrence matrices ( <ref type="bibr" target="#b0">Bengio et al., 2006;</ref><ref type="bibr" target="#b3">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b17">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013;</ref><ref type="bibr" target="#b15">Mikolov et al., 2010;</ref><ref type="bibr" target="#b21">Pennington et al., 2014)</ref>. Standard neural models represent each word with a single unique vector representation.</p><p>Recent work has begun to augment the neu- ral paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to aug- ment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence-e.g., the fruit "apple" tends to co-occur with the words "cider, tree, pear" while the homophonous IT company co-occurs with words like "iphone", "Google" or "ipod".</p><p>For example <ref type="bibr" target="#b25">Reisinger and Mooney (2010)</ref> and <ref type="bibr" target="#b10">Huang et al. (2012)</ref> propose ways to develop mul- tiple embeddings per word type by pre-clustering the contexts of each token to create a fixed num- ber of senses for each word, and then relabel- ing each word token with the clustered sense be- fore learning embeddings. <ref type="bibr" target="#b18">Neelakantan et al. (2014)</ref> extend these models by relaxing the as- sumption that each word must have a fixed num- ber of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; <ref type="bibr" target="#b13">Liu et al. (2015)</ref> learns sense/topic specific embeddings by com- bining neural frameworks with LDA topic mod- els. <ref type="bibr" target="#b31">Wu and Giles (2015)</ref> disambiguate sense em- beddings from Wikipedia by first clustering wiki documents. <ref type="bibr" target="#b2">Chen et al. (2014)</ref> turn to external re- sources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary. Other rele- vant work includes <ref type="bibr" target="#b24">Qiu et al. (2014)</ref> who main- tains separate representations for different part-of- speech tags of the same word.</p><p>Recent work is mostly evaluated on the rela- tively artificial task of matching human word sim- ilarity judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Sense-Specific Embeddings</head><p>We propose to build on this previous literature, most specifically <ref type="bibr" target="#b10">Huang et al. (2012)</ref> and <ref type="bibr" target="#b18">Neelakantan et al. (2014)</ref>, to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense. Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is suffi- ciently different from its early senses. Such a line of thinking naturally points to Chinese Restau- rant Processes (CRP) ( <ref type="bibr" target="#b1">Blei et al., 2004;</ref><ref type="bibr" target="#b30">Teh et al., 2006</ref>) which have been applied in the related field of word sense induction. In the analogy of CRP, the current word could either sit at one of the existing tables (belonging to one of the exist- ing senses) or choose a new table (a new sense). The decision is made by measuring semantic re- latedness (based on local context information and global document information) and the number of customers already sitting at that table (the popu- larity of word senses). We propose such a model and show that it improves over the state of the art on a standard word similarity task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Chinese Restaurant Processes</head><p>We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers ( <ref type="bibr" target="#b1">Blei et al., 2004;</ref><ref type="bibr" target="#b30">Teh et al., 2006;</ref><ref type="bibr" target="#b23">Pitman, 1995)</ref>. CRP can be viewed as a practical interpretation of Dirichlet Processes <ref type="bibr" target="#b6">(Ferguson, 1973)</ref> for non- parametric clustering. In the analogy, each data point is compared to a customer in a restaurant. The restaurant has a series of tables t, each of which serves a dish d t . This dish can be viewed as the index of a cluster or a topic. The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choos- ing a new cluster based on the following probabil- ity distribution: P r(t w = t) ∝ N t P (w|d t ) if t already exists γP (w|d new ) if t is new (1) where N t denotes the number of customers al- ready sitting at table t and P (w|d t ) denotes the probability of assigning the current data point to cluster d t . γ is the hyper parameter controlling the preference for sitting at a new table.</p><p>CRPs exhibit a useful "rich get richer" prop- erty because they take into account the popular- ity of different word senses. They are also more flexible than a simple threshold strategy for set- ting up new clusters, due to the robustness intro- duced by adopting the relative ratio of P (w|d t ) and P (w|d new ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Incorporating CRP into Distributed</head><p>Language Models</p><p>We describe how we incorporate CRP into a stan- dard distributed language model 1 .</p><p>As in the standard vector-space model, each to- ken w is associated with a K dimensional global embedding e w . Additionally, it is associated with a set of senses Z w = {z 1 w , z 2 w , ..., z |Zw| w } where |Z w | denotes the number of senses discovered for word w. Each sense z is associated with a distinct sense-specific embedding e z w . When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current to- ken given its context as in standard language mod- els using the global vector e w : <ref type="bibr" target="#b3">and Weston, 2008)</ref> and CBOW, where g(e neigh ) denotes a function that projects the con- catenation of neighboring vectors to a vector with the same dimension as e w for SENNA and the bag-or-word averaging for CBOW ( <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref>.</p><formula xml:id="formula_0">p(e w |e neigh ) = F (e w , e neigh ) (2) F() can take different forms in different learn- ing paradigms, e.g., F = w ∈neigh p(e w , e w ) for skip-gram or F = p(e w , g(e w )) for SENNA (Collobert</formula><p>Unlike traditional one-word-one-vector frame- works, e neigh includes sense information in addi- tion to the global vectors for neighbors. e neigh can therefore be written as 2 .</p><p>e neigh = {e n−k , , ..., e n−1 , e n+1 , ..., e n−k } (3) Next we would use CRP to decide which sense the current occurrence corresponds to, or construct a new sense if it is a new meaning that we have not encountered before. Based on CRP, the probabil- ity that assigns the current occurrence to each of the discovered senses or a new sense is given by:</p><formula xml:id="formula_1">P r(z w = z) ∝      N w z P (e z w |context) if z already exists γP (w|z new ) if z is new (4)</formula><p>where N w z denotes the number of times already assigned to sense z for token w. P (e z w |context) denotes the probability that current occurrence be- longing to (or generated by) sense z.</p><p>The algorithm for parameter update for the one token predicting procedure is illustrated in <ref type="figure" target="#fig_0">Figure 01</ref>: Input : Token sequence {w n , w neigh }. 02: Update parameters involved in Equ (3)(4) based on current word prediction. 03: Sample sense label z from CRP. 04: If a new sense label z is sampled: 05:</p><p>-add z to Z wn 06:</p><p>-e z wn = argmax p(w n |z m ) 07: else: update parameters involved based on sampled sense label z. 1: Line 2 shows parameter updating through pre- dicting the occurrence of current token. Lines 4-6 illustrate the situation when a new word sense is detected, in which case we would add the newly detected sense z into Z wn . The vector representa- tion e z w for the newly detected sense would be ob- tained by maximizing the function p(e z w |context). As we can see, the model performs word-sense clustering and embedding learning jointly, each one affecting the other. The prediction of the global vector of the current token (line2) is based on both the global and sense-specific embeddings of its neighbors, as will be updated through pre- dicting the current token. Similarly, once the sense label is decided (line7), the model will adjust the embeddings for neighboring words, both global word vectors and sense-specific vectors.</p><p>Training We train embeddings using Giga- word5 + Wikipedia2014. The training approach is implemented using skip-grams (SG) ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>). We induced senses for the top 200,000 most frequent words (and used a unified "unknown" token for other less-frequent tokens). The window size is set to 11. We iterate three times over the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Obtaining Word Representations for NLU tasks</head><p>Next we describe how we decide sense labels for tokens in context. The scenario is treated as a in- ference procedure for sense labels where all global word embeddings and sense-specific embeddings are kept fixed. Given a document or a sentence, we have an objective function with respect to sense labels by multiplying Eq.2 over each containing token.</p><p>Computing the global optimum sense labeling- in which every word gets an optimal sense label- requires searching over the space of all senses for all words, which can be expensive. We therefore chose two simplified heuristic approaches:</p><p>• Greedy Search: Assign each token the lo- cally optimum sense label and represent the current token with the embedding associated with that sense.</p><p>• Expectation: Compute the probability of each possible sense for the current word, and represent the word with the expectation vec- tor:</p><formula xml:id="formula_2">e w = z∈Zw p(w|z, context) · e z w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Word Similarity Evaluation</head><p>We evaluate our embeddings by comparing with other multi-sense embeddings on the standard ar- tificial task for matching human word similarity judgments.</p><p>Early work used similarity datasets like WS353 ( <ref type="bibr" target="#b7">Finkelstein et al., 2001</ref>) or RG ( <ref type="bibr" target="#b26">Rubenstein and Goodenough, 1965)</ref>, whose context-free nature makes them a poor evaluation. We therefore adopt Stanford's Contextual Word Similarities (SCWS) ( <ref type="bibr" target="#b10">Huang et al., 2012)</ref>, in which human judgments are associated with pairs of words in context. Thus for example "bank" in the context of "river bank" would have low relatedness with "deficit" in the context "financial deficit".</p><p>We first use the Greedy or Expectation strate- gies to obtain word vectors for tokens given their context. These vectors are then used as input to get the value of cosine similarity between two words.</p><p>Performances are reported in <ref type="table">Table 1</ref>. Con- sistent with earlier work (e.g.., <ref type="bibr" target="#b18">Neelakantan et al. (2014)</ref>), we find that multi-sense em- beddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG). As expected, per- formance is not as high when global level in- formation is ignored when choosing word senses (SG+Greedy) as when it is included (SG+Expect), as neighboring words don't provide sufficient in- formation for word sense disambiguation.</p><p>To note, the proposed CRF models work a little better than earlier baselines, which gives some ev- idence that it is sufficiently strong to stand in for this class of multi-sense models and serves as a promise for being extended to NLU tasks.</p><p>Visualization <ref type="table" target="#tab_1">Table 2</ref> shows examples of se- mantically related words given the local context. Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity. Like earlier models (e.g., <ref type="bibr" target="#b18">Neelakantan et al. (2014)</ref>)., the model can disambiguate different word senses (in examples like bank, rock and apple) based on their local context; although of course the model is also capable of dealing with polysemy-senses that are less distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments on NLP Tasks</head><p>Having shown that multi-sense embeddings im- prove word similarity tasks, we turn to ask whether they improve real-world NLU tasks: POS tagging, NER tagging, sentiment analysis at the phrase and sentence level, semantic relationship identification and sentence-level semantic related- ness. For each task, we experimented on the fol- lowing sets of embeddings, which are trained us- ing the word2vec package on the same corpus:</p><p>• Standard one-word-one-vector embeddings from skip-gram (50d).</p><p>• Sense disambiguated embeddings from Sec- tion 3 and 4 using Greedy Search and Expec- tation (50d)</p><p>• The concatenation of global word embed- dings and sense-specific embeddings (100d).</p><p>• Standard one-word-one-vector skip-gram embeddings with dimensionality doubled (100d) (100d is the correct corresponding baseline since the concatenation above doubles the dimensionality of word vectors)</p><p>• Embeddings with very high dimensionality (300d).</p><p>As far as possible we try to perform an apple- to-apple comparison on these tasks, and our goal is an analytic one-to investigate how well se- mantic information can be encoded in multi-sense embeddings and how they can improve NLU performances-rather than an attempt to create state-of-the-art results. Thus for example, in tag- ging tasks (e.g., NER, POS), we follow the proto- cols in <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) using the concate- nation of neighboring embeddings as input fea- tures rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in <ref type="bibr" target="#b21">Pennington et al. (2014)</ref>. Or for experiments on senti- ment and other tasks where sentence level embed- dings are required we only employ standard recur- rent or recursive models for sentence embedding rather than models with sophisticated state-of-the- art methods (e.g., <ref type="bibr" target="#b29">Tai et al. (2015;</ref><ref type="bibr" target="#b11">Irsoy and Cardie (2014)</ref>).</p><p>Significance testing for comparing models is done via the bootstrap test <ref type="bibr" target="#b5">(Efron and Tibshirani, 1994)</ref>. Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expec- tation inference (50d) and one-vector embedding (100d) versus Expectation (100d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Tasks</head><p>Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data. We follow the protocols in <ref type="bibr" target="#b4">Collobert et al. (2011)</ref>, using the concatenation of neighboring embeddings as input to a multi-layer neural model. We employ a five-layer neural architecture, comprised of an input layer, three convolutional layers with rectifier linear activation function and a softmax output layer. Training is done by gradient descent with minibatches where each sentence is treated as one batch. Learning rate, window size, number of hidden units of hidden layers, L2 regulariza- tions and number of iterations are tuned on the development set.</p><p>Part-of-Speech Tagging We use Sections 0-18 of the Wall Street Journal (WSJ) data for train-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest Neighbors</head><p>Apple is a kind of fruit.</p><p>pear, cherry, mango, juice, peach, plum, fruit, cider, apples, tomato, orange, bean, pie Apple releases its new ipads. microsoft, intel, dell, ipad, macintosh, ipod, iphone, google, computer, imac, hardware He borrowed the money from banks.</p><p>banking, credit, investment, finance, citibank, currency, assets, loads, imf, hsbc along the shores of lakes, banks of rivers land, coast, river, waters, stream, inland, area, coasts, shoreline, shores, peninsula Basalt is the commonest volcanic rock. boulder, stone, rocks, sand, mud, limestone, volcanic, sedimentary, pelt, lava, basalt Rock is the music of teenage rebellion. band, pop, bands, song, rap, album, jazz. blues, singer, hip-pop, songs, guitar, musician     <ref type="formula">(2013)</ref> we obtained em- beddings for tree nodes by using a recursive neu- ral network model, where the embedding for par- ent node is obtained in a bottom-up fashion based on its children. The embeddings for each parse tree constituent are output to a softmax layer; see <ref type="bibr" target="#b28">Socher et al. (2013)</ref>.</p><p>We focus on the standard version of recursive neural models. Again we fixed word embeddings to each of the different embedding settings de- scribed above <ref type="bibr">3</ref> . Similarly, we adopted AdaGrad with mini-batch. Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set. The number of iterations is treated as a variable to tune and parameters are harvested based on the best performance on the development set.</p><p>Standard <ref type="formula" target="#formula_3">(50)</ref> Greedy <ref type="formula" target="#formula_3">(50)</ref> Expectation (50) 0.818 0.815 (-0.03) 0.820 (+0.02) Standard (100) Global+G <ref type="formula">(100)</ref> Global+E (100) 0.838 0.840 (+0.02) 0.838 (+0.00) Standard <ref type="formula">(300)</ref> 0.854 <ref type="table">Table 6</ref>: Accuracy for Different Models on Sen- timent Analysis (binary classification on Stanford Sentiment Treebank.). P-value 0.250 for 50d and 0.401 for 100d. <ref type="bibr">Classification SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2009</ref>) is to find semantic relationships between pairs of nominals, e.g., in "My [apartment] e1 has a pretty large <ref type="bibr">[kitchen]</ref> e2 " classifying the relation between <ref type="bibr">[apartment]</ref> and <ref type="bibr">[kitchen]</ref> as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classifica- tion problem, with directed relations treated as separate labels; see <ref type="bibr" target="#b8">Hendrickx et al. (2009)</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship</head><p>We follow the recursive implementations de- fined in . The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive mod- els and fed to a softmax classifier. For pure com- parison purpose, we only use embeddings as fea- tures and do not explore other combination of ar- tificial features. We adopt the same training strat- egy as for the sentiment task (e.g., Adagrad, mini- batches, etc).</p><p>Standard <ref type="formula" target="#formula_3">(50)</ref> Greedy <ref type="formula" target="#formula_3">(50)</ref> Expectation (50) 0.748 0.760 (+0.12) 0.762 (+0.14) Standard(100) Global+G <ref type="formula">(100)</ref> Global+E (100) 0.770 0.782 (+0.12) 0.778 (+0.18) Standard(300) 0.798 <ref type="table">Table 7</ref>: Accuracy for Different Models on Se- mantic Relationship Identification. P-value 0.017 for 50d and 0.020 for 100d.</p><p>( <ref type="bibr" target="#b28">Socher et al., 2013</ref>) where word vectors were treated as pa- rameters to optimize.</p><p>Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset <ref type="bibr" target="#b14">(Marelli et al., 2014</ref>) consist- ing of 9927 sentence pairs, split into train- ing(4500)/development(500)/Testing(4927). Each sentence pair is associated with a gold-standard la- bel ranging from 1 to 5, indicating how semanti- cally related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very re- lated).</p><p>In our setting, the similarity between two sen- tences is measured based on sentence-level em- beddings. Let s 1 and s 2 denote two sentences and e s 1 and e s 2 denote corresponding embeddings. e s 1 and e s 2 are achieved through recurrent or re- cursive models (as illustrated in Appendix sec- tion). Again, word embeddings are obtained by simple table look up in one-word-one-vector set- tings and inferred using the Greedy or Expecta- tion strategy in multi-sense settings. We adopt two different recurrent models for acquiring sentence- level embeddings, a standard recurrent model and an LSTM model <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>The similarity score is predicted using a regres- sion model built on the structure of a three layer convolutional model, with concatenation of e s1 and e s2 as input, and a regression score from 1- 5 as output. We adopted the same training strat- egy as described earlier. The trained model is then used to predict the relatedness score between two new sentences. Performance is measured using Pearson's r between the predicted score and gold- standard labels.</p><p>Standard <ref type="formula" target="#formula_3">( 50)</ref> Greedy <ref type="formula" target="#formula_3">(50)</ref> Expectation <ref type="formula" target="#formula_3">(50)</ref>   028 for 50d and 0.042 for 100d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discussions</head><p>Results for different tasks are represented in Ta- bles 3-9. At first glance it seems that multi-sense em- beddings do indeed offer superior performance, since combining global vectors with sense-specific vectors introduces a consistent performance boost <ref type="bibr">Standard(50)</ref> Greedy <ref type="formula" target="#formula_3">(50)</ref> Expectation(50) 0.843 0.848 (+0.05) 0.846 (+0.03) Standard(100) Global+G (100) Global+E <ref type="formula">(100)</ref> 0.850 0.853 (+0.03) 0.854 (+0.04) Standard <ref type="formula">(300)</ref> 0.850 <ref type="table">Table 9</ref>: Pearson's r for Different Models on Se- mantic Relatedness for LSTM Models. P-value 0.145 for 50d and 0.170 for 100d.</p><p>for every task, when compared with the standard (50d) setting. But of course this is an unfair comparison; combining global vector with sense- specific vector doubles the dimensionality of vec- tor to 100, making comparison with standard di- mensionality (50d) unfair. When comparing with standard (100), the conclusions become more nu- anced.</p><p>For every task, the +Expectation method has performances that often seem to be higher than the simple baseline (both for the 50d case or the 100d case). However, only some of these differences are significant.</p><p>(1) Using multi-sense embeddings is signifi- cantly helpful for tasks like semantic relatedness <ref type="table" target="#tab_5">(Tables 7-8</ref>). This is sensible since sentence mean- ing here is sensitive to the semantics of one partic- ular word, which could vary with word sense and which would directly be reflected on the related- ness score.</p><p>(2) By contrast, for sentiment analysis <ref type="table" target="#tab_4">(Tables  5-6</ref>), much of the task depends on correctly identi- fying a few sentiment words like "good" or "bad", whose senses tend to have similar sentiment val- ues, and hence for which multi-sense embeddings offer little help. Multi-sense embeddings might promise to help sentiment analysis for some cases, like disambiguating the word "sound" in "safe and sound" versus "movie sound". But we suspect that such cases are not common, explaining the non- significance of the improvement. Furthermore, the advantages of neural models in sentiment analysis tasks presumably lie in their capability to capture local composition like negation, and it's not clear how helpful multi-sense embeddings are for that aspect.</p><p>(3) Similarly, multi-sense embeddings help for POS tagging, but not for NER tagging <ref type="table" target="#tab_2">(Table 3-4)</ref>. Word senses have long been known to be related to POS tags. But the largest proportion of NER tags consists of the negative not-a-NER ("O") tag, each of which is likely correctly labelable regard- less of whether senses are disambiguated or not (since presumably if a word is not a named entity, most of its senses are not named entities either).</p><p>(4) As we apply more sophisticated models like LSTM to semantic relatedness tasks (in <ref type="table">Table 9</ref>), the advantages caused by multi-sense embeddings disappears.</p><p>(5) Doubling the number of dimensions is suf- ficient to increase performance as much as using the complex multi-sense algorithm. (Of course in- creasing vector dimensionality (to 300) boosts per- formance even more, although at the significant cost of exponentially increasing time complexity.) We do larger one-word-one-vector embeddings do so well? We suggest some hypotheses:</p><p>• though information about distinct senses is encoded in one-word-one-vector embeddings in a mixed and less structured way, we sus- pect that the compositional nature of neural models is able to separate the informational chaff from the wheat and choose what infor- mation to take up, bridging the gap between single vector and multi-sense paradigms. For models like LSTMs which are better at do- ing such a job by using gates to control in- formation flow, the difference between two paradigms should thus be further narrowed, as indeed we found.</p><p>• The pipeline model proposed in the work re- quires sense-label inference (i.e., step 2). We proposed two strategies: GREEDY and EX- PECTATION, and found that GREEDY mod- els perform worse than EXPECTATION, as we might expect 4 . But even EXPECTATION can be viewed as another form of one-word- one-vector models, just one where different senses are entangled but weighted to empha- size the important ones. Again, this suggests another cause for the strong relative perfor- mance of larger-dimensioned one-word-one- vector models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we expand ongoing research into multi-sense embeddings by first proposing a new version based on Chinese restaurant processes that achieves state of the art performance on simple word similarity matching tasks. We then intro- duce a pipeline system for incorporating multi- sense embeddings into NLP applications, and ex- amine multiple NLP tasks to see whether and when multi-sense embeddings can introduce per- formance boosts. Our results suggest that sim- ply increasing the dimensionality of baseline skip-gram embeddings is sometimes sufficient to achieve the same performance wins that come from using multi-sense embeddings. That is, the most straightforward way to yield better perfor- mance on these tasks is just to increase embedding dimensionality.</p><p>Our results come with some caveats. In partic- ular, our conclusions are based on the pipelined system that we introduce, and other multi-sense embedding systems (e.g., a more advanced sense learning model or a better sense label model or a completely different pipeline system) may find stronger effects of multi-sense models. Nonethe- less we do consistently find improvements for multi-sense embeddings in some tasks (part-of- speech tagging and semantic relation identifica- tion), suggesting the benefits of our multi-sense models and those of others. Perhaps the most im- portant implication of our results may be the ev- idence they provide for the importance of going beyond simple human-matching tasks, and testing embedding models by using them as components in real NLP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix</head><p>In sentiment classification and sentence seman- tic relatedness tasks, classification models require embeddings that represent the input at a sentence or phrase level. We adopt recurrent networks (standard ones or LSTMs) and recursive networks in order to map a sequence of tokens with various length to a vector representation.</p><p>Recurrent Networks A recurrent network suc- cessively takes word w t at step t, combines its vec- tor representation e t with the previously built hid- den vector h t−1 from time t − 1, calculates the re- sulting current embedding h t , and passes it to the next step. The embedding h t for the current time t is thus:</p><formula xml:id="formula_3">h t = tanh(W · h t−1 + V · e t )<label>(5)</label></formula><p>where W and V denote compositional matrices. If N s denote the length of the sequence, h Ns repre- sents the whole sequence S.</p><p>Recursive Networks Standard recursive models work in a similar way by working on neighbor- ing words by parse tree order rather than sequence order. They compute the representation for each parent node based on its immediate children re- cursively in a bottom-up fashion until reaching the root of the tree. For a given node η in the tree and its left child η left (with representation e left ) and right child η right (with representation e right ), the standard recursive network calculates e η :</p><formula xml:id="formula_4">e η = tanh(W · e η left + V · e η right )<label>(6)</label></formula><p>Long Short Term Memory (LSTM) LSTM models <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) are defined as follows: given a sequence of inputs X = {x 1 , x 2 , ..., x n X }, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as i t , f t and o t . We nota- tionally disambiguate e and h, where e t denote the vector for an individual text unit (e.g., word or sen- tence) at time step t while h t denotes the vector computed by the LSTM model at time t by com- bining e t and h t−1 . σ denotes the sigmoid func- tion. W ∈ R 4K×2K . The vector representation h t for each time-step t is given by:</p><formula xml:id="formula_5">i t f t o t l t = σ σ σ tanh W · h t−1 e t<label>(7)</label></formula><p>c t = f t · c t−1 + i t · l t (8)</p><formula xml:id="formula_6">h s t = o t · c t<label>(9)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Incorporating CRP into Neural Language Models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Nearest neighbors of words given context. The embeddings from context words are first in-
ferred with the Greedy strategy; nearest neighbors are computed by cosine similarity between word 
embeddings. Similar phenomena have been observed in earlier work (Neelakantan et al., 2014) 

Standard (50) 
Greedy (50) 
Expectation( 50) 
0.852 
0.852 (+0) 
0.854 (+0.02) 
Standard (100) Global+G (100) 
Global+E (100) 
0.867 
0.866 (-0.01) 
0.871 (+0.04) 
Standard (300) 
0.882 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Accuracy for Different Models on Name Entity Recognition. Global+E stands for Global+Expectation inference and Global+G stands for Global+Greedy inference. p-value 0.223 for Standard(50) verse Expectation (50) and 0.310 for Standard(100) verse Expectation (100). ing, sections 19-21 for validation and sections 22-24 for testing. Similar to NER, we trained 5- layer neural models which take the concatenation of neighboring embeddings as inputs. We adopt a similar training and parameter tuning strategy as for POS tagging.</figDesc><table>Standard (50) 
Greedy (50) 
Expectation (50) 
0.925 
0.934 (+0.09) 
0.938 (+0.13) 
Standard (100) Global+G (100) 
Global+E (100) 
0.940 
0.946 (+0.06) 
0.952 (+0.12) 
Standard (300) 
0.954 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy for Different Models on Part of 
Speech Tagging. P-value 0.033 for 50d and 0.031 
for 100d. 

Sentence-level Sentiment Classification (Pang) 
The sentiment dataset of Pang et al. (2002) con-
sists of movie reviews with a sentiment label for 
each sentence. We divide the original dataset 
into training(8101)/dev(500)/testing(2000). Word 
embeddings are initialized using the aforemen-
tioned types of embeddings and kept fixed in the 
learning procedure. Sentence level embeddings 
are achieved by using standard sequence recur-
rent neural models (Pearlmutter, 1989) (for de-
tails, please refer to Appendix section). The ob-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy for Different Models on Sen-
timent Analysis (Pang et al.'s dataset). P-value 
0.442 for 50d and 0.375 for 100d. 

Sentiment Analysis-Stanford Treebank The 
Stanford Sentiment Treebank (Socher et al., 2013) 
contains gold-standard labels for each constituent 
in the parse tree (phrase level), thus allowing us to 
investigate a sentiment task at a finer granularity 
than the dataset in Pang et al. (2002) where 
labels are only found at the top of each sentence, 
The sentences in the treebank were split into a 
training(8544)/development(1101)/testing(2210) 
dataset. 
Following Socher et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Pearson's r for Different Models on Se-
mantic Relatedness for Standard Models. P-value 
0.</table></figure>

			<note place="foot">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1722-1732, Lisbon, Portugal, 17-21 September 2015. c 2015 Association for Computational Linguistics.</note>

			<note place="foot" n="1"> We omit details about training standard distributed models; see Collobert and Weston (2008) and Mikolov et al. (2013).</note>

			<note place="foot" n="2"> For models that predict succeeding words, sense labels for preceding words have already been decided. For models that predict words using both left and right contexts, the labels for right-context words have not been decided yet. In such cases we just use its global word vector to fill up the position.</note>

			<note place="foot" n="3"> Note that this is different from the settings used in</note>

			<note place="foot" n="4"> GREEDY models work in a more aggressive way and likely make mistakes due to the non-global-optimum nature and limited context information</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We would like to thank Sam Bowman, Ignacio Cases, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as anonymous reviewers for their helpful advice on various aspects of this work. We gratefully acknowledge the support of the NSF via award IIS-1514268, the Defense Advanced Re-search Projects Agency (DARPA) Deep Explo-ration and Filtering of Text (DEFT) Program un-der Air Force Research Laboratory (AFRL) con-tract no. FA8750-13-2-0040. Any opinions, find-ings, and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of NSF, DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A bayesian analysis of some nonparametric problems. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas S Ferguson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="page" from="209" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kintsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Predication. Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="202" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TwentyNinth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Topical word embeddings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>SemEval-2014</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thumbs up?: Sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning state space trajectories in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A simple and efficient method to generate word sense representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis Nieto</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6045</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exchangeable and partially exchangeable random partitions. Probability theory and related fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="145" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning word representation considering proximity and ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
