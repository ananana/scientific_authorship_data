<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do You See What I Mean? Visual Resolution of Linguistic Ambiguities</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
							<email>berzak@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="department" key="dep3">CSAIL MIT</orgName>
								<orgName type="department" key="dep4">Weizmann Institute of Science</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
							<email>andrei@0xab.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="department" key="dep3">CSAIL MIT</orgName>
								<orgName type="department" key="dep4">Weizmann Institute of Science</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Harari</surname></persName>
							<email>hararid@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="department" key="dep3">CSAIL MIT</orgName>
								<orgName type="department" key="dep4">Weizmann Institute of Science</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="department" key="dep3">CSAIL MIT</orgName>
								<orgName type="department" key="dep4">Weizmann Institute of Science</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
							<email>shimon.ullman@weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="department" key="dep3">CSAIL MIT</orgName>
								<orgName type="department" key="dep4">Weizmann Institute of Science</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do You See What I Mean? Visual Resolution of Linguistic Ambiguities</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ambiguity is one of the defining characteristics of human languages, and language understand- ing crucially relies on the ability to obtain un- ambiguous representations of linguistic content. While some ambiguities can be resolved using intra-linguistic contextual cues, the disambigua- tion of many linguistic constructions requires in- tegration of world knowledge and perceptual in- formation obtained from other modalities.</p><p>In this work, we focus on the problem of grounding language in the visual modality, and in- troduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context in which the linguistic content is expressed. This type of inference is fre- quently called for in human communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguis- tic content refers to the visual surroundings of the child <ref type="bibr" target="#b22">(Snow, 1972)</ref>.</p><p>Our task is also fundamental to the problem of grounding vision in language, by focusing on phe- nomena of linguistic ambiguity, which are preva- lent in language, but typically overlooked when using language as a medium for expressing un- derstanding of visual content. Due to such ambi- guities, a superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating a correct understanding of the rel- evant visual content. Our task addresses this issue by introducing a deep validation protocol for vi- sual understanding, requiring not only providing a surface description of a visual activity but also demonstrating structural understanding at the lev- els of syntax, semantics and discourse.</p><p>To enable the systematic study of visually grounded processing of ambiguous language, we create a new corpus, LAVA (Language and Vision Ambiguities). This corpus contains sentences with linguistic ambiguities that can only be resolved us- ing external information. The sentences are paired with short videos that visualize different interpre- tations of each sentence. Our sentences encom- pass a wide range of syntactic, semantic and dis-course ambiguities, including ambiguous preposi- tional and verb phrase attachments, conjunctions, logical forms, anaphora and ellipsis. Overall, the corpus contains 237 sentences, with 2 to 3 inter- pretations per sentence, and an average of 3.37 videos that depict visual variations of each sen- tence interpretation, corresponding to a total of 1679 videos.</p><p>Using this corpus, we address the problem of selecting the interpretation of an ambiguous sen- tence that matches the content of a given video. Our approach for tackling this task extends the sentence tracker introduced in ( <ref type="bibr" target="#b20">Siddharth et al., 2014</ref>). The sentence tracker produces a score which determines if a sentence is depicted by a video. This earlier work had no concept of ambi- guities; it assumed that every sentence had a sin- gle interpretation. We extend this approach to rep- resent multiple interpretations of a sentence, en- abling us to pick the interpretation that is most compatible with the video.</p><p>To summarize, the contributions of this paper are threefold. First, we introduce a new task for vi- sually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence's con- tent. Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in vi- sual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous language and vision studies focused on the development of multimodal word and sentence representations ( <ref type="bibr" target="#b2">Bruni et al., 2012;</ref><ref type="bibr" target="#b23">Socher et al., 2013;</ref><ref type="bibr" target="#b21">Silberer and Lapata, 2014;</ref><ref type="bibr" target="#b6">Gong et al., 2014;</ref><ref type="bibr" target="#b13">Lazaridou et al., 2015)</ref>, as well as methods for describing images and videos in natural lan- guage <ref type="bibr" target="#b4">(Farhadi et al., 2010;</ref><ref type="bibr" target="#b12">Kulkarni et al., 2011;</ref><ref type="bibr" target="#b16">Mitchell et al., 2012;</ref><ref type="bibr">Socher et al., 2014;</ref><ref type="bibr" target="#b28">Thomason et al., 2014;</ref><ref type="bibr" target="#b7">Karpathy and Fei-Fei, 2014;</ref><ref type="bibr" target="#b20">Siddharth et al., 2014;</ref><ref type="bibr" target="#b29">Venugopalan et al., 2015;</ref><ref type="bibr" target="#b30">Vinyals et al., 2015)</ref>. While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities.</p><p>Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation ( <ref type="bibr" target="#b0">Barnard et al., 2003)</ref>. How- ever, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution <ref type="bibr" target="#b17">(Ramanathan et al., 2014;</ref><ref type="bibr" target="#b9">Kong et al., 2014</ref>). Our work ex- pands this line of research, and addresses further discourse ambiguities in the interpretation of el- lipsis. More importantly, to the best of our knowl- edge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision.</p><p>The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology <ref type="bibr" target="#b27">(Tanenhaus et al., 1995)</ref>. A considerable fraction of this work focused on the processing of ambiguous language <ref type="bibr" target="#b26">(Spivey et al., 2002;</ref><ref type="bibr" target="#b3">Coco and Keller, 2015)</ref>, providing evidence for the importance of visual information for lin- guistic ambiguity resolution by humans. Such in- formation is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environ- ment <ref type="bibr" target="#b22">(Snow, 1972)</ref>. Over time, children develop mechanisms for grounded disambiguation of lan- guage, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content <ref type="bibr" target="#b8">(Kidd and Holler, 2009)</ref>. Our study leverages such insights to develop a com- plementary framework that enables addressing the challenge of visually grounded disambiguation of language in the realm of artificial intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task</head><p>In this work we provide a concrete framework for the study of language understanding with vi- sual context by introducing the task of grounded language disambiguation. This task requires to choose the correct linguistic representation of a sentence given a visual context depicted in a video. Specifically, provided with a sentence, n candidate interpretations of that sentence and a video that depicts the content of the sentence, one needs to choose the interpretation that corresponds to the content of the video.</p><p>To illustrate this task, consider the example in <ref type="figure">figure 1</ref>, where we are given the sentence "Sam approached the chair with a bag" along with two different linguistic interpretations. In the first in-  <ref type="figure">Figure 1</ref>: An example of the visually grounded language disambiguation task. Given the sentence "Sam approached the chair with a bag", two poten- tial parses, (a) and (b), correspond to two different semantic interpretations. In the first interpretation Sam has the bag, while in the second reading the bag is on the chair. The task is to select the correct interpretation given the visual context (c).</p><p>terpretation, which corresponds to parse 1(a), Sam has the bag. In the second interpretation associ- ated with parse 1(b), the bag is on the chair rather than with Sam. Given the visual context from fig- ure 1(c), the task is to choose which interpretation is most appropriate for the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach Overview</head><p>To address the grounded language disambiguation task, we use a compositional approach for deter- mining if a specific interpretation of a sentence is depicted by a video. In this framework, described in detail in section 6, a sentence and an accom- panying interpretation encoded in first order logic, give rise to a grounded model that matches a video against the provided sentence interpretation.</p><p>The model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words, and trackers which locate objects in video frames. To represent an interpretation of a sen- tence, word models are combined with trackers through a cross-product which respects the seman- tic representation of the sentence to create a single model which recognizes that interpretation.</p><p>Given a sentence, we construct an HMM based representation for each interpretation of that sen- tence. We then detect candidate locations for ob- jects in every frame of the video. Together the re- forestation for the sentence and the candidate ob- ject locations are combined to form a model which can determine if a given interpretation is depicted by the video. We test each interpretation and re- port the interpretation with highest likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Corpus</head><p>To enable a systematic study of linguistic ambi- guities that are grounded in vision, we compiled a corpus with ambiguous sentences describing vi- sual actions. The sentences are formulated such that the correct linguistic interpretation of each sentence can only be determined using external, non-linguistic, information about the depicted ac- tivity. For example, in the sentence "Bill held the green chair and bag", the correct scope of "green" can only be determined by integrating ad- ditional information about the color of the bag. This information is provided in the accompany- ing videos, which visualize the possible interpreta- tions of each sentence. <ref type="figure" target="#fig_1">Figure 2</ref> presents the syn- tactic parses for this example along with frames from the respective videos. Although our videos contain visual uncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting, and hence a video always corresponds to a single candidate representation of a sentence. The corpus covers a wide range of well known syntactic, semantic and discourse ambigu- ity classes. While the ambiguities are associated with various types, different sentence interpreta- tions always represent distinct sentence meanings, and are hence encoded semantically using first or- der logic. For syntactic and discourse ambiguities we also provide an additional, ambiguity type spe- cific encoding as described below.</p><p>• Syntax Syntactic ambiguities include Prepo- sitional Phrase (PP) attachments, Verb Phrase (VP) attachments, and ambiguities in the in- terpretation of conjunctions. In addition to logical forms, sentences with syntactic am- biguities are also accompanied with Context Free Grammar (CFG) parses of the candidate interpretations, generated from a determinis- tic CFG parser.</p><p>• Semantics The corpus addresses several classes of semantic quantification ambigui- ties, in which a syntactically unambiguous sentence may correspond to different logical forms. For each such sentence we provide the respective logical forms.</p><p>• Discourse The corpus contains two types of discourse ambiguities, Pronoun Anaphora and Ellipsis, offering examples comprising two sentences. In anaphora ambiguity cases, an ambiguous pronoun in the second sen- tence is given its candidate antecedents in the first sentence, as well as a corresponding log- ical form for the meaning of the second sen- tence. In ellipsis cases, a part of the second sentence, which can constitute either the sub- ject and the verb, or the verb and the object, is omitted. We provide both interpretations of the omission in the form of a single unam- biguous sentence, and its logical form, which combines the meanings of the first and the second sentences. The corpus videos are filmed in an indoor environment containing background objects and pedestrians. To account for the manner of per- forming actions, videos are shot twice with differ- ent actors. Whenever applicable, we also filmed the actions from two different directions (e.g. ap- proach from the left, and approach from the right). Finally, all videos were shot with two cameras from two different view points. Taking these vari- ations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a to- tal of 1679 videos. The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames).     <ref type="figure">figure 1</ref> in order to generate the corpus.</p><formula xml:id="formula_0">chair(x), chair(y), x = y, person(u), person(v), u = v, move(u, x), move(v, y) Each</formula><p>aim to control for more aspects of the videos than just the main action being performed but they do not provide the range of ambiguities discussed here. The closest dataset is that of <ref type="bibr" target="#b20">Siddharth et al. (2014)</ref> as it controls for object appearance, color, action, and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks. Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable for evaluating the work described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Model</head><p>To perform the disambiguation task, we extend the sentence recognition model of <ref type="bibr" target="#b20">Siddharth et al. (2014)</ref> which represents sentences as compo- sitions of words. Given a sentence, its first order logic interpretation and a video, our model pro- duces a score which determines if the sentence is depicted by the video. It simultaneously tracks the participants in the events described by the sentence while recognizing the events themselves. This al- lows it to be flexible in the presence of noise by integrating top-down information from the sen- tence with bottom-up information from object and property detectors. Each word in the query sen- tence is represented by an HMM ( <ref type="bibr" target="#b1">Baum et al., 1970</ref>), which recognizes tracks (i.e. paths of de- tections in a video for a specific object) that satisfy the semantics of the given word. In essence, this model can be described as having two layers, one in which object tracking occurs and one in which words observe tracks and filter tracks that do not satisfy the word constraints. Given a sentence interpretation, we construct a sentence-specific model which recognizes if a video depicts the sentence as follows. Each pred- icate in the first order logic formula has a cor- responding HMM, which can recognize if that predicate is true of a video given its arguments. Each variable has a corresponding tracker which attempts to physically locate the bounding box corresponding to that variable in each frame of a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PP Attachment</head><p>Sam looked at Bill with a telescope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VP Attachment</head><p>Bill approached the person holding a green chair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conjunction</head><p>Sam and Bill picked up the yellow bag and chair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logical Form</head><p>Someone put down the bags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anaphora</head><p>Sam picked up the bag and the chair. It is yellow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ellipsis</head><p>Sam left Bill. Also Clark.  video. This creates a bipartite graph: HMMs that represent predicates are connected to trackers that represent variables. The trackers themselves are similar to the HMMs, in that they comprise a lat- tice of potential bounding boxes in every frame.</p><p>To construct a joint model for a sentence interpre- tation, we take the cross product of HMMs and trackers, taking only those cross products dictated by the structure of the formula corresponding to the desired interpretation. Given a video, we em- ploy an object detector to generate candidate de- tections in each frame, construct trackers which select one of these detections in each frame, and fi- nally construct the overall model from HMMs and trackers.</p><p>Provided an interpretation and its correspond- ing formula composed of P predicates and V vari- ables, along with a collection of object detections, b frame detection index , in each frame of a video of length T the model computes the score of the video- sentence pair by finding the optimal detection for each participant in every frame. This is in essence the Viterbi algorithm <ref type="bibr" target="#b31">(Viterbi, 1971)</ref>, the MAP al- gorithm for HMMs, applied to finding optimal ob- ject detections j frame variable for each participant, and the optimal state k frame predicate for each predicate HMM, in every frame. Each detection is scored by its con- fidence from the object detector, f and each ob- ject track is scored by a motion coherence metric g which determines if the motion of the track agrees with the underlying optical flow. Each predicate, p, is scored by the probability of observing a par- ticular detection in a given state h p , and by the probability of transitioning between states a p . The structure of the formula and the fact that multi- ple predicates often refer to the same variables is recorded by θ, a mapping between predicates and their arguments. The model computes the MAP estimate as:</p><formula xml:id="formula_1">max j 1 1 ,..., j T 1 . . . j 1 V ,..., j T V max k 1 1 ,..., k T 1 . . . k 1 P ,..., k T P V v=1 T t=1 f (b t j t v ) + T t=2 g(b t−1 j t−1 v , b t j t v )+ P p=1 T t=1 hp(k t p , b t j t θ 1 p , b t j t θ 2 p ) + T t=2 ap(k t−1 p , k t p )</formula><p>for sentences which have words that refer to at most two tracks (i.e. transitive verbs or binary predicates) but is trivially extended to arbitrary ar- ities. <ref type="figure" target="#fig_3">Figure 3</ref> provides a visual overview of the model as a cross-product of tracker models and word models. Our model extends the approach of <ref type="bibr" target="#b20">Siddharth et al. (2014)</ref> in several ways. First, we depart from the dependency based representation used in that work, and recast the model to encode first order logic formulas. Note that some complex first or- der logic formulas cannot be directly encoded in the model and require additional inference steps. This extension enables us to represent ambiguities in which a given sentence has multiple logical in- terpretations for the same syntactic parse.</p><p>Second, we introduce several model compo- nents which are not specific to disambiguation, but are required to encode linguistic constructions that are present in our corpus and could not be handled by the model of <ref type="bibr" target="#b20">Siddharth et al. (2014)</ref>. These new components are the predicate "not equal", disjunc- tion, and conjunction. The key addition among these components is support for the new predicate "not equal", which enforces that two tracks, i.e. objects, are distinct from each other. For example, in the sentence "Claire and Bill moved a chair" one would want to ensure that the two movers are distinct entities. In earlier work, this was not re- quired because the sentences tested in that work were designed to distinguish objects based on con- straints rather than identity. In other words, there might have been two different people but they were distinguished in the sentence by their actions or appearance. To faithfully recognize that two ac- tors are moving the chair in the earlier example, we must ensure that they are disjoint from each other. In order to do this we create a new HMM for this predicate, which assigns low probability to tracks that heavily overlap, forcing the model to fit two different actors in the previous example. By combining the new first order logic based seman- tic representation in lieu of a syntactic represen- tation with a more expressive model, we can en- code the sentence interpretations required to per- form the disambiguation task. <ref type="figure" target="#fig_3">Figure 3</ref>(left) shows an example of two differ- ent interpretations of the above discussed sentence "Claire and Bill moved a chair". Object track- ers, which correspond to variables in the first order logic representation of the sentence interpretation, are shown in red. Predicates which constrain the possible bindings of the trackers, corresponding to predicates in the representation of the sentence, are shown in blue. Links represent the argument structure of the first order logic formula, and de- termine the cross products that are taken between the predicate HMMs and tracker lattices in order to form the joint model which recognizes the en- tire interpretation in a video.</p><p>The resulting model provides a single unified formalism for representing all the ambiguities in table 2. Moreover, this approach can be tuned to different levels of specificity. We can create mod- els that are specific to one interpretation of a sen- tence or that are generic, and accept multiple inter- pretations by eliding constraints that are not com- mon between the different interpretations. This al- lows the model, like humans, to defer deciding on a particular interpretation or to infer that multiple interpretation of the sentence are plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>We tested the performance of the model described in the previous section on the LAVA dataset pre- sented in section 5. Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes. We employed a mix- ture of CNN ( <ref type="bibr" target="#b10">Krizhevsky et al., 2012</ref>) and DPM <ref type="bibr" target="#b5">(Felzenszwalb et al., 2010</ref>) detectors, trained on held out sections of our corpus. For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring func- tion to map both results into the same space. The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set. As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity. Instead, any sentence which con- tains names was automatically converted to one which contains arbitrary "person" labels.</p><p>The sentences in our corpus have either two or three interpretations. Each interpretation has one or more associated videos where the scene was shot from a different angle, carried out either by different actors, with different objects, or in differ- ent directions of motion. For each sentence-video pair, we performed a 1-out-of-2 or 1-out-of-3 clas- sification task to determine which of the interpre- tations of the corresponding sentence best fits that video. Overall chance performance on our dataset is 49.04%, slightly lower than 50% due to the 1- out-of-3 classification examples.</p><p>The model presented here achieved an accuracy of 75.36% over the entire corpus averaged across all error categories. This demonstrates that the model is largely capable of capturing the under- lying task and that similar compositional cross- modal models may do the same. For each of the 3 major ambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for se- mantic ambiguities, and 64.44% for discourse am- biguities.</p><p>The most significant source of model failures are poor object detections. Objects are often ro- tated and presented at angles that are difficult to recognize. Certain object classes like the telescope are much more difficult to recognize due to their small size and the fact that hands tend to largely occlude them. This accounts for the degraded per- formance of the semantic ambiguities relative to the syntactic ambiguities, as many more seman- tic ambiguities involved the telescope. Object de- tector performance is similarly responsible for the lower performance of the discourse ambiguities which relied much more on the accuracy of the person detector as many sentences involve only people interacting with each other without any ad- ditional objects. This degrades performance by re- moving a helpful constraint for inference, accord- ing to which people tend to be close to the objects they are manipulating. In addition, these sentences introduced more visual uncertainty as they often involved three actors.</p><p>The remaining errors are due to the event mod- els. HMMs can fixate on short sequences of events which seem as if they are part of an action, but in fact are just noise or the prefix of another action. Ideally, one would want an event model which has a global view of the action, if an object went up from the beginning to the end of the video while a person was holding it, it's likely that the object was being picked up. The event models used here cannot enforce this constraint, they merely assert that the object was moving up for some number of frames; an event which can happen due to noise in the object detectors. Enforcing such local con- straints instead of the global constraint of the mo- tion of the object over the video makes joint track- ing and event recognition tractable in the frame- work presented here but can lead to errors. Finding models which strike a better balance between local information and global constraints while maintain- ing tractable inference remains an area of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We present a novel framework for studying am- biguous utterances expressed in a visual context. In particular, we formulate a new task for resolv- ing structural ambiguities using visual signal. This is a fundamental task for humans, involving com- plex cognitive processing, and is a key challenge for language acquisition during childhood. We release a multimodal corpus that enables to ad- dress this task, as well as support further inves- tigation of ambiguity related phenomena in visu- ally grounded language processing. Finally, we present a unified approach for resolving ambigu- ous descriptions of videos, achieving good perfor- mance on our corpus.</p><p>While our current investigation focuses on structural inference, we intend to extend this line of work to learning scenarios, in which the agent has to deduce the meaning of words and sentences from structurally ambiguous input. Furthermore, our framework can be beneficial for image and video retrieval applications in which the query is expressed in natural language. Given an ambigu- ous query, our approach will enable matching and clustering the retrieved results according to the dif- ferent query interpretations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Linguistic and visual interpretations of the sentence "Bill held the green chair and bag". In the first interpretation (a,c) both the chair and bag are green, while in the second interpretation (b,d) only the chair is green and the bag has a different color.</figDesc><graphic url="image-4.png" coords="3,310.73,171.88,93.87,70.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>A</head><label></label><figDesc>custom corpus is required for this task be- cause no existing corpus, containing either videos or images, systematically covers multimodal am- biguities. Datasets such as UCF Sports (Ro- driguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accom- panied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed. Datasets for im- age and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013), Someone moved the two chairs. chair(x), chair(y), x = y, person(u), move(u, x), move(u, y) One person moves both chairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (left) Tracker lattices for every sentence participant are combined with predicate HMMs. The MAP estimate in the resulting cross-product lattice simultaneously finds the best tracks and the best state sequences for every predicate. (right) Two interpretations of the sentence "Claire and Bill moved a chair" having different first order logic formulas. The top interpretation corresponds to Bill and Claire moving the same chair, while the bottom one describes them moving different chairs. Predicates are highlighted in blue at the top and variables are highlighted in red at the bottom. Each predicate has a corresponding HMM which recognizes its presence in a video. Each variable has a corresponding tracker which locates it in a video. Lines connect predicates and the variables which fill their argument slots. Some predicates, such as move and =, take multiple arguments. Some predicates, such as move, are applied multiple times between different pairs of variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 listsNNP2] V DT JJ NN1 and NN2. 40 NNP V DT NN1 or DT NN2 and DT NN3.</head><label>2</label><figDesc></figDesc><table>examples of the different ambiguity 
classes, along with the candidate interpretations of 
each example. 
The corpus is generated using Part of Speech 
(POS) tag sequence templates. For each template, 
the POS tags are replaced with lexical items from 
the corpus lexicon, described in table 3, using all 
the visually applicable assignments. This gener-
ation process yields an overall of 237 sentences, 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>An overview of the different ambiguity types, along with examples of ambiguous sentences with their linguistic and visual interpretations. Note that similarly to semantic ambiguities, syntactic and discourse ambiguities are also provided with first order logic formulas for the resulting sentence interpretations. Table 4 shows additional examples for each ambiguity type, with frames from sample videos corresponding to the different interpretations of each sentence.</figDesc><table>Syntactic Category Visual Category Words 
Nouns 
Objects, People 
chair, bag, telescope, someone, proper names 

Verbs 
Actions 
pick up, put down, hold, move (transitive), look at, approach, leave 

Prepositions 
Spacial Relations with, left of, right of, on 

Adjectives 
Visual Properties yellow, green 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : The lexicon used to instantiate the templates in</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Examples of the six ambiguity classes described in table 2. The example sentences have at least two interpretations, which are depicted by different videos. Three frames from each such video are shown on the left and on the right below each sentence.</figDesc><table>predicate 1 

predicate W 

.. 
. 
.. 
. 
.. 
. 
.. 
. 

. . . 

. . . 

. . . 

. . . 

h 
a 

× · · · × 

.. 
. 
.. 
. 
.. 
. 
.. 
. 

. . . 

. . . 

. . . 

. . . 

h 
a 

× 

.. 
. 
.. 
. 
.. 
. 
.. 
. 

. . . 

. . . 

. . . 

. . . 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF-1231216. SU was also supported by ERC Ad-vanced Grant 269627 Digital Baby.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Word sense disambiguation with pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 2003 workshop on Learning word meaning from non-linguistic data</title>
		<meeting>the HLT-NAACL 2003 workshop on Learning word meaning from non-linguistic data</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The interaction of visual and linguistic saliency during syntactic ambiguity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Coco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="74" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Pedro F Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
	<note>Julia Hockenmaier, and Svetlana Lazebnik</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Children&apos;s use of gesture to resolve lexical ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Holler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="903" to="913" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
	<note>Raquel Urtasun, and Sanja Fidler</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno>abs/1501.02598</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer VisionECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1996" to="2003" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="747" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linking people in videos with their names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Vignesh Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Grounding action descriptions in videos</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action MACH A Spatio-temporal Maximum Average Correlation Height Filter for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing what you&apos;re told: Sentence-guided activity recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanaswamy</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="732" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mothers&apos; speech to children learning language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><forename type="middle">E</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child development</title>
		<imprint>
			<biblScope unit="page" from="549" to="565" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eye movements and spoken language comprehension: Effects of visual context on syntactic ambiguity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Spivey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">C</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="481" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integration of visual and linguistic information in spoken language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Michael K Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">C</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5217</biblScope>
			<biblScope unit="page" from="1632" to="1634" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver<address><addrLine>Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional codes and their performance in communication systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the IEEE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="751" to="772" />
			<date type="published" when="1971-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
