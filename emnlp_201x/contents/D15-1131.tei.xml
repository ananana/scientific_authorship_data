<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trans-gram, Fast Cross-lingual Word-embeddings rey es − Mann</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jocelyn Coulmance</orgName>
								<address>
									<addrLine>105 rue La Fayette, 105 rue La Fayette, 105 rue La Fayette</addrLine>
									<postCode>75010, 75010, 75010</postCode>
									<settlement>Paris, Paris, Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Amine Benhalloum</orgName>
								<address>
									<addrLine>105 rue La Fayette</addrLine>
									<postCode>75010</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trans-gram, Fast Cross-lingual Word-embeddings rey es − Mann</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce Trans-gram, a simple and computationally-efficient method to simultaneously learn and align word-embeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned word-embeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classification and word translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word-embeddings are a representation of words with fixed-sized vectors. It is a distributed rep- resentation <ref type="bibr" target="#b5">(Hinton, 1984)</ref> in the sense that there is not necessarily a one-to-one correspondence be- tween vector dimensions and linguistic properties. The linguistic properties are distributed along the dimensions of the space.</p><p>A popular method to compute word- embeddings is the Skip-gram model <ref type="bibr" target="#b11">(Mikolov et al., 2013a</ref>). This algorithm learns high-quality word vectors with a computation cost much lower than previous methods. This allows the processing of very important amounts of data. For instance, a 1.6 billion words dataset can be processed in less than one day. Several authors came up with different methods to align word-embeddings across two languages ( <ref type="bibr" target="#b6">Klementiev et al., 2012;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b8">Lauly et al., 2014;</ref><ref type="bibr" target="#b2">Gouws et al., 2015)</ref>. * These authors contributed equally.</p><p>In this article, we introduce a new method called Trans-gram, which learns word embed- dings aligned across many languages, in a simple and efficient fashion, using only sentence align- ments rather than word alignments. We compare our method with previous approaches on a cross- lingual document classification task and on a word translation task and obtain state of the art results on these tasks. Additionally, word-embeddings for twenty-one languages are learned simultaneously -to our knowledge -for the first time, in less than two and a half hours. Furthermore, we illustrate some interesting properties that are captured such as cross-lingual analogies, e.g rey es − Mann de + femme fr ≈ regina it which can be used for dis- ambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of Previous Work</head><p>A number of methods have been explored to train and align bilingual word-embeddings. These methods pursue two objectives: first, similar rep- resentations (i.e. spatially close) must be assigned to similar words (i.e. "semantically close") within each language -this is the mono-lingual objec- tive; second, similar representations must be as- signed to similar words across languages -this is the cross-lingual objective.</p><p>The simplest approach consists in separating the mono-lingual optimization task from the cross- lingual optimization task. This is for example the case in ( <ref type="bibr" target="#b12">Mikolov et al., 2013b</ref>). The idea is to sep- arately train two sets of word-embeddings for each language and then to do a parametric estimation of the mapping between word-embeddings across languages. This method was further extended by <ref type="bibr" target="#b1">(Faruqui and Dyer, 2014)</ref>. Even though those al- gorithms proved to be viable and fast, it is not clear whether or not a simple mapping between whole languages exists. Moreover, they require word alignments which are a rare and expensive resource.</p><p>Another approach consists in focusing entirely on the cross-lingual objective. This was explored in ( <ref type="bibr">Hermann and Blunsom, 2013;</ref><ref type="bibr" target="#b8">Lauly et al., 2014</ref>) where every couple of aligned sentences is transformed into two fixed-size vectors. Then, the model minimizes the Euclidean distance between both vectors. This idea allows processing corpus aligned at sentence-level rather than word-level. However, it does not leverage the abundance of ex- isting mono-lingual corpora .</p><p>A popular approach is to jointly optimize the mono-lingual and cross-lingual objectives simul- taneously. This is mostly done by minimizing the sum of mono-lingual loss functions for each lan- guage and the cross-lingual loss function. ( <ref type="bibr" target="#b6">Klementiev et al., 2012</ref>) proved this approach to be useful by obtaining state-of-the-art results on sev- eral tasks. ( <ref type="bibr" target="#b2">Gouws et al., 2015</ref>) extends their work with a more computationally-efficient implemen- tation.</p><p>3 From Skip-Gram to Trans-Gram</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skip-gram</head><p>We briefly introduce the Skip-gram algorithm, as we will need it for further explanations. Skip- gram allows to train word embeddings for a lan- guage using mono-lingual data. This method uses a dual representation for words. Each word w has two embeddings: a target vector, w (∈ R D ), and a context vector, w (∈ R D ). The algorithm tries to estimate the probability of a word w to appear in the context of a word c. More pre- cisely we are learning the embeddings w, c so that:</p><formula xml:id="formula_0">σ( w · c) = P (w|c)</formula><p>where σ is the sigmoid func- tion.</p><p>A simplified version of the loss function mini- mized by Skip-gram is the following:</p><formula xml:id="formula_1">J = s∈C w∈s c∈s[w−l:w+l] − log σ( w · c) (1)</formula><p>where C is the set of sentences constituting the training corpus, and s[w − l : w + l] is a word win- dow on the sentence s centered around w. For the sake of simplicity this equation does not include the "negative-sampling" term, see (Mikolov et al., 2013a) for more details. Skip-gram can be seen as a materialization of the distributional hypothesis <ref type="bibr" target="#b3">(Harris, 1968)</ref>: "Words used in similar contexts have similar meanings". We will now see how to extend this idea to cross-lingual contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Trans-gram</head><p>In this section we introduce Trans-gram, a new method to compute aligned word-embeddings for a variety of languages.</p><p>Our method will minimize the summation of mono-lingual losses and cross-lingual losses. Like in BilBOWA ( <ref type="bibr" target="#b2">Gouws et al., 2015)</ref>, we use Skip- gram as a mono-lingual loss. Assuming we are trying to learn aligned word vectors for languages e (e.g. English) and f (e.g. French), we note J e and J f the two mono-lingual losses.</p><p>In BilBOWA, the cross-lingual loss function is a distance between bag-of-words representa- tions of two aligned sentences. But as ( <ref type="bibr" target="#b9">Levy and Goldberg, 2014)</ref> showed that the Skip-gram loss function extracts interesting linguistic features, we wanted to use a loss function for the cross-lingual objective that will be closer to Skip-gram than Bil- BOWA.</p><p>Therefore, we introduce a new task, Trans- gram, similar to Skip-gram. Each English sen- tence s e in our aligned corpus A e,f is aligned with a French sentence s f . In Skip-gram, the context picked for a target word w e in a sentence s e is the set of words c e appearing in the window centered around w e : s e [w e − l : w e + l]. In Trans-gram, the context picked for a target word w e in a sentence s e will be all the words c f appearing in s f . The loss can thus be written as:</p><formula xml:id="formula_2">Ω e,f = (se,s f )∈A e,f we∈se c f ∈s f − log σ( w e · c f )<label>(2)</label></formula><p>This loss isn't symmetric with respect to the lan- guages. We, therefore, use two cross-lingual ob- jectives: Ω e,f aligning e's target vectors and f 's context vectors and Ω f,e aligning f 's target vectors and e's context vectors. By comparison BilBOWA only aligns e's target vectors and f 's target vec- tors. The <ref type="figure">figure 1</ref> illustrates the four objectives.</p><p>Notice that we make the assumption that the meaning of a word is uniformly distributed in the whole sentence. This assumption, although a naive one, gave us in practice excellent results. Also our method uses only sentence-aligned cor- pus and not word-aligned corpus which are rarer.  <ref type="figure">Figure 1</ref>: The four partial objectives contributing to the alignment of English and French: a Skip-gram objective per language (Je and J f ) over a window surrounding a target word (blue) and two Trans-gram objectives (Ω e,f and Ω f,e ) over the whole sentence aligned with the sentence from which the target word is extracted (red).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>In our experiments, we used the Europarl ( <ref type="bibr" target="#b7">Koehn, 2005</ref>) aligned corpora. Europarl-v7 has two pe- culiarities: firstly, the corpora are aligned at sentence-level; secondly each pair of languages contains English as one of its members: for in- stance, there is no French/Italian pair. In other words, English is used as a pivot language. No bi-lingual lexicons nor other bi-lingual datasets aligned at the word level were used.</p><p>Using only the Europarl-v7 texts as both mono- lingual and bilingual data, it took 10 minutes to align 2 languages, and two and a half hours to align the 21 languages of the corpus, in a 40 dimensional space on a 6 core computer. We also computed 300 dimensions vectors using the Wikipedia extracts provided by (Al-Rfou et al., 2013) as monolingual data for each language. The training time was 21 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reuters Cross-lingual Document Classification</head><p>We used a subset of the English and German sec- tions of the Reuters RCV1/RCV2 corpora ( <ref type="bibr" target="#b10">Lewis and Li, 2004</ref>) (10000 documents each), as in ( <ref type="bibr" target="#b6">Klementiev et al., 2012</ref>), and we replicated the exper- imental setting. In the English dataset, there are four topics: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). We used these topics as our la- bels and we only selected documents labeled with a single topic. We trained our classifier on the ar- ticles of one language, where each document was represented using an IDF weighted sum of the vec- tors of its words, we then tested it on the articles of the other language. The classifier used was an averaged perceptron, and we used the implemen- tation from (Klementiev et al., 2012) <ref type="bibr">1</ref> . The word vectors were computed on the Europarl-v7 paral- lel corpus with size 40 like other methods. For this task only the target vectors where used. We report the percentage precision obtained with our method, in comparison with other meth- ods, in <ref type="table" target="#tab_1">Table 1. The table also</ref> include results obtained with 300 dimensions vectors trained by Trans-gram with the Europarl-v7 as parallel cor- pus and the Wikipedia as mono-lingual corpus. The previous state of the art results were detained ( <ref type="bibr" target="#b2">Gouws et al., 2015</ref>) with BilBOWA and ( <ref type="bibr" target="#b8">Lauly et al., 2014</ref>) with their Bilingual Auto-encoder model. This model learns word embeddings dur- ing a translation task that uses an encoder-decoder approach. We also report the scores from Kle- mentiev et al. who introduced the task and the BiCVM model scores from <ref type="bibr">(Hermann and Blunsom, 2013)</ref>.</p><p>The results show an overall significant improve- ment over the other methods, with the added ad- vantage of being computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">P@k Word Translation</head><p>Next we evaluated our method on a word transla- tion task, introduced in ( <ref type="bibr" target="#b12">Mikolov et al., 2013b</ref>) and used in <ref type="figure">(Gouws et al., 2015)</ref>. The words were ex- tracted from the publicly available WMT11 2 cor- pus. The experiments were done for two sets of translation: English to Spanish and Spanish to En- glish. ( <ref type="bibr" target="#b12">Mikolov et al., 2013b</ref>) extracted the top 6K most frequent words and translated them with Google Translate. They used the top 5K pairs to train a translation matrix, and evaluated their method on the remaining 1K. As our English and</p><note type="other">Method En → De De → En Speed-up in training time Klementiev et al.</note><p>77.6% 71.1% ×1 Bilingual Auto-encoder 91.8% 72.8% ×3 BiCVM 83.7% 71.4% ×320 BilBOWA 86,5% 75% ×800 Trans-gram 87,8% 78,7% ×600 Trans-gram (size 300 vectors EP+WIKI) 91,1% 78,4%  <ref type="table">Table 2</ref>: Results on the translation task Spanish vectors are already aligned we don't need the 5K training pairs and use only the 1K test pairs. The reported score, the translation precision P @k, is the fraction of test-pairs where the tar- get translation (Google Translate) is one of the k translations proposed by our model. For a given English word, w, our model takes its target vectors w and proposes the k closest Spanish word using the co-similarity of their vectors to w. We com- pare ourselves to the "translation matrix" method and to the BilBowa aligned vectors. We also re- port the scores obtained by a trivial algorithm that uses edit-distance to determine the closest transla- tion and by the Bing Translator service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Interesting properties 6.1 Cross-lingual disambiguation</head><p>We now present the task of cross-lingual dis- ambiguation as an example of possible uses of aligned multilingual vectors. The goal of this task is to find a suitable representation of each sense of a given polysemous word. The idea of our method is to look for a language in which the undesired senses are represented by unambiguous words and then to perform some arithmetic operation.</p><p>Let's illustrate the process with a concrete ex- ample: consider the French word "train", train fr . The three closest Polish words to train fr translate in English into "now", "a train" and "when". This seems a poor matching. In fact, train fr is polyse- mous. It can name a line of railroad cars, but it is also used to form progressive tenses. The French "Il est en train de manger" translates into "he is eating", or in Italian "sta mangiando".</p><p>As the Italian word "sta" is used to form pro- gressive tenses, it's a good candidate to disam- biguate train fr . Let's introduce the vector v = train fr − sta it . Now the three polish words clos- est to v translate in English into "a train", "a train" and "railroad". Therefore v is a better representa- tion for the railroad sense of train fr .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Transfer of linguistic features</head><p>Another interesting property of the vectors gener- ated by Trans-gram is the transfer of linguistic fea- tures through a pivot language that does not pos- sess these features.</p><p>Let's illustrate this by focusing on Latin lan- guages, which possess some features that En- glish does not, like rich conjugations. For ex- ample, in French and Italian the infinitives of "eat" are manger fr and mangiare it , and the first plural persons are mangeons fr and mangiamo it . Actually in our models we observe the follow- ing alignments:</p><p>manger fr ≈ mangiare it and mangeons fr ≈ mangiamo it . It is thus remark- able to see that features not present in English match in languages aligned through English as the only pivot language. We also found similar transfers for the genders of adjectives and are cur- rently studying other similar properties captured by Trans-gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we provided the following contri- butions: Trans-gram, a new method to compute cross-lingual word-embeddings in a single word space; state of the art results on cross-lingual NLP tasks; a sketch of a cross-lingual calculus to help disambiguate polysemous words; the exhibition of linguistic features transfers through a pivot- language not possessing those features.</p><p>We are still exploring promising properties of the generated vectors and their applications in other NLP tasks (Sentiment Analysis, NER...).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of Trans-gram with various methods for Reuters English/German classification 

Method 
En → Es P@1 En → Es P@5 
Es → En P@1 Es → En P@5 
Edit distance 
13% 
18% 
24% 
27% 
Bing 
55% 
71% 
Translation Matrix 
33% 
35% 
51% 
52% 
BilBOWA 
39% 
44% 
51% 
55% 
Trans-gram 
45% 
61% 
47% 
62% 

</table></figure>

			<note place="foot" n="1"> Thanks to S. Gouws for providing this implementation 2 http://www.statmt.org/wmt11/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.1662</idno>
		<title level="m">Polyglot: Distributed word representations for multilingual nlp</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving vector space word representations using multilingual correlation. Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mathematical structures of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Zellig Sabbettai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno type="arXiv">arXiv:1312.6173</idno>
		<title level="m">Multilingual distributed representations without word alignment</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>Distributed representations</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT Summit</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
