<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hybrid Convolutional Variational Autoencoder for Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Neuro-und Bioinformatik</orgName>
								<orgName type="institution">Universität zu Lübeck</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
							<email>severyn@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Neuro-und Bioinformatik</orgName>
								<orgName type="institution">Universität zu Lübeck</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Hybrid Convolutional Variational Autoencoder for Text Generation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="627" to="637"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we explore the effect of architectural choices on learning a variational autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvo-lutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly , it helps to avoid the issue of the VAE collapsing to a deterministic model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative models of texts are currently at the cornerstone of natural language understanding en- abling recent breakthroughs in machine transla- tion ( <ref type="bibr" target="#b2">Bahdanau et al., 2014;</ref>, dia- logue modelling <ref type="bibr" target="#b32">(Serban et al., 2016)</ref>, abstractive summarization ( <ref type="bibr" target="#b31">Rush et al., 2015)</ref>, etc.</p><p>Currently, RNN-based generative models hold state-of-the-art results in both unconditional <ref type="bibr" target="#b9">Ha et al., 2016</ref>) and con- ditional ( <ref type="bibr" target="#b34">Vinyals et al., 2014</ref>) text generation. At a high level, these models represent a class of au- toregressive models that work by generating out- puts sequentially one step at a time where the next predicted element is conditioned on the history of elements generated thus far.</p><p>Variational autoencoders (VAE), recently intro- duced by <ref type="bibr" target="#b18">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b30">Rezende et al., 2014</ref>), offer a different approach to genera- tive modeling by integrating stochastic latent vari- ables into the conventional autoencoder architec- ture. The primary purpose of learning VAE-based generative models is to be able to generate realis- tic examples as if they were drawn from the input data distribution by simply feeding noise vectors through the decoder. Additionally, the latent rep- resentations obtained by applying the encoder to input examples give a fine-grained control over the generation process that is harder to achieve with more conventional autoregressive models. Similar to compelling examples from image generation, where it is possible to condition generated human faces on various attributes such as hair, skin color and style <ref type="bibr" target="#b36">(Yan et al., 2015;</ref><ref type="bibr" target="#b20">Larsen et al., 2015)</ref>, in text generation it should be possible to also control various attributes of the generated sentences, such as, for example, sentiment or writing style.</p><p>While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images <ref type="bibr" target="#b1">(Bachman, 2016;</ref><ref type="bibr" target="#b8">Gulrajani et al., 2016</ref>) and speech ( <ref type="bibr" target="#b7">Fraccaro et al., 2016)</ref>, their application to natural text generation requires additional care <ref type="bibr" target="#b4">(Bowman et al., 2016;</ref><ref type="bibr" target="#b23">Miao et al., 2015)</ref>. As discussed by <ref type="bibr" target="#b4">Bowman et al. (2016)</ref>, the core difficulty of training VAE models is the collapse of the latent loss (represented by the KL divergence term) to zero. In this case the gen- erator tends to completely ignore latent represen- tations and reduces to a standard language model. This is largely due to the high modeling power of the RNN-based decoders which with sufficiently small history can achieve low reconstruction er- rors while not relying on the latent vector provided by the encoder.</p><p>In this paper, we propose a novel VAE model for texts that is more effective at forcing the decoder to make use of latent vectors. Contrary to existing work, where both encoder and decoder layers are LSTMs, the core of our model is a feed-forward architecture composed of one-dimensional convo- lutional and deconvolutional <ref type="bibr" target="#b39">(Zeiler et al., 2010)</ref> layers. This choice of architecture helps to gain more control over the KL term, which is crucial for training a VAE model. Given the difficulty of generating long sequences in a fully feed-forward manner, we augment our network with an RNN language model layer. To the best of our knowl- edge, this paper is the first work that successfully applies deconvolutions in the decoder of a latent variable generative model of natural text. We em- pirically verify that our model is easier to train than its fully recurrent alternative, which, in our experiments, fails to converge on longer texts. To better understand why training VAEs for texts is difficult we carry out detailed experiments, dis- cuss optimization difficulties, and propose effec- tive ways to address them. Finally, we demon- strate that sampling from our model yields realistic texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>So far, the majority of neural generative mod- els of text are built on the autoregressive as- sumption ( <ref type="bibr" target="#b19">Larochelle and Murray, 2011;</ref><ref type="bibr" target="#b25">van den Oord et al., 2016</ref>). Such models assume that the current data element can be accurately predicted given sufficient history of elements generated thus far. The conventional RNN based language mod- els fall into this category and currently dominate the language modeling and generation problem in NLP. Neural architectures based on recurrent <ref type="bibr" target="#b42">Zoph and Le, 2016;</ref><ref type="bibr" target="#b9">Ha et al., 2016)</ref> or convolutional decoders <ref type="bibr" target="#b25">(Kalchbrenner et al., 2016;</ref> provide an effective solution to this problem.</p><p>A recent work by <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> tack- les language generation problem within the VAE framework ( <ref type="bibr" target="#b18">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b30">Rezende et al., 2014</ref>). The authors demonstrate that with some care it is possible to successfully learn a la- tent variable generative model of text. Although their model is slightly outperformed by a tradi- tional LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> language model, their model achieves a similar ef- fect as in computer vision where one can (i) sam- ple realistic sentences by feeding randomly gen- erated novel latent vectors through the decoder and (ii) linearly interpolate between two points in the latent space. <ref type="bibr" target="#b23">Miao et al. (2015)</ref> apply VAE to bag-of-words representations of documents and the answer selection problem achieving good re- sults on both tasks.  discuss a VAE consisting of RNN encoder and CNN de- coder so that the decoder's receptive field is lim- ited. They demonstrate that this allows for a better control of KL and reconstruction terms.  build a VAE for text generation and de- sign a cost function that encourages interpretabil- ity of the latent variables. <ref type="bibr" target="#b40">Zhang et al. (2016)</ref>, <ref type="bibr" target="#b32">Serban et al. (2016)</ref> and <ref type="bibr" target="#b41">Zhao et al. (2017)</ref> ap- ply VAE to sequence-to-sequence problems, im- proving over deterministic alternatives. <ref type="bibr" target="#b35">Chen et al. (2016)</ref> propose a hybrid model combining autore- gressive convolutional layers with the VAE. The authors make an argument based on the Bit-Back coding <ref type="bibr" target="#b10">(Hinton and van Camp, 1993</ref>) that when the decoder is powerful enough the best thing for the encoder to do is to make the posterior distri- bution equivalent to the prior. While they exper- iment on images, this argument is very relevant to the textual data. A recent work by <ref type="bibr" target="#b3">Bousquet et al. (2017)</ref> approaches VAEs and GANs from the optimal transport point of view. The authors show that commonly known blurriness of sam- ples from VAEs trained on image data are a nec- essary property of the model. While the implica- tions of their argument to models combining la- tent variables and autoregressive layers trained on non-image data are still unclear, the argument sup- ports the hypothesis of <ref type="bibr" target="#b35">Chen et al. (2016)</ref> that dif- ficulty of training a hybrid model is not caused by a simple optimization difficulty but rather may be a more principled issue.</p><p>Various techniques to improve training of VAE models where the total cost represents a trade-off between the reconstruction cost and KL term have been used so far: KL-term annealing and input dropout <ref type="bibr" target="#b4">(Bowman et al., 2016;</ref><ref type="bibr" target="#b33">Sønderby et al., 2016)</ref>, imposing structured sparsity on latent vari- ables ( <ref type="bibr" target="#b38">Yeung et al., 2016</ref>) and more expressive for- mulations of the posterior distribution ( <ref type="bibr" target="#b29">Rezende and Mohamed, 2015;</ref><ref type="bibr" target="#b17">Kingma et al., 2016)</ref>. A work by <ref type="bibr" target="#b22">(Mescheder et al., 2017)</ref> follows the same motivation and combines GANs and VAEs allow- ing a model to use arbitrary complex formulations of both prior and posterior distributions. In Sec- tion 3.4 we propose another efficient technique to control the trade-off between KL and reconstruc- tion terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section we first briefly explain the VAE framework of <ref type="bibr" target="#b18">Kingma and Welling (2013)</ref>, then describe our hybrid architecture where the feed- forward part is composed of a fully convolutional encoder and a decoder that combines deconvolu- tional layers and a conventional RNN. Finally, we discuss optimization recipes that help VAE to re- spect latent variables, which is critical training a model with a meaningful latent space and being able to sample realistic sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational Autoencoder</head><p>The VAE is a recently introduced latent vari- able generative model, which combines varia- tional inference with deep learning. It modifies the conventional autoencoder framework in two key ways. Firstly, a deterministic internal representa- tion z (provided by the encoder) of an input x is re- placed with a posterior distribution q(z|x). Inputs are then reconstructed by sampling z from this posterior and passing them through a decoder. To make sampling easy, the posterior distribution is usually parametrized by a Gaussian with its mean and variance predicted by the encoder. Secondly, to ensure that we can sample from any point of the latent space and still generate valid and diverse outputs, the posterior q(z|x) is regularized with its KL divergence from a prior distribution p(z). The prior is typically chosen to be also a Gaussian with zero mean and unit variance, such that the KL term between posterior and prior can be computed in closed form ( <ref type="bibr" target="#b18">Kingma and Welling, 2013)</ref>. The total VAE cost is composed of the reconstruction term, i.e., negative log-likelihood of the data, and the KL regularizer:</p><formula xml:id="formula_0">J vae = KL(q(z|x)||p(z)) −E q(z|x) [log p(x|z)]</formula><p>(1) <ref type="bibr" target="#b18">Kingma and Welling (2013)</ref> show that the loss function from Eq (1) can be derived from the probabilistic model perspective and it is an upper bound on the true negative likelihood of the data.</p><p>One can view a VAE as a traditional Autoen- coder with some restrictions imposed on the in- ternal representation space. Specifically, using a sample from the q(z|x) to reconstruct the input instead of a deterministic z, forces the model to map an input to a region of the space rather than to a single point. The most straight-forward way to achieve a good reconstruction error in this case is to predict a very sharp probability distribution ef- fectively corresponding to a single point in the la- tent space ( <ref type="bibr" target="#b28">Raiko et al., 2014</ref>). The additional KL term in Eq (1) prevents this behavior and forces the model to find a solution with, on one hand, low re- construction error and, on the other, predicted pos- terior distributions close to the prior. Thus, the de- coder part of the VAE is capable of reconstructing a sensible data sample from every point in the la- tent space that has non-zero probability under the prior. This allows for straightforward generation of novel samples and linear operations on the la- tent codes. <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> demonstrate that this does not work in the fully deterministic Autoencoder framework . In addition to regulariz- ing the latent space, KL term indicates how much information the VAE stores in the latent vector.</p><p>Bowman et al. <ref type="formula" target="#formula_1">(2016)</ref> propose a VAE model for text generation where both encoder and decoder are LSTM networks <ref type="figure" target="#fig_0">(Figure 1</ref>). We will refer to this model as LSTM VAE in the remainder of the paper. The authors show that adapting VAEs to text generation is more challenging as the decoder tends to ignore the latent vector (KL term is close to zero) and falls back to a language model. Two training tricks are required to mitigate this issue: (i) KL-term annealing where its weight in Eq (1) gradually increases from 0 to 1 during the training; and (ii) applying dropout to the inputs of the de- coder to limit its expressiveness and thereby forc- ing the model to rely more on the latent variables. We will discuss these tricks in more detail in Sec- tion 3.4. Next we describe a deconvolutional layer, which is the core element of the decoder in our VAE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deconvolutional Networks</head><p>A deconvolutional layer (also referred to as trans- posed convolutions ( <ref type="bibr" target="#b8">Gulrajani et al., 2016</ref>) and fractionally strided convolutions ( <ref type="bibr" target="#b27">Radford et al., 2015)</ref>) performs spatial up-sampling of its inputs and is an integral part of latent variable genera- tive models of images ( <ref type="bibr" target="#b27">Radford et al., 2015;</ref><ref type="bibr" target="#b8">Gulrajani et al., 2016</ref>) and semantic segmentation algo- rithms ( <ref type="bibr" target="#b24">Noh et al., 2015)</ref>. Its goal is to perform an "inverse" convolution operation and increase spa- tial size of the input while decreasing the number of feature maps. This operation can be viewed as a backward pass of a convolutional layer and can be implemented by simply switching the forward and backward passes of the convolution operation. In the context of generative modeling based on global representations, the deconvolutions are typ- ically used as follows: the global representation is first linearly mapped to another representation with small spatial resolution and large number of feature maps. A stack of deconvolutional layers is then applied to this representation, each layer progressively increasing spatial resolution and de- creasing the amount of feature channels. The out- put of the last layer is an image or, in our case, a text fragment. A notable example of such a model is the deep network of ( <ref type="bibr" target="#b27">Radford et al., 2015</ref>) trained with adversarial objective. Our model uses a similar approach but is instead trained with the VAE objective.</p><p>There are two primary motivations for choos- ing deconvolutional layers instead of the dom- inantly used recurrent ones: firstly, such lay- ers have extremely efficient GPU implementations due to their fully parallel structure. Secondly, feed-forward architectures are typically easier to optimize than their recurrent counterparts, as the number of back-propagation steps is constant and potentially much smaller than in RNNs. Both points become significant as the length of the gen- erated text increases. Next, we describe our VAE architecture that blends deconvolutional and RNN layers in the decoder to allow for better control over the KL-term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hybrid Convolutional-Recurrent VAE</head><p>Our model is composed of two relatively inde- pendent modules. The first component is a stan- dard VAE where the encoder and decoder modules are parametrized by convolutional and deconvolu- tional layers respectively (see <ref type="figure">Figure 2(a)</ref>). This architecture is attractive for its computational effi- ciency and simplicity of training.</p><p>The other component is a recurrent language model consuming activations from the deconvo- lutional decoder concatenated with the previous output characters. We consider two flavors of re- current functions: a conventional LSTM network <ref type="figure">(Figure 2(b)</ref>) and a stack of masked convolutions also known as the ByteNet decoder from <ref type="bibr" target="#b25">Kalchbrenner et al. (2016)</ref>  <ref type="figure">(Figure 2(c)</ref>). The primary reason for having a recurrent component in the decoder is to capture dependencies between ele- ments of the text sequences -a hard task for a fully feed-forward architecture. Indeed, the condi- tional distribution P (x|z) = P (x 1 , . . . , x n |z) of generated sentences cannot be richly represented with a feed-forward network. Instead, it factor-izes as: P (x 1 , . . . , x n |z) = i P (x i |z) where components are independent of each other and are conditioned only on z. To minimize the recon- struction cost the model is forced to encode ev- ery detail of a text fragment. A recurrent lan- guage model instead models the full joint distribu- tion of output sequences without having to make independence assumptions P (x 1 , . . . , x n |z) = i P (x i |x i−1 , . . . , x 1 , z). Thus, adding a re- current layer on top of our fully feed-forward encoder-decoder architecture relieves it from en- coding every aspect of a text fragment into the la- tent vector and allows it to instead focus on more high-level semantic and stylistic features.</p><p>Note that the feed-forward part of our model is different from the existing fully convolutional approaches of  and <ref type="bibr" target="#b25">Kalchbrenner et al. (2016)</ref> in two respects: firstly, while being fully parallelizable during training, these models still require predictions from previous time steps during inference and thus behave as a vari- ant of recurrent networks. In contrast, expansion of the z vector is fully parallel in our model (ex- cept for the recurrent component). Secondly, our model down-and up-samples a text fragment dur- ing processing while the existing fully convolu- tional decoders do not. Preserving spatial reso- lution can be beneficial to the overall result, but comes at a higher computational cost. Lastly, we note that our model imposes an upper bound on the size of text samples it is able to generate. While it is possible to model short texts by adding special padding characters at the end of a sample, generat- ing texts longer than certain thresholds is not pos- sible by design. This is not an unavoidable restric- tion, since the model can be extended to generate variable sized text fragments by, for example, vari- able sized latent codes. These extensions however are out of scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization Difficulties</head><p>The addition of the recurrent component results in optimization difficulties that are similar to those described by <ref type="bibr" target="#b4">Bowman et al. (2016)</ref>. In most cases the model converges to a solution with a vanish- ingly small KL term, thus effectively falling back to a conventional language model. <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> have proposed to use input dropout and KL term annealing to encourage their model to encode meaningful representations into the z vector. We found that these techniques also help our model to achieve solutions with non-zero KL term.</p><p>KL term annealing can be viewed as a grad- ual transition from conventional deterministic Au- toencoder to a full VAE. In this work we use linear annealing from 0 to 1. We have experimented with other schedules but did not find them to have a sig- nificant impact on the final result. As long as the KL term weight starts to grow sufficiently slowly, the exact shape and speed of its growth does not seem to affect the overall result. We have found the following heuristic to work well: we first run a model with KL weight fixed to 0 to find the num- ber of iterations it needs to converge. We then con- figure the annealing schedule to start after the un- regularized model has converged and last for no less than 20% of that amount.</p><p>While helping to regularize the latent vector, in- put dropout tends to slow down convergence. We propose an alternative technique to encourage the model to compress information into the latent vec- tor: in addition to the reconstruction cost com- puted on the outputs of the recurrent language model, we also add an auxiliary reconstruction term computed from the activations of the last de- convolutional layer:</p><formula xml:id="formula_1">J aux = −E q(z|x) [log p(x|z)] = −E q(z|x) [ t log p(x t |z)].<label>(2)</label></formula><p>Since at this layer the model does not have ac- cess to previous output elements it needs to rely on the z vector to produce a meaningful reconstruc- tion. The final cost minimized by our model is:</p><formula xml:id="formula_2">J hybrid = J vae + αJ aux (3)</formula><p>where α is a hyperparameter, J aux is the interme- diate reconstruction term and J vae is the bound from Eq (1). Expanding the two terms from Eq <ref type="formula">(3)</ref> gives:</p><formula xml:id="formula_3">J hybrid = KL(q(z|x)||p(z)) −E q(z|x) [ t log p(x t |z, x &lt;t )] −αE q(z|x) [ t log p(x t |z)].<label>(4)</label></formula><p>The objective function from Eq (4) puts a mild constraint on the latent vector to produce features useful for historyless reconstruction. Since the autoregressive part reuses these features, it also improves the main reconstruction term. We are thus able to encode information in the latent vector without hurting expressiveness of the decoder. One can view the objective function in Eq 4 as a joint objective for two VAEs: one only feed- forward, as in <ref type="figure">Figure 2(a)</ref>, and the other combin- ing feed-forward and recurrent parts, as in <ref type="figure">Fig- ures 2(b) and 2(c)</ref>, that partially share parameters. Since the feed-forward VAE is incapable of pro- ducing reasonable reconstructions without making use of the latent vector, the full architecture also gains access to the latent vector through shared parameters. We note that this trick comes at a cost of worse result on the density estimation task, since part of the parameters of the full model are trained to optimize an objective that does not cap- ture all the dependencies that exist in the textual data. However, the gap between purely determin- istic LM and our model is small and easily control- lable by the α hyperparameter. We refer the reader to <ref type="figure">Figure 4</ref> for quantitative results regarding the effect of α on the performance of our model on the LM task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We use KL term annealing and input dropout when training the LSTM VAE models from <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> and KL term annealing and regular- ized objective function from Eq (3) when train- ing our models. All models were trained with the Adam optimization algorithm <ref type="bibr" target="#b16">(Kingma and Ba, 2014</ref>) with decaying learning rate. We use Layer Normalization ( <ref type="bibr" target="#b0">Ba et al., 2016</ref>) in LSTM lay- ers and Batch Normalization ( <ref type="bibr" target="#b13">Ioffe and Szegedy, 2015</ref>) in convolutional and deconvolutional layers. To make our results easy to reproduce we have re- leased the source code of all our experiments 1 . Data. Our first task is character-level language generation performed on the standard Penn Tree- bank dataset <ref type="bibr" target="#b21">(Marcus et al., 1993)</ref>. One of the goals is to test the ability of the models to success- fully learn the representations of long sequences. For training, fixed-size data samples are selected from random positions in the standard training and validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with LSTM VAE</head><p>Historyless decoding. We start with an exper- iment where the decoder is forced to ignore the history and has to rely fully on the latent vec- tor. By conditioning the decoder only on the la- tent vector z we can directly compare the expres- siveness of the compared models. For the LSTM VAE model historyless decoding is achieved by using the dropout on the input elements with the dropout rate equal to 1 so that its decoder is only conditioned on the z vector and, implicitly, on the number tokens generated so far. We compare it to our fully-feedforward model without the recurrent layer in the decoder <ref type="figure">(Figure 2(a)</ref>). Both networks are parametrized to have comparable number of parameters.</p><p>To test how well both models can cope with the stochasticity of the latent vectors, we minimize only the reconstruction term from Eq. (1). This is equivalent to a pure autoencoder setting with stochastic internal representation and no regular- ization of the latent space. This experiment cor- responds to an initial stage of training with KL term annealing when its weight is set to 0. We pursue two goals with this experiment: firstly, we investigate how do the two alternative encoders behave in the beginning of training and establish a lower bound on the quality of the reconstruc- tions. Secondly, we attempt to put the Bit Back coding argument from <ref type="bibr" target="#b35">Chen et al. (2016)</ref> in con- text. The authors assume the encoder to be power- ful enough to produce a good representation of the data. One interpretation of this argument applied to textual data is that factorizing the joint proba- bility as p(x) = t p(x t |x &lt;t ) provides the model with a sufficiently powerful decoder that does not need the latent variables. However, our experi- mental results suggest that LSTM encoder may not be a sufficiently expressive encoder for VAEs for textual data, potentially making the argument in- applicable.</p><p>The results are presented in <ref type="figure">Figure 3</ref>. Note that when the length of input samples reaches 30 char- acters, the historyless LSTM autoencoder fails to fit the data well, while the convolutional architec- ture converges almost instantaneously. The results appear even worse for LSTMs on sequences of 50 characters. To make sure that this effect is not caused by optimization difficulties, i.e. exploding gradients ( <ref type="bibr" target="#b26">Pascanu et al., 2013</ref>), we have searched over learning rates, gradient clipping thresholds and sizes of LSTM layers but were only able to get results comparable to those shown in <ref type="figure">Figure 3</ref>. Note that LSTM networks make use of Layer <ref type="table">Nor-  632   0 2000 4000 6000 8000 10000   Iteration   0  1  2  3  4  5  6</ref> Bits-per-character convvae lstmvae <ref type="table">(a) 10 characters   0 2000 4000 6000 8000 10000   Iteration   0  1  2  3  4  5  6</ref> Bits-per-character convvae lstmvae <ref type="table">(b) 20 characters   0 2000 4000 6000 8000 10000   Iteration   0  1  2  3  4  5  6</ref> Bits-per-character convvae lstmvae <ref type="table">(c) 30 characters   0 2000 4000 6000 8000 10000   Iteration   0  1  2  3  4  5  6</ref> Bits malization ( <ref type="bibr" target="#b0">Ba et al., 2016</ref>) which has been shown to make training of such networks easier. These results suggest that our model is easier to train than the LSTM-based model, especially for modeling longer pieces of text. Additionally, our model is computationally faster by a factor of roughly two, since we run only one recurrent network per sam- ple and time complexity of the convolutional part is negligible in comparison.</p><p>Decoding with history. We now move to a case where the decoder is conditioned on both the la- tent vector and previous output elements. In these experiments we pursue two goals: firstly, we ver- ify whether the results obtained on the historyless decoding task also generalize to a less restricted case. Secondly, we study how well the models cope with stochasticity introduced by the latent variables. Note that we do not attempt to improve the state-of-the-art result on the Language Mod- eling task but instead focus on providing an ap- proach capable of generating long and diverse se- quences. We experiment on the task to obtain a detailed picture of how are our model and LSTM VAE affected by various choices and compare the two models, focusing on how effective is the en- coder at producing meaningful latent vector. How- ever, we note that our model performs fairly well on the LM task and is only slightly worse than purely deterministic Language Model, trained in the same environment, and is comparable to the one of <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> in this regard.</p><p>We fix input dropout rates at 0.2 and 0.5 for LSTM VAE and use auxiliary reconstruction loss (Section 3.4) with 0.2 weight in our Hybrid model. The bits-per-character scores on differently sized text samples are presented in <ref type="figure">Figure 4</ref>. As dis- cussed in Section 3.1, the KL term value indi- cates how much information the network stores in the latent vector. We observe that the amount of information stored in the latent vector by our model and the LSTM VAE is comparable when we train on short samples and largely depends on hyper-parameters α and p. When the length of a text fragment increases, LSTM VAE is able to put less information into the latent vector (i.e., the KL component is small) and for texts longer than 48 characters, the KL term drops to almost zero while for our model the ratio between KL and recon- struction terms stays roughly constant. This sug- gests that our model is better at encoding latent representations of long texts since the amount of information in the latent vector does not decrease as the length of a text fragment grows. In con- trast, there is a steady decline of the KL term of the LSTM VAE model. This result is consistent with our findings from the historyless decoding exper- iment. Note that in both of these experiments the LSTM VAE model fails to produce meaningful la- tent vectors with inputs over 50 characters long. This further suggests that our Hybrid model en- codes long texts better than the LSTM VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Controlling the KL term</head><p>We study the effect of various training techniques that help control the KL term which is crucial for training a generative VAE model.</p><p>Aux cost weight. First, we provide a detailed view of how optimization tricks discussed in Sec- tion 3.4 affect the performance of our Hybrid model. <ref type="figure" target="#fig_3">Figure 5</ref> presents results of our model trained with different values of α from Eq. (3). Note that the inclusion of the auxiliary reconstruc- tion loss slightly harms the bound on the likeli- hood of the data but helps the model to rely more on the latent vector as α grows. A similar effect on model's bound was observed by <ref type="bibr" target="#b4">Bowman et al. (2016)</ref>: increased input dropout rates force their model to put more information into the z vector but at the cost of increased final loss values. This is a trade-off that allows for sampling outputs in the VAE framework. Note that our model can find a solution with non-trivial latent vectors when trained with the full VAE loss provided that the α hyper-parameter is large enough. Combining it with KL term annealing helps to find non-zero KL term solutions at smaller α values.</p><p>Receptive field. The goal of this experiment is to study the relationship between the KL term val- ues and the expressiveness of the decoder. Without KL term annealing and input dropout, the RNN decoder in LSTM VAE tends to completely ignore information stored in the latent vector and essen-  <ref type="figure">(Figure 2(c)</ref>), which is similar to the decoder in ByteNet model from <ref type="bibr" target="#b25">Kalchbrenner et al. (2016)</ref>. We fix the size of the convolutional kernels to 2 and do not use di- lated convolutions and skip connections as in the original ByteNet.</p><p>The resulting receptive field size of the recurrent layer in our decoder is equal to N + 1 characters, where N is the number of convolutional layers. We vary the number of layers to find the amount of preceding characters that our model can consume without collapsing the KL term to zero.</p><p>Results of these experiments are presented in <ref type="figure">Figure 6</ref>. Interestingly, with the receptive field size larger than 3 and without the auxiliary re- construction term from Eq. (3) (α = 0) the KL term collapses to zero and the model falls back to a pure language model. This suggests that the training signal received from the previous charac- ters is much stronger than that from the input to be reconstructed. Using the auxiliary reconstruction term, however, helps to find solutions with non- zero KL term component irrespective of receptive field size. Note that increasing the value of α re- sults in stronger values of KL component. This is consistent with the results obtained with LSTM decoder in <ref type="figure" target="#fig_3">Figure 5</ref>. @userid @userid @userid @userid @userid ... I want to see you so much @userid #FollowMeCam ... @userid @userid @userid @userid @userid ... Why do I start the day today? @userid thanks for the follow back no matter what I'm doing with my friends they are so cute @userid Hello How are you doing I wanna go to the UK tomorrow!! #feelinggood #selfie #instago @userid @userid I'll come to the same time and it was a good day too xx <ref type="table">Table 1</ref>: Random sample tweets generated by LSTM VAE (top) and our Hybrid model (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rec</head><p>KL LSTM, p = 0.2 67.4</p><p>1.0 LSTM, p = 0.5 77.1 2.1 LSTM, p = 0.8 93. <ref type="bibr">7</ref> 3.8 Hybrid, α = 0.2 58.5 12.5 <ref type="table">Table 2</ref>: Breakdown into KL and reconstruction terms for char-level tweet generation. p refers to input dropout rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generating Tweets</head><p>In this section we present qualitative results on the task of generating tweets.</p><p>Data. We use 1M tweets 2 to train our model and test it on a held out dataset of 10k samples. We minimally preprocess tweets by only replac- ing user ids and urls with "@userid" and "url".</p><p>Setup. We use 5 convolutional layers with the ReLU non-linearity, kernel size 3 and stride 2 in the encoder. The number of feature maps is <ref type="bibr">[128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">512,</ref><ref type="bibr">512]</ref> for each layer respectively. The decoder is configured equivalently but with the amount of feature maps decreasing in each consecutive layer. The top layer is an LSTM with 1000 units. We have not observed significant over- fitting. The baseline LSTM VAE model contained two distinct LSTMs both with 1000 cells. The models have comparable number of parameters: 10.5M for the LSTM VAE model and 10.8M for our hybrid model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>. Both VAE models are trained on the character-level generation. The breakdown of to- tal cost into KL and reconstruction terms is given in <ref type="table">Table 2</ref>. Note that while the total cost values are comparable, our model puts more information into the latent vector, further supporting our obser- vations from Section 4.1. This is reflected in the random samples from both models, presented in <ref type="table">Table 1</ref>. We perform greedy decoding during gen- eration so any variation in samples is only due to the latent vector. LSTM VAE produces very lim- ited range of tweets and tends to repeat "@userid" sequence, while our model produces much more diverse samples. <ref type="bibr">2</ref> a random sample collected using the Twitter API</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have introduced a novel generative model of natural texts based on the VAE framework. Its core components are a convolutional encoder and a deconvolutional decoder combined with a recur- rent layer. We have shown that the feed-forward part of our model architecture makes it easier to train a VAE and avoid the problem of KL-term col- lapsing to zero, where the decoder falls back to a standard language model thus inhibiting the sam- pling ability of VAE. Additionally, we propose an efficient way to encourage the model to rely on the latent vector by introducing an additional cost term in the training objective. We observe that it works well on long sequences which is hard to achieve with purely RNN-based VAEs using the previously proposed tricks such as KL-term an- nealing and input dropout. Finally, we have ex- tensively evaluated the trade-off between the KL- term and the reconstruction loss. In particular, we investigated the effect of the receptive field size on the ability of the model to respect the latent vector which is crucial for being able to generate realistic and diverse samples. In future work we plan to ap- ply our VAE model to semi-supervised NLP tasks and experiment with conditioning generation on text attributes such as sentiment and writing style.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: LSTM VAE model of (Bowman et al., 2016)</figDesc><graphic url="image-1.png" coords="3,79.09,62.81,204.09,54.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Figure 2: Illustrations of our proposed models.</figDesc><graphic url="image-2.png" coords="4,101.85,69.79,263.04,261.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Training curves of LSTM autoencoder and our model on samples of different length. Solid and dashed lines show training and validation curves respectively. Note that the model exhibits little to no overfitting since the validation curve follows the training one almost perfectly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The full cost (solid line) and the KL component (dashed line) of our Hybrid model with LSTM decoder trained with various α, with and without KL term weight annealing, measured on the validation partition.</figDesc></figure>

			<note place="foot" n="1"> https://github.com/stas-semeniuta/ textvae</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Enrique Alfonseca, Katja Filippova, Sylvain Gelly and Jason Lee for their useful feed-back while preparing this paper. This project has received funding from the European Union's Framework Programme for <ref type="bibr">Research and Innovation HORIZON 2020</ref><ref type="bibr">-2020</ref> under the Marie Skodowska-Curie Agreement No. 641805. Stanislau Semeniuta thanks the support from Pat-tern Recognition Company GmbH. We thank the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An architecture for deep, hierarchical generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4826" to="4834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">From optimal transport to generative modeling: the vegan cookbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carljohann</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno>abs/1705.07642</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONLL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1611.02731</idno>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. 2016. Variational lossy autoencoder. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
		<idno>abs/1612.08083</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Søren Kaae Sø Nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pixelvae: A latent variable model for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><forename type="middle">Ali</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1611.05013</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1609.09106</idno>
		<title level="m">Hypernetworks. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, COLT 1993</title>
		<meeting>the Sixth Annual ACM Conference on Computational Learning Theory, COLT 1993<address><addrLine>Santa Cruz, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-07-26" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>abs/1703.00955</idno>
		<title level="m">Controllable text generation. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016. Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1606.04934</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno>abs/1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno>abs/1701.04722</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1511.06038</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1601.06759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Techniques for learning binary stochastic feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno>abs/1406.2989</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1605.06069</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<idno>abs/1602.02282</idno>
		<title level="m">Ladder variational autoencoders. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno>abs/1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1512.00570</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno>abs/1702.08139</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Epitomic variational autoencoder. In submission to ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno>abs/1605.07869</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno>abs/1703.10960</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
