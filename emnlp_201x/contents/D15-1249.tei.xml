<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variable-Length Word Encodings for Neural Translation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chitnis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variable-Length Word Encodings for Neural Translation Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work in neural machine translation has shown promising performance, but the most effective architectures do not scale naturally to large vocabulary sizes. We propose and compare three variable-length encoding schemes that represent a large vocabulary corpus using a much smaller vocabulary with no loss in information. Common words are unaffected by our encoding , but rare words are encoded using a sequence of two pseudo-words. Our method is simple and effective: it requires no complete dictionaries, learning procedures , increased training time, changes to the model, or new parameters. Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head> <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref> <p>propose a neural transla- tion model that learns vector representations for in- dividual words as well as word sequences. Their approach jointly predicts a translation and a la- tent word-level alignment for a sequence of source words. However, the architecture of the network does not scale naturally to large vocabularies ( <ref type="bibr" target="#b5">Jean et al., 2014)</ref>.</p><p>In this paper, we propose a novel approach to circumvent the large-vocabulary challenge by pre- processing the source and target word sequences, encoding them as a longer token sequence drawn from a small vocabulary that does not discard any information. Common words are unaffected, but rare words are encoded as a sequence of two pseudo-words. The exact same learning and infer- ence machinery applied to these transformed data yields improved translations.</p><p>We evaluate a family of 3 different encoding schemes based on Huffman codes. All of them eliminate the need to replace rare words with the unknown word symbol. Our approach is simpler than other methods recently proposed to address the same issue. It does not introduce new param- eters into the model, change the model structure, affect inference, require access to a complete dic- tionary, or require any additional learning proce- dures. Nonetheless, compared to a baseline system that replaces all rare words with an unknown word symbol, our encoding approach improves English- French news translation by up to 1.7 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>Neural machine translation describes approaches to machine translation that learn from corpora in a single integrated model that embeds words and sentences into a vector space <ref type="bibr" target="#b6">(Kalchbrenner and Blunsom, 2013;</ref>. We focus on one recent approach to neu- ral machine translation, proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref>, that predicts both a translation and its alignment to the source sentence, though our tech- nique is relevant to related approaches as well.</p><p>The architecture consists of an encoder and a de- coder. The encoder receives a source sentence x and encodes each prefix using a recurrent neural network that recursively combines embeddings x j for each word position j:</p><formula xml:id="formula_0">− → h j = f (x j , − → h j−1 )<label>(1)</label></formula><p>where f is a non-linear function. Reverse encod- ings ← − h j are computed similarly to represent suf- fixes of the sentence. These vector representa- tions are stacked to form h j , a representation of the whole sentence focused on position j.</p><p>The decoder predicts each target word y i se- quentially according to the distribution</p><formula xml:id="formula_1">P (y i |y i−1 , ..., y 1 , x) = g(y i−1 , s i , c i ) (2)</formula><p>where s i is a hidden decoder state summarizing the prefix of the translation generated so far, c i is a summary of the entire input sequence, and g is another non-linear function. Encoder and decoder parameters are jointly optimized to maximize the log-likelihood of a training corpus.</p><p>Depending on the approach to neural transla- tion, c can take multiple forms. <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref> propose integrating an attention mechanism in the decoder, which is trained to determine on which portions of the source sentence to focus. The decoder computes c i , the summarizing con- text vector, as a convex combination of the h j . The coefficients of this combination are propor- tional (softmax) to an alignment model prediction exp a(h j , s i ), where a is a non-linear function.</p><p>The speed of prediction scales with the output vocabulary size, due to the denominator of Equa- tion 2 ( <ref type="bibr" target="#b5">Jean et al., 2014</ref>). The input vocabulary size is also a challenge for storage and learning. As a re- sult, neural machine translation systems only con- sider the top 30K to 100K most frequent words in a training corpus, replacing the other words with an unknown word symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>There has been much recent work in improving translation quality by addressing these vocabulary size challenges. <ref type="bibr" target="#b7">Luong et al. (2014)</ref> describe an approach that, similar to ours, treats the translation system as a black box. They eliminate unknown symbols by training the system to recognize from where in the source text each unknown word in the target text came, so that in a postprocessing phase, the unknown word can be replaced by a dictionary lookup of the corresponding source word. In con- trast, our method does not rely on access to a com- plete dictionary, and instead transforms the data to allow the system itself to learn translations for even the rare words.</p><p>Some approaches have altered the model to cir- cumvent the expensive normalization computa- tion, rather than applying preprocessing and post- processing on the text. <ref type="bibr" target="#b5">Jean et al. (2014)</ref> de- velop an importance sampling strategy for ap- proximating the softmax computation. Mnih and Kavukcuoglu (2013) present a technique for ap- proximation of the target word probability using noise-contrastive estimation.</p><p>Sequential or hierarchical encodings of large vo- cabularies have played an important role in recur- rent neural network language models, primarily to address the inference time issue of large vocabu- laries. <ref type="bibr" target="#b9">Mikolov et al. (2011b)</ref> describe an architec- ture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. <ref type="bibr" target="#b10">Mikolov et al. (2013)</ref> describe an encoding of the output vocabu- lary as a binary tree. To our knowledge, hierarchi- cal encodings have not been applied to the input vocabulary of a machine translation system.</p><p>Other methods have also been developed to work around large-vocabulary issues in language model- ing. <ref type="bibr" target="#b13">Morin and Bengio (2005)</ref>, <ref type="bibr" target="#b11">Mnih and Hinton (2009)</ref>, and <ref type="bibr" target="#b8">Mikolov et al. (2011a)</ref> develop hierar- chical versions of the softmax computation; <ref type="bibr" target="#b3">Huang et al. (2012)</ref> and <ref type="bibr" target="#b2">Collobert and Weston (2008)</ref> re- move the need for normalization, thus avoiding computation of the summation term over the entire vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Huffman Codes</head><p>An encoding can be used to represent a sequence of tokens from a large vocabulary V using a small vocabulary W. In the case of translation, let V be the original corpus vocabulary, which can number in the millions of word types in a typical corpus. Let W be the vocabulary size of a neural transla- tion model, typically set to a much smaller number such as 30,000.</p><p>A deterministically invertible, variable-length encoding maps each v ∈ V to a sequence w ∈ W+ such that no other v ∈ V is mapped to a prefix of w. Encoding simply replaces each element of V according to the map, and decoding is unambigu- ous because of this prefix restriction. An encoding can be represented as a tree in which each leaf cor- responds to an element of V, each node contains a symbol from W, and the encoding of any leaf is its path from the root.</p><p>A Huffman code is an optimal encoding that uses as few symbols from W as possible to encode an original sequence of symbols from V. Although binary codes are typical, W can have any size. An optimal encoding can be found using a greedy al- gorithm <ref type="bibr" target="#b4">(Huffman, 1952)</ref>.</p><p>to be or not to be take it or leave it to be or not it (take) (leave) s0 be s0</p><p>to be or not to be s0 be it or s0 s0 it</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Corpus</head><p>Repeat-All Encoding to be s0 s1</p><formula xml:id="formula_2">(not) (it)</formula><p>or s0 s1</p><p>Repeat-Symbol Encoding to be or s0 s0 to be s1 s0 s0 s1 or s1 s1 s0 s1</p><formula xml:id="formula_3">(take) (leave) s0 s1 s0 s1 s2 (not)(it) (to) t0 t1</formula><p>No-Repeats Encoding s0 t0 s0 t1 or s1 t0 s0 t0 s0 t1 s2 t0 s1 t1 or s2 t1 s1 t1 Figure 1: Our three encoding schemes are applied to a two-sentence toy corpus for which each word type appears one or two times, and the total vocab- ulary size V is 7. An optimal encoding tree under each scheme is shown for an encoded vocabulary size W of 6. As stricter constraints are imposed on the encoding, the encoded corpus length increases and the number of elements of V that can be rep- resented using a single symbol decreases. Two- symbol encodings of rare words are underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Variable-Length Encoding Methods</head><p>We consider three different encoding schemes that are based on Huffman codes. The encoding for a toy corpus under each scheme is depicted in <ref type="figure">Fig- ure 1</ref>. While a Huffman code achieves the shortest possible encoded length using a fixed vocabulary size W , symbols are often shared between both common words and rare words. The variants we consider are designed to prevent specific forms of symbol sharing across encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Schemes</head><p>Repeat-All. The first scheme is a standard Huff- man code. In our experiments with V ≈ 2 · 10 6 , W = 3 · 10 4 , and frequencies drawn from the WMT corpus, all words in V are encoded as either a single symbol or two symbols of W. We denote the single-symbol words (which have the high- est frequency) as common, and we call the other words rare. The Repeat-All encoding scheme has the highest number of common words. In <ref type="figure">Fig- ure 1</ref>, common words are represented as them- selves. Rare words are represented by two words, and the first is always a pseudo-word symbol intro- duced into W of the form sX for an integer X. Repeat-Symbol. The Repeat-Symbol encoding scheme does not allow common-word symbols to appear in the encoding of rare words. Instead, each rare word is encoded as a two-symbol sequence of the form "sX sY," where X and Y are integers that may be the same or different. This scheme decreases the number of common words in order to encode all rare words using a restricted set of symbols. In this scheme, a common word in the encoded vocabulary always corresponds to a com- mon word in the original vocabulary, reducing am- biguity of common word symbols at the expense of increasing ambiguity of pseudo-word symbols. No-Repeats. Our final encoding scheme, No- Repeats, uses a different vocabulary for the first and second symbols in each rare word. That is, rare words are represented as "sX tY," where X and Y are integers that may be the same or different. In this scheme, common words and rare words do not share symbols, and each symbol can immediately be identified as common, the first of a rare encod- ing pair, or the second of a rare encoding pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Symbol Counts</head><p>To maximize performance, it is critical to set the number of common words (which transform to themselves) as high as possible while satisfying the desired total vocabulary size, counting all the newly introduced symbols. In this section, we al- gebraically derive this optimal number of common words for each encoding scheme. We define the following:</p><p>V : Size of the original vocabulary. T : Number of pseudo-words of the form tX.</p><p>We are interested in maximizing C so that total encoding length is minimized. Repeat-All. We would like to encode the V − C rare words, using only W − C new symbols. To do so, for each new symbol (non-terminal node in our encoding tree), we have all W symbols under it in that branch. Therefore, we maximize C satisfying the constraint that</p><formula xml:id="formula_4">V − C ≤ (W − C) · W</formula><p>Repeat-Symbol. Out of the V − C rare words, we would like to pack them into a complete tree so that they may be encoded using our remaining W − C symbols. Therefore, we maximize C satisfying the constraint that</p><formula xml:id="formula_5">V − C ≤ (W − C) 2</formula><p>No-Repeats. Again, we desire to pack V − C rare words into a complete tree where we may use W − C symbols. To maximize C, we let S = T . Because S + T + C = W , we have that 2S + C = W . Therefore, we maximize C satisfying the con- straint that</p><formula xml:id="formula_6">V − C ≤ W − C 2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We trained a public implementation 1 of the sys- tem described in <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref> on the English-French parallel corpus from ACL WMT 2014, which contains 348M tokens. We evaluated on news-test-2014, also from WMT 2014, which contains 3003 sentences. All experiments used the same learning parameters and vocabulary size of 30,000. We constructed each encoding by the following method. First, we used the formulas derived in the previous section to calculate the optimal number of common words C for each encoding scheme, using V to be the true vocabulary size of the train- ing corpus and W = 30, 000. We then found the C most common words in the text and encoded them as themselves. For the remaining rare words, we encoded them using a distinct symbol whose form matched the one prescribed for each encoding scheme. The encoding was then applied separately 1 github.com/lisa-groundhog/GroundHog  <ref type="table">Table 1</ref>: BLEU scores (%) on detokenized test set for each encoding scheme after training for 5 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding BLEU # Common Words</head><p>to both the source text and the target text. Our en- coding schemes all increased the total number of tokens in the training corpus by approximately 4%.</p><p>To construct the mapping from rare words to their 2-word encodings, we binned rare words by frequency into branches. Thus, rare words of sim- ilar frequency in the training corpus tended to have encodings with the same first symbol. Simi- larly, the standard Huffman construction algorithm groups together rare words with similar frequen- cies within subtrees. More intelligent heuristics for constructing trees, such as using translation statis- tics instead of training corpus frequency, would be an interesting area of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>We used the RNNsearch-50 architecture from Bah- danau et al. (2014) as our machine translation sys- tem. We report results for this system alone, as well as for each of our three encoding schemes, using the BLEU metric ( <ref type="bibr" target="#b14">Papineni et al., 2002</ref>). <ref type="table">Table 1</ref> summarizes our results after training each vari- ant for 5 days, corresponding to roughly 2 passes through the 180K-sentence training corpus.</p><p>Alternative techniques that leverage bilingual resources have been shown to provide larger im- provements. <ref type="bibr" target="#b5">Jean et al. (2014)</ref> demonstrate an im- provement of 3.1 BLEU by using bilingual word co-occurrence statistics in an aligned corpus to re- place unknown word tokens. <ref type="bibr" target="#b7">Luong et al. (2014)</ref> demonstrate an improvement of up to 2.8 BLEU over a series of stronger baselines using an un- known word model that also makes predictions us- ing a bilingual dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>Our results indicate that the encoding scheme that keeps the highest number of common words, Repeat-All, performs best.  <ref type="table" target="#tab_1">Table 2</ref>: Test set precision (%) on common words and rare words for each encoding strategy. 1st Sym- bol denotes the precision of the first pseudo-word symbol in an encoded rare word.</p><p>sions are similar. Larger differences appear in the precision of rare words. The scheme that encodes rare words using both pseudo-words and common words gives substantially higher rare word accu- racy than any other approach. The final column of <ref type="table" target="#tab_1">Table 2</ref> shows the unigram precision of the first pseudo-word in an encoded rare word. The Repeat-All scheme uses only 60 different first symbols to encode all rare words. The other schemes require over 1,000. The fact that Repeat-All has a constrained set of rare word first symbols may account for its higher rare word precision.</p><p>It is possible for the model to predict an in- valid encoded sequence that does not correspond to any word in the original vocabulary. However, in our experiments, we did not observe any such sequences in the decoding of the test set. A rea- sonable way to deal with invalid sequences would be to drop them from the output during decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We described a novel approach for encoding the source and target text based on Huffman cod- ing schemes, eliminating the use of the unknown word symbol. An important continuation of our work would be to develop heuristics for effectively grouping "similar" words in the source and target text, so that they tend to have encodings that share a symbol. Even with our naive grouping by corpus frequency, our approach offers a simple way to pre- dict both common and rare words in a neural trans- lation model. As a result, performance improves by up to 1.7 BLEU. We expect that the simplic- ity of our technique will allow for straightforward combination with other enhancements and neural models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>W</head><label></label><figDesc>: Size of the encoded vocabulary. C: Number of common words. S: Number of pseudo-words of the form sX.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows</head><label>2</label><figDesc>the un- igram precision of each output. The common word translation accuracy is higher for all encoding schemes than for the baseline, although all preci-</figDesc><table>Encoding 

Common Rare 1st Symbol 
None 
62.0 
0.0 -
Repeat-All 
65.8 
28.0 64.8 
Repeat-Symbol 65.5 
16.5 24.8 
No-Repeats 
63.6 
15.8 25.7 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Recurrent continuous translation models</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1410.8206</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Tomáš Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI Stats</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
