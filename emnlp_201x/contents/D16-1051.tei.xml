<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Convex HMM Surrogate for Word Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">Arsene</forename><surname>Simion</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEOR Department New York</orgName>
								<orgName type="institution" key="instit1">Columbia University * New York</orgName>
								<orgName type="institution" key="instit2">Columbia University † Computer Science New York</orgName>
								<orgName type="institution" key="instit3">Columbia University</orgName>
								<address>
									<postCode>10011, 10027, 10027</postCode>
									<region>NY, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEOR Department New York</orgName>
								<orgName type="institution" key="instit1">Columbia University * New York</orgName>
								<orgName type="institution" key="instit2">Columbia University † Computer Science New York</orgName>
								<orgName type="institution" key="instit3">Columbia University</orgName>
								<address>
									<postCode>10011, 10027, 10027</postCode>
									<region>NY, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IEOR Department New York</orgName>
								<orgName type="institution" key="instit1">Columbia University * New York</orgName>
								<orgName type="institution" key="instit2">Columbia University † Computer Science New York</orgName>
								<orgName type="institution" key="instit3">Columbia University</orgName>
								<address>
									<postCode>10011, 10027, 10027</postCode>
									<region>NY, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Convex HMM Surrogate for Word Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="531" to="540"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Among the alignment models used in statistical machine translation (SMT), the hidden Markov model (HMM) is arguably the most elegant: it performs consistently better than IBM Model 3 and is very close in performance to the much more complex IBM Model 4. In this paper we discuss a model which combines the structure of the HMM and IBM Model 2. Using this surrogate, our experiments show that we can attain a similar level of alignment quality as the HMM model implemented in GIZA++ (Och and Ney, 2003). For this model, we derive its convex relaxation and show that it too has strong performance despite not having the local optima problems of non-convex objectives. In particular, the word alignment quality of this new convex model is significantly above that of the standard IBM Models 2 and 3, as well as the popular (and still non-convex) IBM Model 2 variant of (Dyer et al., 2013).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The IBM translation models are widely used in mod- ern statistical translation systems. Typically, one seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) proce- dure. Among the IBM Models, perhaps the most elegant is the HMM model ( <ref type="bibr" target="#b14">Vogel et al., 1996</ref>). The HMM is the last model whose expectation step is * Currently at Google. † Currently on leave at Google. both exact and simple, and it attains a level of ac- curacy that is very close to the results achieved by much more complex models. In particular, experi- ments have shown that IBM Models 1, 2, and 3 all perform worse than the HMM and Model 4 benefits greatly from being seeded by the HMM <ref type="bibr" target="#b15">(Och and Ney, 2003)</ref>.</p><p>In this paper we make the following contributions:</p><p>• We derive a new alignment model which com- bines the structure of the HMM and IBM Model 2 and show that its performance is very close to that of the HMM. There are several reasons why such a result would be of value (for more on this, see  and <ref type="bibr">(Simion et al., 2015a</ref>), for example).</p><p>• The main goal of this work is not to eliminate highly non-convex models such as the HMM entirely but, rather, to develop a new, power- ful, convex alignment model and thus push the boundary of these theoretically justified tech- niques further. Building on the work of (Simion et al., 2015a), we derive a convex relaxation for the new model and show that its performance is close to that of the HMM. Although it does not beat the HMM, the new convex model im- proves upon the standard IBM Model 2 signif- icantly. Moreover, the convex relaxation also performs better than the strong IBM 2 vari- ant FastAlign ( <ref type="bibr" target="#b3">Dyer et al., 2013</ref>), IBM Model 3, and the other available convex alignment models detailed in <ref type="bibr">(Simion et al., 2015a)</ref> and ).</p><p>• We derive a parameter estimation algorithm for new model and its convex relaxation based on the EM algorithm. Our model has both HMM emission probabilities and IBM Model 2's dis- tortions, so we can use Model 2 to seed both the model's lexical and distortion parameters. For the convex model, we need not use any initial- ization heuristics since the EM algorithm we derive is guaranteed to converge to a local op- tima that is also global.</p><p>The goal of our work is to present a model which is convex and has state of the art empirical perfor- mance. Although one step of this task was achieved for IBM Model 2 ( <ref type="bibr">Simion et al., 2015a</ref>), our tar- get goal deals with a much more local-optima-laden, non-convex objective. Finally, whereas IBM 2 in some ways leads to a clear method of attack, we will discuss why the HMM presents challenges that re- quire the insertion of this new surrogate.</p><p>Notation. We adopt the notation introduced in <ref type="bibr" target="#b15">(Och and Ney, 2003</ref>) of having 1 m 2 n denote the training scheme of m IBM Model 1 EM iterations followed by initializing Model 2 with these parame- ters and running n IBM Model 2 EM iterations. We denote by H the HMM and note that it too can be seeded by running Model 1 followed by Model 2. Additionally, we denote our model as 2 H , and note that it has distortion parameters like IBM Model 2 and emission parameters like that of the HMM. Under this notation, we let 1 m 2 n 2 o H denote running Model 1 for m iterations, then Model 2 for n iter- ation, and then finally our Model for o iterations. As before, we are seeding from the more basic to the more complex model in turn. We denote the convex relaxation of 2 H by 2 HC . Throughout this paper, for any integer N , we use [N ] to denote {1 . . . N } and [N ] 0 to denote {0 . . . N }. Finally, in our presentation, "convex function" means a func- tion for which a local maxima also global, for exam- ple, f (x) = −x 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IBM Models 1 and and the HMM</head><p>In this section we give a brief review of IBM Models 1, 2, and the HMM, as well as the the optimization problems arising from these models. The standard approach for optimization within these latent vari- able models is the EM algorithm. Throughout this section, and the remainder of the paper, we assume that our set of training examples is (e (k) , f (k) ) for k = 1 . . . n, where e (k) is the k'th English sentence and f (k) is the k'th French sen- tence. Following standard convention, we assume the task is to translate from French (the "source" lan- guage) into English (the "target" language) <ref type="bibr">1</ref> . We use E to denote the English vocabulary (set of pos- sible English words), and F to denote the French vocabulary. The k'th English sentence is a sequence of words e</p><formula xml:id="formula_0">(k) 1 . . . e (k) l k</formula><p>where l k is the length of the k'th English sentence, and each e (k) i ∈ E; similarly the k'th French sentence is a sequence f</p><formula xml:id="formula_1">(k) 1 . . . f (k) m k ,</formula><p>where m k is the length of the k'th French sentence, and each f</p><formula xml:id="formula_2">(k) j ∈ F . We define e (k)</formula><p>0 for k = 1 . . . n to be a special NULL word (note that E contains the NULL word).</p><p>For each English word e ∈ E, we will assume that D(e) is a dictionary specifying the set of possi- ble French words that can be translations of e. The set D(e) is a subset of F . In practice, D(e) can be derived in various ways; in our experiments we sim- ply define D(e) to include all French words f such that e and f are seen in a translation pair.</p><p>Given these definitions, the IBM Model 2 opti- mization problem is presented in several sources, for example, ). The parameters in this problem are t(f |e) and d(i|j, l, m). The ob- jective function for IBM Model 2 is then the log- likelihood of the training data; we can simplify the log-likelihood <ref type="bibr" target="#b9">(Koehn, 2008)</ref> as</p><formula xml:id="formula_3">1 n n k=1 m k j=1 log p(f (k) j |e (k) ) , where p(f (k) j |e (k) ) = l k i=0 t(f (k) j |e (k) i )d(i|j, l k , m k ) .</formula><p>1 Technically, in most standard sources <ref type="bibr" target="#b9">(Koehn, 2008)</ref>, this goes as follows: when we want to translate from French to En- glish we note that p(e|f ) ∝ p(f |e)p(e) by Bayes's Theorem. When translating, the alignment models we consider are con- cerned with modeling p(f |e) while the rest of the translation is handled by the language model p(e). Therefore, in the context of the original task, we have that English is the target language while French is the source. However, for the sake of clarity, we emphasize that the alignment models we study are concerned with the development of p(f |e).</p><p>This last simplification is crucial as it allows for a simple multinomial EM implementation, and can be done for IBM Model 1 as well <ref type="bibr" target="#b9">(Koehn, 2008)</ref>. Fur- thermore, the ability to write out the marginal like- lihood per sentence in this manner has seen other applications: it was crucial, for example, in deriving a convex relaxation of IBM Model 2 and solving the new problem using subgradient methods ).</p><p>An improvement on IBM Model 2, called the HMM alignment model, was introduced by Vogel et al ( <ref type="bibr" target="#b14">Vogel et al., 1996</ref>). For this model, the dis- tortion parameters are replaced by emission parame- ters d(a j |a j−1 , l). These emission parameters spec- ify the probability of the next alignment variable for the j th target word is a j , given that the previ- ous source word was aligned to a target word whose position was a j−1 in a target sentence with length of l. The objective of the HMM is given by</p><formula xml:id="formula_4">1 n n k=1 a (k) 1 ...a (k) m k log m k j=1 t(f (k) j |e (k) a k j )d(a (k) j |a (k) j−1 , l k )</formula><p>and we present this in <ref type="figure" target="#fig_0">Fig 1.</ref> We note that unlike IBM Model 2, we cannot simplify the exponential sum within the log-likelihood of the HMM, and so EM training for this model requires the use of a spe- cial EM implementation knows as the Baum-Welch algorithm <ref type="bibr" target="#b16">(Rabiner and Juang., 1986)</ref>.</p><p>Once these models are trained, each model's high- est probability (Viterbi) alignment is computed. For IBM Models 1 and 2, the Viterbi alignment splits easily <ref type="bibr" target="#b9">(Koehn, 2008)</ref>. For the HMM, dynamic pro- gramming is used ( <ref type="bibr" target="#b14">Vogel et al., 1996)</ref>. Although it is non-convex and thus its initialization is important, the HMM is the last alignment model in the classi- cal setting that has an exact EM procedure <ref type="bibr" target="#b15">(Och and Ney, 2003</ref>): from IBM Model 3 onwards heuristics are used within the expectation and maximization steps of each model's associated EM procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distortion and emission parameter structure</head><p>The structure of IBM Model 2's distortion param- eters and the HMM's emission parameters is im- portant and is used in our model as well, so we</p><formula xml:id="formula_5">Input: Define E, F , (e (k) , f (k) , l k , m k ) for k = 1 . . . n, D(e) for e ∈ E as in Section 2. Parameters: • A parameter t(f |e) for each e ∈ E, f ∈ D(e). • A parameter d(i|i, l k ) for each i ∈ [l k ] 0 , i ∈ [l k ] 0 . Constraints: ∀e ∈ E, f ∈ D(e), t(f |e) ≥ 0 (1) ∀e ∈ E, f ∈D(e) t(f |e) = 1 (2) ∀i, i ∈ [l k ] 0 , d(i |i, l k ) ≥ 0 (3) ∀i ∈ [l k ] 0 , i ∈[l k ] 0 d(i |i, l k ) = 1 (4) Objective: Maximize 1 n n k=1 a (k) 1 ...a (k) m k log m k j=1 t(f (k) j |e (k) a k j )d(a (k) j |a (k) j−1 , l k )</formula><p>with respect to the t(f |e) parameters d(i |i, l). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Distortion Parameters for IBM2</head><p>Let λ &gt; 0. For the IBM Model 2 distortions we set the NULL word probability as d(0|j, l, m) = p 0 , where p 0 = 1 l+1 and note that this will generally de- pend on the target sentence length within a bitext training pair that we are considering. For i = 0 which satisfies we set</p><formula xml:id="formula_6">d(i|j, l, m) = (1 − p 0 )e −λ| i l − j m | Z λ (j, l, m) ,</formula><p>where Z λ (j, l, m) is a normalization constant as in (Dyer et al., 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Emission Parameters for HMM</head><p>Let θ &gt; 0. For the HMM emissions we first set the NULL word generation to d(0|i, l) = p 0 , with</p><formula xml:id="formula_7">p 0 = 1 l+1 . For target word position i, i = 0, we set d(i |i, l) = (1 − p 0 )e −θ| i l − i l | Z θ (i, l, m) ,</formula><p>where Z θ (i, l, m) is a suitable normalization con- stant. Lastly, if i = 0 so that we are jump- ing from the NULL word onto a possibly different word, we set d(i |0, l) = p 0 . Aside from making the NULL word have uniform jump probability, the above emission parameters are modeled to favor a jumping to an adjacent English word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combining IBM Model 2 and the HMM</head><p>In deriving the new HMM surrogate, our main goal was to allow the current alignment to know as much as possible about the previous alignment variable and still have a likelihood that factors as that of IBM Model 2 ( <ref type="bibr" target="#b9">Koehn, 2008)</ref>. We combine IBM Model 2 and the HMM by incorpo- rating the generation of words using the structure of both models. The model we introduce, IBM2- HMM, is displayed in <ref type="figure" target="#fig_1">Fig 2.</ref> Consider a target-source sentence pair (e, f ) with |e| = l and |f | = m. For source sentence positions j and j + 1 we have source words f j and f j+1 and we assign a joint probability involving the alignments a j and a j+1 as:</p><formula xml:id="formula_8">q(j, a j , a j+1 , l, m) = (5) t(f j |e aj )d(a j |j, l, m)t(f j+1 |e aj+1 )d(a j+1 |a j , l) . (6)</formula><p>From the equation above, we note that we use the IBM Model 2's word generation method for posi- tion j and the HMM generative structure for position j + 1. The generative nature of the above procedure introduces dependency between adjacent words two at a time. Since we want to mimic the HMM's struc- ture as much as possible, we devise our likelihood function to mimic the HMM's dependency between alignments using q. Essentially, we move the source word position j from 1 to m and allow for overlap- ping terms when j ∈ {2, . . . , m − 1}. In what fol- lows, we describe this representation in detail.</p><p>The likelihood in Eq. 16 is actually the sum of two likelihoods which use equations Eq. 5 and 6 repeat- edly. To this end, we will discuss how our objective is actually</p><formula xml:id="formula_9">1 n n k=1 log a (k) ,b (k) p(f (k) , a (k) , b (k) |e (k) ) ,<label>(7)</label></formula><p>where a (k) and b (k) both are alignment vectors whose components are independent and can take on any values in [l k ] 0 . To see how p(f, a, b|e) comes about, note that we could generate the sentence f by generating pairs (1, 2), (3, 4), (5, 6), . . . using equa- tions Eqs. 5 and 6 for each pair. Taking all this to- gether, the upshot of our discussion is that generat- ing the pair (e, f ) in this way gives us that the like- lihood for an alignment a would be given by:</p><formula xml:id="formula_10">p 1 (f, a|e) = m−1 j odd q(j, a j , a j+1 , l, m) .<label>(8)</label></formula><p>Using the same idea as above, we could also skip the first target word position and generate pairs (2, 3), (4, 5), . . . using Eqs. 5 and 6. Under this sec- ond generative method, the joint probability for f and alignment b is:</p><formula xml:id="formula_11">p 2 (f, b|e) = m−1 j even q(j, b j , b j+1 , l, m) ,<label>(9)</label></formula><p>Finally, we note that if m is even we do not generate f 1 and f m under p 2 but we do generate these words under p 1 . Similarly, if m is odd we do not generate f 1 under p 2 and we do not gen- erate f m under p 1 ; however in this case as in the first, we still generate these missing words under the other generative method. Using p(f, a, b|e) = p 1 (f, a|e)p 2 (f, b|e) and factoring the log-likelihood as in IBM Model 1 and 2 <ref type="bibr" target="#b9">(Koehn, 2008)</ref>, we get the log-likelihood in <ref type="figure" target="#fig_1">Fig 2.</ref> Finally, we note that our model's log-likelihood could be viewed as the sum of the log-likelihoods of a model which generates (e, f ) using p 1 and another model which generates sentences using p 2 . These models share parameters but generate words using different recipes, as dis- cussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The parameter estimation for IBM2-HMM</head><p>To fully optimize our new model (over t, λ, and θ), we can use an EM algorithm in the same fashion as</p><formula xml:id="formula_12">Input: Define E, F , (e (k) , f (k) , l k , m k ) for k = 1 . . . n, D(e)</formula><p>for e ∈ E as in Section 2. Parameters:</p><p>• A parameter t(f |e) for each e ∈ E, f ∈ D(e).</p><p>• A parameter λ &gt; 0 for distortion centering.</p><p>• A parameter θ &gt; 0 for emission centering.</p><p>Constraints:</p><formula xml:id="formula_13">∀e ∈ E, f ∈ D(e), t(f |e) ≥ 0 (10) ∀e ∈ E, f ∈D(e) t(f |e) = 1 (11) ∀i ∈ [l k ] 0 , j ∈ [m k ], d(i|j, l k , m k ) ≥ 0 (12) ∀j ∈ [m k ], i∈[l k ] 0 d(i|j, l k , m k ) = 1 (13) ∀i, i ∈ [l k ] 0 , d(i |i, l k ) ≥ 0 (14) ∀i ∈ [l k ] 0 , i ∈[l k ] 0 d(i |i, l k ) = 1 (15) Objective: Maximize 1 n n k=1 m k −1 j=1 log l k i=0 l k i =0 q(j, i, i , l k , m k )<label>(16)</label></formula><p>with respect to the parameters t(f |e), d(i |i, l) d(i|j, l, m), and q(j, i, i , l k , m k ) set as ( <ref type="bibr" target="#b3">Dyer et al., 2013)</ref>. Specifically, for the model in question the EM algorithm still applies but we have to use a gradient-based algorithm within the learning step. On the other hand, since such a gradient-based method introduces the necessary complication of a learning rate, we could also optimize the objective by picking θ and λ via cross-validation and using a multinomial EM algorithm for the learning of the lexical t terms. For our experiments, we opted for this simpler choice: we derived a multinomial EM algorithm and cross-validated the centering param- eters for the distortion and emission terms. With λ and θ fixed, the derivation of this algorithm is very similar to the one used for IBM2-HMM's convex re- laxation and this uses the path discussed in (Simion et al., 2015a) and ( <ref type="bibr">Simion et al., 2015b</ref>). We detail the EM algorithm for the convex relaxation below.</p><formula xml:id="formula_14">t(f (k) j |e (k) i )d(i|j, l, m)t(f (k) j+1 |e i )d(i |i, l) (17)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">A Convex HMM Surrogate</head><p>In this section we detail a procedure to get a con- vex relaxation for IBM2-HMM. Let (t, d) be all the parameters of the HMM. As a first step in getting a convex HMM, one could follow the path devel- oped in (Simion et al., 2015a) and directly replace the HMM's objective terms</p><formula xml:id="formula_15">m k j=1 t(f (k) j |e (k) a k j )d(a (k) j |a (k) j−1 , l k ) by ( m k j=1 t(f (k) j |e (k) a k j )d(a (k) j |a (k) j−1 , l k )) 1 2m k .</formula><p>In particular, the geometric mean function <ref type="bibr" target="#b0">Boyd and Vandenberghe, 2004)</ref>) and, for a given sentence pair (e (k) , f (k) ) with alignment a (k) we can find a projection matrix P so that P(t, d) = ( ˜ t, ˜ d) where˜twhere˜where˜t = {t(f</p><formula xml:id="formula_16">h(x 1 , . . . , h 2m k ) = ( 2m k j=1 x j ) 1 2m k is convex ((</formula><formula xml:id="formula_17">(k) j |e (k) a k j )} m k j=1 and˜dand˜ and˜d = {d(a (k) j |a (k) j−1 , l k )} m k j=1</formula><p>are exactly the parameters used in the term above (in particular, t, d are the set of all parameters while˜twhile˜while˜t, ˜ d are the set of parameters for the specific training pair k; P projects from the full space onto only the parameters used for training pair k). Given this, we then have that g(t, d) = h(P(t, d)) = h( ˜ t, ˜ d) is convex and, by composition, so is log g(t, d) (see <ref type="bibr">(Simion et al., 2015a;</ref><ref type="bibr" target="#b0">Boyd and Vandenberghe, 2004</ref>) for details; the main idea lies in the fact that as linear transformations preserve convexity, so do compositions of convex functions with increasing convex functions such as log). Finally, if we run this plan for all terms in the objective, the new objective is convex since it is the sum of convex functions (the new optimization problem is convex as it has linear constraints). Although this gives a convex program, we observed that the powers being so small made the optimized probabilities very uninformative (i.e. uniform). The above makes sense: no matter what the parameters are, we will easily get the 1 we seek for each term in the objective since all terms are taken to a low ( 1 2m k ) power .</p><p>Since this direct relaxation does not yield fruit, we next could turn to our model. Developing its re- laxation in the vein of (Simion et al., 2015a), we could be to let d(i|j, l, m) and d(i |i, l) be multino- mial probabilities (that is, these parameters would not have centering parameters λ and θ and would be just standard probabilities as in the GIZA++ ver- sions of the HMM and IBM Model 2 ( <ref type="bibr" target="#b15">Och and Ney, 2003)</ref>) and replace all the terms q(j, i , i, l, m) in <ref type="formula" target="#formula_13">(16)</ref>  <ref type="figure">by (q(j, i , i, l, m)</ref>) 1 4 . Although this method is feasible, experiments showed that the relaxation is not very competitive and performs on par with IBM Model 2; this relaxation is far in performance from the HMM even though we are relaxing (only) the product of 4 terms (lastly, we mention that we tried other variants were we replaced d(i|j, l, m)d(i |i, l) by d(i, i |j, l, m) so that we would have only three terms; unfortunately, this last attempt also produced parameters that were "too uniform").</p><p>The above analysis motivates why we defined our model as we did: we now have only two terms to relax. In particular, to rectify the above, we left in place the structure discussed in Section 3 and made λ and θ be tuning parameters which we can cross-validate for on a small held-out data set. This last constraint effectively removes the distortion and emission parameters from the model but we still maintain the structural property of these parame- ters: we maintain their favoring the diagonal or ad- jacent alignment. To get the relaxation, we replace q(j, i, i , l, m) by</p><formula xml:id="formula_18">p(j, i, i , l, m) ∝ t(f (k) j |e (k) i )t(f (k) j+1 |e i )</formula><p>and set the proportionality constant to be d(i|j, l, m)d(i |i, l). Using this setup we now have a convex objective to optimize over. In particular, we've formulated a convex relaxation of the IBM2-HMM problem which, like the Support Vector Machine, includes parameters that can be cross-validated over ( <ref type="bibr" target="#b0">Boyd and Vandenberghe, 2004)</ref>.</p><formula xml:id="formula_19">Input: Define E, F , (e (k) , f (k) , l k , m k ) for k = 1 . .</formula><p>. n, D(e) for e ∈ E as in Section 2. Pick λ, θ &gt; 0 as in Section 3 via cross-validation. Parameters:</p><p>• A parameter t(f |e) for each e ∈ E, f ∈ D(e).</p><p>Constraints:</p><formula xml:id="formula_20">∀e ∈ E, f ∈ D(e), t(f |e) ≥ 0<label>(18)</label></formula><p>∀e ∈ E,</p><formula xml:id="formula_21">f ∈D(e) t(f |e) = 1<label>(19)</label></formula><p>Objective: Maximize</p><formula xml:id="formula_22">1 n n k=1 m k −1 j=1 log l k i=0 l k i =0 p(j, i, i , l k , m k )<label>(20)</label></formula><p>with respect to the parameters t(f |e) and </p><formula xml:id="formula_23">p(j, i, i , l k , m k ) set as t(f (k) j |e (k) i )d(i|j, l, m) t(f (k) j+1 |e i )d(i |i, l)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">An EM algorithm for the convex surrogate</head><p>The EM algorithm for the convex relaxation of our surrogate is given in <ref type="figure" target="#fig_5">Fig 4.</ref> As the model's objective is the sum of the objectives of two models generated by a multinomial rule, we can get a very succinct EM algorithm. For more details on this and a simi- lar derivation, please refer to ( <ref type="bibr">Simion et al., 2015a)</ref>, <ref type="bibr" target="#b9">(Koehn, 2008)</ref> or <ref type="bibr">(Simion et al., 2015b</ref>). For this al- gorithm, we again note that the distortion and emis- sion parameters are constants so that the only esti- mation that needs to be conducted is on the lexical t terms.</p><p>To be specific, we have that the M step requires optimizing</p><formula xml:id="formula_24">1 n n k=1 log a (k) ,b (k) q(a (k) , b (k) |e (k) , f (k) )p(f (k) , a (k) , b (k) |e (k) ) .</formula><p>In the above, we have that</p><formula xml:id="formula_25">q(a (k) , b (k) |e (k) , f (k) )</formula><p>are constants proportional to</p><formula xml:id="formula_26">m k −1 j=1 t(f (k) j |e (k) a (k) j )t(f (k) j+1 |e (k) a (k) j+1 ) m k j=2 t(f (k) j |e (k) b (k) j )t(f (k) j+1 |e (k) b (k) j+1 )</formula><p>and gotten through the E step. This optimization step is very similar to the regular Model 2 M step since the β drops down using log t β = β log t; the exact same count-based method can be applied. The upshot of this is given in <ref type="figure" target="#fig_5">Fig 4;</ref> similar to the logic above for 2 HC , we can get the EM algorithm for 2 H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Decoding methods for IBM2-HMM</head><p>When computing the optimal alignment we wanted to compare our model with the HMM as closely as possible. Because of this, the most natural method of evaluating the quality of the parameters would be to use the same rule as the HMM. Specifically, for a sentence pair (e, f ) with |e| = l and |f | = m, in HMM decoding we aim to find (a 1 . . . a m ) which maximizes</p><formula xml:id="formula_27">max a 1 ,...,am m j=1 t(f j |e a j )d(a j |a j−1 , l).</formula><p>As is standard, dynamic programming can now be used to find the Viterbi alignment. Although there are a number of ways we could define the opti- mal alignment, we felt that the above would be the best since it tests dependance between alignment variables and allows for easy comparison with the GIZA++ HMM. Finding the optimal alignment un- der the HMM setting is labelled "HMM" in <ref type="table" target="#tab_2">Table 1</ref>. We can also find the optimal alignment by taking the objective literally (see <ref type="bibr">(Simion et al., 2014</ref>) for a similar argument dealing with the convex relaxation of IBM Model 2) and computing max a 1 ...am</p><formula xml:id="formula_28">p 1 (f, a|e)p 2 (f, a|e).</formula><p>Above, we are asking for the optimal alignment that yields the highest probability alignment through generating technique p 1 and p 2 . This method of de- coding is a lot like the HMM style and also relies 1: Input: Define E, F , (e (k) , f (k) , l k , m k ) for k = 1 . . . n, D(e) for e ∈ E as in Section 2. Two pa- rameters λ, θ &gt; 0 picked by cross-validation so that the distortions and emissions are constants obeying the structure in Section 3. An integer T specifying the number of passes over the data. 2: Parameters:</p><p>• A parameter t(f |e) for each e ∈ E, f ∈ D(e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3: Initialization:</head><p>• ∀e ∈ E, f ∈ D(e), set t(f |e) = 1 D(e) . 4: EM Algorithm: Expectation 5: for all k = 1 . . . N do </p><formula xml:id="formula_29">for all i = 0 . . . l k do 15: δ[i, i ] = δ[i,i ] ∆ 16: counts(f (k) j , e (k) i )+ = δ[i, i ] 17: counts(e (k) i )+ = δ[i, i ] 18: counts(f (k) j+1 , e (k) i )+ = δ[i, i ] 19:</formula><p>counts(e  on dynamic programming. In this case we have the recursion for Q Joint given by</p><formula xml:id="formula_30">Q Joint (1, i) = t(f 1 |e i )d 2 (i|1, l, m) , ∀i ∈ [l] 0 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and</head><formula xml:id="formula_31">Q Joint (j, i ) = t 2 (f j |e i )d(i |j, l, m)M Joint (j − 1, i ) , where M Joint (j − 1, i ) is M Joint (j − 1, i ) = l max i=0 {d(i |i, l)Q Joint (j − 1, i)} , ∀ 2 ≤ j ≤ m, ∀ i ∈ [l] 0 .</formula><p>The alignment results got- ten by decoding with this method is labelled "Joint" in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>537</head><p>In this section we describe experiments using the IBM2-HMM optimization problem combined with the EM algorithm for parameter estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Data Sets</head><p>We use data from the bilingual word alignment workshop held at HLT-NAACL 2003 <ref type="bibr" target="#b12">(Michalcea and Pederson, 2003)</ref>. We use the Canadian Hansards bilingual corpus, with 743,989 English-French sen- tence pairs as training data, 37 sentences of devel- opment data, and 447 sentences of test data (note that we use a randomly chosen subset of the origi- nal training set of 1.1 million sentences, similar to the setting used in <ref type="bibr" target="#b13">(Moore, 2004)</ref>). The develop- ment and test data have been manually aligned at the word level, annotating alignments between source and target words in the corpus as either "sure" (S) or "possible" (P ) alignments, as described in <ref type="bibr" target="#b15">(Och and Ney, 2003)</ref>. As is standard, we lower-cased all words before giving the data to GIZA++ and we ig- nored NULL word alignments in our computation of alignment quality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Methodology</head><p>We test several models in our experiments. In par- ticular, we empirically evaluate our models against the GIZA++ IBM Model 3 and HMM, as well as the FastAlign IBM Model 2 implementation of ( <ref type="bibr" target="#b3">Dyer et al., 2013</ref>) that uses Variational Bayes. For each of our models, we estimated parameters and got align- ments in turn using models in the source-target and target-source directions; using the same setup as , we present the gotten inter- sected alignments. In training, we employ the stan- dard practice of initializing non-convex alignment models with simpler non-convex models. In par- ticular, we initialize, the GIZA++ HMM with IBM Model 2, IBM Model 2 with IBM Model 1, and IBM2-HMM and IBM Model 3 with IBM Model 2 preceded by Model 1. Lastly, for FastAlign, we initialized all parameters uniformly since this em- pirically was a more favorable initialization, as dis- cussed in <ref type="bibr" target="#b3">(Dyer et al., 2013)</ref>.</p><p>We measure the performance of the models in terms of Precision, Recall, F-Measure, and AER us- ing only sure alignments in the definitions of the first three metrics and sure and possible alignments in the definition of AER , as in (  and ( <ref type="bibr" target="#b11">Marcu et al., 2006</ref>). For our experiments, we report results in both AER (lower is better) and F-Measure (higher is better) <ref type="bibr" target="#b15">(Och and Ney, 2003)</ref>. <ref type="table" target="#tab_2">Table 1</ref> shows the alignment summary statistics for the 447 sentences present in the Hansard test data. We present alignments quality scores using either the FastAlign IBM Model 2, the GIZA++ HMM, and our model and its relaxation using either the "HMM" or "Joint" decoding. First, we note that in deciding the decoding style for IBM2-HMM, the HMM method is better than the Joint method. We expected this type of performance since HMM de- coding introduces positional dependance among the entire set of words in the sentence, which is shown to be a good modeling assumption ( <ref type="bibr" target="#b14">Vogel et al., 1996)</ref>.</p><p>From the results in <ref type="table" target="#tab_2">Table 1</ref> we see that the HMM outperforms all other models, including IBM2- HMM and its convex relaxation. However, IBM2- HMM is not far in AER performance from the HMM and both it and its relaxation do better than FastAl- ign or IBM Model 3 (the results for IBM Model 3 are not presented; a one-directional English-French run of 1 5 2 5 3 15 gave AER and F-Measure numbers of 0.1768 and 0.6588, respectively, and this was behind both the IBM Model 2 FastAlign and our models).</p><p>As a further set of experiments, we also appended an IBM Model 1 or IBM Model 2 objective to our models's original objectives, so that the constraints and parameters are the same but now we are maxi- mizing the average of two log-likelihoods. With re- gard to the EM optimization, we would only need to add another δ parameter: we'd now have proba-  <ref type="bibr">(Simion et al., 2015a)</ref>). We note that the appended IBM Model 2 objective is still convex if we fix the distortions' λ parameter and then optimize for the t parameters via EM (thus, model 2 HC is still convex). For us, there were significant gains, especially in the con- vex model. The results for all these experiments are shown in <ref type="table" target="#tab_4">Table 2</ref>, with IBM 2 smoothing for the con- vex model displayed in the rightmost column.</p><formula xml:id="formula_32">bilities δ 1 [i] ∝ t(f (k) j |e (k) i )d(i|j, l l , m k ) (</formula><p>Finally, we also tested our model in the full  SMT pipeline using the cdec system ( <ref type="bibr" target="#b3">Dyer et al., 2013</ref>). For our experiments, we compared our models' alignments (gotten by training 1 5 2 5 H and 2 5 HC ) against the alignments gotten by the HMM (1 5 2 5 H 5 ), IBM Model 4 (1 5 H 5 3 3 4 3 ), and FastAl- ign. Unfortunately, we found that all 4 systems led to roughly the same BLEU score of 40 on a Spanish-English training set of size 250000 which was a subset of version 7 of the Europarl dataset ( <ref type="bibr" target="#b3">Dyer et al., 2013</ref>). For our development and test sets, we used data each of size roughly 1800 and we preprocessed all data by considering only sen- tences of size less than 80 and filtering out sentences which had a very large (or small) ratio of target and source sentence lengths. Although the SMT results were not a success in that our gains were not signif- icant, we felt that the experiments at least highlight that our model mimics the HMM's alignments even though its structure is much more local. Lastly, we in regards to the new convex model's performance, we observe much better alignment quality than any other convex alignment models in print, for exam- ple, <ref type="bibr">(Simion et al., 2015a</ref>).   </p><note type="other">Training 1 5 2 10 H 1 5 2 10 H 2 10 HC 2 10 HC FA 10 1 5 2 5 H</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusions and Future Work</head><p>Our work has explored some of the details of a new model which combines the structure of IBM Model 2 the alignment HMM Model. We've shown that this new model and its convex relaxation performs very close to the standard GIZA++ implementation of the HMM. Bridging the gap between the HMM and convex models proves difficult for a number of reasons ( <ref type="bibr" target="#b6">Guo and Schuurmans, 2007)</ref>. In this pa- per, we have introduced a new set of ideas aimed at tightening this gap.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The HMM Optimization Problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The IBM2-HMM Optimization Problem. We use equation (5) within the likelihood definition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The IBM2-HMM convex relaxation optimization problem. Note that the distortions d(i|j, l, , m) and emissions d(i |i, l) are constants held fixed and parameterized by crossvalidated parameters λ and θ as in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for all i = 0 . . . l k do 10: for all i = 0 . . . l k do 11: δ[i, i ] = p(j, i , i, l k , m k ) 12: ∆+ = δ[i, i ] 13: for all i = 0 . . . l k do 14:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>EM Algorithm: Maximization 21: for all e ∈ E do 22: for all f ∈ D(e) do 23: t(f |e) = counts(e,f ) counts(e) 24: Output: t parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pseudocode for the EM algorithm of the IBM2HMM's convex relaxation. As the distortion and emission parameters are constants, the algorithm is very similar to that of IBM Model 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>"</head><label></label><figDesc>smoothing" means adding these log-likelihoods to the original objective as in (Simion et al., 2013). For the convex relaxation of IBM2-HMM, we can only smooth by adding in the convex IBM Model 1 objective, or by adding in an IBM Model 2 objec- tive where the distortions are taken to be constants (these distor- tions are identical to the ones that are used within the relaxation itself and are cross-validated for optimal λ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Alignment quality results for IBM2-HMM (2H) and 

its convex relaxation (2HC) using either HMM-style dynamic 

programming or "Joint" decoding. The first and last columns 

above are for the GIZA++ HMM initialized either with IBM 

Model 1 or Model 1 followed by Model 2. FA above refers to 

the improved IBM Model 2 (FastAlign) of (Dyer et al., 2013). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Alignment quality results for IBM2-HMM and its 

relaxation using IBM 1 and IBM 2 smoothing (in this case, 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Andrei Simion was supported by a Google research award. Cliff Stein was partially supported by NSF grant CCF-1421161. We thank the reviewers for their insightful commentary and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<title level="m">The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum Likelihood From Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society, series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Simple, Fast, and Effective Reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring Word Alignment Quality for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="303" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expectation Maximization and Posterior Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">V</forename><surname>Graca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convex Relaxations of Latent Variable Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word Alignment via Quadratic Assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL</title>
		<meeting>the HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical Significance Tests for Machine Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Alignment by Agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SPMT: Statistical Machine Translation with Syntactified Target Language Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Evaluation Exercise in Word Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Michalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2003: Workshop in building and using Parallel Texts: Data Driven Machine Translation and Beyond</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving IBM WordAlignment Model 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HMM-Based Word Alignment in Statistical Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational-Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="52" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An Introduction to Hidden Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ASSP Magazine</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Convex Alternative to IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Simion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Some Experiments with a Convex IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Simion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Family of Latent Variable Convex Relaxations for IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Simion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On a Strictly Concave IBM Model 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Simion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the L0-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
