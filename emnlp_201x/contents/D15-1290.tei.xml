<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Single Word is not Enough: Ranking Multiword Expressions Using Distributional Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Computer Science Department</orgName>
								<orgName type="institution">Technische Universität Darmstadt Hochschulstrasse</orgName>
								<address>
									<addrLine>10</addrLine>
									<postCode>D-64289</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Computer Science Department</orgName>
								<orgName type="institution">Technische Universität Darmstadt Hochschulstrasse</orgName>
								<address>
									<addrLine>10</addrLine>
									<postCode>D-64289</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Single Word is not Enough: Ranking Multiword Expressions Using Distributional Semantics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new unsupervised mechanism , which ranks word n-grams according to their multiwordness. It heavily relies on a new uniqueness measure that computes, based on a distributional thesaurus , how often an n-gram could be replaced in context by a single-worded term. In addition with a downweighting mechanism for incomplete terms this forms a new measure called DRUID. Results show large improvements on two small test sets over competitive baselines. We demonstrate the scalability of the method to large corpora, and the independence of the measure of shallow syntactic filtering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While it seems intuitive to treat certain sequences of tokens as single terms, there is still consider- able controversy about the definition of what ex- actly such a multiword expression (MWE) con- stitutes. <ref type="bibr" target="#b22">Sag et al. (2001)</ref> pinpoint the need of treating MWEs correctly and classify a range of syntactic formations that could form MWEs and define MWEs as being non-compositional with re- spect to the meaning of their parts. While the exact requirements on MWEs is bound to specific tasks (such as parsing, keyword extraction, etc.), we op- erationalize the notion of non-compositionality by using distributional semantics and introduce a new measure that works well for a range of task-based MWE definitions.</p><p>Most previous MWE ranking approaches use the following mechanisms to determine multiwordness:</p><p>part-of-speech (POS) tags, word/multiword frequency and significance of co-occurrence of the parts. In this paper we do not want to introduce "yet another ranking function" but rather an additional mechanism, which performs ranking based on distributional semantics.</p><p>Distributional semantics has already been used for MWE identification, but mainly to discriminate between compositional and non- compositional MWEs ( <ref type="bibr" target="#b24">Schone and Jurafsky, 2001;</ref><ref type="bibr" target="#b23">Salehi et al., 2014;</ref><ref type="bibr">Hermann and Blunsom, 2014</ref>). Here we introduce a new concept to de- scribe the multiwordness of a term by its unique- ness. Using the uniqueness score we measure how likely a term in context can be replaced by a single word. This measure is motivated by the semiotic consideration that due to parsimony con- cepts are often expressed as single words. Further- more, we implement a context-aware punishment, called incompleteness, which degrades the score of terms that seem incomplete regarding their con- texts. Both concepts are combined into a single score we call DRUID, which is calculated based on a distributional thesaurus. In the following, we show the impact of that new method for French and English and also examine the effect of cor- pus size on MWE extraction. Additionally, we report on results without using any linguistic pre- processing except tokenization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The generation of MWE dictionaries has drawn much attention in the field of Natural Lan- guage Processing (NLP). Early computational approaches (e.g. <ref type="bibr" target="#b11">Justeson and Katz (1995)</ref>) use POS sequences as MWE extractors. Other ap- proaches, relying on word frequency, statistically verify the hypothesis whether the parts of the MWE occur more often together than would be expected by chance <ref type="bibr" target="#b16">(Manning and Schütze, 1999;</ref><ref type="bibr" target="#b7">Evert, 2005;</ref><ref type="bibr" target="#b21">Ramisch, 2012)</ref>. One of the first measures that consider context information (co- occurrences) are the C-value and the NC-value in- troduced by <ref type="bibr" target="#b9">Frantzi et al. (1998)</ref>. These meth- ods first extract candidates using POS information and then compute scores based on the frequency of the MWE and the frequency of nested MWE candidates. The method described by <ref type="bibr" target="#b28">Wermter and Hahn (2005)</ref> computes a score by multiplying the frequency of a candidate when placing wild- cards for each word. A newer method is intro- duced in <ref type="bibr" target="#b15">Lossio-Ventura et al. (2014)</ref>, which re- ranks scores based on an extension of the C-value, which uses a POS-based probability and an inverse document frequency. Using different measures and learning a classifier that predicts the multi- wordness was first proposed by <ref type="bibr" target="#b19">Pecina (2010)</ref>, who, however, restricts his experiments to two- word MWEs for the Czech language only. <ref type="bibr" target="#b14">Korkontzelos (2010)</ref> comparatively evaluates several MWE ranking measures. The best MWE extrac- tor reported in his work is the scorer by <ref type="bibr" target="#b17">(Nakagawa and Mori, 2002;</ref><ref type="bibr" target="#b18">Nakagawa and Mori, 2003)</ref>, who use the un-nested frequency (called marginal frequency) of each candidate and multiply these by the geometric mean of the distinct neighbor of each word within the candidate.</p><p>Distributional semantics is mostly used to de- tect compositionality of MWEs ( <ref type="bibr" target="#b23">Salehi et al., 2014;</ref><ref type="bibr" target="#b12">Katz and Giesbrecht, 2006</ref>). Most ap- proaches therefore compare the context vector of a MWE with the combined vectors based on the constituent words of the MWE. The similarity be- tween the vectors is then used as degree for com- positionality. In machine translation, words are sometimes considered as multiwords if they can be translated as single term <ref type="bibr">(cf. (Bouamor et al., 2012;</ref><ref type="bibr" target="#b1">Anastasiou, 2010)</ref>). Whereas this follows the same intuition as our uniqueness measure, we do not require any bilingual corpora.</p><p>Regarding the evaluation, mostly precision at k (P @k) and recall at k (R@k) are applied (e.g. <ref type="bibr" target="#b7">(Evert, 2005;</ref><ref type="bibr" target="#b9">Frantzi et al., 1998;</ref><ref type="bibr" target="#b15">Lossio-Ventura et al., 2014)</ref>). Another general approach is using the average precision (AP), which is also used in In- formation Retrieval (IR) <ref type="bibr" target="#b27">(Thater et al., 2009)</ref> and has also been applied by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baselines and Previous Approaches</head><p>We will evaluate our method by comparing our MWE ranking to multiword lists that have been annotated in corpora. Here, we introduce an up- per bound and two baseline methods and give a brief description of the competitive methods used in this paper. Most of these methods require a list of candidate terms T , usually extracted with POS sequences (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Upper Bound</head><p>We use a perfect ranking as upper bound, where we rank all positive candidates before all negative ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lower Baseline and Frequency Baseline</head><p>The ratio between true candidates and all candi- dates serves as lower baseline, which is also called baseline precision <ref type="bibr" target="#b8">(Evert, 2008)</ref>. The second base- line is the frequency baseline, which ranks can- didate terms t ∈ T according to their frequency f req(t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">C-value/NC-value</head><p>The commonly used C-value (see Eq. 1) was de- veloped by <ref type="bibr" target="#b9">Frantzi et al. (1998)</ref>. The first fac- tor, logarithm of the term length in words, favors longer MWEs. The second factor is the frequency of the term reduced by the average frequency of all candidate terms T , which nest the term t, i.e. t is a substring of the terms we denote as T t .</p><formula xml:id="formula_0">c(t) = log 2 (|t|)(freq(t) − 1 |T t | b∈Tt freq(b)) (1)</formula><p>An extension of the C-value was proposed by <ref type="bibr" target="#b9">Frantzi et al. (1998)</ref> as well and is named NC- value. It takes advantage of context words C t by assigning weights to them. As context words only nouns, adjectives and verbs are considered 1 . Con- text words are weighted with Equation 2, where k denotes the number of times the context word c ∈ C t occurs with any of the candidate terms. This number is normalized by the number of can- didate terms.</p><formula xml:id="formula_1">w(c) = k |T |<label>(2)</label></formula><p>The NC-value is a weighted sum of the C-value and the product of the term t occurring with each context c which form the term t c :</p><formula xml:id="formula_2">nc(t) = 0.8 * c(t) + 0.2 c∈Ct f req(t c )w(c). (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">t-test</head><p>The t-test (see e.g. ( <ref type="bibr">Manning and Schütze, 1999, p.163)</ref>) is a statistical test for the significance of co-occurrence of two words. It relies on the proba- bilities of the term and its single words. The prob- ability of a word p(w) is defined as the frequency of the term divided by the total number of terms of the same length. The t-test statistic is computed using Equation 4 with f req(.) being the total fre- quency of unigrams.</p><formula xml:id="formula_3">t(w 1 . . . w n ) ≈ p(w 1 . . . w n ) − n i=1 p(w i ) p(w 1 . . . w n )/f req(.)<label>(4)</label></formula><p>We then use this score to rank the candidate terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">FGM Score</head><p>Another method inspired by the C/NC-value is proposed in ( <ref type="bibr" target="#b17">Nakagawa and Mori, 2002;</ref><ref type="bibr" target="#b18">Nakagawa and Mori, 2003)</ref>. The method was developed on a Japanese dataset and outperformed a modi- fied C-value 2 measure. The method is composed of two scoring mechanisms for the candidate term t as shown in Equation 5.</p><formula xml:id="formula_4">F GM (t) = GM (t) × M F (t)<label>(5)</label></formula><p>The first term in the equation is a geometric mean GM (.) of the number of distinct direct left l(.) and right r(.) neighboring words for each single word t i within t.</p><formula xml:id="formula_5">GM (t) = 2|t| t i ∈t (|l(t i )| + 1)(|r(t i )| + 1) (6)</formula><p>The neighboring words are extracted directly from the corpus; the method does neither rely on can- didate lists nor POS tags. To the contrary, the marginal frequency M F (t) relies on the candi- date list and the underlying corpus. This frequency counts how often the candidate term occurs within the corpus and is not a subset of a candidate. In Korkontzelos (2010) it was shown that while scor- ing according to Equation 5 leads to comparatively good results, it is consistently outperformed by MF only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Uniqueness and Incompleteness</head><p>We present two new mechanisms relying on a Dis- tributional Thesaurus (DT), which we use to rank terms regarding their multiwordness: A score for the uniqueness of a term and a punishing score that conveys the incompleteness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Similarity Computation</head><p>The DT is computed based on <ref type="bibr" target="#b2">Biemann and Riedl (2013)</ref>. First we extract n-grams from text and consider the left and the right neighbor of each n- gram as context feature. Then, we calculate the Lexicographer's Mutual Information (LMI) sig- nificance score <ref type="bibr" target="#b3">(Bordag, 2008)</ref> between n-grams and features and remove all context features, which co-occur with more than 1000 terms, as these features tend to be to general. In the next step we keep for each n-gram only the 1000 con- text features, with the highest LMI score. The similarity score is then computed based on the overlap of features between two terms. Due to pruning this overlap-based significance measure is proportional to the Jaccard similarity measure, al- beit we do not consider any normalization. After computing the feature overlap between two terms, we keep for each n-gram the 200 most similar n- grams. An example for the most similar n-grams to the terms red blood cell and red blood including their feature overlap are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Uniqness Computation</head><p>The first mechanism of our MWE ranking method is based on the following hypothesis: n-grams, which are MWE, could be substituted by sin- gle words, thus they have many single words amongst their most similar terms. This is moti- vated by semiotic considerations: Because of par- simony, concepts are usually expressed in single words. When a semantically non-compositional word combination is added to the vocabulary, it expresses a concept that is necessarily similar to other concepts. Hence, if a candidate multiword is similar to many single word terms, this indicates multiwordness.</p><p>To compute the uniqueness score (uq) of an n- gram t, we first extract the n-grams it is simi- lar to using the DT as described in Section 4.1. The function similarities(t) returns the 200 most similar n-grams to the given n-gram t. We then compute the ratio between unigrams and all simi- lar n-grams considered using the formula:</p><formula xml:id="formula_6">uq(t) = similarities(t) s:|s|=1 1 |similarities(t)| .<label>(7)</label></formula><p>We illustrate the computation of our measure based on the MWE red blood cell and the non- MWE red blood. When considering only the ten most similar entries for both n-grams as illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>, we observe an uniqueness score of 7/10 = 0.7 for both n-grams. If considering the   <ref type="table">Table 1</ref>: We show the ten most similar entries for the term red blood cell (left) and red blood (right).</p><p>Here, seven out of ten terms are single words.</p><p>top 200 similar n-grams, which are also used in our experiments we will obtain 135 unigrams for the candidate red blood cell and 100 unigrams for the n-gram red blood. We will use these counts for showing the workings of the method in the re- mainder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Incompleteness Computation</head><p>Similar to the C/NC-value method, we also as- sign a context weighting function that punishes incomplete terms, which we call incompleteness (ic). For this function we extract the 1000 most significant context features using the function context(t), which yields tuples of left and right contexts. These context features are the same that are used for the similarity computation in Section 4.1 and have been ranked according to the LMI measure. For the example term red blood, some of the contexts are extravasated, cells, uninfected, cells, nucleated, corpuscles. In the next step we split each tuple to its left and right word in- cluding its relative position (left/right) to the can- didate term. Using the first context feature results in: extravasated, left, cells, right. Then, we sum up the occurrences of for each single context, as shown in <ref type="table">Table 2</ref> for the two terms. We subsequently select the maximal count and normalize it by the counts of features |context(t)| considered, which is 1000. This results into the incompleteness measure ic(t). For our example terms we achieve the values ic(red blood) = 557/1000 and ic(red blood cell) = 48/1000. Whereas the uniqueness scores for the most simi- lar entries were equal, we now have a measure that indicates the incompleteness of an n-gram, with higher scores indicating more incomplete terms. <ref type="table" target="#tab_2">red blood cell  transfusions  right  48  (  right  42  transfusion  right  33  red blood  cells  right  557  cell  right  344  corpuscles  right  13   Table 2</ref>: Top three most frequent context words for the term red blood cell and red blood in the Medline corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context term Position Count</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combining Both Measures</head><p>As shown in the previous two sections, a high uniqueness score indicates the multiwordness and a high incompleteness score should decrease the overall score. In experiments, we found the best combination if we subtract 3 the incompleteness score from the uniqueness score. This mechanism is inspired by the C-value and motivated as terms that are often preceded/followed by the same word do not cover the full multiword expression and need to be downranked. This leads to Equation 8, which we call DistRibutional Uniqueness and Incompleteness Degree:</p><formula xml:id="formula_7">DRUID(t) = uq(t) − ic(t).<label>(8)</label></formula><p>Applying the DRUID score to our example terms (considering the 200 most similar terms) we will achieve the scores DRUID(red blood cell) = 135/200 − 48/1000 = 0.627 and DRUID(red blood) = 100/200 − 557/1000 = −0.057. As a higher DRUID score indicates the multiwordness of an n-gram, we can summarize that the n-gram red blood cell is a better MWE than the n-gram red blood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setting</head><p>We examine two experimental settings: First, we compute all measures on a small corpus that has been annotated for MWEs, which serves as the gold standard. In the second setting we compute the measures on a larger in-domain corpus. The evaluation is again performed for the same candi- date terms as given by the gold standard. Results for the top k ranked entries are reported using the precision at k (P @k = 1 k k i=1 x i with x i equals 1 if the i-th ranked candidate is annotated as MWE and 0 otherwise). For an overall performance we use the average precision (AP) as defined in <ref type="bibr" target="#b27">Thater et al. (2009)</ref>: AP = 1 |Tmwe| |T | k=1 x k P @k, with T mwe beeing the set of positive MWE. When fac- ing tied scores we mix false and true candidates randomly cf. <ref type="bibr" target="#b5">Cabanac et al. (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Corpora</head><p>For the experiments we consider two annotated (small) corpora and two unannotated (large) cor- pora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">GENIA corpus and SPMRL 2013: French Treebank</head><p>In the first experiments we use two small anno- tated corpora that serve the gold standard MWEs. We use the medical GENIA corpus ( <ref type="bibr" target="#b13">Kim et al., 2003)</ref>  <ref type="bibr">4</ref> which consists of 1999 abstracts from Medline 5 and encompasses 0.4 million words. This corpus has annotations regarding important and biomedical terms. Also single terms are anno- tated in this data set, which we ignore.</p><p>The second small corpus is based on the French Treebank ( <ref type="bibr" target="#b0">Abeillé and Barrier, 2004</ref>), which was extended for the SPMRL task ( <ref type="bibr" target="#b26">Seddah et al., 2013)</ref>. This version of the corpus also contains compounds annotated as MWEs. In our experi- ments we use the training data, which covers 0.4 million words.</p><p>Whereas the GENIA MWEs target term match- ing and medical information retrieval, the SPMRL MWEs mainly focus on improving parsing through compound recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Medline Corpus and Est Républican</head><p>Corpus (ERC) In a second experiment the scalability to larger corpora is tested. For this, we make use of the en- tire Medline 5 abstracts, which consist of about 1.1 billion words. The Est Républican Corpus (ERC) ( <ref type="bibr" target="#b25">Seddah et al., 2012)</ref>  <ref type="bibr">6</ref> is our large French corpus. It consists of local French news from the east- ern part of France and comprises of 150 million words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Candidate Selection</head><p>In the first two experiments, we use POS filters to select candidates. We concentrate on filters that extract noun MWEs and avoid further pre- processing like lemmatization. We use the filter introduced by <ref type="bibr" target="#b11">Justeson and Katz (1995)</ref>  <ref type="bibr">7</ref> for the English medical datasets. Considering only terms that appear more than ten times leads to 1,340 can- didates for the GENIA dataset and 29,790 can- didates for the Medline dataset. According to <ref type="table">Table 3</ref> we observe that most candidates are bi- grams. Whereas for both corpora still around 20% of trigrams are contained, the number of 4-grams is only marginally represented. For the French datasets we apply the filter proposed by <ref type="bibr">Daille et al. (1994)</ref>   <ref type="table">Table 3</ref>: Number of candidates after filtering for the expected POS-tag and their distribution over n-grams with n ∈ {1, 2, 3, 4}.</p><note type="other">8 , which is suited to match nomi- nal MWEs. Applying the same filtering as for the medical corpora leads to 330 candidate terms for the SPMRL and 7,365 candidate terms for the ERC. Here the ratio between bi-and trigrams is more balanced but again the number of 4-grams constitutes the smallest class</note><p>In comparison to the Medline dataset, the ratio of multiwords extracted by the POS filter on the French corpus is much lower. The reason for that property is that in the French data, many adverbial, prepositional MWEs are annotated, which are not covered by the POS filter.</p><p>The third experiment shows the performance of the method in absence of language-specific pre- processing. Thus, we only filter the candidates by frequency and do not make use of POS filtering. As most previous methods rely on POS-filtered data we cannot make use of them in this setting.</p><p>For the evaluation, we compute the scores of the competitive methods in two settings: First, we compute the scores based on the full candidate list without any frequency filter and prune low- frequent candidates only for the evaluation (post- prune). In the second setting we filter candidates <ref type="bibr">7</ref> A regular expression for matching POS tag se- quences is summarized by <ref type="bibr" target="#b14">Korkontzelos (2010)</ref>: <ref type="bibr">[NP]</ref>? <ref type="bibr">[JN]</ref>?)N). Each letter is a truncated POS tag of length one where J is an adjective N a noun and P a preposition.</p><formula xml:id="formula_8">(([JN]+[JN]?</formula><p>8 Following the same convention as for English the regular expression can be expressed as N <ref type="bibr">[J]</ref>?|NN|NPDN according to their frequency before the computa- tion of scores <ref type="bibr">(pre-prune)</ref>. This leads to differ- ences for context-aware measures, since in the pre- pruned case, a lower number of less noisier con- texts is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Small Corpora Results</head><p>First we show the results based on the GENIA cor- pus (see <ref type="table" target="#tab_2">Table 4</ref>). Almost all competitive methods  beat the lower baseline. The C/NC-value perform best when the pruning is done after a frequency filter. In line with the findings of <ref type="bibr" target="#b14">Korkontzelos (2010)</ref> and in contrast to <ref type="bibr" target="#b9">Frantzi et al. (1998)</ref> the AP of the C-value is slightly higher than for the NC-value. All the FGM based methods except the GM measure alone outperform the C-value. The results in <ref type="table" target="#tab_2">Table 4</ref> indicate that the best compet- itive system is the post-pruned FGM system as it has much higher average precision scores and misses only 50 MWEs in the first 500 entries. A slightly different picture is presented in <ref type="figure" target="#fig_2">Figure 1</ref> where the P @k scores against the number of can- didates are plotted. Here DRUID performs well for the top-k list for small k, i.e. finds many good MWEs with high confidence thus combines well with MF, which extends to larger k, but places too much importance of frequency when used alone. Common errors are frequent chunks such as "in patience", see <ref type="table">Table 9</ref> in Section 7. Whereas for the post-pruned case FGM scores higher than MF, the inverse is observed when pre-pruning. Us- ing our DRUID methods can surmount the FGM method only for the first 300 ranked terms (see <ref type="figure" target="#fig_2">Figure 1</ref> and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GENIA (GENIA frequency &gt;= 10)</head><p>Number of instances  do not exceed the lower baseline. Data analysis revealed that for the French dataset only ten out of the 330 candidate terms are nested within any of the candidates. This is much lower than the 637 terms nested in the 1340 candidate terms for the GENIA dataset. As both the FGM-based methods and the C/NC-value heavily rely on nested can- didates, they cannot profit from the candidates of this dataset and achieve similar scores as ordering candidates according to frequency. Comparing the baselines to our scoring method, this time we ob- tain the best result for DRUID without additional factors. However, multiplying DRUID with MF or log(frequency) still outperforms the other methods and the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Large Corpora Results</head><p>Most MWE evaluations have been performed on rather small corpora. Here we want to inspect the performance of the measures for large corpora, so as to realistically simulate a situation where the MWEs should be found automatically for an entire domain.</p><p>Using the Medline corpus, all methods except the GM score outperform the lower baseline and the frequency baseline (see <ref type="table" target="#tab_5">Table 6</ref>). Regarding  the AP the best results are obtained when combin- ing our DRUID method with the MF, whereas for P @100 and P @500 the log-frequency weighted DRUID scores best. Using solely the DRUID method or the combined variation with the log- frequency lead to the best ranking for the first 1000 ranked candidates and is then outperformed by the MF based DRUID variations. In this exper- iment the C-value achieves the best performance from the competitive methods for the P @100 and P @500, followed by the t-test. But the highest AP is reached with the post-pruned MF method, which also outperforms the sole DRUID slightly. Contrary to the GENIA results, the MF scores are consistently higher than the FGM scores. When using the French ERC we figured out that no nested terms are found within the candidates. Thus, the post-and pre-pruned settings are equiv- alent and thus MF equals frequency. The best re- sults are again obtained with our method with and without the logarithmic frequency weighting (see <ref type="table">Table 7</ref>). Again the AP of the C-value and most Method P @100 P @500 AP upper baseline 1.000  <ref type="table">Table 7</ref>: Results computed based on the ERC.</p><p>of the FGM-based methods are inferior to the fre- quency scoring. Only the t-test and the MF are slightly higher than the frequency <ref type="bibr">9</ref> . But in con- trast to the results based on the smaller SPMRL dataset, the MF, FGM and C-value can outperform the lower baseline. In comparison to the smaller corpora, the performance for the larger corpora is much lower. Especially low-frequent terms in the small corpora that have high frequencies in the larger corpora have not been annotated as MWEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results without POS Filtering</head><p>In the last experiment, we apply our method to candidates without any POS filtering and report results using a frequency threshold of ten. As the competitive methods from the previous section rely on POS tags, we use the t-test for compari- son. Analysis revealed that the top-scored candi-  dates according to the t-test begin with stop words.</p><p>As an additional heuristic for the t-test, we shift MWEs, which start or end with one of the ten most frequent words, to the last ranks. For the smaller dataset the best results are achieved with the sole DRUID (see <ref type="table" target="#tab_8">Table 8</ref>) and the frequency weight- ing does not seem to be beneficial, as highly fre- quent n-grams ending with stopwords are ranked higher in absence of POS filtering. This, how- ever, is not observed for larger corpora. Here the best results for Medline are achieved with the frequency weighted DRUID. Whereas for French, the sole DRUID method performs best, the differ- ence between the DRUID and the log-frequency- weighted DRUID is rather small. The low APs throughout can be explained by the large number of considered candidates. The second best scores are achieved with stop word based t-test (t-test + sw). C-value performs en par with frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Components of DRUID</head><p>Here, we show different parameters for DRUID, relying on the English GENIA dataset without POS filtering of MWE candidates and by consid- ering only terms with a frequency of 10 or more.</p><p>Inspecting the two different components of the DRUID measure (see upper graph in <ref type="figure" target="#fig_4">Figure 2</ref>), we observe that the uniqueness measure contributes most to the DRUID score. The main effect of the incompleteness component is the downranking of a rather small number of terms with high unique- ness scores, which improves the overall ranking. We can also see that for the top ranked terms the negative incompleteness score does not improve over the frequency baseline but outperforms the frequency in the middle ranked candidates. Used in DRUID we observe a slight improvement for the complete ranking. We achieve a P@500 of 0.474 for the uniqueness scoring and 0.498 for the DRUID score. When filtering similar entries, used for the uq scoring, by their similarity score (see bottom graph in <ref type="figure" target="#fig_4">Figure 2</ref>), we observe that the amount of similar n-grams considered seems to be more im- portant then the quality of the similar entries: With the increasing filtering also the quality of extracted candidate MWEs diminishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Data Analysis</head><p>The experiments confirm that our DRUID mea- sure, either weighted with the MF or alone, works best across two languages and across different cor-  pus sizes. It also achieves the best results in ab- sence of POS filtering for candidate term extrac- tion. The optimal weighting of DRUID depends on the nestedness of the MWEs: Using DRUID with the MF should be used when there are more than 20% of nested candidates and using the log- frequency or no frequency weighting when there are almost no nested candidate terms. We show the best-ranked candidates obtained with our method and with the best competitive method in terms of P @100 for the two smaller corpora. Using the GENIA dataset, our log- frequency based DRUID (see left column in <ref type="table">Table  9</ref>) ranks only true MWE within the 15 top-scored candidates.</p><p>The right-hand side shows results extracted with the pre-pruned MF method that yields three non- MWE terms. Whereas that could be a POS error, log(freq)DRUID MF (pre-pruned) NF-kappa <ref type="table">B  1 T cells  1  transcription factors  1 NF-kappa B  1  transcription factor  1 transcription factors  1  I kappa B alpha  1 activated T cells  1  activated T cells  1 T lymphocytes  1  nuclear factor  1 human monocytes  1  human monocytes  1 I kappa B alpha  1  gene expression  1 nuclear factor  1  T lymphocytes  1 gene expression  1  NF-kappa B activation</ref> 1 NF-kappa B activation 1 binding sites 1 in patients 0 MHC class II 1 important role 0 tyrosine phosphorylation 1 binding sites 1 transcriptional activation 1 in B cells 0 nuclear extracts 1 transcriptional activation 1 <ref type="table">Table 9</ref>: Top ranked candidates from the GENIA dataset using our method (left) and the competitive method (right). Each term is marked, whether the term is an MWE (1) or not (0).</p><p>the MF and also the C-value are not capable to re- move terms starting with stop words. The DRUID score alleviates this problem with the uniqueness factor. For the French dataset our method ranks only one false candidate whereas the MF (post- pruned) ranks eight non-annotated candidates in the top 15 list (see <ref type="table" target="#tab_11">Table 10</ref>).  Whereas the unweighted DRUID method scores better than its competitors on the large corpora, the best results are achieved when using DRUID with frequency-based weights on the smaller cor- pora. For a direct comparison we evaluated the small and large corpora using an equal candidate set. We observed that all methods computed on the large corpora achieve slightly inferior results than when computing them using the small cor- pora. Data analysis revealed that we would con- sider many of the high ranked "false" candidates as MWE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DRUID</head><p>Therefore we extracted the top ten ranked terms, which are not annotated as MWE from the meth- ods with the best P @100 performance, resulting to the log(freq) DRUID and the pre-pruned C-value methods.</p><p>First, we observed that the first 'false' candidate for our method appears at rank 26 and at rank 1 for the C-value. Additionally, only ten out of the top 74 candidates are not annotated as MWEs for our method and 48 for the competitor. When search- ing the terms within the MeSH dictionary 10 , we find seven terms ranked from our method and two for the competitive method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Uniqueness is a new mechanism in MWE model- ing. Whereas frequency and co-occurrence have been captured in many previous approaches (see <ref type="bibr" target="#b16">Manning and Schütze (1999)</ref>,  and <ref type="bibr" target="#b14">Korkontzelos (2010)</ref> for a survey), we boost multiword candidates t by their grade of dis- tributional similarity with single word terms. We implement such contextual substitutability with a model where the term t can consist of multiword tokens and similarity is measured based on the right and neighboring word between all (single and multiword) terms. Since it is the default to ex- press concepts with single words, a high unique- ness score is given to multiwords that belong to a category just as single words would. E.g. for an English open-domain corpus hot dog is most similar to the terms: food, burger, hamburger, sausage and roadside. Candidates with a low number of single word similarities also serve the same function, but more frequently we observe single n-grams with function words or modifying adjectives concatenated with content words, i.e. small dog is most similar to "various cat", "large amount of ", "large dog", "certain dog", "dog". To be able to kick in, the measure requires a certain minimum frequency for candidates in order to find enough contextual overlap with other terms. Ad- ditionally, we also demonstrate effective perfor- mance on larger corpora and show its applicability when used in a complete unsupervised evaluation setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results for the GENIA corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results for the components of the DRUID measure (top) and for different filtering thresholds of the similar entries considered for the uniqueness scoring (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Corpus 
Candidates 2-gram 3-gram 4-gram 
GENIA 
1,340 
1,056 
243 
41 
Medline 
29,790 
22,236 
6,400 
1,154 
SPMRL 
330 
197 
116 
17 
ERC 
7,365 
3,639 
2,889 
837 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Results for the GENIA dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 )</head><label>4</label><figDesc></figDesc><table>. Multiplying the logarith-
mic frequency to the DRUID, the results improve 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Results for the French SPMRL dataset</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 : Results computed on the Medline corpus.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Results without linguistic pre-processing. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>Top ranked candidates from the SPMRL 
dataset for the best DRUID method (left) and the 
best competitive method (right). Each term is 
marked, if it is an MWE (1) or not (0). 

</table></figure>

			<note place="foot" n="1"> the context window size is not specified in Frantzi et al. (1998)</note>

			<note place="foot" n="2"> They adjust the logarithmic length in order to be able to use the C-value to detect single worded terms.</note>

			<note place="foot" n="3"> multiplicative combinations consistently performed worse</note>

			<note place="foot" n="4"> freely available at http://www.nactem.ac.uk/ genia/genia-corpus/pos-annotation. 5 http://www.nlm.nih.gov/bsd/licensee/ access/medline_pubmed.html 6 http://www.cnrtl.fr/corpus/ estrepublicain</note>

			<note place="foot" n="9"> This is achieved by chance for the MF, as it is equal to the frequency. The different scores are due to the randomly sorted tied scores used during our evaluation and reflect the variance of the randomness.</note>

			<note place="foot" n="10"> http://www.nlm.nih.gov/mesh/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work has been supported by the German Fed-eral Ministry of Education and Research (BMBF) within the context of the Software Campus project LiCoRes under grant No. 01IS12054 and the Deutsche Forschungsgemeinschaft (DFG) within the SeMSch project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching a French Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Abeillé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Barrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC &apos;04)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC &apos;04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2233" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Idiom Treatment Experiments in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Anastasiou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Saarbrücken, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universität des Saarlandes</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Text: Now in 2D! A Framework for Lexical Expansion with Contextual Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Modelling</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="95" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Comparison of Co-occurrence and Similarity Measures As Simulations of Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bordag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing &apos;14)</title>
		<meeting>the 9th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing &apos;14)<address><addrLine>Haifa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
	<note>Israel</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying bilingual MultiWord Expressions for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhouha</forename><surname>Bouamor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasredine</forename><surname>Semmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC &apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC &apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Cabanac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohand</forename><surname>Boughanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Chrisment</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards automatic extraction of monolingual and bilingual terminology</title>
	</analytic>
	<monogr>
		<title level="m">Tie-breaking Bias: Effect of an Uncontrolled Parameter on Information Retrieval Evaluation</title>
		<editor>Béatrice Daille, ´ Eric Gaussier, and Jean-Marc Langé</editor>
		<meeting><address><addrLine>Padua, Italy; Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="515" to="521" />
		</imprint>
	</monogr>
	<note>Proceedings of the 15th Conference on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Evert</forename></persName>
		</author>
		<title level="m">The Statistics of Word Cooccurrences: Word Pairs and Collocations</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Institut für maschinelle Sprachverarbeitung, University of Stuttgart</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A lexicographic evaluation of German adjective-noun collocations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2008 Workshop Towards a Shared Task for Multiword Expressions (MWE &apos;08)</title>
		<meeting>the LREC 2008 Workshop Towards a Shared Task for Multiword Expressions (MWE &apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The C-value/NC-value Method of Automatic Recognition for Multi-Word Terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><forename type="middle">T</forename><surname>Frantzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries (ECDL &apos;98)</title>
		<meeting>the Second European Conference on Research and Advanced Technology for Digital Libraries (ECDL &apos;98)<address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="585" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilingual Models for Compositional Distributed Semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;14)</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;14)<address><addrLine>Baltimore, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Technical terminology: some linguistic properties and an algorithm for identification in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Justeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic Identification of Non-compositional Multiword Expressions Using Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenie</forename><surname>Giesbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties (MWE &apos;06)</title>
		<meeting>the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties (MWE &apos;06)<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>Australia</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GENIA corpus-a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Multiword Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis Korkontzelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of York</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yet Another Ranking Function for Automatic Multiword Term Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan Antonio Lossio-</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Jonquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maguelonne</forename><surname>Teisseire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Natural Language Processing (PolTAL &apos;14)</title>
		<meeting>the 9th International Conference on Natural Language Processing (PolTAL &apos;14)<address><addrLine>Warsaw, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="52" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple but powerful automatic term extraction method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02 on COMPUTERM 2002: Second International Workshop on Computational Terminology</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>COMPUTERM &apos;02</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic term recognition based on statistics of compound nouns and their components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Terminology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="219" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lexical association measures and collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="137" to="158" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A broad evaluation of techniques for automatic acquisition of multiword expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Vitor De Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Villavicencio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop of the 50th Meeting of the Association for Computational Linguistics (ACL Student Workshop &apos;12)</title>
		<meeting>the Student Research Workshop of the 50th Meeting of the Association for Computational Linguistics (ACL Student Workshop &apos;12)<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A generic and open framework for multiword expressions treatment: from acquisition to applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Universidade Federal Do Rio Grande do Sul</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiword expressions: A pain in the neck for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002)</title>
		<meeting>the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002)<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using distributional similarity of multi-way translations to predict multiword expression compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahar</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL &apos;14)</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL &apos;14)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="472" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is Knowledge-Free Induction of Multiword Unit Dictionary Headwords a Solved Problem?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;01)</title>
		<meeting>the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;01)<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ubiquitous Usage of a Broad Coverage French Corpus: Processing the Est Republicain corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Crabbé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><forename type="middle">Henestroza</forename><surname>Anguiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC &apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC &apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3249" to="3254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nskiWoli´nski, Alina Wróblewska, and Eric Villemonte de la Clergerie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koldo</forename><forename type="middle">Gojenola</forename><surname>Galletebeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Przepiórkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="146" to="182" />
		</imprint>
	</monogr>
	<note>Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ranking Paraphrases in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on Applied Textual Inference (TextInfer &apos;09) in conjunction with the ACL &apos;09</title>
		<meeting>the 2009 Workshop on Applied Textual Inference (TextInfer &apos;09) in conjunction with the ACL &apos;09<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="44" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective grading of termhood in biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wermter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual AMIA Symposium Proceedings</title>
		<meeting><address><addrLine>Washington D.C., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="809" to="813" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
