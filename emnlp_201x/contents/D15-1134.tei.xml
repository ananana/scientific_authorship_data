<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A model of rapid phonotactic generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
							<email>linzen@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Brain and Cognitive Sciences Massachusetts Institute of Technology</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>O&amp;apos;donnell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Brain and Cognitive Sciences Massachusetts Institute of Technology</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A model of rapid phonotactic generalization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The phonotactics of a language describes the ways in which the sounds of the language combine to form possible morphemes and words. Humans can learn phonotactic patterns at the level of abstract classes, generalizing across sounds (e.g., &quot;words can end in a voiced stop&quot;). Moreover , they rapidly acquire these generalizations , even before they acquire sound-specific patterns. We present a probabilis-tic model intended to capture this early-abstraction phenomenon. The model represents both abstract and concrete generalizations in its hypothesis space from the outset of learning. This-combined with a parsimony bias in favor of compact descriptions of the input data-leads the model to favor rapid abstraction in a way similar to human learners.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural languages place restrictions on the ways in which sounds can combine to form words (the phonotactics of the language). The velar nasal <ref type="bibr">[N]</ref>, for example, occurs at the end of English sylla- bles, as in ring <ref type="bibr">[ôIN]</ref> or finger <ref type="bibr">[fINg@ô]</ref>, but never at the beginning of a syllable: English does not have words like *ngir <ref type="bibr">[NIô]</ref>. English speakers are aware of this constraint, and judge forms that start with a <ref type="bibr">[N]</ref> as impossible English words.</p><p>Sounds that share articulatory and/or perceptual properties often have similar phonotactic distribu- tions. German, for example, allows voiced obstru- ents, 1 such as <ref type="bibr">[b]</ref> and <ref type="bibr">[g]</ref>, to occur anywhere in the word except at its end: <ref type="bibr">[bal]</ref> is a valid German word, but <ref type="bibr">[lab]</ref> isn't.</p><p>Speakers use such features of sounds to form phonotactic generalizations, which can then apply to sounds that do not appear in their language. Al- though no English words start with either <ref type="bibr">[sô]</ref> or <ref type="bibr">[mb]</ref>, English speakers judge srip to be a better po- tential word of English than mbip <ref type="bibr" target="#b19">(Scholes, 1966)</ref>; this is likely because <ref type="bibr">[sô]</ref> shares properties with strident-liquid clusters that do exist in English, such as <ref type="bibr">[sl]</ref> as in slip and <ref type="bibr">[Sô]</ref> as in shrewd, whereas <ref type="bibr">[mb]</ref> does not benefit from any sonorant-stop on- set sequences <ref type="bibr">(*[nt]</ref>)-none exist in English.</p><p>Recent studies have investigated how humans acquire generalizations over phonological classes in an artificial language paradigm <ref type="bibr" target="#b13">(Linzen and Gallagher, 2014;</ref><ref type="bibr" target="#b14">Linzen and Gallagher, 2015)</ref>. The central finding of these studies was that participants rapidly learned abstract phonotactic constraints and exhibited evidence of generaliza- tions over classes of sounds before evidence of phoneme-specific knowledge.</p><p>This paper presents a probabilistic genera- tive model, the Phonotactics As Infinite Mixture (PAIM) model, which exhibits similar behavior. This behavior arises from the combination of two factors: the early availability of abstract phono- logical classes in the learner's hypothesis space; and a parsimony bias implemented as a Dirichlet process mixture, which favors descriptions of the data using a single pattern over ones that make ref- erence to multiple specific patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Summary of behavioral data</head><p>The experiments are described in detail in <ref type="bibr" target="#b13">Linzen and Gallagher (2014)</ref> and <ref type="bibr" target="#b14">Linzen and Gallagher (2015)</ref>; we summarize the main details here.</p><p>Design: Participants were exposed to varying numbers of auditorily-presented words in one of two artificial languages, VOICING and IDENTITY. <ref type="table">Test  ganu gimi   CONF- CONF- NONCONF-  balu  bini   ATT  UNATT UNATT   vimu voni  zonu  dila  tumu  zalu  zili  zini  dimu  talu</ref> Dano Damu Following the exposure phase, they were asked to judge for a set of novel test words whether those words could be part of the language they had learned (the possible answers were "yes" or "no").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exposure</head><p>In the VOICING experiment, all exposure words began with consonants that had the same value for their voicing feature (all voiced or all voiceless, e.g., A 1 = {g, v, z, D, b} or A 2 = {k, f, s, T, p}). Some of the sounds with the relevant voicing were held out to be used during testing (e.g.,  <ref type="table" target="#tab_0">Table 1</ref>).</p><p>Participants judged three types of novel test words: ones with the same onset as one or more of the words in exposure (CONF-ATT; 2 e.g., zonu for A 1 ); ones whose onset was not encountered in exposure but had the same voicing as the ex- posure onsets (CONF-UNATT; e.g., dila); and ones whose onset had different voicing from the expo- sure words (NONCONF-UNATT, e.g., tomu). The vowels and the second consonant were random- ized across conditions such that only the onsets reliably discriminated the three conditions.</p><p>In the IDENTITY language, words had the form C 1 V C 2 V . The generalization in this language was C 1 = C 2 (e.g., pipa). Here it was probabilistic: only half of the words in the exposure stage con- formed to the generalization. As such, there was a fourth test condition, NONCONF-ATT, of exposure words that did not conform to the generalization.</p><p>Participants were recruited on Amazon Me- chanical Turk (280 participants in the VOICING 2 ATT (attested): the consonants in the word (though not the full word) were encountered in exposure; UNATT (unat- tested): consonants were not encountered in exposures; CONF (conforming): the consonants conform to the abstract pat- tern (voicing or identity); NONCONF (nonconforming): con- sonants don't conform to the abstract pattern.  experiment and 288 in the IDENTITY experiment). They were divided into four groups, which re- ceived 1, 2, 4 or 8 sets of words. In the VOICING experiment, each of the sets contained five words, one starting with each of the five CONF-ATT on- sets; in the IDENTITY experiment, each of the sets contained eight words, one with each of the CONF- ATT and NONCONF-ATT consonant pairs <ref type="table" target="#tab_0">(Tables 1  and 2)</ref>.</p><p>Results: Human experimental results are plotted in <ref type="figure" target="#fig_1">Figure 1</ref>. Endorsement rates represent the pro- portion of trials in which participants judged the word to be well-formed. Participants learned gen- eralizations involving abstract classes of sounds after a single exposure set: in the VOICING exper- iment, they judged voiced word onsets to be bet- ter than voiceless ones, and in the IDENTITY ex- periment they judged words with identical conso- nants as better than words with nonidentical ones. 3 Participants did not start distinguishing CONF-ATT from CONF-UNATT patterns until they received two or more sets of exposure to the language. Participants continued to generalize to CONF- UNATT patterns even after significant exposure to the language. Endorsement rates were higher than 50% across the board, likely because even words with NONCONF-UNATT consonant patterns were similar to the exposure words in all other respects (e.g., length, syllable structure, number of vowels </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The model</head><p>PAIM is a generative model: it describes the prob- abilistic process by which the phonological forms of words are generated. Phonotactic knowledge is expressed as a set of word-form templates, repre- sented as sequences of phonological classes. For example, the template [+voiced], V, C, V cap- tures a generalization over words beginning with a voiced consonant.</p><p>Prior over phonological classes: Phonemes are represented as phonological feature-value matri- ces. <ref type="bibr">4</ref> We generate a phonological class for each position in the template using this feature sys- tem and a parameter p ∈ [0, 1], which con- trols the model's willingness to consider more or less abstract phonological classes: low val- ues of p encourage underspecified classes, such as [] or <ref type="bibr">[+voice]</ref>, whereas high values of p favor highly specified classes, such as [+voice, labial, -continuant]. Given a particular value of p, we de- fine the distribution G(c) over classes as follows:</p><p>• For each phonological feature f whose set of possible values is V f :</p><formula xml:id="formula_0">1. Draw P ∼ Bernoulli(p).</formula><p>2. If P = 1, draw v ∼ Uniform(V f ) and include it in c. 3. Otherwise, leave the feature unspeci- fied, allowing the class to be abstract. <ref type="bibr">4</ref> The particular feature system is treated as a parameter of the model. In the simulation below we used a simplified ver- sion of the phonological feature inventory described in Hayes (2011), which only included features that are distinctive in English consonants.</p><p>Generating words from templates: Given a choice of phonological template t, we assume that each of the segments that instantiate t has the same probability of being sampled (cf. the "size prin- ciple" of <ref type="bibr" target="#b20">Tenenbaum and Griffiths (2001)</ref>). Con- sider again the class c 0 = [+continuant, labial]. Under the assumption that the model's segment in- ventory is the English one, there are only two seg- ments that are labial continuants: <ref type="bibr">[v]</ref> and <ref type="bibr">[f]</ref>. The probability of each one of them being generated from c 0 will be P (s|c 0 ) = 1/2.</p><p>Prior over template sets: The sounds of the language can be generated from a variety of tem- plates at varying levels of abstraction. We there- fore extend the model to be a mixture of template distributions of the type described above. The number of templates is inferred from the data us- ing a Dirichlet process mixture model <ref type="bibr" target="#b1">(Antoniak, 1974)</ref>.</p><p>This prior can be constructed as a process. Sup- pose that s i is an ordering of the input sounds, and that we know which templates generated the first n − 1 sounds s 1 , . . . , s n−1 . If K is the num- ber of templates that have been posited so far and n 1 , . . . , n K indicate the number of sounds that have been drawn from each template, then the probability distribution over the template z n that the sound s n will be drawn from is given by</p><formula xml:id="formula_1">P (z n = k|z 1:n−1 ) ∝    n k if k ≤ K α otherwise<label>(1)</label></formula><p>Since the probability that an existing template generated s n is proportional to the number of seg- ments currently assigned to that template, this prior encourages partitions in which a few tem- plates explain most of the sounds (the "rich get richer" property), which amounts to a parsimony bias. Higher values of α can make this bias weaker.</p><p>Modeling phoneme spreading: To simulate the generalization made by participants in the IDEN- TITY experiment, templates must be able to state that two phonemes need to be identical. This is analogous to mechanisms of "spreading" widely assumed in phonology ( <ref type="bibr" target="#b2">Colavin et al., 2010;</ref><ref type="bibr" target="#b5">Goldsmith, 1976;</ref><ref type="bibr" target="#b15">McCarthy, 1986)</ref>. For our simula- tions below, we simplify by only considering iden- tity constraints between the initial and medial con- sonants in exposure and test forms. We sample a template over these positions as follows:</p><p>1. Draw a class c 1 ∼ G, where G is the distribu- tion over phonological classes defined above. 2. Draw Q ∼ Bernoulli(q). 3. If Q = 1, return an identity template, i.e., s, s such that s ∈ c 1 . 4. Otherwise, draw c 2 ∼ G and return the tem- plate s 1 , s 2 such that s 1 ∈ c 1 and s 2 ∈ c 2 .</p><p>Inference: We perform inference to find the posterior over template sets given the exposure datasets used in the human experiments described above. We also infer the hyperparameters using the following prior distributions:</p><formula xml:id="formula_2">p ∼ Beta(1, 1) α ∼ Gamma(2, 4) q ∼ Beta(1, 1)<label>(2)</label></formula><p>Inference for the Dirichlet process mixture was performed using the Gibbs sampler described in <ref type="bibr" target="#b16">Neal (2000)</ref>. After each Gibbs sweep, slice sam- pling <ref type="bibr" target="#b17">(Neal, 2003)</ref> was used to obtain a new value for p and q. A new value for α was sampled using the method described by <ref type="bibr" target="#b3">Escobar and West (1995)</ref>. We ran the sampler for 3000 iterations, discarded the first 100 samples and kept every fifth sample of the remaining samples, for a total of 580 samples from the posterior distribution.</p><p>Predicting human data: Participants in the behavioral experiments gave binary judgments ("could the word be part of the language?") rather than probability estimates. To link our model's predictions to participants' binary responses, we sample m template instantiations from the poste- rior predictive distribution. <ref type="bibr">5</ref> If the relevant part of the test word appeared in these m samples, the model responds "yes"; this can be understood to be related to a sampling-based view of human in- ference ( <ref type="bibr" target="#b21">Vul et al., 2014</ref>). In the simulations below we fix m to be 10.</p><p>Human endorsement rates were consistently above 50%, while the model's ratings were of- ten close to 0%. This is likely to be because hu- man ratings were also informed by the unmodeled (fixed) parts of the templates, such as word length or number of vowels. We therefore linearly trans- form the model's ratings to the range exhibited by human participants: if the untransformed rate is r, the ultimate simulated rate will be (1 + r)/2. <ref type="bibr">5</ref> Template instantiations only include the modeled (speci- fied) part of the template: an onset consonant in our model of the VOICING language or a consonant pair for the IDENTITY language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Simulations</head><p>We only modeled those aspects of phonotactic templates that are relevant to the experimental results. For the VOICING experiment, we con- strained the template to be , V, C, V (inference is only performed on the first consonant); for the IDENTITY experiment, we constrained it to be , V, , V. <ref type="figure" target="#fig_2">Figure 2</ref> shows the simulated endorsement rates. After a single exposure to each pattern (one exposure set), PAIM behaved in a qualitatively similar way to participants in both experiments: it distinguished CONF from NONCONF words, but did not distinguish ATT from UNATT words.</p><p>PAIM was less willing than humans to general- ize to CONF-UNATT items after multiple exposure sets: in the IDENTITY experiment the generaliza- tion had no effect by the eighth exposure set; in the VOICING experiment its effect was weaker after eight than after four exposures sets. By contrast, human generalization in both languages showed no sign of weakening after multiple exposure sets. <ref type="bibr" target="#b9">Hayes and Wilson (2008)</ref> propose a Maximum Entropy model of phonotactics (MaxEnt; see also <ref type="bibr" target="#b6">Goldwater and Johnson (2003)</ref>). Like PAIM, MaxEnt is based on phonological classes defined as feature matrices. Each class c is assigned a weight w c . The predicted probability in MaxEnt of a sound s isˆp</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison to related models</head><formula xml:id="formula_3">isˆ isˆp(s) = 1 Z e c wcIc(s)<label>(3)</label></formula><p>where Z = s ˆ p(s) and I c (s) = 1 if s ∈ c and 0 otherwise. We simulated endorsement rates from a Max- Ent model for the VOICING language. Following <ref type="bibr" target="#b9">Hayes and Wilson (2008)</ref>, we used l 2 regulariza- tion; that is, if the exposure words were s 1 , . . . , s n , the objective function was <ref type="figure" target="#fig_3">Figure 3</ref> shows the simulated endorsement rates for different values of σ (we set µ = 0 in all simulations). For σ = 0.05, the model showed little learning after a single exposure set. When σ was set to higher values, MaxEnt rapidly pre- ferred attested to unattested items, failing to re- produce the human early generalization pattern. Like PAIM, but unlike humans, generalization to CONF-UNATT items diminished after multiple ex- posure sets (in particular for σ = 0.5). A straight- forward implementation of MaxEnt is therefore incapable of simulating the human results; better results could potentially be achieved with a regu- larization method that encouraged sparsity <ref type="bibr" target="#b8">(Goodman, 2004;</ref><ref type="bibr" target="#b12">Johnson et al., 2015</ref>).</p><formula xml:id="formula_4">n i=1 logˆplogˆ logˆp(s i ) − c (w c − µ) 2 2σ 2 (4)</formula><p>Another proposed model of phonotactics is the Minimal Generalization Learner, or MGL (Albright, 2009); <ref type="bibr" target="#b13">Linzen and Gallagher (2014)</ref> showed that MGL can simulate relevant human behavioral data in some circumstances. In contrast with PAIM and MaxEnt, which converge to the empirical distribution given sufficient data, MGL reserves a fixed amount of probability mass to un- seen events. It would therefore able to simulate a sustained generalization pattern.</p><p>Our prior over phonological classes bears some resemblance to the Rational Rules model of vi- sual categorization <ref type="bibr" target="#b7">(Goodman et al., 2008</ref>). In that model, classes are generated from a probabilis- tic context free grammar (PCFG); highly specified rules are therefore implicitly less probable, as in our model. Relatedly, <ref type="bibr" target="#b9">Hayes and Wilson (2008)</ref> use a greedy feature selection procedure that starts from simpler phonological classes and gradually adds more complex ones; this procedure also en- codes an implicit bias in favor of simple classes. Finally, our implementation of a parsimony bias using a Dirichlet process is related to similar bi- ases incorporated into other models of language learning <ref type="bibr" target="#b4">(Frank and Tenenbaum, 2011;</ref><ref type="bibr" target="#b11">Johnson et al., 2007;</ref><ref type="bibr" target="#b18">O'Donnell, 2015)</ref>.  </p><formula xml:id="formula_5">σ = 0.05 σ = 0.3 σ = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have presented a probabilistic model of phono- tactic generalization that captures the pattern of rapid, abstract generalization characteristic of hu- man learners. The model's performance is driven by two crucial assumptions. First, it allows hy- potheses that make reference to abstract, broad classes of phones from the beginning of the learn- ing process. Second, it prefers to learn compact or parsimonious explanations of the input corpus, using a small number of phonotactic templates. This second property is enforced by our use of the Dirichlet process as a prior over template sets. These two properties interact. When the model has seen only a few data items, the availability of abstract generalizations allows it to explain all items using a single template, and the prior bias towards parsimony drives it to do so. As the num- ber of data items increases, repeated instances of specific phonemes no longer seem like acciden- tal observations from a more general template, but rather like significant templates in their own right; the model begins to capture such item-specificity.</p><p>The model stopped generalizing earlier than hu- mans did; we intend to explore ways to explain this discrepancy. Additional human data would need to be collected to determine whether humans keep generalizing indefinitely, or eventually con- verge on the attested sounds. Finally, to facilitate inference, we only tested our model on the parts of the word that were relevant to the human data. In future work, we intend to extend the model to learn larger templates that include syllable struc- ture and phonological tiers <ref type="bibr" target="#b5">(Goldsmith, 1976)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[d] for A 1 or [t] for A 2 ). All exposure and test words had the form CVMV (tumi), where C stands for the onset consonant, V for a randomized vowel, and M for [m], [n] or [l] (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Human behavioral results. Error bars indicate bootstrapped 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PAIM: Simulated endorsement rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Maximum entropy model: Simulated endorsement rates for the VOICING language, with different values of the regularization parameter σ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>VOICING: Design for one of the lists 
(voiced exposure, [d] held out). The table shows 
the complete list of exposure words that a partici-
pant in the two exposure sets group might receive. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>IDENTITY: A complete list of exposure 
and test words that a participant in the one expo-
sure set group might receive. 

</table></figure>

			<note place="foot" n="1"> See Hayes (2011) for an introduction to phonological features.</note>

			<note place="foot" n="3"> All differences discussed in this section are statistically significant.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature-based generalisation as a source of gradient acceptability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Albright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phonology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="41" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Antoniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1152" to="1174" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling OCP-Place in Amharic with the Maximum Entropy phonotactic learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">S</forename><surname>Colavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><forename type="middle">Rose</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th meeting of the Chicago Linguistics Society</title>
		<meeting>the 46th meeting of the Chicago Linguistics Society</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian density estimation and inference using mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three ideal observer models for rule learning in simple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="371" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Autosegmental Phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Goldsmith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, MIT</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning OT constraint rankings using a maximum entropy model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Stockholm workshop on variation within Optimality Theory</title>
		<meeting>the Stockholm workshop on variation within Optimality Theory</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A rational analysis of rule-based concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="154" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exponential priors for maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A maximum entropy model of phonotactics and phonotactic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Inquiry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="440" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introductory phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Hayes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>WileyBlackwell</publisher>
			<pubPlace>Malden, MA and Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sign constraints on feature weights improve a joint model of word segmentation and phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Pater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Staubs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="303" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The timecourse of generalization in phonotactic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gillian</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Phonology</title>
		<meeting>Phonology</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rapid generalization in phonotactic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gillian</forename><surname>Gallagher</surname></persName>
		</author>
		<ptr target="http://tallinzen.net/media/papers/linzen_gallagher_2015.pdf" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OCP effects: Gemination and antigemination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Inquiry</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="263" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for Dirichlet process mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Slice sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="705" to="767" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Productivity and reuse in language: A theory of linguistic computation and storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>O&amp;apos;donnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Phonotactic grammaticality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Scholes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<pubPlace>Mouton, The Hague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalization, similarity, and Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="640" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One and done? Optimal decisions from very few samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Vul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="637" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
