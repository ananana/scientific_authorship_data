<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernández-González</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Computación Campus de Elviña</orgName>
								<orgName type="laboratory">LyS Research Group</orgName>
								<orgName type="institution">Universidade da Coruña FASTPARSE Lab</orgName>
								<address>
									<postCode>15071 A</postCode>
									<settlement>s/n, Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Computación Campus de Elviña</orgName>
								<orgName type="laboratory">LyS Research Group</orgName>
								<orgName type="institution">Universidade da Coruña FASTPARSE Lab</orgName>
								<address>
									<postCode>15071 A</postCode>
									<settlement>s/n, Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1303" to="1313"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1303</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce novel dynamic oracles for training two of the most accurate known shift-reduce algorithms for constituent parsing: the top-down and in-order transition-based parsers. In both cases, the dynamic oracles manage to notably increase their accuracy, in comparison to that obtained by performing classic static training. In addition, by improving the performance of the state-of-the-art in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained by a fully-supervised single-model greedy shift-reduce constituent parser on the WSJ benchmark.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The shift-reduce transition-based framework was initially introduced, and successfully adapted from the dependency formalism, into constituent pars- ing by <ref type="bibr" target="#b18">Sagae and Lavie (2005)</ref>, significantly in- creasing phrase-structure parsing performance.</p><p>A shift-reduce algorithm uses a sequence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, <ref type="bibr" target="#b18">Sagae and Lavie (2005)</ref> suggested that those par- tial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy was followed by many researchers <ref type="bibr" target="#b24">(Zhang and Clark, 2009;</ref><ref type="bibr" target="#b25">Zhu et al., 2013;</ref><ref type="bibr" target="#b22">Watanabe and Sumita, 2015;</ref><ref type="bibr" target="#b17">Mi and Huang, 2015;</ref><ref type="bibr" target="#b3">Crabbé, 2015;</ref><ref type="bibr" target="#b5">Cross and Huang, 2016b;</ref><ref type="bibr" target="#b2">Coavoux and Crabbé, 2016;</ref><ref type="bibr">FernándezGonzález and Gómez-Rodríguez, 2018</ref>) who man- aged to improve the accuracy and speed of the ori- ginal Sagae and Lavie's bottom-up parser. With this, shift-reduce algorithms have become com- petitive, and are the fastest alternative to perform phrase-structure parsing to date.</p><p>Some of these attempts <ref type="bibr" target="#b5">(Cross and Huang, 2016b;</ref><ref type="bibr" target="#b2">Coavoux and Crabbé, 2016;</ref><ref type="bibr">FernándezGonzález and Gómez-Rodríguez, 2018</ref>) intro- duced dynamic oracles ( <ref type="bibr" target="#b11">Goldberg and Nivre, 2012)</ref>, originally designed for transition-based de- pendency algorithms, to bottom-up constituent parsing. They propose to use these dynamic or- acles to train shift-reduce parsers instead of a tra- ditional static oracle. The latter follows the stand- ard procedure that uses a gold sequence of trans- itions to train a model for parsing new sentences at test time. A shift-reduce parser trained with this approach tends to be prone to suffer from error propagation (i.e. errors made in previous states are propagated to subsequent states, causing fur- ther mistakes in the transition sequence). Dy- namic oracles <ref type="bibr" target="#b11">(Goldberg and Nivre, 2012)</ref> were developed to minimize the effect of error propaga- tion by training parsers under closer conditions to those found at test time, where mistakes are inevit- ably made. They are designed to guide the parser through any state it might reach during learning time. This makes it possible to introduce error ex- ploration to force the parser to go through non- optimal states, teaching it how to recover from mistakes and lose the minimum number of gold constituents.</p><p>Alternatively, some researchers decided to fol- low a different direction and explore non-bottom- up strategies for producing phrase-structure syn- tactic analysis.</p><p>On the one hand, ( <ref type="bibr" target="#b13">Kuncoro et al., 2017</ref>) proposed a top-down transition-based algorithm, which creates a phrase structure tree in the stack by first choosing the non-terminal on the top of the tree, and then considering which should be its child nodes. In contrast to the bottom-up approach, this top-down strategy adds a lookahead guidance to the parsing process, while it loses rich local features from partially-built trees.</p><p>On the other hand, Liu and Zhang (2017a) re- cently developed a novel strategy that finds a com- promise between the strengths of top-down and bottom-up approaches, resulting in state-of-the-art accuracy. Concretely, this parser builds the tree following an in-order traversal: instead of starting the tree from the top, it chooses the non-terminal of the resulting subtree after having the first child node in the stack. In that way each partial con- stituent tree is created in a bottom-up manner, but the non-terminal node is not chosen when all child nodes are in the stack (as a purely bottom-up parser does), but after the first child is considered. <ref type="bibr" target="#b14">Liu and Zhang (2017a)</ref> report that the top-down approach is on par with the bottom-up strategy in terms of accuracy and the in-order parser yields the best accuracy to date on the WSJ. However, despite being two adequate alternatives to the tra- ditional bottom-up strategy, no further work has been undertaken to improve their performance. <ref type="bibr">1</ref> We propose what, to our knowledge, are the first optimal dynamic oracles for both the top- down and in-order shift-reduce parsers, allowing us to train these algorithms with exploration. The resulting parsers outperform the existing versions trained with static oracles on the WSJ Penn Tree- bank ( <ref type="bibr" target="#b16">Marcus et al., 1993)</ref> and Chinese Treebank (CTB) benchmarks <ref type="bibr" target="#b23">(Xue et al., 2005</ref>). The version of the in-order parser trained with our dynamic or- acle achieves the highest accuracy obtained so far by a single fully-supervised greedy shift-reduce system on the WSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>The original transition system of Sagae and Lavie (2005) parses a sentence from left to right by read- ing (moving) words from a buffer to a stack, where partial subtrees are built. This process is per-formed by a sequence of Shift (for reading) and Reduce (for building) transitions that will lead the parser through different states or parser configur- ations until a terminal one is reached. While in the bottom-up strategy the Reduce transition is in charge of labeling the partial subtree with a non- terminal at the same time the tree is built,  and <ref type="bibr" target="#b14">Liu and Zhang (2017a)</ref> intro- duce a novel transition to choose the non-terminal on top, leaving the Reduce transition just to cre- ate the subtree under the previously decided non- terminal. We will now explain more in detail both the top-down and the in-order transition systems.</p><p>In both transition systems, parser configurations have the form c = Σ, i, f, γ, α, where Σ is a stack of constituents, i is the position of the left- most unprocessed word in the buffer (which is the next to be pushed onto the stack), f is a boolean variable used by the in-order transition system to mark if a configuration is terminal or not and with no value in top-down parser configurations, γ is the set of constituents that have already been built, and α is the set of non-terminal nodes that are cur- rently in the stack.</p><p>Each constituent is represented as a tuple (X, l, r), where X is a non-terminal and l and r are integers defining its span. Constituents are composed of one or several words or constituents, and just one non-terminal node on top. Each word w i is represented as (w, i, i + 1). To define our or- acles, we will need to represent each non-terminal node of the tree as (X, j), where j has the value of i when X is included in the stack and is used to keep them in order. <ref type="bibr">2</ref> For instance, the phrase-structure tree in <ref type="figure" target="#fig_0">Fig- ure 1</ref> can be decomposed as the following set of gold constituents: {(S, 0, 6), (NP, 0, 2), (VP, 2, 5), (ADVP, 3, 4), (ADJP, 4, 5)}. In addition, the ordered set of gold non-terminal nodes added to the stack while following a top-down strategy will be {(S, 0), (NP, 0), (VP, 2), (ADVP, 3), (ADJP, 4)} and, according to an in-order approach, {(NP, 1), (S, 2), (VP, 3), (ADVP, 4), (ADJP, 5)}. It is worth mentioning that the index of non-terminal nodes in the top-down method is the same as the left- most span index of the constituent that it will pro- duce. However, this does not hold in the in-order approach, as the leftmost child is fully processed before the node is added to the stack, so the index for the node will point to the leftmost span index of the second leftmost child.</p><p>Note that the information about the span of a constituent, the set of predicted constituents γ and the set α of predicted non-terminal nodes in the stack is not used by the original top-down and in- order parsers. However, we need to include it in parser configurations at learning time to allow an efficient implementation of the proposed dynamic oracles.</p><p>Given an input string w 0 · · · w n−1 , the in-order parsing process starts at the initial configuration c s (w 0 . . . w n−1 ) = [ ], 0, false, {}, {}} and, after applying a sequence of transitions, it ends in a terminal configuration (S, 0, n), n, true, γ, α, where n is the number of words in the input sen- tence. The top-down transition system shares the same form for the initial and terminal configura- tions, except for the fact that variable f has no value in both cases. <ref type="figure">Figure 2</ref> shows the available transitions in the top-down algorithm. In particular, the Shift trans- ition moves the first (leftmost) word in the buf- fer to the stack; the Non-Terminal-X transition pushes onto the stack the non-terminal node X that should be on top of a coming constituent, and the Reduce transition pops the topmost stack nodes until the first non-terminal node appears (which is also popped) and combines them into a con- stituent with this non-terminal node as their par- ent, pushing this new constituent into the stack. Note that every reduction action will add a new constituent to γ and remove a non-terminal node from α, and every Non-Terminal transition will include a new non-terminal node in α. <ref type="figure" target="#fig_1">Figure 3</ref> shows the top-down transition sequence that pro- duces the phrase-structure tree in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In <ref type="figure">Figure 4</ref> we describe the available trans- itions in the in-order algorithm. The Shift, Non-Terminal-X and Reduce transitions have the same behavior as defined for the top-down trans- ition system, except that the Reduce transition not only pops stack nodes until finding a non-terminal node (also removed from the stack), but also the node below this non-terminal node, and combines them into a constituent spanning all the popped nodes with the non-terminal node on top. And, fi- nally, a Finish transition is also available to end the parsing process. <ref type="figure" target="#fig_2">Figure 5</ref> shows the in-order trans- ition sequence that outputs the constituent tree in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The standard procedure to train a greedy shift- reduce parser consists of training a classifier to ap- proximate an oracle, which chooses optimal trans- itions with respect to gold parse trees. This classi- fier will greedily choose which transition sequence the parser should apply at test time.</p><p>Depending on the strategy used for training the parser, oracles can be static or dynamic. A static oracle trains the parser only on gold transition se- quences, while a dynamic one can guide the parser through any possible transition path, allowing the exploration of non-optimal sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic Oracles</head><p>Previous work such as <ref type="bibr" target="#b5">(Cross and Huang, 2016b;</ref><ref type="bibr" target="#b2">Coavoux and Crabbé, 2016;</ref><ref type="bibr">Fernández-González and Gómez-Rodríguez, 2018</ref>) has introduced and successfully applied dynamic oracles for bottom- up phrase-structure parsing. We present dynamic oracles for training the top-down and in-order transition-based constituent parsers. <ref type="bibr" target="#b11">Goldberg and Nivre (2012)</ref> show that imple- menting a dynamic oracle reduces to defining a loss function on configurations to measure the dis- tance from the best tree they can produce to the gold parse. This allows us to compute which trans- itions will lead the parser to configurations where the minimum number of mistakes are made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Loss function</head><p>According to <ref type="bibr" target="#b7">Fernández-González and GómezRodríguez (2018)</ref>, we can define a loss function in constituent parsing as follows: given a parser configuration c and a gold tree t G , a loss func- tion (c) is implemented as the minimum Ham- ming loss between t and t G , (L(t, t G )), where t is the already-built tree of a configuration c reach- able from c (written as c t). This Hamming loss is computed as the size of the symmetric dif- ference between the set of constituents γ and γ G in the trees t and t G , respectively. Therefore, the loss function is defined as:</p><formula xml:id="formula_0">(c) = min γ|cγ L(γ, γ G ) = |γ G \ γ| + |γ \ γ G |</formula><p>and, according to the authors, it can be efficiently computed for a non-binary bottom-up transition system by counting the individually unreachable arcs from configuration c (|U(c, γ G )|) plus the er- roneous constituents created so far (|γ c \ γ G |):</p><formula xml:id="formula_1">(c) = min γ|cγ L(γ, γ G ) = |U(c, γ G )| + |γ c \ γ G | Shift: Σ, i, /, γ, α ⇒ Σ|(wi, i, i + 1), i + 1, /, γ ∪ {(wi, i, i + 1)}, α</formula><p>Non-Terminal-X: <ref type="figure">Figure 2</ref>: Transitions of a top-down constituent parser. We adapt the latter to efficiently implement a loss function for the top-down and in-order strategies. While in bottom-up parsing constituents are created at once by a Reduce transition, in the other two approaches a Non-Terminal transition begins the process by naming the future constituent and a Reduce transition builds it by setting its span and children. Therefore, a Non-Terminal trans- ition that deviates from the non-terminals expected in the gold tree will eventually produce a wrong constituent in future configurations, so it should be penalized. Additionally, a sequence of gold Non-Terminal transitions may also lead to a wrong final parse if they are applied in an incorrect or- der. Then, the computation of the Hamming loss in top-down and in-order phrase-structure parsing adds two more terms to the bottom-up loss expres- sion: (1) the number of predicted non-terminal nodes that are currently in the stack (α c ), <ref type="bibr">3</ref> but not included in the set of gold non-terminal nodes (α G ), and (2) the number of gold non-terminal nodes in the stack that are out of order with respect to the order needed in the gold tree:</p><formula xml:id="formula_2">Σ, i, /, γ, α ⇒ Σ|(X, i), i, /, γ, α ∪ {(X, i)}} Reduce: Σ|(X, j)|(Y1, m0, m1)|...|(Y k , m k−1 , m k ), i, /, γ, α ⇒ Σ|(X, j, m k ), i, /, γ ∪ {(X, j, m k )}, α \ {(X, j)}}</formula><formula xml:id="formula_3">Transition Σ Buffer [ ] [ The, ...] NTS [ S ] [ The, ...] NTNP [ S, NP ] [ The, ...] SH [ S, NP, The ] [ public, ...] SH [ S, NP, The, public ] [ is, ...] RE [ S, NP ] [ is, ...] NTVP [ S, NP, VP ] [ is, ...] SH [ S, NP, VP, is ] [ still, ...] NTADVP [ S, NP, VP, is, ADVP ] [ still, ...] SH [ S, NP, VP, is, ADVP, still ] [ cautious, ...] RE [ S, NP, VP, is, ADVP ] [ cautious, ...] NTADJP [ S, NP, VP, is, ADVP, ADJP ] [ cautious, ...] SH [S, NP,VP, is, ADVP, ADJP, cautious] [ . ] RE [ S, NP, VP, is, ADVP, ADJP ] [ . ] RE [ S, NP, VP ] [ . ] SH [ S, NP, VP, . ] [ ] RE [ S ] [ ]</formula><formula xml:id="formula_4">(c) = min γ|cγ L(γ, γ G ) = |U(c, γ G )| + |γ c \ γ G | +|α c \ α G | + out of order (α c , α G )</formula><p>This loss function is used to implement a dynamic oracle that, when given any parser configuration, will return the set of transitions τ that do not in- crease the overall loss (i.e., (τ (c)) − (c) = 0), leading the system through optimal configurations that minimize Hamming loss with respect to t G .</p><p>As suggested by <ref type="bibr" target="#b2">(Coavoux and Crabbé, 2016;</ref><ref type="bibr">Fernández-González and Gómez-Rodríguez, 2018)</ref>, constituent reachability can be used to efficiently compute the first term of the symmetric difference (|γ G \ γ|), by simply counting the gold constituents that are individually unreachable from configuration c, as we describe in the next subsection.</p><p>The second and third terms of the loss (|γ c \ γ G | and |α c \ α G |) can be trivially computed and are used to penalize false positives (extra erroneous constituents) so that final F-score is not harmed due to the decrease of precision, as pointed out by <ref type="bibr" target="#b2">(Coavoux and Crabbé, 2016;</ref><ref type="bibr">Fernández-González and Gómez-Rodríguez, 2018)</ref>. Note that it is cru- cial that the creation of non-gold Non-Terminal transitions is avoided, since these might not affect the creation of gold constituents, however, they will certainly lead the parser to the creation of ex- tra erroneous constituents in future steps.</p><p>Finally, the function out of order of the last term can be implemented by computing the longest increasing subsequence of gold non- terminal nodes in the stack, where the order re- lation is given by the order of non-terminals (provided by their associated index) in the trans- ition sequence that builds the gold tree (this or- der is unique, as none of our two parsers of in- terest have spurious ambiguity). Obtaining the longest increasing subsequence is a well-known problem solvable in time O(n log n) <ref type="bibr" target="#b8">(Fredman, 1975)</ref>, where n denotes the length of the input se- quence. Once we have the largest possible sub-</p><note type="other">Shift: Σ, i, false, γ, α ⇒ Σ|(wi, i, i + 1), i + 1, false, γ ∪ {(wi, i, i + 1)}, α</note><p>Non-Terminal-X:</p><formula xml:id="formula_5">Σ, i, false, γ, α ⇒ Σ|(X, i), i, false, γ, α ∪ {(X, i)}} Reduce: Σ|(Y1, m0, m1)|(X, j)|...|(Y k , m k−1 , m k ), i, false, γ, α ⇒ Σ|(X, m0, m k ), i, false, γ ∪ {(X, m0, m k )}, α \ {(X, j)}} Finish:</formula><p>(S, 0, n), n, false, γ, α ⇒ (S, 0, n), n, true, γ, α <ref type="figure">Figure 4</ref>: Transitions of a in-order constituent parser. sequence of gold non-terminal nodes in our con- figuration's stack that is compatible with the gold order, the remaining ones give us the number of er- roneous constituents that we will unavoidably gen- erate, even in the best case, due to building them in an incorrect order. We will prove below that this loss formulation returns the exact loss and the resulting dynamic oracle is correct.</p><formula xml:id="formula_6">Transition Σ Buffer [ ] [ The, ...] SH [ The ] [ public, ...] NTNP [ The, NP ] [ public, ...] SH [ The, NP, public ] [ is, ...] RE [ NP ] [ is, ...] NTS [ NP, S ] [ is, ...] SH [ NP, S, is ] [ still, ...] NTVP [ NP, S, is, VP ] [ still, ...] SH [ NP, S, is, VP, still ] [ cautious, ...] NTADVP [ NP, S, is, VP, still, ADVP ] [ cautious, ...] RE [ NP, S, is, VP, ADVP ] [ cautious, ...] SH [ NP, S, is, VP, ADVP, cautious ] [ . ] NTADJP [NP, S, is,VP, ADVP, cautious, ADJP] [ . ] RE [ NP, S, is, VP, ADVP, ADJP ] [ . ] RE [ NP, S, VP ] [ . ] SH [ NP, S, VP, . ] [ ] RE [ S ] [ ] FI [ S ] [ ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constituent reachability</head><p>We now show how the computation of the set of reachable constituents developed for bottom- up parsing in <ref type="bibr" target="#b2">(Coavoux and Crabbé, 2016;</ref><ref type="bibr">Fernández-González and Gómez-Rodríguez, 2018)</ref> can be extended to deal with the top-down and in-order strategies.</p><p>Top-down transition system Let γ G and α G be the set of gold constituents and the set of gold non-terminal nodes, respectively, for our current input. We say that a gold constitu- ent (X, l, r) ∈ γ G is reachable from a con-</p><formula xml:id="formula_7">figuration c = Σ, j, /, γ c , α c with Σ = [(Y p , i p , i p−1 ) · · · (Y 2 , i 2 , i 1 )|(Y 1 , i 1 , j)]</formula><p>, and it is included in the set of individually reachable con- stituents R(c, γ G ), iff it satisfies one of the follow- ing conditions: <ref type="bibr">4</ref> (i) (X, l, r) ∈ γ c (i.e. it has already been created and, therefore, it is reachable by definition). (ii) j ≤ l &lt; r ∧ (X, l) / ∈ α c (i.e. the words dominated by the gold constituent are still in the buffer and the non-terminal node that be- gins its creation has not been added to the stack yet; therefore, it can be still created after pushing the correct non-terminal node and shifting the necessary words).</p><formula xml:id="formula_8">(iii) l ∈ {i k | 1 ≤ k ≤ p} ∧ j ≤ r ∧ (X, l) ∈ α c</formula><p>(i.e. its span is partially or completely in the stack and the corresponding non-terminal node was already added to the stack, then, by shifting more words or/and reducing, the con- stituent can still be created).</p><p>In-order transition system Let γ G and α G be the set of gold constituents and the set of gold non-terminal nodes, respectively, for our current input. We say that a gold constitu- ent (X, l, r) ∈ γ G is reachable from a con-</p><formula xml:id="formula_9">figuration c = Σ, j, false, γ c , α c with Σ = [(Y p , i p , i p−1 ) · · · (Y 2 , i 2 , i 1 )|(Y 1 , i 1 , j)]</formula><p>, and it is included in the set of individually reachable con- stituents R(c, γ G ), iff it satisfies one of the follow- ing conditions: (i) (X, l, r) ∈ γ c (i.e. it has already been cre- ated). (ii) j ≤ l &lt; r (i.e. the constituent is entirely in the buffer, then it can be still built).</p><formula xml:id="formula_10">(iii) l ∈ {i k | 1 ≤ k ≤ p} ∧ j ≤ r ∧ (X, m) /</formula><p>∈ α c (i.e. its first child is still a totally-or partially- built constituent on top of the stack and the non-terminal node has not been created yet; therefore, it has to wait till the first child is completed (if it is still pending) and, then, it can be still created by pushing onto the stack the correct non-terminal node and shift- ing more words if necessary).</p><formula xml:id="formula_11">(iv) l ∈ {i k | 1 ≤ k ≤ p} ∧ j ≤ r ∧ (X, m) ∈ α c ∧ ∃(Y, l, m) ∈ Σ (i.</formula><p>e. its span is par- tially or completely in the stack, and its first child (which is an alredy-built constituent) and the non-terminal node assigned are ad- jacent, thus, by shifting more words or/and reducing, the constituent can still be built). In both transition systems, the set of individually unreachable constituents U(c, γ G ) with respect to the set of gold constituents γ G can be easily com- puted as γ G \ R(c, γ G ) and will contain the gold constituents that can no longer be built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Correctness</head><p>We will now prove that the above expression of (c) indeed provides the minimum possible Ham- ming loss to the gold tree among all the trees that are reachable from configuration c. This implies correctness (or optimality) of our oracle.</p><p>To do so, we first show that both algorithms are constituent-decomposable. This amounts to say- ing that if we take a set of m constituents that are tree-compatible (can appear together in a constitu- ent tree, meaning that no pair of constituent spans overlap unless one is a subset of the other) and in- dividually reachable from a configuration c, then the set is also reachable as a whole.</p><p>We prove this by induction on m. The base case (m = 1) is trivial. Let us suppose that constituent-decomposability holds for any set of m tree-compatible constituents. We will show that it also holds for any set T of m+1 tree-compatible constituents.</p><p>Let (X, l, r) be one of the constituents in T such that r = min{r | (X , l , r ) ∈ T } and l = max{l | (X , l , r) ∈ T }. Let T = T \ {(X, l, r)}. Since T has m constituents, by induction hypothesis, T is a reachable set from configuration c.</p><p>Since (X, l, r) is individually reachable by hy- pothesis, it must satisfy at least one of the condi- tions for constituent reachability. As these con- ditions are different for each particular algorithm, we continue the proof separately for each:</p><p>Top-down constituent-decomposability In this case, we enumerated three constituent reachability conditions, so we divide the proof into three cases:</p><p>If the first condition holds, then the constitu- ent (X, l, r) has already been created in c. Thus, it will still be present after applying any of the possible transition sequences that build T starting from c. Hence, T = T ∪ {(X, l, r)} is reachable from c.</p><p>If the second condition holds, then j ≤ l &lt; r and the constituent (X, l, r) can be created by l−j Shift transitions, followed by one Non-Terminal transition, r − l Shift transitions and one Reduce transition. This will leave the parser in a configur- ation whose value of j is r, and where stack ele- ments with left span index ≤ l (apart from those referencing the new non-terminal and its leftmost child) have not changed. Thus, constituents of T are still individually reachable in this configura- tion, as their left span index is either ≥ r (and then they meet the second reachability condition) or ≤ l (and then they meet the third), so T is reach- able from c.</p><p>Finally, if the third condition holds, then we can create (X, l, r) by applying r − j Shift transitions followed by a sequence of Reduce transitions stop- ping when we obtain (X, l, r) on the stack (this will always happen after a finite number of such transitions, as the reachability condition guaran- tees that l is the left span index of some constitu- ent already on the stack, and that (X, l) is on the stack). Following the same reasoning as in the pre- vious case regarding the resulting parser configur- ation, we conclude that T is reachable from c.</p><p>With this we have shown the induction step, and thus constituent decomposability for the top-down parser.</p><p>In-order constituent decomposability The in- order parser has four constituent reachability con- ditions. Analogously to the previous case, we prove the reachability of T by case analysis.</p><p>If the first condition holds, then we have a situ- ation where the constituent (X, l, r) has already been created in c, so reachability of T follows from the same reasoning as for the first condition in the top-down case.</p><p>If the second condition holds, we have j ≤ l &lt; r and the constituent (X, l, r) can be created by l − j + 1 Shift transitions (where the last one shifts a word that will be assigned as left child of the new constituent), followed by the relevant Non-Terminal-X transition, r − l − 1 more Shift transitions and one Reduce transition. After this, the parser will be in a configuration where j takes the value r, where we can use the same reasoning as in the second condition of the top-down parser to show that all constituents of T are still reach- able, proving reachability of T .</p><p>For the third condition, the proof is analogous but the combination of transitions that creates the non-terminal starts with a sequence composed of Reduce transitions (when there is a non-terminal at the top of the stack) or Non-Terminal-Y trans- itions for arbitrary Y (when the top of the stack is a constituent) until the top node on the stack is a constituent with left span index l (this ensures that the constituent at the top of the stack can serve as leftmost child for our desired constituent), fol- lowed by a Non-Terminal-X, r−j Shift transitions and one Reduce transition.</p><p>Finally, for the fourth condition, the reasoning is again analogous, but the computation leading to the non-terminal starts with as many Reduce trans- itions as non-terminal nodes located above (X, m) in the stack (if any). If we call j the index asso- ciated to the resulting transition, then it only re- mains to apply r − j Shift transitions followed by a Reduce transition.</p><p>Optimality With this, we have shown constitu- ent decomposability for both parsing algorithms. This means that, for a configuration c, and a set of constituents that are individually reachable from c, there is always some computation that can build them all. This facilitates the proof that the loss function is correct.</p><p>To finish the proof, we observe the following:</p><p>• Let c be a final configuration reachable from c. The set (γ c \ γ G ), representing erroneous constituents that have been built, will always contain at least |γ c \ γ G |, as the algorithm never deletes constituents.</p><p>• In addition, c will contain one erroneous constituent for each element of (α c \ α G ), as once a non-terminal node is on the stack, there is no way to reach a final configura- tion without using it to create an erroneous constituent. Note that these erroneous con- stituents do not overlap those arising from the previous item, as γ c stores already-built con- stituents and α c non-terminals that have still not been used to build a constituent.</p><p>• Given a subset S of R(c, γ G ), the previously shown constituent decomposability property implies that there exists at least one transition sequence starting from c that generates the tree S ∪(γ c \γ G )∪E, where E is a set of erro- neous constituents containing one such con- stituent per element of (α c \ α G ). This tree has loss</p><formula xml:id="formula_12">|t G |−(|γ c ∪S|)+|γ c \γ G |+|α c \α G |.</formula><p>The term |t G | − (|γ c ∪ S|) corresponds to missed constituents (gold constituents that have not been already created and are not cre- ated as part of S), the other two to erroneous constituents.</p><p>• As we have shown that the erroneous con- stituents arising from (γ c \γ G ) and (α c \α G ) are unavoidable, computations yielding a tree with minimum loss are those that maximize |γ c ∪ S| in the previous term. In general, the largest possible |S| is for S = R(c, γ G ). In that case, we would correctly generate every reachable constituent and the loss would be</p><formula xml:id="formula_13">(c) = |U(c, γ G )| + |γ c \ γ G | +|α c \ α G |</formula><p>However, we additionally want to generate constituents in the correct order, and this may not be possible if we have already shifted some of them into the stack in a wrong or- der. The function out of order gives us the number of reachable constituents that are lost for this cause in the best case. Thus, indeed, the expression</p><formula xml:id="formula_14">(c) = |U(c, γ G )| + |γ c \ γ G | +|α c \ α G | + out of order (α c , α G )</formula><p>provides the minimum loss from configura- tion c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We test the two proposed approaches on two widely-used benchmarks for constituent parsers: the Wall Street Journal (WSJ) sections of the Eng- lish Penn Treebank <ref type="bibr">5 (Marcus et al., 1993</ref>) and ver- sion 5.1 of the Penn Chinese Treebank (CTB) <ref type="bibr">6 (Xue et al., 2005</ref>). We use the same predicted POS tags and pre-trained word embeddings as  and Liu and Zhang (2017a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Model</head><p>To perform a fair comparison, we define the novel dynamic oracles on the original implementations of the top-down parser by  and in-order parser by <ref type="bibr" target="#b14">Liu and Zhang (2017a)</ref>, where parsers are trained with a traditional static oracle. Both implementations follow a stack-LSTM ap- proach to represent the stack and the buffer, as well as a vanilla LSTM to represent the action history.</p><p>In addition, they also use a bi-LSTM as a composi- tional function for representing constituents in the stack. Concretely, this consists in computing the composition representation s comp as:</p><formula xml:id="formula_15">s comp = (LST M f wd [e nt , s 0 , ..., s m ]; LST M bwd [e nt , s m , ..., s 0 ])</formula><p>where e nt is the vector representation of a non- terminal, and s i , i ∈ [0, m] is the ith child node. Finally, the exact same word representation strategy and hyper-parameter values as (  and ( <ref type="bibr" target="#b14">Liu and Zhang, 2017a</ref>) are used to conduct the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Error exploration</head><p>In order to benefit from training a parser by a dy- namic oracle, errors should be made during the training process so that the parser can learn to avoid and recover from them. Unlike more com- plex error-exploration strategies as those studied in ( <ref type="bibr" target="#b5">Cross and Huang, 2016b;</ref><ref type="bibr" target="#b9">Fried and Klein, 2018)</ref>, we decided to consider a simple one that follows a non-optimal transition when it is the highest-scoring one, but with a certain probability. In that way, we eas- ily simulate test time conditions, when the parser greedily chooses the highest-scoring transition, even when it is not an optimal one, placing the parser in an incorrect state.</p><p>In particular, we run experiments on devel- opment sets for each benchmark/algorithm with three different error exploration probabilities and choose the one that achieves the best F-score. <ref type="table">Table 1</ref> reports all results, including those ob- tained by the top-down and in-order parsers trained by a dynamic oracle without error explor- ation (equivalent to a traditional static oracle).  <ref type="bibr">Fernández-G and Gómez-R, 2018)</ref> gs bu 91.5 <ref type="bibr">(Fernández-G and Gómez-R, 2018</ref>  <ref type="table" target="#tab_0">Table 2</ref>: Accuracy comparison of state-of-the-art single-model fully-supervised constituent parsers on WSJ §23 (top) and CTB §271-300 (bottom). The "Type" column shows the type of parser: gs is a greedy parser trained with a static oracle, gd a greedy parser trained with a dynamic oracle, b a beam search parser, bp a beam search parser trained with a policy gradient method, bd a beam search parser trained with a non- optimal dynamic oracle, bg a generative beam search parser, and ch a chart-based parser. Finally, the "Strat" column describes the strategy followed (bu=bottom-up, td=top-down and in=in-order). <ref type="table" target="#tab_0">Oracle  #1  #2  #3  #4  #5  Top-down</ref>   <ref type="table">Table 3</ref>: F-score on constituents with a number of children ranging from one to five on WSJ §23.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parser</head><p>we also include some recent state-of-the-art pars- ers with global chart decoding that achieve the highest accuracies to date on WSJ, but are much slower than shift-reduce algorithms.</p><p>Top-down and in-order parsers benefit from be- ing trained by these new dynamic oracles in both datasets. The top-down strategy achieves a gain of 0.5 and 0.7 points in F-score on WSJ and CTB benchmarks, respectively. The in-order parser obtains similar improvements on the CTB (0.5 points), but less notable accuracy gain on the WSJ (0.2 points). Although a case of diminishing re- turns might explain the latter, the in-order parser trained with the proposed dynamic oracle still achieves the highest accuracy to date in greedy transition-based constituent parsing on the WSJ. <ref type="bibr">7</ref> While this work was under review, <ref type="bibr" target="#b9">Fried and Klein (2018)</ref> proposed to train the top-down and in-order parsers with a policy gradient method in- stead of custom designed dynamic oracles. They also present a non-optimal dynamic oracle for the top-down parser that, combined with more complex error-exploration strategies and size-10 beam search, significantly outperforms the policy gradient-trained version, confirming that even non-optimal dynamic oracles are a good option. <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>Dan Bikel's randomized parsing evaluation com- parator <ref type="bibr" target="#b1">(Bikel, 2004</ref>) was used to perform signi- ficance tests on precision and recall metrics on WSJ §23 and CTB §271-300. The top-down parser trained with dynamic oracles achieves statistically significant improvements (p &lt; 0.05) in precision <ref type="bibr">7</ref> Note that the proposed dynamic oracles are orthogonal to approaches like beam search, re-ranking or semi-supervision, that can boost accuracy but at a large cost to parsing speed.</p><p>8 Unfortunately, we cannot directly compare our approach to theirs, since they use beam-search decoding with size 10 in all experiments, gaining up to 0.3 points in F-score, while penalizing speed with respect to greedy decoding. However, by extrapolating the results above, we hypothesize that our optimal dynamic oracles (especially the one designed for the in-order algorithm) with their same training and beam-search decoding setup might achieve the best scores to date in shift- reduce parsing. both on the WSJ and CTB benchmarks, and in re- call on WSJ. The in-order parser trained with the proposed technique obtains significant improve- ments (p &lt; 0.05) in recall in both benchmarks, although not in precision.</p><p>We also undertake an analysis to check if dy- namic oracles are able to mitigate error propaga- tion. We report in <ref type="table">Table 3</ref> the F-score obtained in constituents with different number of children on WSJ §23 by the top-down and in-order al- gorithms trained with both static and dynamic or- acles. Please note that creating a constituent with a great number of children is more prone to suffer from error propagation, since a larger number of transitions is required to build it. The results seem to confirm that, indeed, dynamic oracles manage to alleviate error propagation, since improvements in F-score are more notable for larger constituents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We develop the first optimal dynamic oracles for training the top-down and the state-of-the-art in- order parsers. Apart from improving the sys- tems' accuracies in both cases, we achieve the best result to date in greedy shift-reduce pars- ing on the WSJ. In addition, these promising techniques could easily benefit from recent stud- ies in error-exploration strategies and yield state- of-the-art accuracies in transition-based parsing in the near future. The parser's source code is freely available at https://github.com/ danifg/Dynamic-InOrderParser.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Simplified constituent tree, taken from English WSJ §22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Transition sequence for producing the constituent tree in Figure 1 using a top-down parser. SH = Shift, NT X = Non-Terminal-X and RE = Reduce. Already-built constituents are marked in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Transition sequence for producing the constituent tree in Figure 1 using an in-order parser. SH = Shift, NT X = Non-Terminal-X, RE = Reduce and FI = Finish. Already-built constituents are marked in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 compares</head><label>2</label><figDesc></figDesc><table>our system's accuracy to other 
state-of-the-art shift-reduce constituent parsers on 
the WSJ and CTB benchmarks. For comparison, 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>55 (+0.19) 89.43 (+0.22) 85.34 (+0.19) 77.57 (+0.49) 81.03 (+2.01)</head><label></label><figDesc></figDesc><table>static 
90.98 
88.76 
85.01 
76.63 
77.35 
dynamic 91.34 (+0.36) 89.18 (+0.42) 85.17 (+0.16) 77.12 (+0.49) 80.02 (+2.67) 
In-order 
static 
91.36 
89.21 
85.15 
77.08 
79.02 
dynamic 91.</table></figure>

			<note place="foot" n="1"> In parallel to this work, Fried and Klein (2018) present a non-optimal dynamic oracle for training the top-down parser.</note>

			<note place="foot" n="2"> When two or more non-terminals share their labels within the tree, we use a secondary index to make them unique.</note>

			<note place="foot" n="3"> Note that we only consider predicted non-terminal nodes still in the stack, since wrong non-terminal nodes that have been already reduced are included in the loss as erroneous constituents.</note>

			<note place="foot" n="4"> Please note that elements from the stack can be an already-built constituent, a shifted word or a non-terminal node. Therefore, (Yp, ip, ip−1), (Y2, i2, i1) and (Y1, i1, j) should be represented as (Yp, ip−1), (Y2, i1) and (Y1, j), respectively, when they are non-terminal nodes. We omit this for simplicity.</note>

			<note place="foot" n="5"> Sections 2-21 are used as training data, Section 22 for development and Section 23 for testing 6 Articles 001-270 and 440-1151 are taken for training, articles 301-325 for system development, and articles 271300 for final testing</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has received funding from the European Research Council (ERC), under the European Union's Horizon 2020 research and innovation programme (FASTPARSE, grant agreement No 714150), from MINECO (FFI2014-51978-C2-2-R, TIN2017-85160-C2-1-R) and from Xunta de Galicia (ED431B 2017/01).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training with exploration improves a greedy stack LSTM parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="2005" to="2010" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the Parameter Space of Generative Lexicalized Statistical Parsing Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural greedy constituent parsing with dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilingual discriminative lexicalized phrase structure parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benoit Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1847" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Incremental parsing with minimal features using bi-directional LSTM. In ACL (2). The Association for Computer Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Faster shift-reduce constituent parsing with a non-binary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-González</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gómezrodríguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>bottom-up strategy. arXiv, 1804.07961 [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On computing the length of longest increasing subsequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fredman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Policy gradient as a proxy for dynamic oracles in constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What&apos;s going on in neural constituency parsers? an analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="999" to="1010" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="959" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2675" to="2685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">In-order transition-based constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="413" to="424" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shift-reduce constituent parsing with neural lookahead features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shift-reduce constituency parsing with dynamic programming and pos tag lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1030" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A classifier-based parser with linear run-time complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 9th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective inference for generative neural parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="1695" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature optimization for constituent parsing via neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1138" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-31" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transition-based parsing of the chinese treebank using a global discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies, IWPT &apos;09</title>
		<meeting>the 11th International Conference on Parsing Technologies, IWPT &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08-09" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
