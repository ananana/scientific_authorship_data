<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangze</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper , we introduce a large corpus of Chi-nese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public 1. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, individuals or organizations can eas- ily share or post information to the public on the social network. Take the popular Chinese mi- croblogging website (Sina Weibo) as an example, the People's Daily, one of the media in China, posts more than tens of weibos (analogous to tweets) each day. Most of these weibos are well- written and highly informative because of the text length limitation (less than140 Chinese charac- ters). Such data is regarded as naturally annotated web resources <ref type="bibr" target="#b18">(Sun, 2011</ref>). If we can mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data.  In the Natural Language Processing (NLP) community, automatic text summarization is a hot and difficult task. A good summarization system should understand the whole text and re-organize the information to generate coherent, informative, and significantly short summaries which convey important information of the original text <ref type="bibr" target="#b6">(Hovy and Lin, 1998)</ref>, <ref type="bibr" target="#b14">(Martins, 2007)</ref>. Most of tradi- tional abstractive summarization methods divide the process into two phrases <ref type="bibr" target="#b1">(Bing et al., 2015)</ref>. First, key textual elements are extracted from the original text by using unsupervised methods or lin- guistic knowledge. And then, unclear extracted components are rewritten or paraphrased to pro- duce a concise summary of the original text by using linguistic rules or language generation tech- niques. Although extensive researches have been done, the linguistic quality of abstractive sum- mary is still far from satisfactory. Recently, deep learning methods have shown potential abilities to learn representation ( <ref type="bibr" target="#b7">Hu et al., 2014;</ref><ref type="bibr" target="#b21">Zhou et al., 2015</ref>) and generate language ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref>) from large scale data by utilizing GPUs. Many researchers realize that we are closer to generate abstractive summariza- tions by using the deep learning methods. How- ever, the publicly available and high-quality large scale summarization data set is still very rare and not easy to be constructed manually. For exam- ple, the popular document summarization dataset DUC 2 , TAC <ref type="bibr">3</ref>   per, we take one step back and focus on construct- ing LCSTS, the Large-scale Chinese Short Text Summarization dataset by utilizing the naturally annotated web resources on Sina Weibo. <ref type="figure" target="#fig_1">Figure 1</ref> shows one weibo posted by the People's Daily. In order to convey the import information to the pub- lic quickly, it also writes a very informative and short summary (in the blue circle) of the news. Our goal is to mine a large scale, high-quality short text summarization dataset from these texts. This paper makes the following contributions: (1) We introduce a large scale Chinese short text summarization dataset. To our knowledge, it is the largest one to date; (2) We provide standard splits for the dataset into large scale training set and human labeled test set which will be easier for benchmarking the related methods; (3) We explore the properties of the dataset and sample 10,666 instances for manually checking and scoring the quality of the dataset; (4) We perform recurrent neural network based encoder-decoder method on the dataset to generate summary and get promis- ing results, which can be used as one baseline of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is related to recent works on automatic text summarization and natural language process- ing based on naturally annotated web resources, which are briefly introduced as follows.</p><p>Automatic Text Summarization in some form has been studied since 1950. Since then, most re- searches are related to extractive summarizations by analyzing the organization of the words in the document ( <ref type="bibr" target="#b16">Nenkova and McKeown, 2011)</ref>  <ref type="bibr" target="#b12">(Luhn, 1998)</ref>; Since it needs labeled data sets for su- pervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods <ref type="bibr" target="#b15">(Mihalcea, 2004</ref>). The scale of existing data sets are usually very small (most of them are less than 1000). For example, DUC2002 dataset contains 567 docu- ments and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset written in English for head- line generation ( <ref type="bibr" target="#b4">Colmenares et al., 2015)</ref>. How- ever, the data set is not publicly available.</p><p>Naturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011). Naturally Annotated Web Re- sources is the data generated by users for commu- nicative purposes such as web pages, blogs and microblogs. We can mine knowledge or useful data from these raw data by using marks generated by users unintentionally. Jure et.al track 1.6 mil- lion mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle ( <ref type="bibr" target="#b8">Leskovec et al., 2009)</ref>. Sepandar et.al use the users' naturally annotated pattern 'we feel' and 'i feel' to extract the 'Feeling' sentence collec- tion which is used to collect the world's emotions. In this work, we use the naturally annotated re- sources to construct the large scale Chinese short text summarization data to facilitate the research on text summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Collection</head><p>A lot of popular Chinese media and organizations have created accounts on the Sina Weibo. They use their accounts to post news and information. These accounts are verified on the Weibo and la- beled by a blue 'V'. In order to guarantee the qual- ity of the crawled text, we only crawl the verified organizations' weibos which are more likely to be clean, formal and informative. There are a lot of human intervention required in each step. The pro- cess of the data collection is shown as <ref type="figure" target="#fig_2">Figure 2</ref> and summarized as follows:</p><p>1) We first collect 50 very popular organiza- tion users as seeds. They come from the domains of politic, economic, military, movies, game and etc, such as People's Daily, the Economic Observe press, the Ministry of National Defense and etc. 2) We then crawl fusers followed by these seed users and filter them by using human written rules such as the user must be blue verified, the number of followers is more than 1 million and etc. 3) We use the chosen users and text crawler to crawl their weibos. 4) we filter, clean and extract (short text, summary) pairs. About 100 rules are used to ex- tract high quality pairs. These rules are concluded by 5 peoples via carefully investigating of the raw text. We also remove those paris, whose short text length is too short (less than 80 characters) and length of summaries is out of <ref type="bibr">[10,</ref><ref type="bibr">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Properties</head><p>The dataset consists of three parts shown as Ta- ble 1 and the length distributions of texts are shown as <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Part I is the main content of LCSTS that con- tains 2,400,591 (short text, summary) pairs. These pairs can be used to train supervised learning model for summary generation.</p><p>Part II contains the 10,666 human labeled (short text, summary) pairs with the score ranges from 1 to 5 that indicates the relevance between the short text and the corresponding summary. '1' denotes " the least relevant " and '5' denotes "the most relevant". For annotating this part, we recruit 5 volunteers, each pair is only labeled by one an- notator. These pairs are randomly sampled from Part I and are used to analysize the distribution of pairs in the Part I. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates examples of different scores. From the examples we can see that pairs scored by 3, 4 or 5 are very relevant to the corresponding summaries. These summaries are highly informative, concise and significantly short compared to original text. We can also see that many words in the summary do not appear in the original text, which indicates the significant difference of our dataset from sentence compres- sion datasets. The summaries of pairs scored by 1 or 2 are highly abstractive and relatively hard to conclude the summaries from the short text. They are more likely to be headlines or comments in- stead of summaries. The statistics show that the percent of score 1 and 2 is less than 20% of the data, which can be filtered by using trained classi- fier.</p><p>Part III contains 1,106 pairs. For this part, 3 annotators label the same 2000 texts and we ex- tract the text with common scores. This part is independent from Part I and Part II. In this work, we use pairs scored by 3, 4 and 5 of this part as the test set for short text summary generation task. Mingzhong Chen, the Chief Secretary of the Water Devision of the Ministry of Water Resources, revealed today at a press conference, according to the just&lt; completed assessment of water resources management system, some provinces are closed to the red line indicator, some provinces are over the red line indicator. In some places over the red lineIt will enforce regional approval restrictions on some water projects , implement strictly w ater resources assessment and the approval of water licensing. Summarization: Some provinces exceeds the red line indicator of annual water using, some water project will be. limited approved Human Score: 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short Text: 30%PC O2OO2O O2O</head><p>Groupons' sales on mobile terminals are below 30 percent. User's preference of shopping through PCs can not be changed in the short term. In the future Chinese O2O catering market, mobile terminals will become the strategic development direction. And also, it will become offDline driving from onDline driving. The first and second tier cities are facing growth difficulties. However, O2O market in the third and fourth tier cities contains opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization: O2O</head><p>The mobile terminals will become catering's strategic development direction. Centaline Property Agency, said that because the first and second2tier city gathers too many resources, the price of house is likely to rise and hard to fall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization: 14</head><p>1002cities' house prices gain "14th consecutive rising", the first and second2tier cities rise more. and the subset of Part III, which is scored by 3, 4 and 5, as test set.</p><p>Two approaches are used to preprocess the data: 1) character-based method, we take the Chinese character as input, which will reduce the vocab- ulary size to 4,000. 2) word-based method, the   text is segmented into Chinese words by using jieba <ref type="bibr">5</ref> . The vocabulary is limited to 50,000. We adopt two deep architectures, 1) The local con- text is not used during decoding. We use the RNN as encoder and it's last hidden state as the input of decoder, as shown in <ref type="figure">Figure 5;</ref> 2) The context is used during decoding, following <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>), we use the combination of all the hidden states of encoder as input of the decoder, as shown in <ref type="figure" target="#fig_6">Figure 6</ref>. For the RNN, we adopt the gated recurrent unit (GRU) which is proposed by <ref type="bibr" target="#b3">(Chung et al., 2015)</ref> and has been proved comparable to LSTM <ref type="bibr" target="#b2">(Chung et al., 2014</ref>). All the parameters (including the embeddings) of the two architectures are randomly initialized and ADADELTA <ref type="bibr" target="#b20">(Zeiler, 2012</ref>) is used to update the learning rate. After the model is trained, the beam search is used to generate the best summaries in the process of decoding and the size of beam is set to 10 in our experiment.   For evaluation, we adopt the ROUGE met- rics <ref type="bibr" target="#b11">(Lin, 2004)</ref> proposed by <ref type="bibr" target="#b10">(Lin and Hovy, 2003)</ref>, which have been proved strongly correlated with human evaluations. ROUGE-1, ROUGE- 2 and ROUGE-L are used. All the models are trained on the GPUs tesla M2090 for about one week. <ref type="table" target="#tab_5">Table 2</ref> lists the experiment results. As we can see in <ref type="figure" target="#fig_7">Figure 7</ref>, the summaries generated by RNN with context are very close to human written summaries, which indicates that if we feed enough data to the RNN encoder and decoder, it may gen- erate summary almost from scratch.</p><formula xml:id="formula_0">model data R-1 R-2 R-L RNN Word</formula><p>The results also show that the RNN with con- text outperforms RNN without context on both character and word based input. This result indi- cates that the internal hidden states of the RNN encoder can be combined to represent the context of words in summary. And also the performances of the character-based input outperform the word- based input. As shown in <ref type="figure" target="#fig_8">Figure 8</ref>, the summary generated by RNN with context by inputing the character-based short text is relatively good, while the the summary generated by RNN with context</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short Text: 20 74</head><p>On The factory's door is locked. About 20 works are scattered to sit under the shade. "We are ordinary workers, we are waiting for our salary here." one of them said. On the morning of July 4 th , reporters arrived at Shenzhen Yuanjing Photoelectron Corporation located on Qinghu Road, Longhua District, Shenzhen. Just as the rumor, Yuanjing Photoelectron Corporation is closed down and the large shareholder Xing Yi is missing. Human: LED HundredRmillionRyuanRclass LED enterprise is closed down and workers wait for the boss under the under the scorching sun.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN+Context+Char:</head><p>Shenzhen Yuanjing PhotoElectron Corporation is closed down. RNN+Context+Word: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK Shenzhen UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK on word-based input contains many UNKs. This may attribute to that the segmentation may lead to many UNKs in the vocabulary and text such as the person name and organization name. For exam- ple, "愿景光电子" is a company name which is not in the vocabulary of word-based RNN, the RNN summarizer has to use the UNKs to replace the "愿景光电子" in the process of decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We constructed a large-scale Chinese short text summarization dataset and performed RNN-based methods on it, which achieved some promising re- sults. This is just a start of deep models on this task and there is much room for improvement. We take the whole short text as one sequence, this may not be very reasonable, because most of short texts contain several sentences. A hierarchical <ref type="bibr">RNN (Li et al., 2015</ref>) is one possible direction. The rare word problem is also very important for the gener- ation of the summaries, especially when the input is word-based instead of character-based. It is also a hot topic in the neural generative models such as neural translation machine(NTM) ( <ref type="bibr" target="#b13">Luong et al., 2014)</ref>, which can benefit to this task. We also plan to construct a large document summarization data set by using naturally annotated web resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>http://icrc.hitsz.edu.cn/Article/show/139.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Weibo Posted by People's Daily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagram of the process for creating the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Box plot of lengths for short text(ST), segmented short text(Segmented ST), summary(SUM) and segmented summary(Segmented SUM). The red line denotes the median, and the edges of the box the quartiles.</figDesc><graphic url="image-31.png" coords="3,307.28,62.81,218.27,163.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Five examples of different scores.</figDesc><graphic url="image-36.png" coords="4,71.09,535.21,220.32,123.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The graphical depiction of the RNN encoder and decoder framework with context.</figDesc><graphic url="image-35.png" coords="4,70.74,418.79,220.90,118.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example of the generated summaries.</figDesc><graphic url="image-111.png" coords="5,71.49,191.74,219.42,236.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An example of the generated summaries with UNKs.</figDesc><graphic url="image-112.png" coords="5,306.00,62.51,221.36,150.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>and TREC 4 have only hundreds of human written English text summarizations. The problem is even worse for Chinese. In this pa-</figDesc><table>User 
crawler 

selecting 

Text 
crawler 

filtering,, 
cleaning,and 
extracting 
Data,set 

Social,Media 

Raw,Text 

Seeds 
Chosen,Users 

Users,Collection 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The experiment result: "Word" and 
"Char" denote the word-based and character-
based input respectively. 

Short Text: 932014'2015 

28 

On September 3, the Geneva'based World Economic Forum released " The 
Global Competitiveness Report 2014'2015 ". Switzerland topped the list for six 
consecutive years , becoming the world's most competitive country. Singapore 
and the United States are in the second and third place respectively. China is in 
the 28 th place, ranking highest among the BRIC countries. 

Human: 2 8 
the Global competitiveness ranking list, China is in the 28 th place, the highest 
among BRIC countries. 

RNN+Char: 9302012 
5000 
It is not a fluent sentence and can not be translated. There are only some 
segments related to original text. 
RNN+Word: :31111 
11 
Global competitiveness ranking: Switzerland ranks 3 rd place, China 1 st place, 1 st 
place, China 1 st place, Switzerland 1 st place, first 

RNN+Context+Char: 28 
In the Global competitiveness ranking list, China is in the 28th place which is 
highest among the BRIC countries. 
RNN+Context+Word: 28 
( 
"The Global Competitiveness Report" is released. China is in the 28 th place, 
rank highest among the BRIC countries (ninth 

</table></figure>

			<note place="foot" n="2"> http://duc.nist.gov/data.html 3 http://www.nist.gov/tac/2015/KBP/ 4 http://trec.nist.gov/</note>

			<note place="foot" n="5"> https://pypi.python.org/pypi/jieba/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Natu-ral Science Foundation of China: 61473101, 61173075 and 61272383, Strategic Emerg-ing Industry Development Special Funds of Shenzhen: JCYJ20140417172417105, JCYJ20140508161040764 and JCYJ20140627163809422. We thank to Baolin Peng, Lin Ma, Li Yu and the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstractive multidocument summarization via phrase selection and merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1587" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1502.02367</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Heads: Headline generation as sequence prediction using an abstract feature-rich space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Mantrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT</title>
		<meeting>eddings of 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1303.5778</idno>
		<title level="m">Speech recognition with deep recurrent neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated text summarization and the summarist system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of a Workshop on</title>
		<meeting>a Workshop on<address><addrLine>Baltimore, Maryland; Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998-10-13" />
			<biblScope unit="page" from="197" to="214" />
		</imprint>
	</monogr>
	<note>TIPSTER &apos;98</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meme-tracking and the dynamics of the news cycle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2003 Language Technology Conference (HLT-NAACL 2003)</title>
		<meeting>2003 Language Technology Conference (HLT-NAACL 2003)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL</title>
		<meeting>Workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The automatic creation of literature abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1410.8206</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A survey on automatic text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Dipanjan Das Andr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph-based ranking algorithms for sentence extraction, applied to text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, companion volume</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics, companion volume<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic summarization. Foundations and Trend in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="103" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1503.02364</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural language procesing based on naturaly annotated web resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Song Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="32" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Answer sequence learning with neural networks for answer selection in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP</title>
		<meeting>the ACL-IJCNLP<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="713" to="718" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
