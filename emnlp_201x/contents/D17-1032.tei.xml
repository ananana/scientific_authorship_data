<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VecShare: A Framework for Sharing Word Representation Vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>ddowney@eecs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VecShare: A Framework for Sharing Word Representation Vectors</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="316" to="320"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many Natural Language Processing (NLP) models rely on distributed vector representations of words. Because the process of training word vectors can require large amounts of data and computation , NLP researchers and practitioners often utilize pre-trained embeddings downloaded from the Web. However, finding the best embeddings for a given task is difficult, and can be computation-ally prohibitive. We present a framework, called VecShare, that makes it easy to share and retrieve word embeddings on the Web. The framework leverages a public data-sharing infrastructure to host embedding sets, and provides automated mechanisms for retrieving the embed-dings most similar to a given corpus. We perform an experimental evaluation of VecShare&apos;s similarity strategies, and show that they are effective at efficiently retrieving embeddings that boost accuracy in a document classification task. Finally, we provide an open-source Python library for using the VecShare framework. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings capture syntactic and semantic properties of words, and are a key component of many modern NLP models ( <ref type="bibr" target="#b9">Turian et al., 2010)</ref>. However, high-quality embeddings can be expen- sive to train. As a result, rather than training their own embeddings, NLP researchers and practition- ers often download pre-trained embeddings from the Web, e.g. ( <ref type="bibr" target="#b4">Limsopatham and Collier, 2016;</ref><ref type="bibr" target="#b0">Cheng et al., 2016</ref>).</p><p>1 https://github.com/JaredFern/VecShare However, existing methods for sharing embed- dings on the Web are suboptimal. Current prac- tice primarily consists of contributors posting em- bedding sets to their own Web sites. No central embedding repository exists, and it is difficult for users to know which embedding sets are available. Furthermore, determining the utility of an embed- ding set for a particular NLP task entails signifi- cant time and computational costs, as users must manually download and evaluate multiple com- plete embedding sets. Methods exist for automati- cally scoring an embedding set, but these are lim- ited to specific tasks and lack integration with ex- isting code bases <ref type="bibr" target="#b2">(Faruqui and Dyer, 2014)</ref>.</p><p>Our goal in this paper is to introduce a frame- work that makes sharing word embeddings easier for NLP researchers and practitioners. It should be simple and fast to post, browse, and retrieve em- beddings from a public data store. Additionally, integration with existing NLP codebases should be more seamless: software libraries should automat- ically identify and download the particular embed- dings that are likely to be relevant to a user's cor- pus.</p><p>This paper presents VecShare, a framework for sharing word embeddings. As its data store, it uses an existing public data-sharing platform, which provides searching and browsing capabil- ity. To solve the critical challenge of helping users quickly find relevant embeddings, we in- troduce embedding indexers. The indexers com- pute and share compact representations, called sig- natures, for each embedding set. Users employ a software library that downloads the signatures and compares them against the user's corpus. Us- ing the signatures, the library efficiently evaluates the utility of each shared embedding set and de- termines which sets are most likely to be rele- vant. The library can then automatically download the relevant embeddings and make them available within the user's code. Finally, VecShare is an open source framework: new embeddings, index- ers, and signature methods can be independently added at any time.</p><p>We perform experiments evaluating different signature methods in selecting embeddings for document classification tasks, and demonstrate that an ensemble signature based on simple fea- tures (e.g. vocabulary overlap with the user's cor- pus, or similarities between a sample of embed- ding pairs) can select helpful embeddings. We release a Python library for NLP researchers and practitioners that can query the embedding library, and automatically select and download relevant embeddings for a given corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The VecShare Framework</head><p>The VecShare framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Contributors upload embedding sets to a Public Embedding Store, hosted on a public data-sharing platform. Indexers periodically poll the embed- ding store, detecting and indexing newly uploaded embedding sets. For each embedding set, index- ers store a signature that compactly represents the content of the embedding set. The indexer uses these signatures to return relevant embedding sets to users. A user can then retrieve embeddings for their particular corpus from the data store, using a software library built for this purpose.</p><p>The current VecShare implementation uses the public data sharing website data.world as its em- bedding store. <ref type="bibr">2</ref> We chose data.world for its ease of use and robustness, but any publicly-available data share that allows search by tags and programmatic access is sufficient to house VecShare. Below, we describe the details of the framework.</p><p>2 http://data.world/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contributors</head><p>A contributor who has computed a set of word em- beddings adds the embeddings to VecShare by up- loading the data to the share, following a simple standard format with n + 1 fields where the first field is a word, and the remaining fields give the n-dimensional embedding of the word. The con- tributor tags the data set with a designated tag so that indexers can automatically identify that the data set is an indexable word embedding set. For embedding sets to be applicable to certain kinds of signatures, contributors can also elect to up- load additional metadata with their embeddings (in our initial implementation, metadata includes a frequency ranking of terms in the corpus, and the total number of tokens used to construct the embeddings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indexers and Signatures</head><p>Indexers periodically poll the embedding store, looking for new embedding sets uploaded by con- tributors. For each embedding set, the indexer computes and stores a signature designed to cap- ture characteristics of the embeddings. To esti- mate the relevance of each embedding set for the user's task, these signatures are later compared to corresponding signatures created from the user's corpus. Thus, each signature method has an asso- ciated similarity measure, which takes in a pair of signatures (one from a VecShare embedding set, and the other from the user's corpus) and outputs a numeric similarity score for the pair.</p><p>We explore two primary signature methods in this paper. The first, VocabRk, consists of (up to) the T v most frequent words in the embedding cor- pus, excluding a set of stop words. The similar- ity method is the negative average rank of the sig- nature words within the user's frequency-ordered vocabulary. Words not in the user's corpus are as- signed a rank of T . Thus, the most similar embed- ding set under the VocabRk signature is thus the one with the lowest average rank.</p><p>The VocabRk signature method relies only on vocabulary overlap, and entirely ignores the em- beddings themselves. We also experiment with a second signature method that does utilize the em- beddings. The SimCorr signature for a set of em- beddings E consists of the embeddings for the T s most frequent words in E's corpus (again exclud- ing stop words). To estimate the similarity be- tween the user's corpus C and the embeddings E, the SimCorr method first computes a set of embed- dings on the user's corpus. For all words found in both E's signature and in the user's corpus, Sim- Corr computes all pairwise cosine similarities be- tween pairs of E embeddings, and pairs of C em- beddings. The Pearson correlation coefficient be- tween the E-based cosine similarities and the C- based ones is taken as the SimCorr similarity mea- sure. Thus, two embedding sets that estimate sim- ilarity of terms in a similar manner will be deemed similar according the SimCorr measure, even if the vector values of the embeddings differ substan- tially between the two sets.</p><p>The indexers make their signatures available to users by simply sharing a signature data set on the public data store, which is read by the VecShare software library. Like the other components of VecShare, the indexers of the framework are ex- tensible -new indexers, signature and similarity measures can be created at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Libraries</head><p>Users access the VecShare framework using a code library. Embeddings can be requested by name if the user desires a particular embedding (e.g. "300 dimensional Google News word2vec embeddings"), or the user can query an indexer to find embedding sets most likely to be useful for the user's task. Importantly, the software li- brary performs embedding selection locally on the user's machine, using signature similarity methods described previously. The user's corpus does not need to be uploaded or shared.</p><p>Once the target embedding set has been identi- fied, the library downloads the target embeddings for the particular words in the user's vocabulary. Thus, if the user's vocabulary is much smaller than that of the embedding set, this download can be much more compact than the full embedding set.</p><p>A contribution of this paper is the release of a Python library for the framework, which imple- ments the VocabRk similarity computation. With this library, leveraging the framework to select and retrieve the top-ranked embeddings for a given corpus requires just one line of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We now evaluate the effectiveness of the signature methods described in the previous section at iden- tifying high-quality embedding sets for a given corpus, for the task of text classification. We also quantify the improvement in efficiency when using VecShare rather than following the current practice of downloading and testing multiple em- bedding sets.</p><p>In our experiments, we set the parameters T v = 5, 000 and T s = 1, 000, and we discard the top 100 most frequent words as stopwords. When training embeddings on the user's corpus, we use word2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Methodology</head><p>We perform experiments in two settings: first with large-corpus embeddings, where we use word2vec and GloVe embedding sets trained on billions of tokens. The large-corpus embeddings are repre- sentative of state-of-the-art models, but are trained over very broad-topic corpora (billions of tokens of newswire, Web or social media text). To bet- ter measure whether VecShare can harness more specific, targeted embedding sets, we also evalu- ate over small-corpus embeddings.</p><p>For the large-corpus embeddings, we utilize three sets of GloVe embeddings <ref type="bibr" target="#b7">(Pennington et al., 2014</ref>): wik+, 100-dimensional embeddings trained on six billion tokens of Wikipedia and the Gigaword corpus; web, 300-dimensional embed- dings trained on 42 billion tokens of the Common Crawl Web dataset; and twtr, 100-dimensional embeddings trained on 27 billion tokens of Twit- ter posts. We also utilize gnws, 300-dimensional word2vec embeddings trained on three billion to- kens of Google News data. <ref type="bibr">3</ref> For the small corpus embeddings, we created a topically diverse collection of subsets of the New York Times corpus <ref type="bibr" target="#b8">(Sandhaus, 2008)</ref>, across seven categories (agriculture, arts, books, eco- nomics, government, movies, and weather). We then trained word2vec embeddings on each sub- set, to create seven distinct similarly-sized, small corpus embedding sets.</p><p>For our experiments, we utilize the embed- dings as features for document classification within a convolutional neural network <ref type="bibr" target="#b1">(Chollet, 2017)</ref>. We evaluate on four document classifica- tion tasks: Reuters-21578 newswire topic classi- fication ( <ref type="bibr" target="#b3">Lewis, 1997)</ref>, subjectivity classification ( <ref type="bibr" target="#b6">Pang and Lee, 2004</ref>), IMDB movie review classi- fication ( <ref type="bibr" target="#b5">Maas et al., 2011)</ref>, and the 20news clas- sification task. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reuters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjectivity</head><formula xml:id="formula_0">IMDB 20news Average ρ Sel. Acc. ρ Sel. Acc. ρ Sel. Acc. ρ Sel. Acc. ρ Acc Random - -0.862 - -0.688 - -0.868 - -0.763 -0.795 MaxTkn</formula><p>0.63 web 0.888 0.19 web 0.728 0.38 web 0.881 0.97 web 0.863 0.54 0.840 VocabRk 0.46 gnws 0.882 0.02 gnws 0.759 0.40 gnws 0.886 0.20 gnws 0.719 0.27 0.812 SimCorr -0.65 wik+ 0.84 0.81 gnws 0.759 0.45 gnws 0.886 0.60 twtr 0.748 0.30 0.808 All 0.26 gnws 0.882 0.43 gnws 0.759 0.49 gnws 0.886 0.87 web 0.863 0.51 0.848 Oracle - web 0.888 -gnws 0.759 -gnws 0.886 - web 0.863 - 0.85 <ref type="table">Table 1</ref>: Experimental results using large-corpus embeddings. All of the signature methods outperform the random baseline, and the All method performs best in terms of both correlation ρ and text classifica- tion accuracy.  <ref type="table">Table 2</ref>: Experimental results using small-corpus embeddings. The VocabRk and SimCorr methods outperform the baselines, and the All method performs best in terms of both correlation ρ and text clas- sification accuracy.</p><formula xml:id="formula_1">Reuters Subjectivity IMDB 20news Average ρ Sel. Acc. ρ Sel. Acc. ρ Sel. Acc. ρ Sel. Acc. ρ Acc Random - -0.844 - -0.667 - -0.829 - -0</formula><p>In addition to the VocabRk and SimCorr meth- ods described in the previous section, we also eval- uate against two simple baselines: Random, which selects an embedding set at random, and Max- Tkn, which adopts a "bigger is better" strategy, al- ways selecting the embedding set trained over the largest text corpus.</p><p>Finally, as discussed below our experimental re- sults reveal that the different signature methods have distinct strengths. Thus, we also evaluate All, a simple ensemble of the VocabRk, SimCorr, and MaxTkn methods. All simply takes an even aver- age of the rankings output by its three constituent signature methods. The All method selects the sin- gle embedding set with the lowest average rank- ing, breaking ties in favor of the VocabRk method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Our results are shown in Tables 1 and 2. For each classification task and each method, "Sel." indicates the embedding set that the method se- lects (that is, the one the method ranks most sim- ilar to the text classification data set). We report two measures of the quality of a signature method: ρ, the Pearson correlation between the similarity scores assigned by the method and the set's accu- racy on the classification task; and "Acc.," the ac- curacy of the embeddings selected by the method.</p><p>The results show that for the small-corpus em- beddings, the VocabRk and SimCorr signature methods perform well, beating the random base- line overall. By contrast, for the large-corpus em- beddings, the MaxTkn method performs the best of the individual methods (primarily due to its strong performance on the idiosyncratic 20news data set). The All ensemble method achieves performance nearly as high as the best possible embedding se- lection (represented as the Oracle method in the tables).</p><p>An alternative to using pre-trained embeddings is to train embeddings on the evaluation corpus it- self. We found that this approach achieved an av- erage accuracy of 0.746 across our four data sets, lower than our results using the All method. Utiliz- ing the pre-trained embeddings, especially those computed over large corpora, provides a signifi- cant boost in text classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficiency Experiment</head><p>We also evaluated the relative gain in time and space efficiency that VecShare provides over the current practice of manually evaluating each em- bedding set and selecting the embedding that per- forms best. The efficiency experiment was per- formed on a single machine with a 2.3 GHz quad- core CPU and 8GB of main memory, using a test framework containing 11 embedding sets.</p><p>Embedding selection was performed using both the VocabRk signature method on VecShare and the conventional method of selecting embeddings, which trains models for each embedding set and then evaluates those models on the test corpus. The conventional approach required an average of 177 minutes to train, evaluate, and select an em- bedding set for each test corpus. Whereas, the Vo- cabRk signature method on the VecShare frame- work required an average of 38 seconds to se- lect an embedding for each test corpus, an aver- age speedup of 280x. Additionally, VecShare sub- stantially reduces space cost: the total size of the signatures in the experiments is 4-5 orders of mag- nitude smaller than the full embedding sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We presented VecShare, a framework for sharing word vector representations. The VecShare frame- work uses signatures to help researchers and prac- titioners quickly identify helpful embeddings for their task. We released a Python library that al- lows practitioners to access the framework. We also performed experiments quantifying the accu- racy and efficiency of VecShare's embedding se- lection approach on text classification. Further ex- periments on additional data sets and NLP tasks are necessary.</p><p>In future work, we wish to explore embedding signatures that leverage richer knowledge of the practitioner's corpus and task. Finally, we hope to extend VecShare's embedding selection meth- ods to consider syntheses of multiple distinct em- bedding sets, tailored to the practitioner's task and corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The VecShare Framework.</figDesc></figure>

			<note place="foot" n="3"> https://github.com/mmihaltz/ word2vec-GoogleNews-vectors. 4 http://qwone.com/ ˜ jason/20Newsgroups/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by NSF grant IIS-1351029 and the Allen Institute for Artificial Intelligence. We thank Mohammed Alam, Dave Demeter, Thanapon Noraset, and Zheng Yuan for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Community evaluation and exchange of word vectors at wordvectors.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reuters-21578 text categorization test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<ptr target="http://www.re-search.att.com/˜lewis/reuters21578.html" />
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Modelling the combination of generic and target domain embeddings in a convolutional neural network for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The new york times annotated corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26752</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
