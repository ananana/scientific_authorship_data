<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discourse Parsing with Attention-based Hierarchical Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
							<email>qi.li@pku.edu.cn lts 417@hotmail.com chbb@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University No</orgName>
								<address>
									<addrLine>5 Yiheyuan Road</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="institution">Haidian District</orgName>
								<address>
									<postCode>100871, 221009</postCode>
									<settlement>Beijing, Xuzhou</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discourse Parsing with Attention-based Hierarchical Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="362" to="371"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>RST-style document-level discourse parsing remains a difficult task and efficient deep learning models on this task have rarely been presented. In this paper, we propose an attention-based hierarchical neural network model for discourse parsing. We also incorporate tensor-based transformation function to model complicated feature interactions. Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A document is formed by a series of coherent text units. Document-level discourse parsing is a task to identify the relations between the text units and to determine the structure of the whole document the text units form. Rhetorical Structure Theory (RST) ( <ref type="bibr" target="#b17">Mann and Thompson, 1988</ref>) is one of the most in- fluential discourse theories. According to RST, the discourse structure of a document can be represented by a Discourse Tree (DT). Each leaf of a DT denotes a text unit referred to as an Elementary Discourse Unit (EDU) and an inner node of a DT represents a text span which is constituted by several adjacent EDUs. DTs can be utilized by many NLP tasks in- cluding automatic document summarization ( <ref type="bibr" target="#b16">Louis et al., 2010;</ref><ref type="bibr" target="#b19">Marcu, 2000</ref>), question-answering <ref type="bibr" target="#b27">(Verberne et al., 2007)</ref> and sentiment analysis <ref type="bibr" target="#b25">(Somasundaran, 2010</ref>) etc.</p><p>Much work has been devoted to the task of RST- style discourse parsing and most state-of-the-art ap- proaches heavily rely on manual feature engineer- ing ( <ref type="bibr" target="#b10">Joty et al., 2013;</ref><ref type="bibr" target="#b3">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b9">Ji and Eisenstein, 2014)</ref>. While neural network mod- els have been increasingly focused on for their abil- ity to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style dis- course parsing except the work of <ref type="bibr" target="#b14">Li et al. (2014a)</ref>. <ref type="bibr" target="#b14">Li et al. (2014a)</ref> propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function they use is weak at modeling complicated interactions which has been stated by <ref type="bibr" target="#b24">Socher et al. (2013)</ref>. As many docu- ments contain more than a hundred EDUs which form quite a long sequence, those weaknesses may lead to inferior results on this task.</p><p>In this paper, we propose to use a hierarchical bidirectional Long Short-Term Memory (bi-LSTM) network to learn representations of text spans. Com- paring with vanilla recursive/recurrent neural net- works, LSTM-based networks can store information for a long period of time and don't suffer from gra- dient vanishing problem. We apply a hierarchical bi-LSTM network because the way words form an EDU and EDUs form a text span is different and thus they should be modeled separately and hierar- chically. On top of that, we apply attention mecha- nism to attend over all EDUs to pick up prominent semantic information of a text span. Besides, we use tensor-based transformation function to model com- plicated feature interactions and thus it can produce combinatorial features.</p><p>We summarize contributions of our work as fol- lows:</p><p>• We propose to use a hierarchical bidirectional LSTM network to learn the compositional se- mantic representations of text spans, which nat- urally matches and models the intrinsic hierar- chical structure of text spans.</p><p>• We extend our hierarchical bi-LSTM network with attention mechanism to allow the network to focus on the parts of input containing promi- nent semantic information for the composi- tional representations of text spans and thus al- leviate the problem caused by the limited mem- ory of LSTM for long text spans.</p><p>• We adopt a tensor-based transformation func- tion to allow explicit feature interactions and apply tensor factorization to reduce the param- eters and computations.</p><p>• We use two level caches to intensively acceler- ate our probabilistic CKY-like parsing process.</p><p>The rest of this paper is organized as follows: Sec- tion 2 gives the details of our parsing model. Section 3 describes our parsing algorithm. Section 4 gives our training criterion. Section 5 reports the experi- mental results of our approach. Section 6 introduces the related work. Conclusions are given in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parsing Model</head><p>Given two successive text spans, our parsing model evaluates the probability to combine them into a larger span, identifies which one is the nucleus and determines what is the relation between them. As with the work of Ji and Eisenstein (2014), we set three classifiers which share the same features as in- put to deal with those problems. The whole pars- ing model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Three classi- fiers are on the top. The semantic representations of the two given text spans which come from the output of attention-based hierarchical bi-LSTM net- work with tensor-based transformation function is the main part of input to the classifiers. Additionally, following the previous practice of <ref type="bibr" target="#b14">Li et al. (2014a)</ref>, a small set of handcrafted features is introduced to enhance the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical Bi-LSTM Network for Text Span Representations</head><p>Long Short-Term Memory (LSTM) networks have been successfully applied to a wide range of NLP tasks for the ability to handle long-term dependen- cies and to mitigate the curse of gradient vanishing <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b5">Hermann et al., 2015)</ref>. A basic LSTM can be described as follows. A sequence {x 1 , x 2 , ..., x n } is given as input. At each time-step, the LSTM computation unit takes in one token x t as input and it keeps some information in a cell state C t and gives an output h t . They are calculated in this way:</p><formula xml:id="formula_0">i t = σ(W i [h t−1 ; x t ] + b i )<label>(1)</label></formula><formula xml:id="formula_1">f t = σ(W f [h t−1 ; x t ] + b f ) (2) ˜ C t = tanh(W C [h t−1 ; x t ] + b C )<label>(3)</label></formula><formula xml:id="formula_2">C t = f t C t−1 + i t ˜ C t (4) o t = σ(W o [h t−1 ; x t ] + b o )<label>(5)</label></formula><formula xml:id="formula_3">h t = o t tanh(C t )<label>(6)</label></formula><p>where</p><formula xml:id="formula_4">W i , b i , W f , b f , W c , b C , W o , b o are LSTM pa-</formula><p>rameters, denotes element-wise product and σ de- notes sigmoid function. The output at the last token, i.e., h n is taken as the representation of the whole sequence. Since an EDU is a sequence of words, we de- rive the representation of an EDU from the sequence constituted by concatenation of word embeddings and the POS tag embeddings of the words as <ref type="figure" target="#fig_1">Figure  2</ref> shows. Previous work on discourse parsing tends to extract some features from the beginning and end of text units partly because discourse clues such as discourse markers(e.g., because, though) are often situated at the beginning or end of text units <ref type="bibr" target="#b3">(Feng and Hirst, 2014;</ref><ref type="bibr" target="#b9">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b14">Li et al., 2014a;</ref><ref type="bibr" target="#b15">Li et al., 2014b;</ref><ref type="bibr" target="#b4">Heilman and Sagae, 2015)</ref>. Considering the last few tokens of a sequence nor- mally have more influence on the representation of the whole sequence learnt with LSTM because they get through less times of forget gate from the LSTM computation unit, to effectively capture the informa- tion from both beginning and end of an EDU, we use bidirectional LSTM to learn the representation of an EDU. In other words, one LSTM takes the word sequence in forward order as input, the other takes the word sequence in reversed order as input. The representation of a sequence is the concatena- tion of the two vector representations calculated by the two LSTMs.</p><p>Since a text span is a sequence of EDUs, its meaning can be computed from the meanings of the EDUs. So we use another bi-LSTM to derive the compositional semantic representation of a text span from the EDUs it contains. The two bi-LSTM networks form a hierarchical structure as <ref type="figure" target="#fig_0">Figure 1</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention</head><p>The representation of a sequence computed by bi- LSTMs is always a vector with fixed dimension de- spite the length of the sequence. Thus when dealing with a text span with hundreds of EDUs, bi-LSTM may not be enough to capture the whole semantic in- formation with its limited output vector dimension. Attention mechanism can attend over the output at every EDU with global context and pick up promi- nent semantic information and drop the subordinate information for the compositional representation of the span, so we employ attention mechanism to al- leviate the problem caused by the limited memory of LSTM networks. The attention mechanism is in- spired by the work of <ref type="bibr">Rocktäschel et al. (2015)</ref>. Our attention-based bi-LSTM network is shown in <ref type="figure" target="#fig_2">Fig- ure 3</ref>.</p><p>We combine the last outputs of the span level bi-</p><formula xml:id="formula_5">LSTM to be h s = [ − → h en , ← − h e 1 ].</formula><p>We also combine the outputs of the two LSTM at every EDU of the span:</p><formula xml:id="formula_6">h t = [ − → h t , ← − h t ] and thus get a matrix H = [h 1 ; h 2 ; ...; h n ] T .</formula><p>Taking H ∈ R d×n and h s ∈ R d as inputs, we get a vector α ∈ R n standing for weights of EDUs to the text span and use it to get a weighted representation of the span r ∈ R d :</p><formula xml:id="formula_7">M = tanh(W y H + W l h s ⊗ e n ) (7) α = sof tmax(w T α M ) (8) r = Hα (9)</formula><p>where ⊗ denotes Cartesian product , M ∈ R k×n , e n is a n dimensional vector of all 1s and we use the Cartesian product W l h s ⊗ e n to repeat the result of W l h s n times in column to form a matrix and</p><formula xml:id="formula_8">W y ∈ R k×d , W l , ∈ R k×d , w α ∈ R k are parameters.</formula><p>We synthesize the information of r and h s to get the final representation of the span:</p><formula xml:id="formula_9">w h = σ(W hr r + W hh h s ) (10) h = w h h s + (1 − w h ) r (11)</formula><p>where W hr , W hh ∈ R d×d are parameters, w h ∈ R d is a computed vector representing the element-wise weight of h s and the element-wise weighted sum- mation h ∈ R d is the final representation of the text span computed by the attention-based bidirectional LSTM network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classifiers</head><p>We concatenate the representations of the two given spans: h = [h s1 , h s2 ] and feed h into a full connec- tion hidden layer to obtain a higher level representa- tion v which is the input to the three classifiers:</p><formula xml:id="formula_10">v = Relu(W h [h s1 , h s2 ] + b h )<label>(12)</label></formula><p>For each classifier, we firstly transform v ∈ R l into a hidden layer:</p><formula xml:id="formula_11">v sp = Relu(W hs v + b hs )<label>(13)</label></formula><formula xml:id="formula_12">v nu = Relu(W hn v + b hn )<label>(14)</label></formula><formula xml:id="formula_13">v rel = Relu(W hr v + b hr )<label>(15)</label></formula><p>where W hs , W hn , W hr ∈ R h×l are transformation matrices and b hs , b hn , b hr ∈ R h are bias vectors. Then we feed these vectors into the respective output layer:</p><formula xml:id="formula_14">y sp = σ(w s v sp + b s )<label>(16)</label></formula><formula xml:id="formula_15">y nu = sof tmax(W n v nu + b n )<label>(17)</label></formula><formula xml:id="formula_16">y rel = sof tmax(W r v rel + b r )<label>(18)</label></formula><p>where w s ∈ R h , b s ∈ R, W n ∈ R 3×h , W n ∈ R 3×h , b n ∈ R 3 , W r ∈ R nr×h , b n ∈ R nr are pa- rameters and n r is the number of different discourse relations.</p><p>The first classifier is a binary classifier which out- puts the probability the two spans should be com- bined. The second classifier is a multiclass classifier which identifies the nucleus to be span 1, span 2 or both. The third classifier is also a multiclass classi- fier which determines the relation between the two spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Tensor-based Transformation</head><p>Tensor-based transformation function has been suc- cessfully utilized in many tasks to allow complicated interaction between features ( <ref type="bibr" target="#b26">Sutskever et al., 2009;</ref><ref type="bibr" target="#b24">Socher et al., 2013;</ref><ref type="bibr" target="#b20">Pei et al., 2014</ref>). Based on the intuition that allowing complicated interaction between the features of the two spans may help to identify how they are related, we adopt tensor-based transformation function to strengthen our model.</p><p>A tensor-based transformation function on x ∈ R d 1 is as follows:</p><formula xml:id="formula_17">y = W x + x T T [1:d 2 ] x + b<label>(19)</label></formula><formula xml:id="formula_18">y i = j W ij x j + j,k T [i] j,k x j x k + b i (20)</formula><p>where</p><formula xml:id="formula_19">y ∈ R d 2 is the output vector, y i ∈ R is the ith element of y, W ∈ R d 2 ×d 1 is the transformation matrix, T [1:d 2 ] ∈ R d 1 ×d 1 ×d 2</formula><p>is a 3rd-order transfor- mation tensor. A normal transformation function in neural network models only has the first term W x with the bias term. It means for normal transfor- mation function each unit of the output vector is the weighted summation of the input vector and this only allows additive interaction between the units of the input vector. With the tensor multiplication term, each unit of the output vector is augmented with the weighted summation of the multiplication of the in- put vector units and thus we incorporate multiplica- tive interaction between the units of the input vector. Inevitably, the incorporation of tensor leads to side effects which include the increase in parameter number and computational complexity. To remedy this, we adopt tensor factorization in the same way as <ref type="bibr" target="#b20">Pei et al. (2014)</ref>: we use two low rank matrices to approximate each tensor slice</p><formula xml:id="formula_20">T [i] ∈ R d 1 ×d 1 :</formula><p>We apply the factorized tensor-based transforma- tion function to the combined text span representa- tion h = [h s1 , h s2 ] to make the features of the two spans explicitly interact with each other:</p><formula xml:id="formula_21">v = Relu(W h [h s1 , h s2 ] + [h s1 , h s2 ] T P [1:d] h Q [1:d] h [h s1 , h s2 ] + b h ) (22)</formula><p>Comparing with Eq. 12, the transformation function is added with a tensor term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Handcrafted Features</head><p>Most previously proposed state-of-the-art systems heavily rely on handcrafted features <ref type="bibr" target="#b6">(Hernault et al., 2010;</ref><ref type="bibr" target="#b3">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b10">Joty et al., 2013;</ref><ref type="bibr" target="#b9">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b4">Heilman and Sagae, 2015)</ref>. <ref type="bibr" target="#b14">Li et al. (2014a)</ref> show that some basic features are still necessary to get a satisfactory result for their recur- sive deep model. Following their practice, we adopt minimal basic features which are utilized by most systems to further strengthen our model. We list these features in <ref type="table">Table 1</ref>. We apply the factorized tensor-based transformation function to Word/POS features to allow more complicated interaction be- tween them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parsing Algorithm</head><p>In this section, we describe our parsing algorithm which utilizes the parsing model to produce the global optimal DT for a segmented document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic CKY-like Algorithm</head><p>We adopt a probabilistic CKY-like bottom-up algo- rithm which is also adopted in ( <ref type="bibr" target="#b10">Joty et al., 2013;</ref><ref type="bibr" target="#b14">Li et al., 2014a</ref>) to produce a DT for a document. This parsing algorithm is a dynamic programming algorithm and produces the global optimal DT with our parsing model. Given a text span which is constituted by [e i , e i+1 , ..., e j ] and the possible sub- trees of [e i , e i+1 , ..., e k ] and [e k+1 , e k+2 , ..., e j ] for all k ∈ {i, i+1, ..., j−1} with their probabilities, we choose k and combine the corresponding subtrees to form a combined DT with the following recurrence formula:</p><formula xml:id="formula_22">k = arg max k {P sp (i, k, j)P i,k P k+1,j }<label>(23)</label></formula><p>where P i,k and P k+1,j are the probabilities of the most probable subtrees of [e i , e i+1 , ..., e k ] and [e k+1 , e k+2 , ..., e j ] respectively, P sp (i, k, j) is the probability which is predicted by our parsing model to combine those two subtrees to form a DT. The probability of the most probable DT of [e i , e i+1 , ..., e j ] is:</p><formula xml:id="formula_23">P i,j = max k {P sp (i, k, j)P i,k P k+1,j }<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parsing Acceleration</head><p>Computational complexity of the original proba- bilistic CKY-like algorithm is O(n 3 ) where n is the number of EDUs of the document. But in this work, given each pair of text spans, we compute the rep- resentations of them with hierarchical bi-LSTM net- work at the expense of an additional O(n) computa- tions. So the computational complexity of our parser becomes O(n 4 ) and it is unacceptable for long docu- ments. However, most computations are duplicated, so we use two level caches to drastically accelerate parsing. Firstly, we cache the outputs of the EDU level bi-LSTM which are the semantic representations of EDUs. As for the forward span level LSTM, after we get the semantic representation of a span, we cache it too and use it to compute the representation of an extended span. For example, after we get the representation of span constituted by [e 1 , e 2 , e 3 ], we take it with semantic representation of e 4 to com- pute the representation of the span constituted by [e 1 , e 2 , e 3 , e 4 ] in one LSTM computation step. For the backward span level LSTM, we do it the same way just in reversed order. Thus we decrease the computational complexity of computing the seman- tic representations for all possible span pairs which is the most time-consuming part of the original pars- ing process from O(n 4 ) to O(n 2 ).</p><p>Secondly, it can be seen that before we apply Relu to the tensor-based transformation function, many calculations from the two spans which include a large part of tensor multiplication are independent. The multiplication between the elements of the rep- resentations of the two spans caused by the tensors and the element-wise non-linear activation function Relu terminate the independence between them. So we can further cache the independent calculation re- sults before Relu operation for each span. Thus we decrease the computational complexity of a large part of tensor-based transformation from O(n 3 ) to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word/POS Features</head><p>One-hot representation of the first two words and of the last word of each span. One-hot representation of POS tags of the first two words and of the last word of each span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shallow Features Number of EDUs of each span. Number of words of each span.</head><p>Predicted relations of the two subtrees' roots. Whether each span is included in one sentence. Whether both spans are included in one sentence. <ref type="table">Table 1</ref>: Handcrafted features used in our parsing model. O(n 2 ) which is the second time-consuming part of the original parsing process.</p><p>The remaining O(n 3 ) computations include a lit- tle part of tensor-based transformation computa- tions, Relu operation and the computations from the three classifiers. These computations take up only a little part of the original parsing model computations and thus we greatly accelerate our parsing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Max-Margin Training</head><p>We use Max-Margin criterion for our model train- ing. We try to learn a function that maps: X → Y , where X is the set of documents and Y is the set of possible DTs. We define the loss function for pre- dicting a DTˆyDTˆ DTˆy i given the correct DT y i as:</p><formula xml:id="formula_24">(y i , ˆ y i ) = r∈ˆyr∈ˆ r∈ˆy i κ1{r ∈ y i } (25)</formula><p>where r is a span specified with nucleus and relation in the predicted DT, κ is a hyperparameter referred to as discount parameter and 1 is indicator function. We expect the probability of the correct DT to be a larger up to a margin to other possible DTs:</p><formula xml:id="formula_25">P rob(x, y i ) ≥ P rob(x i , ˆ y i ) + (y i , ˆ y i )<label>(26)</label></formula><p>The objective function for m training examples is as follows:</p><formula xml:id="formula_26">J(θ) = 1 m m i=1 l i (θ), where<label>(27)</label></formula><formula xml:id="formula_27">l i (θ) = maxˆy maxˆ maxˆy i (P rob(x i , ˆ y i ) + (y i , ˆ y i )) −P rob(x i , y i )<label>(28)</label></formula><p>where θ denotes all the parameters including our neural network parameters and all embeddings. The probabilities of the correct DTs increase and the probabilities of the most probable incorrect DTs decrease during training. We adopt Adadelta <ref type="bibr" target="#b28">(Zeiler, 2012</ref>) with mini-batch to minimize the objective function and set the initial learning rate to be 0.012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model on RST Discourse Treebank 1 (RST-DT) <ref type="bibr" target="#b2">(Carlson et al., 2003)</ref>. It is partitioned into a set of 347 documents for training and a set of 38 documents for test. Non-binary relations are converted into a cascade of right-branching binary relations. The standard metrics of RST-style dis- course parsing evaluation include blank tree struc- ture referred to as span (S), tree structure with nu- clearity (N) indication and tree structure with rhetor- ical relation (R) indication. Following other RST- style discourse parsing systems, we evaluate the re- lation metric in 18 coarse-grained relation classes. Since our work focus does not include EDU segmen- tation, we evaluate our system with gold-standard EDU segmentation and we apply the same setting on this to other discourse parsing systems for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>The dimension of word embeddings is set to be 50 and the dimension of POS embeddings is set to be 10. We pre-trained the word embeddings with GloVe ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>) on English Giga- word 2 and we fine-tune them during training. Con- sidering some words are pretrained by GloVe but don't appear in the RST-DT training set, we want to use their embeddings if they appear in test set. Fol- lowing <ref type="bibr">Kiros et al. (2015)</ref>, we expand our vocabu- lary with those words using a matrix W ∈ R 50×50 that maps word embeddings from the pre-trained word embedding space to the fine-tuned word em- bedding space. The objective function for training the matrix W is as follows:</p><formula xml:id="formula_28">min W,b ||V tuned − V pretrained W − b|| 2 2 (29)</formula><p>where V tuned , V pretrained ∈ R |V |×50 contain fine- tuned and pre-trained embeddings of words appear- ing in training set respectively, |V | is the size of RST-DT training set vocabulary and b is the bias term also to be trained.</p><p>We lemmatize all the words appeared and rep- resent all numbers with a special token. We use Stanford CoreNLP toolkit ( ) to preprocess the text including lemmatization, POS tagging etc. We use Theano library ( <ref type="bibr" target="#b1">Bergstra et al., 2010)</ref> to implement our parsing model. We randomly initialize all parameters within (-0.012, 0.012) except word embeddings. We adopt dropout strategy ( <ref type="bibr" target="#b7">Hinton et al., 2012</ref>) to avoid overfitting and we set the dropout rate to be 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analysis</head><p>To show the effectiveness of the components in- corporated into our model, we firstly test the per- formance of the basic hierarchical bidirectional LSTM network without attention mechanism (ATT), tensor-based transformation (TE) and handcrafted features (HF). Then we add them successively. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The performance is improved by adding each component to our basic model and that shows the ef- fectiveness of attention mechanism and tensor-based transformation function. Even without handcrafted features, the performance is still competitive. It indicates that the semantic representations of text spans produced by our attention-based hierarchical bi-LSTM network are effective and the handcrafted features are complementary to semantic representa- tions produced by the network.</p><p>We also experiment without mapping the OOV word embeddings and use the same embedding for all OOV words. The result is shown in   3. Without mapping the OOV word embeddings the performance decreases slightly, which demon- strates that the relation between pre-trained embed- ding space and the fine-tuned embedding space can be learnt and it is beneficial to train a matrix to trans- form OOV word embeddings from the pre-trained embedding space to the fine-tuned embedding space. We compare our system with other state-of-the-art systems including ( <ref type="bibr" target="#b10">Joty et al., 2013;</ref><ref type="bibr" target="#b9">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b3">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b14">Li et al., 2014a;</ref><ref type="bibr" target="#b15">Li et al., 2014b;</ref><ref type="bibr" target="#b4">Heilman and Sagae, 2015)</ref>. Systems proposed by <ref type="bibr" target="#b10">Joty et al. (2013)</ref>, <ref type="bibr" target="#b4">Heilman (2015)</ref> and <ref type="bibr" target="#b3">Feng and Hirst (2014)</ref> are all based on variants of CRFs. <ref type="bibr" target="#b9">Ji and Eisenstein (2014)</ref> use a projection ma- trix acting on one-hot representations of features to learn representations of text spans and build Support Vector Machine (SVM) classifier on them. <ref type="bibr" target="#b15">Li et al. (2014b)</ref> adopt dependency parsing methods to deal with this task. These systems are all based on hand- crafted features. <ref type="bibr" target="#b14">Li et al. (2014a)</ref> adopt a recursive deep model and use some basic handcrafted features to improve their performances which has been stated before. <ref type="table" target="#tab_3">Table 4</ref> shows the performance for our system and those systems. Our system achieves the best result in span and relatively lower performance in nucleus and relation identification comparing with the corresponding best results but still better than System S N R Joty et al. <ref type="formula" target="#formula_0">(2013)</ref> 82.7 68.4 55.7 <ref type="bibr" target="#b9">Ji and Eisenstein (2014)</ref> 82.1 71.1 <ref type="bibr">61.6 Feng and Hirst (2014)</ref> 85.7 71.0 58.2 <ref type="bibr" target="#b14">Li et al. (2014a)</ref> 84.0 70.8 58.6 <ref type="bibr" target="#b15">Li et al. (2014b)</ref> 83.4 73.8 57.8 Heilman and Sagae (2015) 83.5 68.1 55.1 Ours 85.8 71.1 58.9 Human 88.7 77.7 65.8   <ref type="table" target="#tab_4">Table 5</ref>. It shows our overall perfor- mance outperforms the model proposed by <ref type="bibr" target="#b14">Li et al. (2014a)</ref> which illustrates our model is effective. <ref type="table" target="#tab_6">Table 6</ref> shows an example of the weights (W) of EDUs (see Eq. 8) derived from our attention model. For span1 the main semantic meaning is expressed in EDU32 under the condition described in EDU31. Besides, it is EDU32 that explicitly manifests the contrast relation between the two spans. As can be seen, our attention model assigns less weight to  EDU30 and focuses more on EDU32 which is rea- sonable according to our analysis above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Two most prevalent discourse parsing treebanks are RST Discourse Treebank (RST-DT) <ref type="bibr" target="#b2">(Carlson et al., 2003)</ref> and Penn Discourse TreeBank (PDTB) ( <ref type="bibr" target="#b22">Prasad et al., 2008)</ref>. We evaluate our system on RST-DT which is annotated in the framework of Rhetorical Structure Theory ( <ref type="bibr" target="#b17">Mann and Thompson, 1988)</ref>. It consists of 385 Wall Street Journal arti- cles and is partitioned into a set of 347 documents for training and a set of 38 documents for test. 110 fine-grained and 18 coarse-grained relations are de- fined on RST-DT. Parsing algorithms published on RST-DT can mainly be categorized as shift-reduce parsers and probabilistic CKY-like parsers. Shift- reduce parsers are widely used for their efficiency and effectiveness and probabilistic CKY-like parsers lead to the global optimal result for the parsing models. State-of-the-art systems belonging to shift- reduce parsers include <ref type="bibr" target="#b4">(Heilman and Sagae, 2015;</ref><ref type="bibr" target="#b9">Ji and Eisenstein, 2014)</ref>. Those belonging to prob- abilistic CKY-like parsers include ( <ref type="bibr" target="#b10">Joty et al., 2013;</ref><ref type="bibr" target="#b14">Li et al., 2014a</ref>). Besides, <ref type="bibr" target="#b3">Feng and Hirst (2014)</ref> adopt a greedy bottom-up approach as their pars- ing algorithm. Lexical, syntactic, structural and se- mantic features are extracted in these systems. SVM and variants of Conditional Random Fields (CRFs) are mostly used in these models. <ref type="bibr" target="#b15">Li et al. (2014b)</ref> distinctively propose to use dependency structure to represent the relations between EDUs. Recursive deep model proposed by <ref type="bibr" target="#b14">Li et al. (2014a)</ref> has been the only proposed deep learning model on RST-DT.</p><p>Incorporating attention mechanism into RNN (e.g., LSTM, GRU) has been shown to learn bet- ter representation by attending over the output vec- tors and picking up important information from rel- evant positions of a sequence and this approach has been utilized in many tasks including neural ma- chine translation <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b5">Hermann et al., 2015</ref>), text entailment recognition <ref type="bibr">(Rocktäschel et al., 2015)</ref> etc. Some work also uses tensor-based transforma- tion function to make stronger interaction between features and learn combinatorial features and they get performance boost in their tasks <ref type="bibr" target="#b26">(Sutskever et al., 2009;</ref><ref type="bibr" target="#b24">Socher et al., 2013;</ref><ref type="bibr" target="#b20">Pei et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose an attention-based hier- archical neural network for discourse parsing. Our attention-based hierarchical bi-LSTM network pro- duces effective compositional semantic representa- tions of text spans. We adopt tensor-based trans- formation function to allow complicated interaction between features. Our two level caches accelerate parsing process significantly and thus make it prac- tical. Our proposed system achieves comparable re- sults to state-of-the-art systems. We will try extend- ing attention mechanism to obtain the representation of a text span by referring to another text span at minimal additional cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic structure of our parsing model.</figDesc><graphic url="image-1.png" coords="2,313.13,61.08,219.01,238.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bi-LSTM for computing the compositional semantic representation of an EDU.</figDesc><graphic url="image-2.png" coords="3,74.34,61.08,205.86,140.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention-based bi-LSTM for computing the compositional semantic representation of a text span.</figDesc><graphic url="image-3.png" coords="4,74.34,61.07,187.11,168.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table</head><label></label><figDesc></figDesc><table>System Setting 
S 
N 
R 
Basic 
82.7 
69.7 
55.6 
Basic+ATT 
83.6* 70.2* 56.0* 
Basic+ATT+TE 
84.2* 70.4 56.3* 
Basic+ATT+TE+HF 85.8* 71.1* 58.9* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance comparison for different settings of 

our system on RST-DT. 'Basic' denotes the basic hierarchical 

bidirectional LSTM network; '+ATT' denotes adding attention 

mechanism; '+TE' denotes adopting tensor-based transforma-

tion; '+HF' denotes adding handcrafted features. * indicates 

statistical significance in t-test compared to the result in the line 

above (p &lt; 0.05). 

System Setting 
S 
N 
R 
Without OOV mapping 85.1 
70.7 
58.2 
Full version 
85.8* 71.1* 58.9* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance comparison for whether to map OOV 

embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance comparison with other state-of-the-art 

systems on RST-DT. 

System 
S 
N 
R 
Li et al. (2014a) (no feature) 82.4 69.2 56.8 
Ours (no feature) 
84.2 70.4 56.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance comparison with the deep learning model 

proposed in Li et al. (2014a) without handcrafted features. 

most systems. No system achieves the best result 
on all three metrics. To further show the effective-
ness of the deep learning model itself without hand-
crafted features, we compare the performance be-
tween our model and the model proposed by Li et al. 
(2014a) without handcrafted features and the results 
are shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc>An example of the weights derived from our attention model. The relation between span1 and span2 is Contrast.</figDesc><table></table></figure>

			<note place="foot">T [i] ⇒ P [i] Q [i] (21) where P [i] ∈ R d 1 ×r , Q [i] ∈ R r×d 1 and r d 1. In this way, we drastically reduce parameter number and computational complexity.</note>

			<note place="foot" n="1"> https://catalog.ldc.upenn.edu/LDC2002T07 2 https://catalog.ldc.upenn.edu/LDC2011T07</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their instructive feed-back. We also </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
<note type="report_type">Oral Presentation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current and new directions in discourse and dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="85" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<idno>abs/1505.02425</idno>
		<title level="m">Fast rhetorical structure theory discourse parsing. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Tomá S Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1506.03340</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hilda: a discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>David A Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Speech and language processing, chapter 14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1506.06726</idno>
		<title level="m">Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text-level discourse dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discourse indicators for content selection in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The theory and practice of discourse parsing and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Karl Moritz Hermann, Tomá s Kocisk´yKocisk´y, and Phil Blunsom. 2015. Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno>abs/1509.06664</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Discourse-level relations for Opinion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swapna Somasundaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating discourse-based answer extraction for why-question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peterarno</forename><surname>Coppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="735" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
