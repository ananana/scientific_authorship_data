<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University No</orgName>
								<address>
									<addrLine>1, Sec. 4, Roosevelt Rd. Taipei 10617</addrLine>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruo-Ping</forename><surname>Dong</surname></persName>
							<email>dongruoping@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<addrLine>No. 101, Sec. 2, Kuang-Fu Rd. Hsinchu</addrLine>
									<postCode>30013</postCode>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Siang</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<addrLine>No. 1, Sec. 4, Roosevelt Rd. Taipei 10617</addrLine>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju-Chieh</forename><surname>Chou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<addrLine>No. 1, Sec. 4, Roosevelt Rd. Taipei 10617</addrLine>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
							<email>ma@iis.sinica.edu.tw</email>
							<affiliation key="aff4">
								<orgName type="institution">Academia</orgName>
								<address>
									<addrLine>Sinica No. 128, Sec. 2, Academia Rd. Taipei</addrLine>
									<postCode>11529</postCode>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2664" to="2669"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we utilize the linguistic structures of texts to improve named entity recognition by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that named entities are highly related to linguistic constituents , we propose a constituent-based BRNN-CNN for named entity recognition. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are linguistic constituents. Then it classifies these chunks with a constituency tree structure by recur-sively propagating syntactic and semantic information to each constituent node. This method surpasses current state-of-the-art on OntoNotes 5.0 with automatically generated parses.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) can be seen as a combined task of locating named entity chunks of texts and classifying which named entity cat- egory a chunk falls into. Traditional approaches label each token in texts as a part of a named en- tity chunk, e.g. "person begin", and achieve high performances in several benchmark datasets <ref type="bibr" target="#b11">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b8">Passos et al., 2014;</ref><ref type="bibr" target="#b0">Chiu and Nichols, 2016)</ref>.</p><p>Being formulated as a sequential labeling prob- lem, NER systems could be naturally imple- mented by recurrent neural networks. These net- works process a token at a time, taking, for each token, the hidden features of its previous token as well as its raw features to compute its own hidden features. Then they classify each token by these hidden features. With both forward and backward directions, networks learn how to prop- agate the information of a token sequence to each token. <ref type="bibr" target="#b0">Chiu and Nichols (2016)</ref> utilize a varia- tion of recurrent networks, bidirectional LSTM, attached with a CNN, which learns character-level features instead of handcrafting. They accom- plish state-of-the-art results on both <ref type="bibr">CoNLL-2003 (Tjong Kim Sang and</ref><ref type="bibr">De Meulder, 2003)</ref> and OntoNotes 5.0 ( <ref type="bibr" target="#b5">Hovy et al., 2006;</ref><ref type="bibr" target="#b10">Pradhan et al., 2013</ref>) datasets.</p><p>Classical sequential labeling approaches take little information about phrase structures of sen- tences. However, according to our analysis, most named entity chunks are actually linguistic con- stituents, e.g. noun phrases. This motivates us to focus on a constituent-based approach for NER where the NER problem is transformed into a named entity classification task on every node of a Algorithm 1 Binarization 1: function BINARIZE(node) <ref type="bibr">2:</ref> n ← node.children.length</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>if n &gt; 2 then <ref type="bibr">4:</ref> if HEAD-FINDER(node) = node.children[n] then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>newChild ← GROUP(node.children <ref type="bibr">[</ref> node.children ← [node.children <ref type="bibr">[1]</ref>, newChild]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>newChild.pos ← node.pos</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>for child in node.children do 12:</p><p>BINARIZE(child) <ref type="figure">Figure 1</ref>: Applying Algorithm 1 to the parse of senator Edward Kennedy.</p><p>constituency structure.</p><p>To classify constituents and take into account their structures, we propose BRNN-CNN, a spe- cial bidirectional recursive neural network at- tached with a convolutional network. For each sentence, a constituency parse where every node represents a meaningful chunk of the sentence, i.e. a constituent, is first generated. Then BRNN-CNN recursively computes hidden state features of ev- ery node and classifies each node by these hidden features. To capture structural linguistic informa- tion, bidirectional passes are applied so that each constituent sees what it is composed of as well as what is containing it, both in a near-to-far fashion.</p><p>Our main contribution is the introduction of a novel constituent-based BRNN-CNN for named entity recognition, which successfully utilizes the linguistic structures of texts by recursive neural networks. We show that it achieves better scores than current state-of-the-art on OntoNotes 5.0, where good parses can be automatically gener- ated. Additionally, we analyze the effects of only considering constituents and the effects of con- stituency parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Collobert et al. (2011) achieved near state-of- the-art performance on CoNLL-2003 NER with an end-to-end neural network which had minimal feature engineering and external data. <ref type="bibr" target="#b0">Chiu and Nichols (2016)</ref> achieved the current state-of-the- art on both CoNLL-2003 and OntoNotes 5.0 NER with a sequential bidirectional LSTM-CNN. They also did extensive studies of additional features such as character type, capitalization, and Senna and DBpedia lexicons.</p><p>Finkel and Manning (2009) explored training a parser for an NER-suffixed grammar, jointly tack- ling parsing and NER. They achieved competitive results on OntoNotes with a CRF-CFG parser.</p><p>Recursive neural networks have been success- fully applied for parsing and sentiment analysis on Stanford sentiment treebank ( <ref type="bibr">Socher et al., 2010</ref><ref type="bibr">Socher et al., , 2013a</ref><ref type="bibr">Tai et al., 2015)</ref>. Their recur- sive networks, such as RNTN and Tree-LSTM, do sentiment combinations on phrase structures in a bottom-up fashion, showing the potential of such models in computing semantic compositions. For each input sentence, our constituent-based BRNN-CNN first extracts features from its con- stituency parse, then recursively classifies each constituent, and finally resolves conflicting predic- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preparing Constituency Structures</head><p>For a sentence and its associated constituency parse, our system first sets three features for each node: a POS, a word, and a head. While con- stituency tags and words should come readily, se- mantic head words are determined by a rule-based head finder <ref type="bibr" target="#b1">(Collins, 1999</ref>). Additionally, a fourth feature vector is added to each node to utilize lex- icon knowledge. The 3-bit vector records if the constituent of a node matches some phrases in each of the three SENNA (Collobert et al., 2011) lexicons of persons, organizations, and locations.</p><p>The system then tries to generate more plausible constituents while preserving linguistic structures by applying a binarization process which groups excessive child nodes around the head children. The heuristic is that a head constituent is usually modified by its siblings in a near to far fashion. Algorithm 1 shows the recursive procedure called for the root node of a parse. <ref type="figure">Figure 1</ref> shows the application of the algorithm to the parse of senator Edward Kennedy. With the heuristic that Edward modifies the head node Kennedy before senator. The binarization process successfully adds a new node Edward Kennedy that corresponds to a per- son name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computing Word Embeddings</head><p>For each word, our network retrieves one embed- ding from a trainable lookup table initialized by GloVe ( <ref type="bibr" target="#b9">Pennington et al., 2014</ref>). However, to cap- ture the morphology information of a word and help dealing with unseen words, the network com- putes another character-level embedding. Inspired by <ref type="bibr" target="#b6">Kim et al. (2016)</ref>, the network passes one- hot character vectors through a series of convolu- tional and highway layers to generate the embed- ding. These two embeddings are concatenated as the final embedding of a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computing Hidden Features</head><p>Given a constituency parse tree, where every node represents a constituent, our network recursively computes two hidden state features for every node.</p><p>First, for each node i with left sibling l and right sibling r, the raw feature vector I i is formed by concatenating the one-hot POS vectors of i, l, r, the head embeddings of i, l, r, the word embed- ding of i, the lexicon vector of i, and the mean of word embeddings in the sentence. Then, with the the set of child nodes C and the parent node p, the hidden feature vectors H bot,i and H top,i are com- puted by 2 hidden layers:</p><formula xml:id="formula_0">H bot,i = ReLU ((I i c∈C H bot,c )W bot + b bot ) (1) H top,i = ReLU ((I i H top,p )W top + b top ) (2)</formula><p>where W s are weight matrices, bs are bias vectors, and ReLU (x) = max(0, x). In cases when some needed neighboring nodes do not exist, or when i is a nonterminal and does not have a word, zero  vectors are used as the missing parts of raw or hid- den features. <ref type="figure" target="#fig_0">Figure 2</ref> shows the applications of the equations to the binarized tree in <ref type="figure">Figure 1</ref>. The computa- tions are done recursively in two directions. The bottom-up direction computes the semantic com- position of the subtree of each node, and the top- down counterpart propagates to that node the lin- guistic structures which contain the subtree. To- gether, hidden features of a constituent capture its structural linguistic information.</p><p>In addition, each hidden layer can be extended to a deep hidden network. For example, a 2-layer top-down hidden network is given by</p><formula xml:id="formula_1">H tα,i = ReLU ((I i H tα,p )W tα + b tα ) H tβ,i = ReLU ((H tα,i H tβ,p )W tβ + b tβ )</formula><p>where tα represents the first top-down hidden layer and tβ represents the second. Our best model is tuned to have 3 layers for both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Forming Consistent Predictions</head><p>Given hidden features for every node, our network computes a probability distribution of named en- tity classes plus a special non-entity class by an output layer. For each node i with left sibling l and right sibling r, the probability distribution O i is computed by an output layer:</p><formula xml:id="formula_2">O i = σ((H i H l H r )W out + b out )<label>(3)</label></formula><p>where H x = H bot,x + H top,x , x ∈ {i, l, r}, and σ(x) = (1 + e −x ) −1 . If a sibling does not exist, zero vectors are used as its hidden states. Should deep hidden layers be deployed, the last hidden layer is used. Finally, the system makes predictions for a sen- tence by collecting the constituents whose most probable classes are named entity classes. How- ever, nodes whose ancestors are already predicted as named entities are ignored to prevent predicting overlapping named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate our system on OntoNotes 5.0 NER ( <ref type="bibr" target="#b5">Hovy et al., 2006;</ref><ref type="bibr" target="#b10">Pradhan et al., 2013</ref>) and ana- lyze it with several ablation studies. The project sources are publicly available on https:// github.com/jacobvsdanniel/tf_rnn.  <ref type="table">Table 3</ref>: Dataset statistics for OntoNotes 5.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NE Constituent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Tuning</head><p>To train the model, we minimize the cross entropy loss of the softmax class probabilities in Equation 3 by the Adam optimizer ( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>).</p><p>Other details such as hyperparameters are docu- mented in the supplemental materials as well as the public repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OntoNotes 5.0 NER</head><p>OntoNotes 5.0 annotates 18 types of named enti- ties for diverse sources of texts. Like other pre- vious work <ref type="bibr" target="#b3">(Durrett and Klein, 2014;</ref><ref type="bibr" target="#b0">Chiu and Nichols, 2016)</ref>, we use the format and the train- validate-test split provided by CoNLL-2012. In addition, both gold and auto parses are available. <ref type="table">Table 3</ref> shows the dataset statistics. The last column shows the percentages of named entities that correspond to constituents of auto parses be- fore and after binarization. <ref type="table" target="#tab_2">Table 1 and Table 2</ref> compare our results with others on the whole dataset and different sources respectively. The sample mean, standard devia- tion, and sample count of BRNN-auto and Chiu and Nichols' model are 87.10, 0.14, 3 and 86.41, 0.22, 10 respectively. By one-tailed Welch's T- test, the former significantly surpasses the latter with 99% confidence level (0.000489 p-value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of the Approach</head><p>The training and validation sets contain 1,236,227 tokens and 92,894 named entities, of which 90,371 correspond to some constituents of binarized auto parses. This backs our motivation that more than 97% named entities are linguistic constituents, and 52,729 of them are noun phrases.</p><p>Essentially, the constituent-based approach fil- ters out the other 3% named entities that cross constituent boundaries <ref type="figure" target="#fig_1">(Figure 3)</ref>, i.e. 3% loss of recall. We dig into this problem by analyzing a sequential labeling recurrent network (the sixth model in <ref type="table">Table 1</ref>). The simple model performs reasonably well, but its non-constituent predic- tions are mostly false positive. In fact, it slightly improves if all non-constituent predictions are re- moved in post-processing, i.e., the precision gain of focusing on constituents is more significant than the recall loss. This is one advantage of our system over other sequential models, which try to learn and predict non-constituent named entities but do not perform <ref type="bibr">well.</ref> In addition, to analyze the effects of con- stituency structures, we test our models with dif- ferent qualities of parses (gold vs. auto in <ref type="table">Table 1</ref>). The significant F1 differences suggest that struc- tural linguistic information is crucial and can be learned by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have demonstrated a novel constituent- based BRNN-CNN for named entity recognition which successfully utilizes constituency struc- tures and surpasses the current state-of-the-art on OntoNotes 5.0 NER. Instead of propagating infor- mation by word orders as normal recurrent net- works, the model is able to recursively propa- gate structural linguistic information to every con- stituent. Experiments show that when a good parser is available, the approach will be a good alternative to traditional sequential labeling token- based NER.</p><p>Named entities that cross constituent bound- aries are analyzed and we find out that a na¨ıvena¨ıve sequential labeling model has difficulty predict- ing them without too many false positives. While avoiding them is one of the strengths of our model, generating more consistent parses to reduce this kind of named entities would be one possible di- rection for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The bottom-up and top-down hidden layers applied to the binarized tree in Figure 1.</figDesc><graphic url="image-2.png" coords="3,72.00,62.81,453.54,165.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two sample named entities that cross different branches of syntax parses.</figDesc><graphic url="image-3.png" coords="5,307.28,62.81,218.26,108.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>F1 scores on different data sources. From left to right: broadcast conversation, broadcast news, magazine, newswire, telephone conversation, and blogs &amp; newsgroups.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the 2016 Summer In-ternship Program of IIS, Academia Sinica.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the Thirteenth Conference on Com- putational Natural Language Learning, CoNLL '09, pages 147-155, Stroudsburg, PA, USA. Association</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
