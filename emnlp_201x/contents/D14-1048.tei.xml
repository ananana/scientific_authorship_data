<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aligning English Strings with Abstract Meaning Representation Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Aligning English Strings with Abstract Meaning Representation Graphs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="425" to="429"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We align pairs of English sentences and corresponding Abstract Meaning Representations (AMR), at the token level. Such alignments will be useful for downstream extraction of semantic interpretation and generation rules. Our method involves linearizing AMR structures and performing symmetrized EM training. We obtain 86.5% and 83.1% alignment F score on development and test sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head> <ref type="bibr">(and vice-versa)</ref><p>, so there are no manually-annotated alignment links between English words and AMR concepts. This paper studies how to build such links automatically, us- ing co-occurrence and other information. Auto- matic alignments may be useful for downstream extraction of semantic interpretation and genera- tion rules.</p><p>AMRs are directed, acyclic graphs with labeled edges, e.g., the sentence The boy wants to go is represented as: We have hand-aligned a subset of the 13,050 available AMR/English pairs. We evaluate our automatic alignments against this gold standard. A sample hand-aligned AMR is here ("˜n" speci- fies a link to the nth English word):</p><p>the boy wants to go (w / want-01˜3 :arg0 (b / boy˜2) :arg1 (g / go-01˜5 :arg0 b))</p><p>This alignment problem resembles that of statisti- cal machine translation (SMT). It is easier in some ways, because AMR and English are highly cog- nate. It is harder in other ways, as AMR is graph- structured, and children of an AMR node are un- ordered. There are also fewer available training pairs than in SMT. One approach is to define a generative model from AMR graphs to strings. We can then use EM to uncover hidden derivations, which align- ments weakly reflect. This approach is used in string/string SMT ( <ref type="bibr" target="#b1">Brown et al., 1993)</ref>. How- ever, we do not yet have such a generative graph- to-string model, and even if we did, there might not be an efficient EM solution. For exam- ple, in syntax-based SMT systems ( <ref type="bibr" target="#b3">Galley et al., 2004</ref>), the generative tree/string transduction story is clear, but in the absence of alignment con- straints, there are too many derivations and rules for EM to efficiently consider.</p><p>We therefore follow syntax-based SMT custom and use string/string alignment models in align- ing our graph/string pairs. However, while it is straightforward to convert syntax trees into strings data (by taking yields), it is not obvious how to do this for unordered AMR graph elements. The ex- ample above also shows that gold alignment links reach into the internal nodes of AMR.</p><p>Prior SMT work ( <ref type="bibr" target="#b5">Jones et al., 2012</ref>) describes alignment of semantic graphs and strings, though their experiments are limited to the GeoQuery do- main, and their methods are not described in de- tail. <ref type="bibr" target="#b2">Flanigan et al (2014)</ref> describe a heuristic AMR/English aligner. While heuristic aligners can achieve good accuracy, they will not automat- ically improve as more AMR/English data comes online.</p><p>The contributions of this paper are:</p><p>• A set of gold, manually-aligned AMR/English pairs.</p><p>• An algorithm for automatically aligning AMR/English pairs.</p><p>• An empirical study establishing alignment accuracy of 86.5% and 83.1% F score for de- velopment and test sets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We divide the description of our method into three parts: preprocessing, training, and postprocessing.</p><p>In the preprocessing phase, we linearize the AMR graphs to change them into strings, clean both the AMR and English sides by removing stop words and simple stemming, and add a set of correspond- ing AMR/English token pairs to the corpus to help the training phase. The training phase is based on IBM models, but we modify the learning algo- rithm to learn the parameters symmetrically. Fi- nally, in the postprocessing stage we rebuild the aligned AMR graph. These components are de- scribed in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>The first step of the preprocessing component is to linearize the AMR structure into a string. In this step we record the original structure of nodes in the graph for later reconstruction of AMR. AMR has a rooted graph structure. To linearize this graph we run a depth first search from the root and print each node as soon as it it visited. We print but not expand the nodes that are seen previously. For example the AMR:</p><formula xml:id="formula_0">(w / want-01 :arg0 (b / boy) :arg1 (g / go-01 :arg0 b))</formula><p>is linearized into this order: w / want-01 :arg0 b / boy :arg1 g / go-01 :arg0 b.</p><p>Note that semantically related nodes often stay close together after linearization.</p><p>After linearizing the AMR graph into a string, we perform a series of preprocessing steps includ- ing lowercasing the letters, removing stop words, and stemming.</p><p>The AMR and English stop word lists are gen- erated based on our knowledge of AMR design.</p><p>We know that tokens like an, the or to be verbs will very rarely align to any AMR token; similarly, AMR role tokens like :arg0, :quant, :opt1 etc. as well as the instance-of token /, and tokens like temporal-quantity or date-entity rarely align to any English token. We remove these tokens from the parallel corpus, but remember their position to be able to convert the resulting string/string align- ment back into a full AMR graph/English string alignment. Although some stopwords participate in gold alignments, by removing them we will buy a large precision gain for some recall cost.</p><p>We remove the word sense indicator and quo- tation marks for AMR concepts. For instance we will change want-01 to want and "ohio" to ohio. Then we stem AMR and English tokens into their first four letters, except for role tokens in AMR. The purpose of stemming is to normalize English morphological variants so that they are easier to match to AMR tokens. For example English to- kens wants, wanting, wanted, and want as well as the AMR token want-01 will all convert to want after removing the AMR word sense indicator and stemming.</p><p>In the last step of preprocessing, we benefit from the fact that AMR concepts and their cor- responding English ones are frequently cognates. Hence, after stemming, an AMR token often can be translated to a token spelled similarly in En- glish. This is the case for English token want and AMR token want in the previous paragraph. To help the training model learn from this fact, we extend our sentence pair corpus with the set of AMR/English token pairs that are spelled identi- cally after preprocessing. Also, for English tokens that can be translated into multiple AMR tokens, like higher and high :degree more we add the cor- responding string/string pairs to the corpus. This set is extracted from existing lexical resources, in- cluding lists of comparative/superlative adjectives, negative words, etc.</p><p>After preprocessing, the AMR at the start of this section will change into: want boy go and the sentence The boy wants to go changes into boy want to go, and we will also add the identity pairs want/want, boy/boy, and go/go to the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>Our training method is based on IBM word align- ment models <ref type="bibr" target="#b1">(Brown et al., 1993)</ref>. We modify the objective functions of the IBM models to en-courage agreement between learning parameters in English-to-AMR and AMR-to-English direc- tions of EM. The solution of this objective func- tion can be approximated in an extremely simple way that requires almost no extra coding effort.</p><p>Assume that we have a set of sentence pairs</p><note type="other">{(E, A)}, where each E is an English sentence and each A is a linearized AMR. According to IBM models, A is generated from E through a generative story based on some parameters.</note><p>For example, in IBM Model 2, given E we first decide the length of A based on some prob- ability l = p(len(A)|len(E)), then we decide the distortions based on a distortion table: d = p(i|j, len(A), len(E)). Finally, we translate En- glish tokens into AMR ones based on a translation table t = p(a|e) where a and e are AMR and En- glish tokens respectively.</p><p>IBM models estimate these parameters to max- imize the conditional likelihood of the data:</p><formula xml:id="formula_1">θ A|E = argmaxL θ A|E (A|E) or θ E|A = argmaxL θ E|A (E|A)</formula><p>where θ denotes the set of parameters. The conditional likelihood is intrinsic to the generative story of IBM models. However, word alignment is a symmetric problem. Hence it is more reasonable to estimate the parameters in a more symmetric manner.</p><p>Our objective function in the training phase is:</p><formula xml:id="formula_2">θ A|E , θ E|A = argmaxL θ A|E (A|E)+L θ E|A (E|A) subject to θ A|E θ E = θ E|A θ A = θ A,E</formula><p>We approximate the solution of this objective function with almost no change to the existing implementation of the IBM models. We relax the constraint to θ A|E = θ E|A , then apply the following iterative process: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Iterate</head><p>Note that steps 1 and 3 are nothing more than running the IBM models, and steps 2 and 4 are just initialization of the EM parameters, using ta- bles from the previous iteration. The initialization steps only make sense for the parameters that in- volve both sides of the alignment (i.e., the <ref type="table">transla- tion table and the distortion table)</ref>. For the trans- lation table we set t E|A (e|a) = t A|E (a|e) for En- glish and AMR tokens e and a and then normalize the t table. The distortion table can also be initial- ized in a similar manner. We initialize the fertility table with its value in the previous iteration.</p><p>Previously <ref type="bibr" target="#b7">Liang et al. (2006)</ref> also presented a symmetric method for training alignment parame- ters. Similar to our work, their objective function involves summation of conditional likelihoods in both directions; however, their constraint is on agreement between predicted alignments while we directly focus on agreement between the parame- ters themselves. Moreover their method involves a modification of the E step of EM algorithm which is very hard to implement for IBM Model 3 and above.</p><p>After learning the parameters, alignments are computed using the Viterbi algorithm in both di- rections of the IBM models. We tried merging the alignments of the two directions using meth- ods like grow-diag-final heuristic or taking inter- section of the alignments and adding some high probability links in their union. But these methods did not help the alignment accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Postprocessing</head><p>The main goal of the postprocessing component is to rebuild the aligned AMR graph. We first insert words removed as stop words into their positions, then rebuild the graph using the recorded original structure of the nodes in the AMR graph.</p><p>We also apply a last modification to the align- ments in the postprocessing. Observing that pairs like worker and person :arg0-of work-01 appear frequently, and in all such cases, all the AMR to- kens align to the English one, whenever we see any of AMR tokens person, product, thing or com- pany is followed by arg0-of, arg1-of or arg2-of followed by an AMR concept, we align the two former tokens to what the concept is aligned to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Description</head><p>Our data consists of 13,050 publicly available AMR/English sentence pairs <ref type="bibr">1</ref> . We have hand aligned 200 of these pairs to be used as develop- ment and test sets 2 . We train the parameters on the whole data. <ref type="table" target="#tab_0">Table 1</ref> presents a description of the data. We do not count parenthesis, slash and AMR variables as AMR tokens. Role tokens are those AMR tokens that start with a colon. They do not represent any concept, but provide a link between concepts. For example in: (w / want-01 :arg0 (b / boy) :arg1 (g / go-01 :arg0 b)) the first :arg0 states that the first argument of the concept wanting is the boy and the second argu- ment is going.</p><p>train dev test Sent. pairs 13050 100 100 AMR tokens 465 K 3.8 K (52%) 2.3 K (%55) AMR role tokens 226 K 1.9 K (23%) 1.1 K (%22) ENG tokens 248 K 2.3 K (76%) 1.7 K (%74) <ref type="table" target="#tab_0">Table 1</ref>: AMR/English corpus. The number in parentheses is the percent of the tokens aligned in gold annotation. Almost half of AMR tokens are role tokens, and less than a quarter of role tokens are aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Results</head><p>We use MGIZA++ ( <ref type="bibr" target="#b4">Gao and Vogel, 2008)</ref> as the implementation of the IBM models. We run Model 1 and HMM for 5 iterations each, then run our training algorithm on Model 4 for 4 iterations, at which point the alignments become stable. As alignments are usually many to one from AMR to English, we compute the alignments from AMR to English in the final step. <ref type="table" target="#tab_2">Table 2</ref> shows the alignment accuracy for Model 1, HMM, Model 4, and Model 4 plus the modification described in section 2.2 (Model 4+).</p><p>The alignment accuracy on the test set is lower than the development set mainly because it is in- trinsically a harder set, as we only made small modifications to the system based on the develop- ment set. Recall error due to stop words is one difference.    While the alignment method works very well on non-role tokens, it works poorly on the role tokens. Role tokens are sometimes matched with a word or part of a word in the English sentence. For ex- ample :polarity is matched with the un part of the word unpopular, :manner is matched with most adverbs, or even in the pair: thanks (t / thank-01 :arg0 (i / i) :arg1 (y / you))</p><p>all AMR tokens including :arg0 and :arg1 are matched to the only English word thanks. Incon- sistency in aligning role tokens has made this a hard problem even for human experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>428</head><p>In this paper we present the first set of manually aligned English/AMR pairs, as well as the first published system for learning the alignments be- tween English sentences and AMR graphs that provides a strong baseline for future research in this area. As the proposed system learns the alignments automatically using very little domain knowledge, it can be applied in any domain and for any language with minor adaptations. Computing the alignments between English sentences and AMR graphs is a first step for ex- traction of semantic interpretation and generation rules. Hence, a natural extension to this work will be automatically parsing English sentences into AMR and generating English sentences from AMR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Banarescu et al. (2013) describe a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Represen- tation (AMR). The designers of AMR leave open the question of how meanings are derived from English sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 .</head><label>1</label><figDesc></figDesc><table>Optimize the first part of the objective func-
tion: θ A|E = argmaxL θ A|E (A|E) using EM 

2. Satisfy the constraint: set θ E|A ∝ θ A|E 

3. Optimize the second part of the objective 
function: θ E|A = argmaxL θ E|A (E|A) 
using EM 

4. Satisfy the constraint: set θ A|E ∝ θ E|A 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on different models. Our training 
method (Model 4+) increases the F score by 1.7 
and 3.1 points on dev and test sets respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 breaks</head><label>3</label><figDesc>down precision, recall, and F score for role and non-role AMR tokens, and also shows in parentheses the amount of recall er- ror that was caused by removing either side of the alignment as a stop word.</figDesc><table>token type precision recall 
F score 

Dev 
role 
77.1 
48.7 
59.7 
non-role 97.2 
88.2 
92.5 
all 
94.1 
80.0 (34%) 86.5 

Test 
role 
71.0 
37.8 
49.3 
non-role 95.5 
84.7 
89.8 
all 
92.4 
75.6 (36%) 83.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results breakdown into role and non-
role AMR tokens. The numbers in the parentheses 
show the percent of recall errors caused by remov-
ing aligned tokens as stop words. 

</table></figure>

			<note place="foot" n="1"> LDC AMR release 1.0, Release date: June 16, 2014 https://catalog.ldc.upenn.edu/LDC2014T12</note>

			<note place="foot" n="2"> The development and test AMR/English pairs can be found in /data/split/dev/amr-release-1.0-dev-consensus.txt and /data/split/test/amr-release-1.0-test-consensus.txt, respectively. The gold alignments are not included in these files but are available separately.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by DARPA con-tracts HR0011-12-C-0014 and FA-8750-13-2-0045. The authors would like to thank David Chi-ang, Tomer Levinboim, and Ashish Vaswani (in no particular order) for their comments and sug-gestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Annotation Workshop (LAW VII-ID), ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parallel implementations of word alignment tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering, Testing, and Quality Assurance for Natural Language Processing Workshop, ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<editor>Karl Moritz Hermann, and Kevin Knight</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantics-based machine translation with hyperedge replacement grammars</title>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
