<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Label Embedding for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Network and Information Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Label Embedding for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4545" to="4553"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4545</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-task learning in text classification leverages implicit correlations among related tasks to extract common features and yield performance gains. However, a large body of previous work treats labels of each task as independent and meaningless one-hot vectors, which cause a loss of potential label information. In this paper, we propose Multi-Task Label Embedding to convert labels in text classification into semantic vectors, thereby turning the original tasks into vector matching tasks. Our model utilizes semantic correlations among tasks and makes it convenient to scale or transfer when new tasks are involved. Extensive experiments on five benchmark datasets for text classification show that our model can effectively improve the performances of related tasks with semantic representations of labels and additional information from each other.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is a common Natural Language Processing (NLP) issue that tries to infer the most appropriate label for a given sentence or docu- ment, for example, sentiment analysis, topic clas- sification and so on. With the developments and prosperities of Deep Learning ( ), many neural network based models have been exploited by a large body of literature and achieved inspiring performance gains on various text classification tasks. These models are robust at feature engineering and can represent word se- quences as fixed-length vectors with rich seman- tic information, which are notably ideal for subse- quent NLP tasks.</p><p>Due to numerous parameters to train, neu- ral network based models rely heavily on ade- quate amounts of annotated corpora, which can- not always be met as constructions of large-scale high-quality labeled datasets are extremely time- consuming and labor-intensive. Multi-Task Learn- ing (MTL) solves this problem by jointly train- ing multiple related tasks and leveraging poten- tial correlations among them to increase corpora size implicitly, extract common features and yield performance gains. Inspired by <ref type="bibr" target="#b5">(Caruana, 1997)</ref>, there is a large body of research dedicated to MTL with neural network based models <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b18">Liu et al., 2015b</ref><ref type="bibr" target="#b16">Liu et al., , 2016a</ref><ref type="bibr" target="#b26">Zhang et al., 2017</ref>). These models usually contain a pre- trained lookup layer that map words into dense, low-dimension and real-value vectors with seman- tic implications, namely known as Word Embed- ding ( <ref type="bibr" target="#b22">Mikolov et al., 2013b)</ref>, and utilize some lower layers to capture common features that are further fed to follow-up task-specific layers. How- ever, many existing models may have at least one of the following three disadvantages:</p><p>• Lack of Label Information. Labels of each task are represented by independent and meaningless one-hot vectors, for example, positive and negative in sentiment analysis encoded as <ref type="bibr">[1,</ref><ref type="bibr">0]</ref> and <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, which may cause a loss of potential label information.</p><p>• Inability to Scale. Network structures are elaborately designed to model various corre- lations for MTL, but many of them are struc- turally fixed and some can only deal with in- teractions between two tasks, namely pair- wise interactions. When new tasks are in- volved, the network structures have to be modified and all networks have to be trained again.</p><p>• Inability to Transfer. Human beings can handle a completely new task without any- more efforts after learning with several re- lated tasks, which can be called the ability of Transfer Learning ( <ref type="bibr" target="#b14">Ling et al., 2008</ref>). As discussed above, the network structures of many previous models are fixed and there are no layers specially designed for unannotated new tasks.</p><p>In this paper, we proposed Multi-Task Label Embedding (MTLE) to map labels of each task into semantic vectors, similar to how Word Em- bedding deals with word sequences, thereby turn- ing the original tasks into vector matching tasks. The idea of embedding label is not new and is primarily designed to improve general neural text classification models ( <ref type="bibr" target="#b23">Norouzi et al., 2013;</ref><ref type="bibr" target="#b19">Ma et al., 2016)</ref>, but here we mainly focus on integrating label embedding to enhance MTL. MTLE utilizes semantic correlations among tasks and effectively solves the problems of scal- ing and transferring when new tasks are involved.</p><p>We conduct extensive experiments on five benchmark datasets for text classification. Com- pared to learning separately, jointly learning mul- tiple related tasks based on MTLE demonstrates significant performance gains for each task.</p><p>Our contributions are four-fold:</p><p>• Our model efficiently leverages label infor- mation of each task by mapping labels into dense, low-dimension and real-value vectors with semantic implications.</p><p>• It is particularly convenient for our model to scale for new tasks. The network structures need no modifications and only samples from the new tasks require training.</p><p>• After training on several related tasks, our model can also naturally transfer to deal with new tasks without anymore training, while still achieving appreciable performances.</p><p>• We consider different scenarios of MTL and demonstrate strong results on several bench- mark datasets for text classification. Our model outperforms most of the state-of-the- art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-Task Learning</head><p>For a text classification task, the Input is a word sequence x = {x 1 , x 2 , ..., x T }, where T is the sequence length and we need to output the most appropriate Label from the set {y 1 , y 2 , ..., y C }, or their one-hot representation {y 1 , y 2 , ..., y C }, where C is the number of classes. In the super- vised case, we have the Annotation, that is, the corresponding ground truth label˜ylabel˜ label˜y for each input, while in the unsupervised case, we only know the label set but lack the specific annotations. A pre-trained lookup layer is used to get the embedding vector x t ∈ R d for each word x t . A text classification model f is trained to pro- duce the predicted distributionˆydistributionˆ distributionˆy for each x = {x 1 , x 2 , ..., x T }.</p><formula xml:id="formula_0">f (x 1 , x 2 , ..., x T ) = ˆ y<label>(1)</label></formula><p>and the learning objective is to minimize the over- all cross-entropy on the training set.</p><formula xml:id="formula_1">l = − N ∑ i=1 C ∑ j=1˜y j=1˜ j=1˜y ij logˆylogˆ logˆy ij (2)</formula><p>where N denotes the number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Task Learning</head><p>Given K supervised text classification tasks,</p><formula xml:id="formula_2">T 1 , T 2 , ..., T K , a multi-task learning model F is trained to transform each x (k) from T k into mul- tiple predicted distributions {ˆy{ˆy (1) , ..., ˆ y (K) }. F (x (k) 1 , ..., x (k) T ) = {ˆy{ˆy (1) , ..., ˆ y (K) }<label>(3)</label></formula><p>where onlyˆyonlyˆ onlyˆy (k) is used for loss computation. The overall training loss is a weighted combination of costs for each task.</p><formula xml:id="formula_3">L = − K ∑ k=1 λ k N k ∑ i=1 C k ∑ j=1˜y j=1˜ j=1˜y (k) ij logˆylogˆ logˆy (k) ij<label>(4)</label></formula><p>where λ k , N k and C k denote the weight, the num- ber of training samples and the number of classes for each task T k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ability to Scale</head><p>When new tasks are involved, for ex- ample, given K ′ more supervised tasks T K+1 , T K+2 , ..., T K+K ′ , the MTL model F , which has been trained on the old K tasks, should be able to scale with few structural modifications.</p><formula xml:id="formula_4">F (x (k) 1 , ..., x (k) T ) = {ˆy{ˆy (1) , ..., ˆ y (K+K ′ ) } (5)</formula><p>where x (k) denotes samples from not only the old tasks but also the new ones.</p><p>There are mainly two scaling methods for F to deal with the new tasks. We can continue training F and further tune the model parameters based on samples from the new tasks, which we define as Hot Update, or re-train F again based on training samples from all tasks, which is defined as Cold Update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Ability to Transfer</head><p>Given K ′′ more completely unannotated new tasks</p><formula xml:id="formula_5">T • K+1 , T • K+2 , ..., T</formula><p>• K+K ′′ , F should be able to transfer what it learned from the old tasks, which is defined as Zero Update, as no more training samples are available and the model parameters are not updated anymore.</p><p>For each x (k) from the K ′′ tasks, F should pro- duce the corresponding predicted distributions for the new tasks, even if the annotations of these new tasks are not provided at all.</p><formula xml:id="formula_6">F (x (k) 1 , ..., x (k) T ) = {ˆy{ˆy (K+1) , ..., ˆ y (K+K ′′ ) } (6)</formula><p>which requires that F should be able to understand the meanings of each label from the new tasks and infer the most appropriate one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In text classification tasks, labels can be single- word or double-word, for example, positive and negative in binary sentiment classification, very positive, positive, neutral, negative and very neg- ative in 5-category sentiment classification, but there are few labels that contain three words or more. Inspired by Word Embedding, we pro- pose Multi-Task Label Embedding (MTLE) to convert labels of each task of MTL into dense, low-dimension and real-value vectors with seman- tic implications, thereby disclosing potential intra- task and inter-task correlations from both texts and labels. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the general idea of MTLE, which mainly consists of three parts, the Input Encoder, the Label Encoder and the Matcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>In the Input Encoder, each input sequence</p><formula xml:id="formula_7">x (k) from T k is transformed into embedding represen- tation x (k) = {x (k) 1 , x (k) 2 , ..., x (k)</formula><p>T } by the Embed- ding Layer (E I ). The Learning Layer (L I ) is ap- plied to recurrently comprehend x (k) and generate a fixed-length vector X (k) , which can be regarded Label Encoder In order to classify a sample x (k) from T k , the Matcher obtains the corresponding X (k) from the Input Encoder, all Y (k) j (1 ≤ j ≤ C k ) from the Label Encoder, and conducts vector matching to select the most appropriate class label.</p><formula xml:id="formula_8">T 1 x 1 (1) , x 2 (1) ,..., x T (1) T 2 x 1 (2) , x 2 (2) ,..., x T (2) T K x 1 (K ) , x 2 (K ) ,..., x T (K ) Input Sequences T 1 Y C1 (1) T 2 T K Labels Y 1 (1) Y C2 (2) Y 1 (2) Y CK (K ) Y 1 (K )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matcher</head><p>In MTLE, E I and E L obtain understandings of words from texts and labels respectively, while L I and L L achieve representation abilities of vector sequences. All these four modules are learned within and shared among tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We can explore different embedding methods and neural networks to achieve E I , E L and L I , L L re- spectively, but here we choose to implement them in an easier way and spend more efforts to investi- gate the effectiveness of MTLE.</p><p>The implementation details of MTLE are illus- trated in <ref type="figure" target="#fig_1">Figure 2</ref>.  L I and L L should be trainable models that can transform a vector sequence of arbitrary length into a fixed-length vector, which can be im- plemented by a Bi-directional Long Short-Term Memory Network (BiLSTM) that can recurrently process vector sequences and learn long-term de- pendencies.</p><p>While there are numerous variants of the stan- dard LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>), here we follow the implementation of <ref type="bibr" target="#b9">(Graves, 2013)</ref>. At each time step t, states of the LSTM can be fully described by five vectors in R m , an input gate i t , a forget gate f t , an output gate o t , the hidden state h t and the memory cell c t , which adhere to the following transition equations.</p><formula xml:id="formula_9">i t = σ(W i x t + U i h t−1 + V i c t−1 + b i )<label>(7)</label></formula><formula xml:id="formula_10">f t = σ(W f x t + U f h t−1 + V f c t−1 + b f ) (8) o t = σ(W o x t + U o h t−1 + V o c t−1 + b o ) (9) ˜ c t = tanh(W c x t + U c h t−1 )<label>(10)</label></formula><formula xml:id="formula_11">c t = f t ⊙ c t−1 + i t ⊙ ˜ c t<label>(11)</label></formula><formula xml:id="formula_12">h t = o t ⊙ tanh(c t )<label>(12)</label></formula><p>where x t is the current input, σ denotes logis- tic sigmoid function and ⊙ denotes element-wise multiplication.</p><p>A BiLSTM consists of two LSTM layers that process the input sequences in original and re- versed orders, and its output is the concatenation of hidden states from the forward and backward LSTM at each time step.</p><formula xml:id="formula_13">h t = − → h t ⊕ ← − h t<label>(13)</label></formula><p>where ⊕ denotes vector concatenation. We apply the above equations to implement L I with hidden size m. However, it is inappropriate to apply a BiLSTM for L L , as most labels contain only one or two words. Instead, L L accepts the embedding vectors of each word from a label and calculate the average.</p><p>For an input sample x (k) and all C k labels y (k) j from T k , the corresponding X (k) ∈ R 2m and Y <ref type="bibr">(k)</ref> j ∈ R d are calculated as follows.</p><formula xml:id="formula_14">X (k) = L I (W I (x (k) ))<label>(14)</label></formula><formula xml:id="formula_15">Y (k) j = L L (W L (y (k) j ))<label>(15)</label></formula><p>In this paper, we mainly focus on the idea and effects of MTLE, so rather than exploring some useful mechanisms like gating, external mem- ory or attention for stronger abilities of sequence learning, we choose the vanilla BiLSTM for quick implementations and spend most of our efforts on investigating the effectiveness of MTLE.</p><p>We concatenate</p><formula xml:id="formula_16">X (k) , Y (k) j</formula><p>and apply another fully-connection layer with only one neuron, de- noted by M , to implement the Matcher, which ac- cepts outputs from L I and L L to produce a score of matching. Given the matching scores of each label, we refer to the idea of cross-entropy and cal- culate the loss function for a sample x (k) from T k as follows.</p><formula xml:id="formula_17">s (k) j = σ(M (X (k) ⊕ Y (k) j ))<label>(16)</label></formula><formula xml:id="formula_18">ˆ y (k) j = exp(s (k) j ) ∑ C k c=1 exp(s (k) c )<label>(17)</label></formula><formula xml:id="formula_19">l (k) = − C k ∑ j=1˜y j=1˜ j=1˜y (k) j logˆylogˆ logˆy (k) j<label>(18)</label></formula><p>The overall training objective is to minimize the weighted linear combination of costs for each task.</p><formula xml:id="formula_20">L = − K ∑ k=1 λ k N k ∑ i=1 l (k) i<label>(19)</label></formula><p>MTLE provides an easy and intuitive way to re- alize MTL, where input texts and class labels from different tasks are jointly learned and compactly fused. During training, E I and E L learn better un- derstanding of word semantics for different tasks, L I and L L obtain stronger abilities of sequence representation, while M produces more accurate scores for vector matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scaling and Transferring</head><p>When new tasks are involved, it is particularly convenient for MTLE to scale or transfer as the network structure needs no modifications. For in- put samples x (k) and class labels y (k) j from the new tasks, we can apply E I , E L , L I , L L to get their vector representations X (k) and Y (k) j , calcu- late the matching scores and find the most appro- priate label.  <ref type="figure">Figure 3</ref>, where Before Update denotes the state of MTLE trained on the old tasks before the new tasks are in- troduced. We will further compare these updating methods in the Experiment Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTLE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we design extensive experiments with multi-task learning based on five benchmark datasets for text classification. We investigate the empirical performances of MTLE and compare them with existing state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>As <ref type="table" target="#tab_2">Table 1</ref> shows, we select five benchmark datasets for text classification, which are com- monly used in a large body of research on MTL. We design three experiment scenarios to evaluate the performances of MTLE. Larger text classifica- tion datasets ( <ref type="bibr" target="#b27">Zhang et al., 2015)</ref> are not chosen as there are already enough samples to train and MTL may increase computation workloads. We use the accuracy for evaluations as samples from these datasets are mostly well balanced.</p><p>• Multi-Cardinality Movie review datasets with different average lengths and class num- bers, including SST-1 ( <ref type="bibr" target="#b24">Socher et al., 2013)</ref>, SST-2, IMDB (Maas et al., 2011).</p><p>• Multi-Domain Product review datasets on different domains from Multi-Domain Senti- ment <ref type="bibr">Dataset (Blitzer et al., 2007</ref>).</p><p>• Multi-Objective Text classification datasets with different objectives, including IMDB, <ref type="bibr">RN (Apté et al., 1994)</ref>, QC ( <ref type="bibr" target="#b13">Li and Roth, 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters and Training</head><p>Training of MTLE is conducted through back propagation with batch gradient descent <ref type="bibr" target="#b0">(Amari, 1993)</ref>. We obtain a pre-trained lookup table by applying Word2Vec <ref type="figure" target="#fig_0">(Mikolov et al., 2013a)</ref> on the Google News corpus, which contains more than 100B words with a vocabulary size of about 3M. During each epoch, we randomly divide training samples from different tasks into batches of fixed size. For each iteration, we randomly select one task and choose an untrained batch, calculate the gradient and update the parameters accordingly.</p><p>Parameters of the neural layers are randomly initialized with the Xavier initializer <ref type="bibr" target="#b8">(Glorot and Bengio, 2010)</ref>. We apply 10-fold cross-validation and different combinations of hyperparameters are investigated, of which the best one is described in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of MTLE vs. Single Task</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we compare the performances of MTLE with single-task learning, where only a BiLSTM layer is applied.</p><p>MTLE achieves significant performance gains with label information and additional correla- tions from related tasks. Multi-Domain, Multi- Cardinality and Multi-Objective benefit from MTLE with average improvements of 5.5%, 2.7% and 1.2%, as they contain increasingly weaker rel- evance among tasks. The result of IMDB in Multi- Cardinality is slightly better than that in Multi- Objective (91.3 against 90.9), as SST-1 and SST-2 share more semantically useful information with IMDB than RN and QC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Abilities to Scale and Transfer</head><p>In order to investigate the abilities of MTLE to scale and transfer, we use A + B → C to de- note the case where MTLE is trained on task A and B, while C is the newly involved one. We de- sign three cases based on different scenarios and   compare the influences of Hot Update, Cold Up- date, Zero Update on each task.</p><p>• Case 1 SST-1 + SST-2 → IMDB.</p><p>• Case 2 B + D + E → K.</p><p>• Case 3 RN + QC → IMDB.</p><p>where in Zero Update, we ignore the training set of C and just evaluate our model on the testing set. As <ref type="table" target="#tab_5">Table 4</ref> shows, Before Update denotes the model trained on the old tasks before the new tasks are involved, so only evaluations on the old tasks are conducted.</p><p>Cold Update re-trains the model of Before Up- date with both the old tasks and the new tasks, thus achieving similar performances with the last line in <ref type="table" target="#tab_4">Table 3</ref>. Different from Cold Update, Hot Up- date resumes training only on the new tasks, re- quires much less training time, while still obtains competitive results for all tasks. The new tasks like IMDB and Kitchen benefit more from Hot Update than the old tasks, as the parameters are further tuned according to annotations from these new tasks.</p><p>Zero Update provides inspiring possibilities for completely unannotated tasks. There are no more annotations for additional training from the new tasks, so we just apply the model of Before Update for evaluations on the testing sets of the new tasks. Zero Update achieves competitive performances in Case 1 (90.9 for IMDB) and Case 2 (86.7 for Kitchen), as tasks from these two cases all be- long to sentiment datasets of different cardinalities or domains that contain rich semantic correlations with each other. However, the result for IMDB in Case 3 is only 74.2, as sentiment shares less rel- evance with topic and question type, thus leading to poor transferring performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Multi-Task or Label Embedding</head><p>MTLE mainly employs two mechanisms, label embedding and multi-task learning, so both im- plicit information from labels and potential cor- relations from other tasks make differences. In this section, we conduct experiments to explore the contributions of label embedding and multi- task learning respectively.</p><p>We choose the four tasks from Multi-Domain scenario and train MTLE on each task separately, so their performances are only influenced by label embedding. Then we re-train MTLE from scratch for every two tasks, every three tasks from them and record the performances of each task in differ- ent cases, where both label embedding and multi- task learning matter.</p><p>The results are illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. The first three graphs show the results of MTLE trained on every one, every two and every three tasks. In the first graph, the four tasks are trained separately and achieve improvements of 3.0%, 3.1%, 3.3%, 2.3% compared to the single task in <ref type="table" target="#tab_4">Table 3</ref>. As more tasks are added step by step, MTLE produces increasing performance gains for each task and achieves an average improvement of 5.5% when all the four tasks are trained together. So it can  be concluded that both information from labels as well as correlations from other tasks account for considerable parts of contributions.</p><p>In the last graph, diagonal cells denote improve- ments of every one task, while off-diagonal cells denote average improvements of every two tasks, so an off-diagonal cell of darker color indicates stronger correlation between the two tasks. An interesting finding is that Books is more related with DVDs and Electronics is more relevant to Kitchen. A possible reason may be that Books and DVDs are products targeted for reading or watch- ing, while customers care more about appearances and functionalities when talking about Electronics and Kitchen. We compare MTLE against the following models.</p><p>• NBOW Neural Bag-of-Words that sums up embedding vectors of all words and applies a non-linearity followed by a softmax layer.</p><p>• PV Paragraph Vectors followed by logistic regression ( <ref type="bibr" target="#b12">Le and Mikolov, 2014</ref>).</p><p>• CNN Convolutional Neural Networks for Sentence Classification <ref type="bibr" target="#b11">(Kim, 2014</ref>).</p><p>• MT-CNN Multi-Task learning with Convo- lutional Neural Networks (Collobert and We- ston, 2008) where lookup tables are partially shared.</p><p>• MT-DNN Multi-Task learning with Deep Neural Networks ( <ref type="bibr" target="#b18">Liu et al., 2015b</ref>) that uti- lizes bag-of-word representations and a hid- den shared layer.</p><p>• MT-RNN Multi-Task learning with Re- current Neural Networks with a shared- layer ( <ref type="bibr" target="#b17">Liu et al., 2016b</ref>).</p><p>• DSM Deep multi-task learning with Shared Memory ( <ref type="bibr" target="#b16">Liu et al., 2016a</ref>) where a exter- nal memory and a reading/writing mecha- nism are introduced.</p><p>• GRNN Gated Recursive Neural Network for sentence modeling and text classifica- tion ( <ref type="bibr" target="#b6">Chen et al., 2015</ref>).</p><p>• Tree-LSTM A generalization of LSTMs to tree-structured network topologies <ref type="bibr" target="#b25">(Tai et al., 2015</ref>).</p><p>As <ref type="table" target="#tab_6">Table 5</ref> shows, MTLE achieves competi- tive or better performances on most tasks except as sentences from IMDB are much longer than SST and MDSD, which require stronger abilities of long-term dependency learning. In this paper, we mainly focus the idea and ef- fects of integrating label embedding to enhance multi-task learning, so we apply the BiLSTM to realize L I , which can be further implemented by other more powerful sequence learning mod- els ( <ref type="bibr" target="#b15">Liu et al., 2015a;</ref><ref type="bibr" target="#b6">Chen et al., 2015;</ref><ref type="bibr" target="#b25">Tai et al., 2015)</ref> and produce better performances. Explo- rations of other embedding layers and learning layers may be appreciated, but due to limited pages we choose to research these contents in fu- ture work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There is a large body of literature related to multi- task learning with neural networks in NLP <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b18">Liu et al., 2015b</ref><ref type="bibr" target="#b16">Liu et al., , 2016a</ref><ref type="bibr" target="#b26">Zhang et al., 2017)</ref>.</p><p>( <ref type="bibr" target="#b7">Collobert and Weston, 2008</ref>) use a shared lookup layer for common features, followed by task-specific layers for several traditional NLP tasks including part-of-speech tagging and seman- tic parsing. They use a fix-size window to solve the problem of variable-length input sequences, which can be better addressed by RNN.</p><p>( <ref type="bibr" target="#b18">Liu et al., 2015b</ref><ref type="bibr" target="#b16">Liu et al., , 2016a</ref><ref type="bibr" target="#b26">Zhang et al., 2017</ref>) all investigate MTL for text classification. ( <ref type="bibr" target="#b18">Liu et al., 2015b</ref>) apply bag-of-word representation, but information on word order is lost. ( <ref type="bibr" target="#b16">Liu et al., 2016a</ref>) introduce an external memory for informa- tion sharing with a reading/writing mechanism for communications. ( <ref type="bibr" target="#b17">Liu et al., 2016b)</ref> propose three different models for MTL with RNN and ( <ref type="bibr" target="#b26">Zhang et al., 2017</ref>) constructs a generalized architecture for RNN based MTL. However, models of these papers ignore essential information of labels and mostly can only address pairwise interactions be- tween two tasks. Their network structures are also fixed, thereby failing to scale or transfer when new tasks are involved.</p><p>Different from the above work, MTLE maps la- bels of text classification tasks into semantic vec- tors and provide a more intuitive way to realize MTL with the abilities to scale and transfer. In- put sequences from three or more tasks are jointly learned together with their labels, benefitting from each other and obtaining better sequence represen- tations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose Multi-Task Label Em- bedding to map labels of text classification tasks into semantic vectors. MTLE utilizes semantic correlations among tasks and effectively solves the problems of scaling and transferring when new tasks are involved. We explore three different scenarios of MTL and MTLE can improve per- formances of most tasks with semantic represen- tations of labels and additional information from others in all scenarios.</p><p>In future work, we would like to explore other learning layers and generalize MTLE to address other NLP tasks, for example, sequence labeling and sequence-to-sequence learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General Architecture of MTLE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Implementation Details of MTLE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance gains of each task in different cases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>L I and L L are both fully- connection layers with the weights W I and W L of |V | × d, where |V | denotes the vocabulary size and d is the embedding size. We can get a pre- trained lookup table based on open domain cor- pora to initialize W I , W L and further tune them during training. x (k )</head><label></label><figDesc></figDesc><table>y j 

(k ) 

this 
is 
a 
fantastic 
movie 

very 
positive 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>If the new tasks are annotated, we can apply Hot Update or Cold Update for scaling and better tune the parameters. If the new tasks are completely unannotated, we can use Zero Update for transfer- ring and produce reasonable predictions. The differences among Hot Update, Cold Up- date and Zero Update are illustrated in</figDesc><table>Task A 

Task B 

Task C 
Task D 

Before Update 

Hot Update 
Cold Update 
Zero Update 

Label 
Input 
Annotation 

Figure 3: Three different updating methods 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : Five benchmark text classification datasets</head><label>1</label><figDesc></figDesc><table>Dataset Description 
Type 
Average Length 
Class 
Objective 

SST 

Movie reviews in Stan-
ford Sentiment Treebank 
including SST-1 and SST-2 

Sentence 
19 / 19 
5 / 2 
Sentiment 

IMDB Internet Movie Database 
Document 
279 
2 
Sentiment 

MDSD 

Product reviews on books, 
DVDs, electronics and 
kitchen (BDEK) 

Document 176 / 189 / 115 / 97 
2 
Sentiment 

RN 
Reuters Newswire topics 
classification 
Document 
146 
46 
Topics 

QC 
Question Classification 
Sentence 
10 
6 
Question Types 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Hyperparameter settings</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : Results of MTLE on different scenarios</head><label>3</label><figDesc></figDesc><table>Model 
Multi-Cardinality 
Multi-Domain 
Multi-Objective 
Avg∆ 
SST-1 SST-2 IMDB 
B 
D 
E 
K 
IMDB RN QC 
Single Task 46.2 
86.1 
88.9 
78.3 79.8 81.5 82.3 
89.0 
84.2 92.7 
-
MTLE 
49.8 
88.4 
91.3 
84.5 85.2 87.3 86.9 
90.9 
85.5 93.2 +3.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of Hot Update, Cold Update and Zero Update in different cases 

Model 
Case 1 
Case 2 
Case 3 
SST-1 SST-2 IMDB 
B 
D 
E 
K 
RN QC IMDB 
Before Update 48.6 
87.6 
-
83.7 84.5 85.9 
-
84.8 93.4 
-
Cold Update 
49.8 
88.5 
91.2 
84.4 85.2 87.2 86.9 85.5 93.2 
91.0 
Hot Update 
49.6 
88.1 
91.4 
84.2 84.9 87.0 87.1 85.2 92.9 
91.1 
Zero Update 
-
-
90.9 
-
-
-
86.7 
-
-
74.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 : Comparisons of MTLE against state-of-the-art models</head><label>5</label><figDesc></figDesc><table>Model 
SST-1 SST-2 IMDB Books DVDs Electronics Kitchen QC 
NBOW 
42.4 
80.5 
83.6 
-
-
-
-
88.2 
PV 
44.6 
82.7 
91.7 
-
-
-
-
91.8 
CNN 
48.0 
88.1 
-
-
-
-
-
93.6 
MT-CNN 
-
-
-
80.2 
81.0 
83.4 
83.0 
-
MT-DNN 
-
-
-
79.7 
80.5 
82.5 
82.8 
-
MT-RNN 
49.6 
87.9 
91.3 
-
-
-
-
-
DSM 
49.5 
87.8 
91.2 
82.8 
83.0 
85.5 
84.0 
-
GRNN 
47.5 
85.5 
-
-
-
-
-
93.8 
Tree-LSTM 50.6 
86.9 
-
-
-
-
-
-
MTLE 
49.8 
88.4 
91.3 
84.5 
85.2 
87.3 
86.9 
93.2 

for QC, as it contains less correlations with other 
tasks. Tree-LSTM outperforms our model on SST-
1 (50.6 against 49.8), but it requires an exter-
nal parser to get the sentence topological struc-
ture and utilizes treebank annotations. PV slightly 
surpasses MTLE on IMDB (91.7 against 91.3), 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Backpropagation and stochastic gradient descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="196" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated Learning of Decision Rules for Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chidanand</forename><surname>Apté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sholom</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="251" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label embedding trees for large multi-class tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<title level="m">Multitask Learning. Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Sentence Modeling with Gated Recursive Neural Network. In EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="793" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating Sequences With Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Question Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spectral domain-transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="488" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="2326" to="2335" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Multi-Task Learning with Shared Memory for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network for Text Classification with Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2873" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="912" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Label embedding for zero-shot fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Word Vectors for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A generalized recurrent neural architecture for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3385" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
