<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentence Modeling with Gated Recursive Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentence Modeling with Gated Recursive Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, neural network based sentence modeling methods have achieved great progress. Among these methods, the re-cursive neural networks (RecNNs) can effectively model the combination of the words in sentence. However, RecNNs need a given external topological structure , like syntactic tree. In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in re-cursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classification datasets show the effectiveness of our model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature en- gineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) ( <ref type="bibr" target="#b12">Mikolov et al., 2010)</ref>, Recursive Neural Network (RecNN) <ref type="bibr" target="#b13">(Pollack, 1990;</ref><ref type="bibr" target="#b17">Socher et al., 2013b;</ref><ref type="bibr" target="#b15">Socher et al., 2012)</ref> and Convolutional Neural Network (CNN) ( <ref type="bibr" target="#b8">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b7">Hu et al., 2014</ref>).</p><p>Among these methods, recursive neural net- works (RecNNs) have shown their excellent abil- ities to model the word combinations in sentence. However, RecNNs require a pre-defined topolog- ical structure, like parse tree, to encode sentence, which limits the scope of its application.  proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree * Corresponding author.</p><p>cannot agree with you I more agree with you I more cannot <ref type="figure">Figure 1</ref>: Example of Gated Recursive Neural Networks (GRNNs). Left is a GRNN using a di- rected acyclic graph (DAG) structure. Right is a GRNN using a full binary tree (FBT) structure. (The green nodes, gray nodes and white nodes illustrate the positive, negative and neutral senti- ments respectively.) to model sentences. However, DAG structure is relatively complicated. The number of the hidden neurons quadraticly increases with the length of sentences so that grConv cannot effectively deal with long sentences.</p><p>Inspired by grConv, we propose a gated recur- sive neural network (GRNN) for sentence model- ing. Different with grConv, we use the full binary tree (FBT) as the topological structure to recur- sively model the word combinations, as shown in <ref type="figure">Figure 1</ref>. The number of the hidden neurons lin- early increases with the length of sentences. An- other difference is that we introduce two kinds of gates, reset and update gates ( <ref type="bibr" target="#b3">Chung et al., 2014)</ref>, to control the combinations in recursive structure. With these two gating mechanisms, our model can better model the complicated combinations of fea- tures and capture the long dependency interac- tions.</p><p>In our previous works, we have investigated several different topological structures (tree and directed acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word seg- mentation ( <ref type="bibr" target="#b0">Chen et al., 2015a</ref>) and dependency parsing <ref type="bibr" target="#b1">(Chen et al., 2015b</ref>) tasks. However, these structures are not suitable for modeling sentences.</p><formula xml:id="formula_0">… Softmax(W s × u i + b s ) u i P(·|x i ;θ) w 2 w 1 (i) (i) … … … … … … … … w 3 w 4 w 5 (i) (i) (i) 0 0 0 Figure 2: Architecture of Gated Recursive Neural Network (GRNN).</formula><p>In this paper, we adopt the full binary tree as the topological structure to reduce the model com- plexity.</p><p>Experiments on the Stanford Sentiment Tree- bank dataset <ref type="bibr" target="#b17">(Socher et al., 2013b</ref>) and the TREC questions dataset ( <ref type="bibr" target="#b11">Li and Roth, 2002)</ref> show the ef- fectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Gated Recursive Neural Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>The recursive neural network (RecNN) need a topological structure to model a sentence, such as a syntactic tree. In this paper, we use a full binary tree (FBT), as showing in <ref type="figure">Figure 2</ref>, to model the combinations of features for a given sentence.</p><p>In fact, the FBT structure can model the com- binations of features by continuously mixing the information from the bottom layer to the top layer. Each neuron can be regarded as a complicated feature composition of its governed sub-sentence. When the children nodes combine into their parent node, the combination information of two children nodes is also merged and preserved by their par- ent node. As shown in <ref type="figure">Figure 2</ref>, we put all-zero padding vectors after the last word of the sentence until the length of 2 ⌈log n 2 ⌉ , where n is the length of the given sentence.</p><p>Inspired by the success of the gate mechanism of <ref type="bibr" target="#b3">Chung et al. (2014)</ref>, we further propose a gated recursive neural network (GRNN) by introducing two kinds of gates, namely "reset gate" and "up- date gate". Specifically, there are two reset gates, r L and r R , partially reading the information from Gate z</p><p>Gate rL Gate rR h2j <ref type="bibr">(l-1)</ref> h2j+1 <ref type="bibr">(l-1)</ref> hj ^(l)</p><p>hj (l) <ref type="figure">Figure 3</ref>: Our proposed gated recursive unit.</p><p>left child and right child respectively. And the up- date gates z N , z L and z R decide what to preserve when combining the children's information. Intu- itively, these gates seem to decide how to update and exploit the combination information.</p><p>In the case of text classification, for each given sentence</p><formula xml:id="formula_1">x i = w (i)</formula><p>1:N (i) and the corresponding class y i , we first represent each word w</p><formula xml:id="formula_2">(i) j into its corre- sponding embedding w w (i) j ∈ R d , where N (i) in-</formula><p>dicates the length of i-th sentence and d is dimen- sionality of word embeddings. Then, the embed- dings are sent to the first layer of GRNN as inputs, whose outputs are recursively applied to upper lay- ers until it outputs a single fixed-length vector. Next, we receive the class distribution P(·|x i ; θ) for the given sentence x i by a softmax transforma- tion of u i , where u i is the top node of the network (a fixed length vectorial representation):</p><formula xml:id="formula_3">P(·|x i ; θ) = softmax(W s × u i + b s ),<label>(1)</label></formula><p>where b s ∈ R |T | , W s ∈ R |T |×d . d is the dimen- sionality of the top node u i , which is same with the word embedding size and T represents the set of possible classes. θ represents the parameter set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gated Recursive Unit</head><p>GRNN consists of the minimal structures, gated recursive units, as showing in <ref type="figure">Figure 3</ref>. By assuming that the length of sentence is n, we will have recursion layer l ∈ [1, ⌈log n 2 ⌉+1], where symbol ⌈q⌉ indicates the minimal integer q * ≥ q. At each recursion layer l, the activation of the j-</p><formula xml:id="formula_4">th (j ∈ [0, 2 ⌈log n 2 ⌉−l )) hidden node h (l) j ∈ R d is computed as h (l) j = { zN ⊙ ˆ h l j + zL ⊙ h l−1 2j + zR ⊙ h l−1 2j+1 , l &gt; 1, corresponding word embedding, l = 1,<label>(2)</label></formula><p>where z N , z L and z R ∈ R d are update gates for new activationˆhactivationˆ activationˆh l j , left child node h l−1 2j and right child node h l−1 2j+1 respectively, and ⊙ indi- cates element-wise multiplication.</p><p>The update gates can be formalized as:</p><formula xml:id="formula_5">z =   zN zL zR   =   1/Z 1/Z 1/Z   ⊙ exp(U    ˆ h l j h l−1 2j h l−1 2j+1   ),<label>(3)</label></formula><p>where U ∈ R 3d×3d is the coefficient of update gates, and Z ∈ R d is the vector of the normaliza- tion coefficients,</p><formula xml:id="formula_6">Z k = 3 ∑ i=1 [exp(U    ˆ h l j h l−1 2j h l−1 2j+1   )] d×(i−1)+k ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">1 ≤ k ≤ d.</formula><p>The new activationˆhactivationˆ activationˆh l j is computed as:</p><formula xml:id="formula_8">ˆ h l j = tanh(W ˆ h [ r L ⊙ h l−1 2j r R ⊙ h l−1 2j+1 ] ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">W ˆ h ∈ R d×2d , r L ∈ R d , r R ∈ R d</formula><p>. r L and r R are the reset gates for left child node h l−1 2j and right child node h l−1 2j+1 respectively, which can be formalized as:</p><formula xml:id="formula_10">[ r L r R ] = σ(G [ h l−1 2j h l−1 2j+1 ] ),<label>(6)</label></formula><p>where G ∈ R 2d×2d is the coefficient of two reset gates and σ indicates the sigmoid function. Intuiativly, the reset gates control how to select the output information of the left and right chil- dren, which result to the current new activationˆhactivationˆ activationˆh. By the update gates, the activation of a parent neu- ron can be regarded as a choice among the the cur- rent new activationˆhactivationˆ activationˆh, the left child, and the right child. This choice allows the overall structure to change adaptively with respect to the inputs. This gate mechanism is effective to model the combinations of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>We use the Maximum Likelihood (ML) criterion to train our model. Given training set (x i , y i ) and the parameter set of our model θ, the goal is to minimize the loss function:</p><formula xml:id="formula_11">J(θ) = − 1 m m ∑ i=1 log P(y i |x i ; θ) + λ 2m ∥θ∥ 2 2 ,<label>(7)</label></formula><p>Initial learning rate α = 0.3 Regularization λ = 10 −4 Dropout rate on input layer p = 20% where m is number of training sentences.</p><p>Following <ref type="bibr" target="#b16">(Socher et al., 2013a</ref>), we use the di- agonal variant of AdaGrad ( <ref type="bibr" target="#b6">Duchi et al., 2011</ref>) with minibatchs to minimize the objective.</p><p>For parameter initialization, we use random ini- tialization within (-0.01, 0.01) for all parameters except the word embeddings. We adopt the pre- trained English word embeddings from <ref type="bibr" target="#b5">(Collobert et al., 2011</ref>) and fine-tune them during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>To evaluate our approach, we test our model on three datasets:</p><p>• SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank 1 (Socher et al., 2013b): negative, somewhat negative, neutral, somewhat positive, positive.</p><p>• SST-2 The movie reviews with binary classes in the Stanford Sentiment Treebank 1 (Socher et al., 2013b): negative, positive.</p><p>• QC The TREC questions dataset 2 ( <ref type="bibr" target="#b11">Li and Roth, 2002</ref>) involves six different question types. <ref type="table" target="#tab_0">Table 1</ref> lists the hyper-parameters of our model. In this paper, we also exploit dropout strategy <ref type="bibr" target="#b18">(Srivastava et al., 2014</ref>) to avoid overfitting. In ad- dition, we set the batch size to 20. We set word embedding size d = 50 on the TREC dataset and d = 100 on the Stanford Sentiment Treebank dataset. <ref type="table">Table 2</ref> shows the performance of our GRNN on three datasets. <ref type="bibr">NBoW (Kalchbrenner et al., 2014)</ref> 42.4 80.5 88.2 PV ( <ref type="bibr" target="#b10">Le and Mikolov, 2014)</ref> 44.6 * 82.7 * 91.8 * CNN-non-static <ref type="bibr" target="#b9">(Kim, 2014)</ref> 48.0 87.2 93.6 CNN-multichannel <ref type="bibr" target="#b9">(Kim, 2014)</ref> 47.4 88.1 92.2 <ref type="bibr">MaxTDNN (Collobert and Weston, 2008)</ref> 37.4 77.1 84.4 DCNN ( <ref type="bibr" target="#b8">Kalchbrenner et al., 2014)</ref> 48 <ref type="table">Table 2</ref>: Performances of the different models. The result of PV is from our own implementation based on Gensim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyper-parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST-1 SST-2 QC</head><note type="other">.5 86.8 93.0 RecNTN (Socher et al., 2013b) 45.7 85.4 - RAE (Socher et al., 2011) 43.2 82.4 - MV-RecNN (Socher et al., 2012) 44.4 82.9 - AdaSent (Zhao et al., 2015) - - 92.4 GRNN (our approach) 47.5 85.5 93.8</note><p>Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) ( <ref type="bibr" target="#b10">Le and Mikolov, 2014</ref>) learns continuous distributed vector representations for pieces of texts, which can be regarded as a long term memory of sentences as opposed to short memory in recurrent neural network. 1 https://github.com/piskvorky/gensim/ Moreover, the plain GRNN which does not in- corporate the gate mechanism cannot outperform the GRNN model. Theoretically, the plain GRNN can be regarded as a special case of GRNN, whose parameters are constrained or truncated. As a re- sult, GRNN is a more powerful model which out- performs the plain GRNN. Thus, we mainly focus on the GRNN model in this paper.</p><p>Result Discussion Generally, our model is bet- ter than the previous recursive neural network based models <ref type="bibr">(RecNTN, RAE, MV-RecNN and AdaSent)</ref>, which indicates our model can better model the combinations of features with the FBT and our gating mechanism, even without an exter- nal syntactic tree.</p><p>Although we just use the top layer outputs as the feature for classification, our model still out- performs AdaSent.</p><p>Compared with the CNN based methods <ref type="bibr">(MaxTDNN, DCNN and CNNs)</ref>, our model achieves the comparable performances with much fewer parameters. Although CNN based methods outperform our model on SST-1 and SST-2, the number of parameters 2 of GRNN ranges from 40K to 160K while the number of parameters is about 400K in CNN.  proposed grConv to model sen- tences for machine translation. Unlike our model, grConv uses the DAG structure as the topological structure to model sentences. The number of the internal nodes is n 2 /2, where n is the length of the sentence. <ref type="bibr" target="#b20">Zhao et al. (2015)</ref> uses the same struc- ture to model sentences (called AdaSent), and uti- lizes the information of internal nodes to model sentences for text classification. Unlike grConv and AdaSent, our model uses full binary tree as the topological structure. The number of the in- ternal nodes is 2n in our model. Therefore, our model is more efficient for long sentences. In ad- dition, we just use the top layer neurons for text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Moreover, grConv and AdaSent only exploit one gating mechanism (update gate), which cannot sufficiently model the complicated feature com- binations. Unlike them, our model incorporates two kind of gates and can better model the feature combinations. <ref type="bibr" target="#b7">Hu et al. (2014)</ref> also proposed a similar archi- tecture for matching problems, but they employed the convolutional neural network which might be coarse in modeling the feature combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a gated recursive neu- ral network (GRNN) to recursively summarize the meaning of sentence. GRNN uses full binary tree as the recursive topological structure instead of an external syntactic tree. In addition, we introduce two kinds of gates to model the complicated com- binations of features. In future work, we would like to investigate the other gating mechanisms for better modeling the feature combinations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Here, we use the popular open source implementation of PV in Gensim 1 . Methods in the third block are CNN based models. Kim (2014) reports 4 different CNN models using max-over-time pooling, where CNN-non-static and CNN-multichannel are more sophisticated. MaxTDNN sentence model is based on the architecture of the Time-Delay Neural Network (TDNN) (Waibel et al., 1989; Collobert and Weston, 2008). Dynamic convo- lutional neural network (DCNN) (Kalchbrenner et al., 2014) uses the dynamic k-max pooling operator as a non-linear sub-sampling function, in which the choice of k depends on the length of given sentence. Methods in the fourth block are RecNN based models. Recursive Neural Tensor Network (RecNTN) (Socher et al., 2013b) is an extension of plain RecNN, which also depends on a external syntactic structure. Recursive Autoencoder (RAE) (Socher et al., 2011) learns the representations of sentences by minimizing the reconstruction error. Matrix-Vector Recursive Neural Network (MV-RecNN) (Socher et al., 2012) is a extension of RecNN by assigning a vector and a matrix to every node in the parse tree. AdaSent (Zhao et al., 2015) adopts recursive neural network using DAG structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Hyper-parameter settings.</head><label>1</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/sentiment 2 http://cogcomp.cs.illinois.edu/Data/ QA/QC/</note>

			<note place="foot" n="2"> We only take parameters of network into account, leaving out word embeddings.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), Na-tional High Technology Research and Develop-ment Program of China (2015AA015408), Shang-hai Science and Technology Development Funds (14ZR1403200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing using two heterogeneous gated recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan B Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks. Acoustics, Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">Self-adaptive hierarchical sentence model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
