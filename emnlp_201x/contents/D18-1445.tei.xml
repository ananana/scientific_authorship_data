<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4120" to="4130"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4120</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a method to perform automatic document summarisation without using reference summaries. Instead, our method interactively learns from users&apos; preferences. The merit of preference-based interactive sum-marisation is that preferences are easier for users to provide than reference summaries. Existing preference-based interactive learning methods suffer from high sample complexity , i.e. they need to interact with the oracle for many rounds in order to converge. In this work, we propose a new objective function, which enables us to leverage active learning, preference learning and reinforcement learning techniques in order to reduce the sample complexity. Both simulation and real-user experiments suggest that our method significantly advances the state of the art. Our source code is freely available at https://github.com/UKPLab/ emnlp2018-april.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid growth of text-based information on the Internet, automatic document summarisa- tion attracts increasing research attention from the Natural Language Processing (NLP) community <ref type="bibr" target="#b16">(Nenkova and McKeown, 2012</ref>). Most existing document summarisation techniques require ac- cess to reference summaries to train their systems. However, obtaining reference summaries is very expensive: Lin (2004) reported that 3,000 hours of human effort were required for a simple evalu- ation of the summaries for the Document Under- standing Conferences (DUC). Although previous work has proposed heuristics-based methods to summarise without reference summaries <ref type="bibr" target="#b22">(Ryang and Abekawa, 2012;</ref><ref type="bibr" target="#b21">Rioux et al., 2014</ref>), the gap between their performance and the upper bound is still large: the ROUGE-2 upper bound of .212 on DUC'04 (P.V.S. and Meyer, 2017) is, for example, twice as high as <ref type="bibr">Rioux et al.'s (2014)</ref> .114.</p><p>The Structured Prediction from Partial Infor- mation (SPPI) framework has been proposed to learn to make structured predictions without ac- cess to gold standard data ( <ref type="bibr" target="#b26">Sokolov et al., 2016b)</ref>. SPPI is an interactive NLP paradigm: It inter- acts with a user for multiple rounds and learns from the user's feedback. SPPI can learn from two forms of feedback: point-based feedback, i.e. a numeric score for the presented prediction, or preference-based feedback, i.e. a preference over a pair of predictions. Providing preference-based feedback yields a lower cognitive burden for hu- mans than providing ratings or categorical labels <ref type="bibr" target="#b29">(Thurstone, 1927;</ref><ref type="bibr" target="#b8">Kendall, 1948;</ref><ref type="bibr" target="#b9">Kingsley and Brown, 2010;</ref><ref type="bibr" target="#b33">Zopf, 2018)</ref>. Preference-based SPPI has been applied to multiple NLP applications, in- cluding text classification, chunking and machine translation ( <ref type="bibr" target="#b25">Sokolov et al., 2016a;</ref><ref type="bibr" target="#b10">Kreutzer et al., 2017)</ref>. However, SPPI has prohibitively high sam- ple complexities in the aforementioned NLP tasks, as it needs at least hundreds of thousands rounds of interaction to make near-optimal predictions, even with simulated "perfect" users. <ref type="figure" target="#fig_0">Figure 1a</ref> illus- trates the workflow of the preference-based SPPI.</p><p>To reduce the sample complexity, in this work, we propose a novel preference-based interactive learning framework, called APRIL (Active Prefer- ence ReInforcement Learning). APRIL goes be- yond SPPI by proposing a new objective func- tion, which divides the preference-based interac- tive learning problem into two phases (illustrated in <ref type="figure" target="#fig_0">Figure 1b</ref>): an Active Preference Learning (APL) phase (the right cycle in <ref type="figure" target="#fig_0">Figure 1b)</ref>, and a Reinforcement Learning (RL) phase (the left cy- cle). We show that this separation enables us to query preferences more effectively and to use the collected preferences more efficiently, so as to re- duce the sample complexity. We apply APRIL to Extractive Multi-Document Summarisation (EMDS). The task of EMDS is to extract sentences from the original documents to build a summary under a length constraint. We accommodate multiple APL and RL techniques in APRIL and compare their performance under different simulation settings. We also compare APRIL to a state-of-the-art SPPI implementation using both automatic metrics and human evalua- tion. Our results suggest that APRIL significantly outperforms SPPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>RL has been previously used to perform EMDS without using reference summaries. <ref type="bibr" target="#b22">Ryang and Abekawa (2012)</ref> formulated EMDS as a Markov Decision Process (MDP), designed a heuristics- based reward function considering both informa- tion coverage rate and redundancy level, and used the Temporal Difference (TD) algorithm <ref type="bibr" target="#b28">(Sutton, 1984)</ref> to solve the MDP. In a follow-up work, <ref type="bibr" target="#b21">Rioux et al. (2014)</ref> proposed a different reward func- tion, which also did not require reference sum- maries; their experiments suggested that using their new reward function improved the summary quality. <ref type="bibr" target="#b6">Henß et al. (2015)</ref> proposed a different RL formulation of EMDS and jointly used super- vised learning and RL to perform the task. How- ever, their method requires the access to reference summaries. More recent works applied encoder- decoder-based RL to document summarisation ( <ref type="bibr" target="#b20">Ranzato et al., 2015;</ref><ref type="bibr" target="#b15">Narayan et al., 2018;</ref><ref type="bibr" target="#b18">Paulus et al., 2017;</ref><ref type="bibr" target="#b17">Pasunuru and Bansal, 2018)</ref>. These works outperformed standard encoder-decoder as RL can directly optimise the ROUGE scores and can tackle the exposure bias problems. However, these neural RL methods all used ROUGE scores as their rewards, which in turn relied on reference summaries. APRIL can accommodate these neu- ral RL techniques in its RL phase by using a rank- ing of summaries instead of the ROUGE scores as rewards. We leave neural APRIL for future study. P.V.S. and Meyer (2017) proposed a bigram- based interactive EMDS framework. They asked users to label important bigrams in candidate sum- maries and used integer linear programming (ILP) to extract sentences covering as many important bigrams as possible. Their method requires no ac- cess to reference summaries, but it requires con- siderable human effort during the interaction: in simulation experiments, their system needed to collect up to 350 bigram annotations from a (simu- lated) user. In addition, they did not consider noise in users' annotations but simulated perfect oracles.</p><p>Preference learning aims at obtaining the rank- ing (i.e. total ordering) of objects from pairwise preferences <ref type="bibr">(Fürnkranz and Hüllermeier, 2010)</ref>. <ref type="bibr" target="#b24">Simpson and Gurevych (2018)</ref> proposed to use an improved Gaussian process preference learning (Chu and <ref type="bibr" target="#b4">Ghahramani, 2005</ref>) for learning to rank arguments in terms of convincingness from crowd- sourced annotations. However, such Bayesian methods can hardly scale and suffer from high computation time. <ref type="bibr" target="#b33">Zopf (2018)</ref> recently proposed to learn a sentence ranker from preferences. The resulting ranker can be used to identify the impor- tant sentences and thus to evaluate the quality of the summaries. His study also suggests that pro- viding sentence preferences takes less time than writing reference summaries. APRIL not only learns a ranking over summaries from pairwise preferences, but also uses the ranking to "guide" our RL agent to generate good summaries.</p><p>There is a recent trend in machine learning to combine active learning, preference learning and RL, for learning to perform complex tasks from preferences ( <ref type="bibr" target="#b31">Wirth et al., 2017</ref>). The resulting al- gorithm is termed Preference-based RL (PbRL), and has been used in multiple applications, includ- ing training robots ( <ref type="bibr" target="#b32">Wirth et al., 2016)</ref> and Atari- playing agents ( <ref type="bibr" target="#b3">Christiano et al., 2017</ref>). SPPI and APRIL can both be viewed as PbRL algorithms. But unlike most PbRL methods that learn a utility function of the predictions (in EMDS, predictions are summaries) to guide the RL agent, APRIL is able to directly use a ranking of predictions to guide the RL agent without making assump- tions about the underlying structure of the utility functions. This also enables APRIL to use non- utility-based preference learning techniques (e.g., <ref type="bibr" target="#b13">Maystre and Grossglauser, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we recap necessary details of SPPI, RL and preference learning, and adapt them to the EMDS use case, laying the foundation for APRIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The SPPI Framework</head><p>Let X be the input space and let Y(x) be the set of possible outputs for input x ∈ X . In EMDS, x ∈ X is a cluster of documents and Y(x) is the set of all possible summaries for cluster x. The function ∆ x : Y(x)×Y(x) → {0, 1} is the prefer- ence function such that ∆ x (y i , y j ) = 1 if the user believes y j is better than y i (denoted by y j y i or equivalently y i y j ), and 0 otherwise. Through- out this paper we assume that users do not equally prefer two different items. For a given x, the ex- pected loss is:</p><formula xml:id="formula_0">L SPPI (w|x) = E pw(y i ,y j |x) [∆ x (y i , y j )] = y i ,y j ∈Y(x) ∆ x (y i , y j ) p w (y i , y j |x),<label>(1)</label></formula><p>where p w (y i , y j |x) is the probability of querying the pair (y i , y j ). Formally,</p><formula xml:id="formula_1">p w (y i , y j |x) = exp[w (φ(y i |x) − φ(y j |x))] yp,yq∈Y(x) exp[w (φ(y p |x) − φ(y q |x))] ,<label>(2)</label></formula><p>where φ(y|x) is the vector representation of y given x, and w is the weight vector to be learnt. Eq. (2) is a Gibbs sampling strategy: w (φ(y i |x)− φ(y j |x)) can be viewed as the "utility gap" be- tween y i and y j . The sampling strategy p w en- courages querying pairs with large utility gaps. To minimise L SPPI , SPPI uses gradient descent to update w incrementally. Alg. 1 presents the pseudo code of our adaptation of SPPI to EMDS. In the supplementary material, we provide a de- tailed derivation of w L SPPI (w|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reinforcement Learning</head><p>RL amounts to efficient algorithms for searching optimal solutions in MDPs. MDPs are widely Input : sequence of learning rates γ t ; query budget T ; document cluster x initialise w 0 ; while t = 0 . . . T do sample (y i , y j ) according to Eq. (2); obtain feedback ∆ x (y i , y j );</p><formula xml:id="formula_2">w t+1 := w t − γ t w L SPPI (w|x) end Output: y * = arg max y∈Y (x) w</formula><p>T +1 φ(y, x) Algorithm 1: SPPI for preference-based in- teractive document summarisation (adjusted from Alg. 2 in ( <ref type="bibr" target="#b25">Sokolov et al., 2016a)</ref>).</p><p>used to formulate sequential decision making problems, which EMDS falls into: in EMDS, the summariser has to sequentially select sentences from the original documents and add them to the draft summary. An (episodic) MDP is a tuple (S, A, P, R, T ). S is the set of states, A is the set of actions, P : S × A × S → R is the transition function with P (s |s, a) yielding the probability of performing action a in state s and being transited to a new state s . R : S × A → R is the reward function with R(s, a) giving the immediate reward for performing action a in state s. T ⊆ S is the set of terminal states; visiting a terminal state termi- nates the current episode.</p><p>In EMDS, we follow the same MDP formu- lation as Ryang and Abekawa (2012) and <ref type="bibr" target="#b21">Rioux et al. (2014)</ref>. Given a document cluster, a state s is a draft summary, A includes two types of ac- tions, concatenate a new sentence to the current draft summary, or terminate the draft summary construction. The transition function P in EMDS is trivial because given the current draft summary and an action, the next state can be easily inferred. The reward function R returns an evaluation score of the summary once the action terminate is per- formed; otherwise it returns 0 because the sum- mary is still under construction and thus not ready to be evaluated. Providing non-zero rewards be- fore the action terminate can lead to even worse result, as reported by <ref type="bibr" target="#b21">Rioux et al. (2014)</ref>.</p><p>A policy π : S × A → R in an MDP defines how actions are selected: π(s, a) is the probability of selecting action a in state s. In EMDS, a policy corresponds to a strategy to build summaries for a given document cluster. We let Y π (x) be the set of all possible summaries the policy π can construct in the document cluster x, and we slightly abuse the notation by letting π(y|x) denote the probabil-ity of policy π generating a summary y in cluster x. Then the expected reward of a policy is:</p><formula xml:id="formula_3">R RL (π|x) = E y∈Yπ(x) R(y|x) = y∈Yπ(x) π(y|x)R(y|x),<label>(3)</label></formula><p>where R(y|x) is the reward for summary y in doc- ument cluster x. The goal of an MDP is to find the optimal policy π * that has the highest expected re- ward: π * = arg max π R RL (π).</p><p>Note that the loss function in SPPI (Eq. <ref type="formula" target="#formula_0">(1)</ref>) and the expected reward function in RL (Eq. <ref type="formula" target="#formula_3">(3)</ref>) are in similar forms: if we view the pair selection proba- bility p w in Eq. <ref type="formula" target="#formula_1">(2)</ref> as a policy, and view the pref- erence function ∆ x in Eq. <ref type="formula" target="#formula_0">(1)</ref> as a negative reward function, we can view SPPI as an RL problem. The major difference between SPPI and RL is that SPPI selects and evaluates pairs of outputs, while RL selects and evaluates single outputs. We will exploit their connection to propose our new objec- tive function and the APRIL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Preference Learning</head><p>The linear Bradley-Terry (BT) model <ref type="bibr" target="#b2">(Bradley and Terry, 1952</ref>) is one of the most widely used methods in preference learning. Given a set of items Y, suppose we have observed T preferences: Q = {q 1 (y 1,1 , y 1,2 ), · · · , q T (y T,1 , y T,2 )}, where y i,1 , y i,2 ∈ Y, and q i ∈ {{, } is the oracle's preference in the i th round. The BT model min- imises the following cross-entropy loss:</p><formula xml:id="formula_4">L BT (w) = − q i (y i,1 ,y i,2 )∈Q [ µ i,1 log P w (y i,1 y i,2 ) + µ i,2 log P w (y i,2 y i,1 ) ], (4) where P w (y i y j ) = (1 + exp[w (φ(y j ) − φ(y i ))]) −1</formula><p>, and µ i,1 and µ i,2 indicate the direc- tion of preferences: if y i,1 y i,2 then µ i,1 = 1 and µ i,2 = 0. Let w * = arg min w L BT (w), then w * can be used to rank all items in Y: for any y i , y j ∈ Y, the ranker prefers y i over y j if</p><formula xml:id="formula_5">w * φ(y i ) &gt; w * φ(y j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APRIL: Decomposing SPPI into Active</head><p>Preference Learning and RL A major problem of SPPI is its high sample com- plexity. We believe this is due to two reasons. First, SPPI's sampling strategy is inefficient: From Eq. <ref type="formula" target="#formula_1">(2)</ref> we can see that SPPI tends to select pairs with large quality gaps for querying the user. This strategy can quickly identify the relatively good and relatively bad summaries, but needs many rounds of interaction to find the top summaries. Second, SPPI uses the collected preferences inef- fectively: In Alg. 1, each preference is used only once for performing the gradient descent update and is forgotten afterwards. SPPI does not gener- alise or re-use collected preferences, wasting the useful and expensive information. These two weaknesses of SPPI motivate us to propose a new learning paradigm that can query and generalise preferences more efficiently. Re- call that in EMDS, the goal is to find the optimal summary for a given document cluster x, namely the summary that is preferred over all other pos- sible summaries in Y(x). Based on this under- standing, we define a new expected reward func- tion R APRIL for policy π as follows:</p><formula xml:id="formula_6">R APRIL (π|x) = E y j ∼π [ 1 |Y(x)| y i ∈Y(x) ∆ x (y i , y j )] = 1 |Y(x)| y j ∈Yπ(x) π(y j |x) y i ∈Y(x) ∆ x (y i , y j ) = y∈Yπ(x) π(y|x) r(y|x),<label>(5)</label></formula><p>where r(y|x) = y i ∈Y(x) ∆ x (y i , y j )/|Y(x)|. Note that ∆ x (y i , y j ) equals 1 if y j is preferred over y i and equals 0 otherwise (see §3.1). Thus, r(y|x) is the relative position of y in the (ascend- ing) sorted Y(x), and it can be approximated by preference learning. The use of preference learn- ing enables us to generalise the observed prefer- ences to a ranker (see §3.3), allowing more ef- fective use of the collected preferences. Also, we can use active learning to select summary pairs for querying more effectively. In addition, the resem- blance of R APRIL and RL's reward function R RL (in Eq. <ref type="formula" target="#formula_3">(3)</ref>) enables us to use a wide range of RL algorithms to maximise R APRIL (see §2).</p><p>Based on the new objective function, we split the preference-based interactive learning into two phases: an Active Preference Learning (APL) phase (the right cycle in <ref type="figure" target="#fig_0">Fig. 1b)</ref>, responsible for querying preferences from the oracle and approxi- mating the ranking of summaries, and an RL phase (the left cycle in <ref type="figure" target="#fig_0">Fig. 1b)</ref>, responsible for learning to summarise based on the learned ranking. The resulting framework APRIL allows for integrating any active preference learning and RL techniques. Note that only the APL phase is online (i.e. in-Input : query budget T ; document cluster x;</p><p>RL episode budget N /* Phase 1: active preference learning */ while t = 0 . . . T do sample a summary pair (y i , y j ) using any APL strategy; obtain feedback ∆ x (y i , y j ); update ranker according to <ref type="bibr">Eq. (4)</ref> ; end /* Phase 2: RL-based summarisation */ initialise an arbitrary policy π 0 ; while n = 0 . . . N do evaluate policy π n according to Eq. <ref type="formula" target="#formula_6">(5)</ref>; update policy π n using any RL algorithm; end Output: volving humans in the loop) while the RL phase can be performed offline, helping to improve the real-time responsiveness. Also, the learned ranker can provide an unlimited number of rewards (i.e. r(y|x) in Eq. <ref type="formula" target="#formula_6">(5)</ref>) to the RL agent, enabling us to perform many episodes of RL training with a small number of collected preferences -unlike in SPPI where each collected preference is used to train the system for one round and is forgotten afterwards. Alg. 2 shows APRIL in pseudo code.</p><formula xml:id="formula_7">y * = arg max y∈Yπ N (x) π N (y|x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets. We perform experiments on DUC '04 to find the best performing APL and RL tech- niques. Then we combine the best-performing APL and RL to complete APRIL and compare it against SPPI on the DUC '01, DUC '02 and DUC '04 datasets. 1 Some statistics of these datasets are summarised in <ref type="table">Table 1</ref>.</p><p>Simulated Users. Existing preference-based in- teractive learning techniques assume that the or- acle has an intrinsic evaluation function U * and provides preferences consistent with U * by prefer- ring higher valued candidates. We term this a Per-</p><formula xml:id="formula_8">1 http://duc.nist.gov/</formula><p>fect Oracle (PO). We believe that assuming a PO is unrealistic for real-world applications, because sometimes real users tend to misjudge the prefer- ence direction, especially when the presented can- didates have similar quality. In this work, besides PO, we additionally consider two types of noisy oracles based on the user-response models pro- posed by <ref type="bibr" target="#b30">Viappiani and Boutilier (2010)</ref>:</p><p>• Constant noisy oracle (CNO): with prob- ability c ∈ [0, 1], this oracle randomly se- lects which summary is preferred; otherwise it provides preferences consistent with U * . We consider CNOs with c = 0.1 and c = 0.3.</p><p>• Logistic noisy oracle (LNO): for two sum- maries y i and y j in cluster x, the or- acle prefers y i over y j with probability</p><formula xml:id="formula_9">p U * (y i y j |x; m) = (1 + exp[(U * (y j |x) − U * (y i |x))/m]) −1 .</formula><p>This oracle reflects the in- tuition that users are more likely to misjudge the preference direction when two summaries have similar quality. Note that the parame- ter m ∈ R + controls the "noisiness" of the user's responses: higher values of m result in a less steep sigmoid curve, and the result- ing oracle is more likely to misjudge. We use LNOs with m = 0.3 and m = 1.</p><p>As for the intrinsic evaluation function U * , re- cent work has suggested that human preferences over summaries have high correlations to ROUGE scores <ref type="bibr" target="#b33">(Zopf, 2018)</ref>. Therefore, we define:</p><formula xml:id="formula_10">U * (y|x) = R 1 (y|x) 0.47 + R 2 (y|x) 0.22 + R S (y|x) 0.18<label>(6)</label></formula><p>where R 1 , R 2 and R S stand for ROUGE-1, ROUGE-2 and ROUGE-SU4, respectively. The real values (0.47, 0.22 and 0.18) are used to bal- ance the weights of the three ROUGE scores. We choose them to be around the EMDS upper-bound ROUGE scores reported by P.V.S. and Meyer (2017). As such, an optimal summary's U * value should be around 3.</p><p>Implementation. All code is written in Python and runs on a desktop PC with 8 GB RAM and an i7-2600 CPU. We use NLTK ( <ref type="bibr" target="#b0">Bird et al., 2009</ref>) to perform sentence tokenisation. Our source code is freely available at https://github.com/ UKPLab/emnlp2018-april.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Simulation Results</head><p>We first study the APL phase ( §6.1) and the RL phase ( §6.2)) separately by comparing the perfor-mance of multiple APL and RL algorithms in each phase. Then, in §6.3, we combine the best per- forming APL and RL algorithm to complete Alg. 2 and compare APRIL against SPPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">APL Phase Performance</head><p>Recall that the task of APL is to output a ranking of all summaries in a cluster. In this subsection, we test multiple APL techniques and compare the quality of their resulting rankings. Two metrics are used: Kendall's τ (Kendall, 1948) and Spearman's ρ <ref type="bibr" target="#b27">(Spearman, 1904)</ref>. Both metrics are valued be- tween −1 and 1, with higher values suggesting higher rank correlation. Because the number of possible summaries in a cluster is huge, instead of evaluating the ranking quality on all possible sum- maries, we evaluate rankings on 10,000 randomly sampled summaries, denotedˆYdenotedˆ denotedˆY(x). During query- ing, all candidate summaries presented to the ora- cle are also selected fromˆYfromˆ fromˆY(x). SamplingˆYSamplingˆ SamplingˆY(x) a priori helps us to reduce the response time to un- der 500 ms for all APL techniques we test. We compare four active learning strategies under two query budgets, T = 10 and T = 100:</p><p>• Random Sampling (RND): Randomly se- lect two summaries fromˆYfromˆ fromˆY(x) to query.</p><p>• SPPI Sampling (SBT): Select summary pairs fromˆYfromˆ fromˆY(x) according to the SPPI strat- egy in Eq. (2). After each round, the weight vector w is updated according to Eq. (4).</p><p>• Uncertainty Sampling (Unc): Query the most uncertain summary pairs. In line with P.V.S. and Meyer (2017), the uncertainty of a summary is evaluated as follows: first, we estimate the probability of a summary y being the optimal summary in cluster x as p opt (y|x) = (1 + exp(−w * t φ(x, y))) −1 , where w * t is the weights learned by the BT model (see §3.3) in round t. Given p opt (y|x), we let the uncertainty score unc(y|x</p><formula xml:id="formula_11">) = 1 − p opt (y|x) if p opt (y|x) ≥ 0.5 and unc(y|x) = p opt (y|x) otherwise.</formula><p>• J&amp;N is the robust query selection algorithm proposed by <ref type="bibr" target="#b7">Jamieson and Nowak (2011)</ref>. It assumes that the items' preferences are de- pendent on their distances to an unknown ref- erence point in the embedding space: the far- ther an item to the reference point, the more preferred the item is. After each round of interaction, the algorithm uses all collected preferences to locate the area where the ref- erence point may fall into, and identify the query pairs which can reduce the size of this area, termed ambiguous query pairs. To com- bat noise in preferences, the algorithm se- lects the most-likely-correct ambiguous pair to query the oracle in each round.</p><p>After all preferences are collected, we obtain the ranker as follows: for any y i , y j ∈ Y(x), the ranker prefers y i over y j if</p><formula xml:id="formula_12">αw * φ(y i |x) + (1 − α)HU (y i |x) &gt; αw * φ(y j |x) + (1 − α)HU (y j |x),<label>(7)</label></formula><p>where w * is the weights vector learned by the BT model (see Eq. <ref type="formula">(4)</ref>), HU is the heuristics-based summary evaluation function proposed by <ref type="bibr" target="#b22">Ryang and Abekawa (2012)</ref>, and α ∈ [0, 1] is a param- eter. The aim of using HU and α is to trade off between the prior knowledge (i.e. heuristics-based HU ) and the posterior observation (i.e. the BT- learnt w * ), so as to combat the cold-start problem. Based on some preliminary experiments, we set α = 0.3 when the query budget is 10, and α = 0.7 when the query budget is 100. The intuition is to put more weight to the posterior with increasing rounds of interaction. More systematic research of α can yield better results; we leave it for future work. For the vector φ(y|x), we use the same bag- of-bigram embeddings as <ref type="bibr" target="#b21">Rioux et al. (2014)</ref>, and we let its length be 200.</p><p>In <ref type="table" target="#tab_0">Table 2</ref>, we compare the performance of the four APL methods on the DUC'04 dataset. The baseline we compared against is the prior rank- ing. We find that Unc significantly 2 outperforms all other APL methods, except when the oracle is LNO-1, where the advantage of Unc to SBT is not significant. Also, both Unc and SBT are able to significantly outperform the baseline un- der all settings. The competitive performance of SBT, especially with LNO-1, is due to its unique sampling strategy: LNO-1 is more likely to mis- judge the preference direction when the presented summaries have similar quality, but SBT has high probability to present summaries with large qual- ity gaps (see Eq. <ref type="formula" target="#formula_1">(2)</ref>), effectively reducing the chance that LNOs misjudge preference directions. However, SBT is more "conservative" compared to Unc because it tends to exploit the existing Baseline, α = 0, T = 0: τ = .206, ρ = .304 <ref type="table" target="#tab_0">Table 2</ref>: Performance of multiple APL algorithms (columns) using different oracles and query bud- gets (rows). The baseline is the purely prior rank- ing. All results except the baseline are averaged over 50 document clusters in DUC'04. Aster- isk: significant advantage over other active learn- ing strategies given the same oracle and budget T .</p><p>ranking to select one good and one bad summary to query, while Unc performs more exploration by querying the summaries that are least confident ac- cording to the current ranking. We believe this ex- plains the strong overall performance of Unc. Additional experiments suggest that when we only use the posterior ranking (i.e. letting α = 1), no APL we test can surpass the baseline when T = 10. Detailed results are presented in the sup- plementary material. This observation reflects the severity of the cold-start problem, confirms the ef- fectiveness of our prior-posterior trade-off mecha- nism in combating cold-start, and indicates the im- portance of tuning the α value (see Eq. <ref type="formula" target="#formula_12">(7)</ref>). This opens up exciting avenues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">RL Phase Performance</head><p>We compare two RL algorithms: TD(λ) <ref type="bibr" target="#b28">(Sutton, 1984)</ref> and LSTD(λ) <ref type="bibr" target="#b1">(Boyan, 1999</ref>). TD(λ) has been used in previous RL-based EMDS work <ref type="bibr" target="#b22">(Ryang and Abekawa, 2012;</ref><ref type="bibr" target="#b21">Rioux et al., 2014</ref>). LSTD(λ) is chosen, because it is an improved TD algorithm and has been used in the state-of-the-art PbRL algorithm by <ref type="bibr" target="#b32">Wirth et al. (2016)</ref>. We let the learning round (see Alg. 2) N = 5, 000, which we found to yield good results in reasonable time (less than 1 minute to generate a summary for one doc- ument cluster). Letting N = 3, 000 will result in a significant performance drop, while increasing N to 10,000 will only bring marginal improvement at the cost of doubling the runtime. The learn-  ing parameters we use for TD(λ) are the same as those by <ref type="bibr" target="#b21">Rioux et al. (2014)</ref>. For LSTD(λ), we let λ = 1 and initialise its square matrix as a diag- onal matrix with random numbers between 0 and 1, as suggested by <ref type="bibr" target="#b11">Lagoudakis and Parr (2003)</ref>. The rewards we use are the U * function introduced in §5. Note that this serves as the upper-bound performance, because U * relies on the reference summaries (see Eq. <ref type="formula" target="#formula_10">(6)</ref>), which are not available in the interactive setting. As a baseline, we also present the upper-bound performance of integer linear programming (ILP) reported by P.V.S. and Meyer (2017), optimised for bigram coverage. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of RL and ILP on the DUC'04 dataset. TD(λ) significantly out- performs LSTD(λ) in terms of all ROUGE scores we consider. Although the least-square RL algo- rithms (which LSTD belongs to) have been proved to achieve better performance than standard TD methods in large-scale problems (see <ref type="bibr" target="#b11">Lagoudakis and Parr, 2003)</ref>, their performance is sensitive to many factors, e.g., initialisation values in the di- agonal matrix, regularisation parameters, etc. We note that a similar observation about the inferior performance of least-square RL in EMDS is re- ported by <ref type="bibr" target="#b21">Rioux et al. (2014)</ref>. TD(λ) also significantly outperforms ILP in terms of all metrics except ROUGE-2. This is not surprising, because the bigram-based ILP is opti- mised for ROUGE-2, whereas our reward function U * considers other metrics as well (see Eq. <ref type="formula" target="#formula_10">(6)</ref>). Since ILP is widely used as a strong baseline for EMDS, these results confirm the advantage of us- ing RL for EMDS problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Complete Pipeline Performance</head><p>Finally, we combine the best techniques of the APL and RL phase (namely Unc and TD(λ), re- spectively) to complete APRIL, and compare it against SPPI. As a baseline, we use the heuristic- based rewards HU to train both TD(λ) (ranking- based training, i.e. using HU to produce r(y|x) in Eq. <ref type="formula" target="#formula_6">(5)</ref> to train) and SPPI (preference-based train- ing, i.e. using HU for generating pairs to train <ref type="table" target="#tab_0">Oracle  Method  T  R1  R2  RL  RSU4  R1  R2  RL  RSU4  R1  R2  RL</ref>    SPPI) for up to 5,000 episodes. The baseline re- sults are presented in the bottom rows of <ref type="table" target="#tab_5">Table 4</ref>.</p><note type="other">DUC '01 DUC '02 DUC '04</note><p>We make the following observations from Ta- ble 4. (i) Given the same oracle, the performance of APRIL with 10 rounds of interaction is com- parable or even superior than that of SPPI after 100 rounds of interaction (see boldface in <ref type="table" target="#tab_5">Table  4</ref>), suggesting the strong advantage of APRIL to reduce sample complexity. (ii) APRIL can sig- nificantly improve the baseline with either 10 or 100 rounds of interaction, but SPPI's performance can be even worse than the baseline (marked by † in <ref type="table" target="#tab_5">Table 4</ref>), especially under the high-noise low- budget settings (i.e., CNO-0.3, LNO-0.3, and LNO-1 with T = 10). This is because SPPI lacks a mechanism to balance between prior and poste- rior ranking, while APRIL can adjust this trade-off by tuning α (Eq. <ref type="formula" target="#formula_12">(7)</ref>). This endows APRIL with better noise robustness and lower sample com- plexity in high-noise low-budget settings. Note that the above observations also hold for the other two datasets, indicating the consistently strong performance of APRIL across different datasets.</p><p>As for the overall runtime, when budget T = 100, APRIL on average takes 2 minutes to interact with an oracle and output a summary, while SPPI takes around 15 minutes due to its expensive gra- dient descent computation (see §3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Human Evaluation</head><p>Finally, we invited real users to compare and eval- uate the quality of the summaries generated by SPPI and APRIL. We randomly selected three top- ics (d19 from DUC'01, d117i from DUC'02 and d30042 from DUC'04), and let both SPPI and our best-performing APRIL interact with PO for 10 rounds on these topics. The resulting 100-word summaries, shown in <ref type="figure" target="#fig_2">Figure 2</ref>, were presented to seven users, who had already read two background texts to familiarize with the topic. The users were asked to provide their preference on the presented Topic d30042 (DUC'04), SPPI: After meeting Libyan leader Moammar Gad- hafi in a desert tent, U.N. Secretary-General Kofi Annan said he thinks an ar- rangement for bringing two suspects to trial in the bombing of a Pan Am air- liner could be secured in the "not too distant future." TRIPOLI, Libya (AP) U.N. Secretary-General Kofi Annan arrived in Libya Saturday for talks aimed at bringing to trial two Libyan suspects in the 1988 Pan Am bombing over Lockerbie, Scotland. Secretary General Kofi Annan said Wednesday he was extending his North African tour to include talks with Libyan authorities. An- nan's one-day, 2nd graf pvs During his Algerian stay, Topic d30042 (DUC'04), APRIL: TRIPOLI, Libya (AP) U.N. Secretary- General Kofi Annan arrived in Libya Saturday for talks aimed at bringing to trial two Libyan suspects in the 1988 Pan Am bombing over Lockerbie, Scot- land. Annan's one-day visit to meet with Libyan leader Col. Moammar Gadhafi followed reports in the Libyan media that Gadhafi had no authority to hand over the suspects. The 60-year-old Annan is trying to get Libya to go along with a U.S.-British plan to try the two suspects before a panel of Scottish judges in the Netherlands for the <ref type="bibr">Dec. 21, 1988,</ref>  Topic d19 (DUC'01), SPPI: The issue cuts across partisan lines in the Senate, with Minority Leader Bob Dole (R-Kan.) arguing against the White House po- sition on grounds that including illegal aliens in the census is unfair to Amer- ican citizens.. Loss of Seats Cited. Shelby's amendment says only that the secretary is to "make such adjustments in total population figures as may be necessary, using such methods and procedures as the secretary determines fea- sible and appropriate" to keep illegal aliens from being counted in congres- sional reapportionment. "Some states will lose congressional seats because of illegal aliens," Dole argued. But there's nothing simple about it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>bombing over Lockerbie, Scotland. Sirte is 400 kilometers (250 miles) east of the Libyan capital Tripoli. During his</head><p>Topic d19 (DUC'01), APRIL: In a blow to California and other states with large immigrant populations, the Senate voted Friday to bar the Census Bu- reau from counting illegal aliens in the 1990 population count. But the Senate already has voted to force the Census Bureau to exclude illegal immigrants in preparing tallies for congressional reapportionment. said that Georgia and Indi- ana both lost House seats after the 1980 Census, and California and New York- centers of illegal immigration-each gained seats. A majority of the members of the House of Representatives has signaled support. The national head count will be taken April 1, 1990. In all three topics, all users prefer the APRIL- generated summaries over the SPPI-generated summaries. <ref type="table" target="#tab_6">Table 5</ref> shows the users' ratings. The APRIL-generated summaries consistently receive higher ratings. These results are consistent with our simulation experiments and confirm the sig- nificant advantage of APRIL over SPPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We propose a novel preference-based interactive learning formulation named APRIL (Active Pref- erence ReInforcement Learning), which is able to make structured predictions without referring to the gold standard data. Instead, APRIL learns from preference-based feedback. We designed a novel objective function for APRIL, which natu- rally splits APRIL into an active preference learn- ing (APL) phase and a reinforcement learning (RL) phase, enabling us to leverage a wide spec- trum of active learning, preference learning and RL algorithms to maximise the output quality with a limited number of interaction rounds. We ap- plied APRIL to the Extractive Multi-Document Summarisation (EMDS) problem, simulated the users' preference-giving behaviour using multiple user-response models, and compared the perfor- mance of multiple APL and RL techniques. Sim- ulation experiments indicated that APRIL signif- icantly improved the summary quality with just 10 rounds of interaction (even with high-noise oracles), and significantly outperformed SPPI in terms of both sample complexity and noise robust- ness. Human evaluation results suggested that real users preferred the APRIL-generated summaries over the SPPI-generated ones.</p><p>We identify two major lines of future work. On the technical side, we plan to employ more ad- vanced APL and RL algorithms in APRIL, such as sample-efficient Bayesian-based APL algorithms (e.g., <ref type="bibr" target="#b24">Simpson and Gurevych, 2018)</ref> and neural RL algorithms (e.g. <ref type="bibr" target="#b14">Mnih et al., 2015)</ref> to further reduce the sample complexity of APRIL. On the experimental side, a logical next step is to imple- ment an interactive user interface for APRIL and conduct a larger evaluation study comparing the summary quality before and after the interaction. We also plan to apply APRIL to more NLP appli- cations, including machine translation, informa- tion exploration and semantic parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A comparison of workflows of SPPI (a) and APRIL (b) in the EMDS use case. Notation details, e.g., ∆ x and r(y n ), are discussed in §3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Algerian stay, Topic d117i (DUC'02), SPPI: The Booker Prize is sponsored by Booker, an international food and agriculture business. The novel, a story of Scottish low- life narrated largely in Glaswegian dialect, is unlikely to prove a popular choice with booksellers, who have damned all six books shortlisted for the prize as boring, elitist and-worst of all-unsaleable. The shortlist of six for the Pounds 20,000 Booker Prize for fiction, announced yesterday, immediately prompted the question 'Who ? ' Japanese writer Kazuo Ishiguro won the 1989 Booker Prize, Britain's top literary award, for his novel "The Remains of the Day," judges announced Thursday. He didn't win. Topic d117i (DUC'02), APRIL: Australian novelist Peter Carey was awarded the coveted Booker Prize for fiction Tuesday night for his love story, "Oscar and Lucinda." The Booker Prize is sponsored by Booker, an international food and agriculture business, and administered by The Book Trust. British publish- ers can submit three new novels by British and Commonwealth writers. Six novels have been nominated for the Booker Prize, Britain's most prestigious fiction award, and bookmakers say the favorite is "The Remains of the Day" by Japanese author Kazuo Ishiguro. On the day of the Big Event, Ladbroke, the large British betting agency, posted the final odds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Summaries generated by SPPI and APRIL used in the human evaluation experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Algorithm 2 : Pseudo code of APRIL</head><label>2</label><figDesc></figDesc><table>Dataset 
Lang # Topic # Doc # Token/Doc 

DUC '01 
EN 
30 
308 
781 
DUC '02 
EN 
59 
567 
561 
DUC '04 
EN 
50 
500 
587 

Table 1: Statistics of the datasets. The target sum-
mary length is 100 tokens in all three datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Upper-bound performance comparison. 
Results are averaged over all clusters in DUC'04. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of APRIL and SPPI. All results are averaged over all clusters in each dataset. 
Baselines: HU -trained SPPI and TD(λ), without any interaction (i.e. T = 0). Boldface: Comparable 
(i.e. no significant gaps exist) or significantly better than SPPI with 100 rounds of interaction, under the 
same oracle. Superscript  †: Comparable or significantly worse than the corresponding baseline. 

DUC'01 
DUC'02 
DUC'04 
Overall 

APRIL 3.57±.30 4.14±.14 3.86±.40 3.86±.17 
SPPI 
2.29±.29 2.14±.14 3.14±.34 2.52±.18 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Human ratings for the summaries gener-
ated by APRIL and SPPI (mean±standard error). 

</table></figure>

			<note place="foot" n="2"> In this paper we use double-tailed student t-test to compute p-values, and we let significance level be p &lt; 0.01.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by the German Research Foundation as part of the QA-EduInf project (grant GU 798/18-1 and grant RI 803/12-1). We thank the researchers and students from TU Darmstadt who participated in our human evaluation experiments. We also thank Johannes Fürnkranz, Christian Wirth and the anonymous re-viewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Least-squares temporal difference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999)</title>
		<meeting>the Sixteenth International Conference on Machine Learning (ICML 1999)<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06-27" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. The method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milton</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning from Human Preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miljan</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: 31st Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="4302" to="4310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Preference learning with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005)</title>
		<meeting><address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08-711" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Preference learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preference Learning</title>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach for adaptive single-and multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Henß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Mieskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2015)</title>
		<meeting>the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2015)<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-30" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
		<respStmt>
			<orgName>University of Duisburg-Essen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active ranking using pairwise comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12-12" />
			<biblScope unit="page" from="2240" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rank correlation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">C</forename><surname>Griffin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Preference uncertainty, preference refinement and paired comparison choice experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas C</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Land Economics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="544" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bandit structured prediction for neural sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1503" to="1513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Leastsquares policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Michail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-21" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Just sort it! A simple and effective approach to active preference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Maystre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grossglauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-06" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1747" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey of text summarization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<editor>Charu C. Aggarwal and ChengXiang Zhai</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multireward reinforced summarization with saliency and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="646" to="653" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint optimization of user-desired content in multidocument summaries by learning from user feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V S</forename><surname>Avinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Paper</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1353" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fear the REAPER: A system for automatic multidocument summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Rioux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="681" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Framework of automatic text summarization using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonggi</forename><surname>Ryang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Abekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-12" />
			<biblScope unit="page" from="256" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Finding convincing arguments using scalable bayesian preference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="357" to="371" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning structured predictors from bandit feedback for interactive NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1610" to="1620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic structured prediction under bandit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: 30th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1489" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="101" />
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Temporal Credit Assignment in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Law of Comparative Judgement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louis Leon Thurstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="278" to="286" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimal Bayesian recommendation sets and myopically optimal choice query sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Viappiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2352" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey of preferencebased reinforcement learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riad</forename><surname>Akrour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="4945" to="4990" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Model-free preference-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<idno>Febru- ary 12-17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2222" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating summary quality with pairwise preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018<address><addrLine>LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1687" to="1696" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
