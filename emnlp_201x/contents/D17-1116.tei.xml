<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConStance: Modeling Annotation Contexts to Improve Stance Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Joseph</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Network Science Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Friedland</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Network Science Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hobbs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Network Science Institute</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lazer</surname></persName>
							<email>d.lazer@neu.edu, orentsur@bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Network Science Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Network Science Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Software and Information Systems Engineering</orgName>
								<orgName type="institution">Northeastern University Ben Gurion University of the Negev</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ConStance: Modeling Annotation Contexts to Improve Stance Classification</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1115" to="1124"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular , scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis , sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twit-ter, we show that providing too little context can result in noisy and uncertain annotations , whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple an-notators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of base-lines at predicting political stance, while the model&apos;s interpretable parameters shed light on the effects of each context.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When annotators are asked for objective judg- ments about a text (e.g., POS tags), the broader context in which the text is situated is often ir- relevant. However, many NLP tasks focus on in- ference of factors beyond words and syntax. For example, the present work addresses the task of detecting political stance on Twitter. We ask an- notators to determine whether a given Twitter user supports Donald Trump or Hillary Clinton. How- ever, inferring something about a user from a sin- gle tweet that she writes may prove difficult. Prior work on stance has relied on annotations collected this way ( <ref type="bibr" target="#b16">Mohammad et al., 2016b</ref>), but individual tweets do not always contain clear indicators.</p><p>One solution to this issue is to supply the anno- tator with more information about the user. For ex- ample, for the similar task of classifying a Twitter user's political affiliation, <ref type="bibr" target="#b6">Cohen and Ruths (2013)</ref> display the user's last 10 tweets. <ref type="bibr" target="#b19">Nguyen et al. (2013)</ref>, studying gender and age, ask annotators to label users by leveraging all information available in their profile. Thus, researchers have provided a range of contexts (or more broadly, information conditions) to annotators in an attempt to balance annotators' exposure to the data needed for accu- racy with reasonable costs in terms of time, money and cognitive load. However, while scholars routinely make such decisions about what information to show anno- tators, they rarely examine how such decisions ac- tually impact annotations. The first contribution of this paper (Section 3) is to show that, at least for political stance detection on Twitter, displaying different kinds of context to annotators yields sig- nificantly different annotations for the same user. As a result of these discrepancies, the accuracy of models trained on these annotations varies widely.</p><p>While it is possible one could select a "best" context for a given task, our results suggest that doing so a priori is difficult and that, moreover, different contexts provide complementary infor- mation. What we would prefer, instead, is a model that learns how contexts affect annotators and combines annotations from multiple contexts to create gold standard labels.</p><p>Fortunately, prior work suggests mechanisms for such a model. Typically in annotation tasks, each item is judged by several annotators, and the resulting labels are aggregated, usually by major- ity vote, to create a gold standard. As an alterna- tive to majority vote, <ref type="bibr" target="#b22">Raykar et al. (2010)</ref> develop an elegant probabilistic approach for learning to aggregate labels produced by annotators of vary- ing quality. Their model jointly estimates gold standard labels (in the form of probability scores), infers annotator error rates, and learns a classifier for use on out-of-sample data.</p><p>Our second contribution (Section 4) is an ex- tension of <ref type="bibr">Raykar et al.'</ref>s model to handle labels not only created by annotators of varying quality, but also produced under information conditions of varying quality. We call this model ConStance <ref type="bibr">1</ref> . Like <ref type="bibr" target="#b22">Raykar et al. (2010)</ref>, who find that even low- quality annotators are useful, we find that low- quality contexts can be useful. Specifically, we find that the classifier produced from our model performs better than any classifier trained by ma- jority vote from the same labels. Furthermore, the model provides an unsupervised method for com- paring the information conditions by examining their respective error patterns.</p><p>Intuitively, ConStance performs a role analo- gous to boosting for annotations: for an arbitrary task, it permits collection of labels that capture dif- ferent aspects of the instances at hand, then com- bines them automatically to determine which are more reliable and to produce a classifier that takes all this into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotating Political Stance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Political Stance Detection</head><p>Stance detection is defined as the task of determin- ing whether an individual is in favor of, against, or neutral towards a target concept based on the content they have generated ( <ref type="bibr" target="#b16">Mohammad et al., 2016b)</ref>. It is related to but distinct from sentiment analysis: a given document can have negative sen- timent but a positive stance towards a particular target, or vice versa. Further, for stance detec- tion, the target need not be explicitly mentioned. These points are best illustrated via example: the tweet "I hope that the Democrats get destroyed in this election!" has a negative sentiment (towards Democrats), and (therefore, most likely) implies a positive stance towards Donald Trump.</p><p>As a case study for how context impacts an- notations, we focus on political stance detection on Twitter-specifically, determining stance to- wards Hillary Clinton and Donald Trump during the 2016 U.S. election season. This task illus- trates the challenges of annotation, since individ- ual tweets are often ambiguous with respect to stance, contexts on Twitter are inherently frac- tured, and differing contexts can make annotators lean in different directions.</p><p>Note that a user's stance, as we use the term in this paper, is a latent (and stable) property of the user. However, short of interviewing the user, we can never be completely certain of her stance. As such, the examples here and evaluations later rely on the authors' best estimates of stance, using all available information. A user's tweets, in turn, may or may not reveal her stance. This means that, by our definitions, an annotator might accurately perceive no stance in a tweet, yet have their annotation be considered incorrect with respect to the user's true stance. We would consider this case an annotator error caused by lack of context. As examples of the task, consider annotating the following three tweets: (i) "crooked Hillary -#lock- HerUp," (ii) "Lester thinks he can control the crowd when he can't even keep Trump on topic lmao," and (iii) "Set- tling in for #debatenight Hoping to hear an adult conversa- tion." In the case of (i), a passing familiarity with American politics gives us high confidence that the author is pro-Trump. The tweets in (ii) and (iii) are more ambiguous, but the authors' stances become clearer with access to varying forms of context. For (ii), a Pepe the frog image (a sym- bol used by the American alt-right movement) in the user profile reveals that the user is probably a Trump supporter. Similarly, for (iii), a profile description that reads "Stereotypical Iowan who enjoys Hillary Clinton, progressive politics. Chair of CYDIWomen.</p><p>Previously @HillaryForIA and @NARAL." suggests sup- port for Clinton and distaste for Trump.</p><p>In order to explore the effects on annotation quality of providing these kinds of context to an- notators, we crowd-source annotations for a set of tweets and vary the additional information pro- vided to annotators. For ease of comparison with related work and within our own study, we asso- ciate each user with a single anchor tweet. Thus, both annotators and classifiers are asked to deter-mine the stance of a user using data from one par- ticular time window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data</head><p>We collected tweets during the general election season (7/29/2016-11/7/2016) from over 40,000 Twitter users we had previously matched to voter registration records. Matching Twitter users to voter registrations (using methods similar to <ref type="bibr" target="#b0">Barberá, 2016;</ref><ref type="bibr" target="#b12">Hobbs et al., 2017)</ref> helps ensure that the accounts we study are controlled by humans, and it supplies additional demographic variables: gender, race and party registered with.</p><p>We identified as a political tweet any tweet that mentioned the official handle for Donald Trump (@realDonaldTrump) or Hillary Clinton (@HillaryClinton), or that contained one or more of the following terms or hashtags: Hillary, Clin- ton, Trump, Donald, #maga, #imwithher, #de- batenight, #election2016, #electionnight. We re- moved all reply tweets, quote tweets and tweets that directly retweeted the candidates. Finally, we kept only those users who posted at least three po- litical tweets.</p><p>From these users, we sampled 562 political tweets for crowd-sourced stance annotation, se- lecting at most one tweet per user. These tweets were all sampled from users who were registered Democrats or Republicans. Half the tweets were paired with Hillary Clinton as the target, the other half with Donald Trump. We also sampled and set aside an additional 250 + 318 tweet/target pairs to use as development and validation data, respec- tively (see Section 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Annotation Task</head><p>We used Amazon Mechanical Turk (AMT) for an- notation. Annotators were presented a triplet of {tweet, target, context} and were asked to make their decisions on a 5-point Likert scale, ranging from "Definitely Opposes <ref type="bibr">[target]</ref>" to "Definitely Supports [target]". Both prior work ( <ref type="bibr" target="#b16">Mohammad et al., 2016b</ref>) and our pilot studies suggested con- fusion between options for a tweet's irrelevance towards a target and the tweet's neutrality towards the target, so we used the center of the scale for both options. For this paper, we use a narrower three-point scale formed by merging the "Defi- nitely" and "Probably" options.</p><p>Further, while tweets were annotated with re- spect to different targets, we combine all anno- tations into a single task by assuming that "anti-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>Displays the anchor tweet plus . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No Context</head><p>No additional information <ref type="table">Partial Profile  Profile image, name, and handle   Full Profile  Author's profile image, name, han- dle, and description  Previous  Tweets</ref> Author's two most recent tweets in general prior to the anchor (Previous) Po- litical Tweets</p><p>Author's two most recent political tweets prior to the anchor Political Party Political affiliation (if any) from the author's voter registration <ref type="table">Table 1</ref>: Descriptions of the six contexts (informa- tion conditions) presented to the annotators.</p><p>Trump" means "pro-Clinton", and vice-versa.</p><p>This assumption seems reasonable given that the voting population was strongly polarized during the general (post-primary) election season, and it doubles the amount of data we can use to train the models. Thus, throughout this work the labels we use are taken from the set {"Support Trump / Oppose Clinton" = −1, "Neutral / I don't know" = 0, "Oppose Trump / Support Clinton" = 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Contexts Studied</head><p>Each of the 562 "anchor" tweets was annotated under six different contexts (also referred to as in- formation conditions) described in <ref type="table">Table 1</ref>. (The Supplementary Material provides visual examples of each.) We collected at least three annotations for each tweet/condition pair. Every AMT worker was shown 40 different tweets, one by one, ran- domly distributed across contexts. Two additional artificial tweets were used to control for task com- petency.</p><p>We selected the conditions in <ref type="table">Table 1</ref> based on two factors. First, we included conditions that var- ied in how much we expected them to impact an- notations. For example, we expected the partial profile information to have a relatively small ef- fect, and political party a larger one. Second, we restricted our options to sets of information that we believed would minimally impact task comple- tion times. We confirmed this empirically by re- gressing the (logged) time to completion for each annotator on the number of tweets she saw for each context, finding no significant effects from any context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Gold Standard Labels</head><p>Ideally, we would evaluate annotation quality and downstream performance by comparing to ground truth. Unfortunately, ground truth is difficult to characterize for tasks as subjective as stance de- tection or sentiment analysis ( <ref type="bibr" target="#b21">Passonneau and Carpenter, 2014;</ref><ref type="bibr" target="#b8">DiMaggio, 2015)</ref>. In light of this, we constructed our own labels, using all available information about users, and we use them as an approximation of ground truth.</p><p>We constructed these labels in order to evaluate downstream classification performance, and they cover a set of users not shown to the AMT work- ers. Given our resource constraints and the numer- ous (at least 18), often conflicting labels already available for tweets shown to AMT workers, we did not create definitive labels for that set.</p><p>To create these "gold standard" (GS) labels, we considered all information found on the user's Twitter timeline, including everything AMT an- notators could see, plus friend/following relation- ships, all of their previous tweets, demographics from the voter file, etc. Anecdotally, we found cer- tain cases time-consuming to investigate, which argues for continuing to limit how much informa- tion we ask annotators to consider. All gold stan- dard labels were agreed upon by at least two au- thors, who first labeled the data independently and then came together to discuss disagreements.</p><p>Our GS set consists of 318 users (with their as- sociated anchor tweets). Each user is assigned a label from the tertiary Trump/Neutral/Clinton scale. Another 250 manually labeled accounts were used for model development but are not part of reported results. The GS is approximately equally divided among registered Democrats, reg- istered Republicans, and people not registered with either party; the last category includes self- declared Independents and voters not affiliated with any party. We include this third set in order to ensure the models generalize beyond registered Democrats and Republicans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotation Quality For Individual Contexts</head><p>In this section, we examine how annotator agree- ment varies depending on the context in which the labels were obtained, and how classifiers trained on majority-vote labels from each individual con- text, as well as on labels from all contexts com- bined, perform on the GS. First, we introduce the classifier and features used for the latter task, then discuss results for agreement and classifier perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classifier, Labels, Features, &amp; Evaluation</head><p>For each of the six contexts separately, we con- struct labels with which to train a classifier. Train- ing labels are constructed using majority vote; we also tried weighting the training instances to match the distribution of labels, but it did not perform as well. We also construct a seventh set of labels us- ing all annotations from all conditions. We then train a classifier on each set of labels. We use Random Forest models, as they outperformed reg- ularized logistic regression and SVMs with linear kernels on the development set. Note that the only difference among the models in this section is the labels they are trained on. The feature set used, shown in <ref type="table" target="#tab_1">Table 2</ref>, is meant as a straightforward representation of the informa- tion seen by annotators; parts of it follow <ref type="bibr" target="#b9">Ebrahimi et al. (2016)</ref>. We construct three types of features for each tweet: text, sentiment and user features. For text features, we collapse the anchor tweet plus all additional textual context seen by any an- notator into a single string, then compute vari- ous n-grams from it. For sentiment, we compute various scores from the anchor tweet alone. For user features, we include the user's race and gen- der, which annotators might have learned from the user's profile picture. Note that because we want models to generalize beyond registered Democrats or Republicans, we do not include a feature for po- litical party.</p><p>Classifier performance on the GS is measured, following prior work ( <ref type="bibr" target="#b15">Mohammad et al., 2016a;</ref><ref type="bibr" target="#b9">Ebrahimi et al., 2016)</ref>, on the average of the F1 scores on the two classes of interest ("Clinton" and "Trump"). Additionally, we report the aver- age log-loss (the negative log-likelihood, accord- ing to the classifier, of the true label). Log-loss and F1 can be seen as complementary measures: whereas F1 evaluates the quality of the ranking of test instances, log-loss evaluates the quality of their individual probability estimates. To compute the probability estimate from a Random Forest, we compute mean class probabilities across all trees.</p><p>To assess the statistical significance of differ- ences between two models, we first obtain prob- ability estimates for all GS items. For log-loss, we use a Mann-Whitney test on the scores from the two models being compared. For F1, we create 1000 bootstrap iterations of the sample, compute the average F1 of each, and run a non-parametric difference-in-means test, using 95% confidence</p><note type="other">Category Data Source Feature Representation Text</note><p>Anchor tweet, previous (political) tweets, profile description</p><p>Character n-grams (n ∈ <ref type="bibr">[3,</ref><ref type="bibr">5]</ref>), word n-grams (n ∈ <ref type="bibr">[1,</ref><ref type="bibr">3]</ref>). Preprocessing: only use tokens appearing ≥ 10 times, apply tf-idf weighting.   <ref type="table">Table 3</ref>: Inter-annotator agreement, then perfor- mance of classifier trained on majority vote labels.</p><p>(Best possible is 1 for agreement and F1, 0 for log- loss.) intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effects of the Contexts</head><p>Before evaluating classification results, we con- sider annotator agreement within each context, calculated like <ref type="bibr" target="#b16">Mohammad et al. (2016b)</ref> as the average, across tweets, of the percentage of anno- tations that match the majority vote. As shown in <ref type="table">Table 3</ref>, annotators shown No Context achieve an agreement score of 0.84, similar to the 0.8185 reported by <ref type="bibr" target="#b16">Mohammad et al. (2016b)</ref>. Relative to this baseline, some contexts increase agreement more than others. As one might expect, Previ- ous Political Tweets and Political Party show the strongest signals. Their effects are statistically (p &lt; .01, Mann-Whitney test) and practically sig- nificant, increasing the number of labels having full agreement by 15% and 10%, respectively. However, annotators shown different contexts did not necessarily converge to the same labels. Notice the low agreement for the All Combined condition: the majority labels held stronger ma- jorities within any individual context than across all of them. In fact, if we look at the six major- ity vote labels for each tweet, only in 43% of the tweets are these labels in full agreement. At the end of Section 5, we return to the question of why agreement was so low across conditions, with the help of parameters estimated by ConStance.</p><p>In the classification task, the results in Ta- ble 3 further suggest that Previous Political Tweets serves as the strongest single context. There is a good case to be made for choosing this individ- ual context, which is statistically significantly bet- ter than many others. For example, providing an- notators with Previous Political Tweets provides a statistically significant increase in both average F1 scores and log-loss (with p &lt; .01) over both the No Context and Full Profile conditions. Perhaps most noteworthy is that the All Combined classi- fier, created from the naive combination of all an- notations, is no better than the classifiers from the individual conditions.</p><p>To summarize, results suggest that providing annotators with appropriate additional context can improve annotation quality, as measured via an- notator agreement and downstream classification performance. However, it was not obvious in ad- vance which context would be most helpful, and performing such an analysis as this requires the time-intensive construction of better "gold stan- dard" labels against which to check the labels al- ready being outsourced to annotators. In addition, the heterogeneity of the labels produced in differ- ent contexts suggests that the contexts provide di- verse signals we might be able to leverage; how- ever, simply combining all the annotations does not result in improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ConStance: General Unified Model</head><p>The prior section thus suggests that it may be bet- ter to limit a priori decisions and instead to lever- age multiple kinds of context during annotation. Like <ref type="bibr" target="#b22">Raykar et al. (2010)</ref> assumes for annotators, we might expect (and indeed find) that even those contexts that turn out to be worse on some metrics still might be useful for other purposes. Here, we present a model for such an approach.</p><p>ConStance learns a classifier for items. For our purposes here, an item is a user together with their anchor tweet and the additional information from which features were derived (see <ref type="table" target="#tab_1">Table 2</ref>); more broadly, it is whatever we choose to put into the feature vector. One could choose a differ-  ent setup; for example, an item could be a user and ten anchor tweets. However, the current ar- rangement allows for straightforward comparison to prior stance work on Twitter ( <ref type="bibr" target="#b15">Mohammad et al., 2016a</ref>). Note that in general, the features need not be restricted to those annotators could have seen. Rather, they could include anything useful to a classifier. Note also that the feature set provided to ConStance is the same used by the baseline mod- els; only the models themselves differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The model we develop is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. There are N items to be labeled. Each item can be viewed in up to C different contexts. Finally, there are A total annotators labeling the items; each an- notator sees multiple items. Each item can have a different number of annotations, produced by any assignment of annotators and information condi- tions to items. In our dataset, every item is labeled in 6 conditions (every |C i | = 6), and within every context, every item is labeled by at least 3 annota- tors (every |A c i | ≥ 3). The model's generative story works as follows. Item i has feature vector X i and a "true" label Y i ∈ V . The relationship between X i and Y i can be described by some model M, which we will ultimately learn. When the item is viewed with context c, the item's true label Y i is transformed by noise into a "context-specific" label S c i ∈ V . In other words, the true label may appear differ- ently when seen through the lens of each context. The variable S c i represents what an ideal annotator would say about item i given only as much infor- mation as is preserved by context c.</p><p>The "noise" introduced by context c is de- scribed by parameter γ c . The parameter γ c is a V × V matrix of transition probabilities from true labels to context-specific labels. These probabil- ities depend only on Y i and γ c , not on the item's features X i .</p><p>Importantly, annotators themselves are also im- perfect. When annotator a sees item i, she may also distort the label she sees, S c i , into the observed annotation R ca i ∈ V . The annotator-specific noise process is described by parameter α a , another V × V transition matrix.</p><p>For a better understanding of the role of γ c (and by anology, α a ), consider the depictions in <ref type="figure">Figure 2</ref>. The matrix on the top left refers to the No Context condition. Its top row describes what an annotator with perfect judgment would think about a user whose true label is Trump <ref type="bibr">[supporter]</ref>, with no context. The top left cell, with a value around 0.65, is the probability the annotator would think Trump; the lighter middle cell, with a value around 0.35, is the probability she would think Neutral/Don't know; and the probability she would think Clinton is almost 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning</head><p>Like <ref type="bibr" target="#b22">Raykar et al. (2010)</ref>, we perform infer- ence using Expectation Maximization (EM). A full derivation is provided in the Supplementary Material; here, we sketch the main steps.</p><p>The model's incomplete data likelihood func- tion, Eq. (1), describes the joint probability, across all items, of Y i , all values of S c i , and all values of R ca i assuming X i is known and fixed. Uppercase denotes random variables; lowercase, specific val- ues. In line (2), we substitute in the equivalent model parameters.</p><formula xml:id="formula_0">p(D|θ, X) = N i=1 V y p(Y i = y|x i , M) C i c V s p(S c i = s|y, γ) A c i a p(r ca i |s, α) (1) = N i=1 V y M y (x i ) C i c V s γ c ys A c i a α a sr<label>(2)</label></formula><p>The EM derivation is difficult because both Y i and S i are unobserved. Our solution is to treat the latent variables as a block, describing their joint configuration with a single term T i = (Y i , S i ). In our data, since |C i | = 6, T i can take on 7 |V | pos- sible values, a number small enough to enumerate over when we need to marginalize out T i .</p><p>We define membership indicator variables T i(ys) ∈ {0, 1} such that T i(ys) = 1 if T i has the specific values (y, s). During learning, we use analogous variables τ i(ys) ∈ [0, 1] to represent the posterior probabilities of each configuration:</p><formula xml:id="formula_1">τ i(ys) = p(T i(ys) = 1 | D, θ).</formula><p>The expected value of the complete data log-likelihood is:</p><formula xml:id="formula_2">E Z [(D, Z|θ, X)] = N i=1 V y    V s 1 i . . . V s C i i    τ i(ys) (log p(T i(ys) | x i , M, γ) + C i c A c i a log α a sr )<label>(3)</label></formula><p>For the E step, we update the membership esti- mates τ i(ys) using the current parameters θ. With Bayes' rule, each item's new value of τ i(ys) is shown to be the full joint likelihood of item i (see Eq. <ref type="formula" target="#formula_0">(2)</ref>) when setting Y i = y and S i = s, divided by the sum, over all possible settings of Y i and S i , of that same joint likelihood.</p><p>For the M step, we update the model parame- ters using the current membership estimates. To update the classifier M, following the guidance of <ref type="bibr" target="#b22">Raykar et al. (2010)</ref>, we retrain the classifier using the current estimates of Y i as weights for items. The estimates of Y i can be obtained from τ iys by marginalizing out</p><formula xml:id="formula_3">S i , thus E Z [Y i = y] = V s 1 i . . . V s C i i</formula><p>τ iys . We then use sampling to con- struct a discrete set of labels for model training based on these weights.   <ref type="table">Table 5</ref>: Classification performance of ConStance and model ablations. Boldface highlights best scores. Significance tests use the the p &lt; .05 level for log-loss. Compared to the best baselines, all scores that appear better are statistically signif- icant. Italics indicate the scores that are signifi- cantly worse than ConStance.</p><p>To update γ and α, we maximize them with re- spect to Eq. (3). For γ, the entry γ c ys (i.e., row y, column s of matrix γ c ) denotes p(S c i = s | Y i = y). Each matrix entry can be updated individually by taking the partial derivative of Eq. <ref type="formula" target="#formula_2">(3)</ref> and us- ing, as a Lagrange multiplier term, the constraint that the row must sum to 1. The updated value for γ c ys turns out to be a fraction in which the numera- tor is the weighted (by τ ) number of items having Y i = y and S c i = s, and the denominator is the weighted number of items having Y i = y (and any value for S c i ). For α, a similar derivation yields the following update to α a sr : the weighted number of labels by annotator a, in any context, having S c i = s and R ca i = r, divided by the weighted number of labels by annotator a, in any context, having S c i = s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Results and Discussion</head><p>The top portion of <ref type="table">Table 5</ref> displays ConStance's performance compared to the best results from Section 3. Using the same experimental setup as Section 3-the model type and features, M and X respectively, are the same as in the baselines- ConStance improves over the best baseline mod- els for each metric. This improvement is statisti- cally significant for both metrics (at the p &lt; .05 level for log-loss). Further, the model converges rapidly, within 5-7 iterations of the EM algorithm and 3-5 minutes on a single machine. <ref type="bibr">2</ref> In addition to comparing to the baselines pro- vided in Section 3, we investigate which informa- tion the model is leveraging to be successful. We do so by exploring three ablations of the model. Variation #1 ("Only Political Tweets" in <ref type="table">Table 5)</ref> uses the full model, but only gives it the annota- tions from the Political Tweets condition. This tests whether simply modeling differences in an- notators' error rates, as <ref type="bibr" target="#b22">Raykar et al. (2010)</ref> do, with a single ("best") context is helpful. We find that it is: the performance of this variation is sig- nificantly better on both metrics than the Political Tweets baseline from <ref type="table">Table 3</ref>.</p><p>In the second and third variations, we check whether the effectiveness of ConStance stems from modeling differences between annotators rather than differences in contexts, or vice versa. Variation #2 ("Context Labels Masked"), like #1, models only annotator effects; however, it instead uses the entire set of annotations, treating them as if from a single context (i.e., "masking" context in- formation from the model). Variation #3 ("Anno- tator Labels Masked") is the complement of Vari- ation #2: it models differences in contexts, and it uses the entire set of annotations, treating them as if from a single annotator.</p><p>The results of the model ablation experiments are three-fold. First, we see that each piece of the model on its own is effective in moving beyond baseline approaches that use only one context or naively combine labels across contexts and anno- tators (the "All Combined" baseline). All model variations achieve significantly higher Avg. F1 than the baselines, and Variations #1 and #2 im- prove on log-loss. Second, we see that model- ing annotators alone is clearly better than not: not only does Variation #1 outperform the Political Tweets baseline (significantly), but also Variation #2 outperforms the All Combined baseline (signif- icantly) and ConStance outperforms Variation #3 (with significance in one measure). Finally, the best results come from using the full model. Even if the differences between ConStance and the vari- ations are not all statistically significant, model- ing both annotators and contexts appears to be the most complete and effective approach.</p><p>In addition to model performance, we can also examine what ConStance has learned about the quality of labels from each context. Recall that the model produces a parameter matrix for each con- text, γ c , which describes how a context distorts the "true" labels the model assumes. Each γ c is a tran- sition matrix, so a context that perfectly preserves true labels would show up as the identity matrix; off-diagonal entries show error patterns. We see that in the No Context, Partial Profile and Full Profile conditions, annotators often selected the "Neutral" option (x-axis) when the model in- ferred the true label was "Clinton" or "Trump" (y- axis). This finding is in line with intuitions; an- notators who saw these conditions simply lacked enough information to determine any label.</p><p>On the other extreme, in the Political Party con- text, annotators selected "Trump" or "Clinton" too often when the model settled on the "Neutral" op- tion. That is, even when a user's stance was not clear to annotators in other conditions, annotators who saw political party still inferred stance from the text. Here, one could argue annotators were shown "too much" or "too strong" a context-they saw stance even where the content produced by the user did not suggest one. Indeed, further manual inspection of 90 tweets on which annotations dis- agreed across contexts implies that annotators who saw political affiliation were often wrong because they focused too little on text content relative to the provided political affiliations.</p><p>In presenting these findings, a key point to high- light is that unlike the results of Section 3, <ref type="figure">Figure 2</ref> was produced without access to any full informa- tion labels, which depend on a significant level of manual effort beyond annotations gathered on AMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Recent work has shown that cognitive biases such as stereotypes <ref type="bibr" target="#b4">(Carpenter et al., 2016</ref>) and anchor- ing ( <ref type="bibr" target="#b1">Berzak et al., 2016)</ref> can negatively impact text annotation and resulting models, even for ob- jective tasks like POS tagging <ref type="bibr" target="#b2">(Blodgett et al., 2016)</ref>. Still, researchers often decide what con- text to show annotators without rigorously evalu-ating how their decisions will affect annotations, on tasks from gender identification to political leanings ( <ref type="bibr" target="#b5">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Nguyen et al., 2014;</ref><ref type="bibr" target="#b3">Burger et al., 2011;</ref><ref type="bibr" target="#b6">Cohen and Ruths, 2013)</ref>. Our work suggests an interesting avenue of develop- ment towards reducing annotation bias by explic- itly modeling it and reducing the need for a priori decisions on which context is best for which par- ticular task.</p><p>In doing so, we draw on a large body of work around improving annotation quality for NLP data. Our work aligns with efforts to improve task design (e.g. <ref type="bibr" target="#b24">Schneider et al., 2013;</ref><ref type="bibr" target="#b17">Morstatter and Liu, 2016;</ref><ref type="bibr" target="#b23">Schneider, 2015)</ref>, and to develop better models of annotation. With respect to the former and specific to Twitter, <ref type="bibr" target="#b10">Frankenstein et al. (2016)</ref> show that for the task of labeling the senti- ment of reply tweets, annotations vary depending on whether or not the original tweet (being replied to) is also shown. With respect to the latter, sev- eral recent models beyond <ref type="bibr" target="#b22">Raykar et al. (2010)</ref> have been proposed <ref type="bibr" target="#b11">(Guan et al., 2017;</ref><ref type="bibr" target="#b26">Tian and Zhu, 2012;</ref><ref type="bibr" target="#b27">Wauthier and Jordan, 2011;</ref><ref type="bibr" target="#b21">Passonneau and Carpenter, 2014</ref>). However, our work is most similar to efforts outside the domain of NLP, where <ref type="bibr" target="#b7">Dai et al. (2013)</ref> have developed a method of switching between task workflows based on an- notation quality for particular items, and <ref type="bibr" target="#b18">Nguyen et al. (2016)</ref> have developed a Bayesian model similar to ours to study annotation quality for other kinds of slightly subjective tasks.</p><p>In a closely related vein, recent work has also considered how text annotations may vary in im- portant ways based on the characteristics of anno- tators (rather than how the task is posed, as we study here) <ref type="bibr" target="#b25">(Sen et al., 2015</ref>). An interesting av- enue of future work is to understand the intersec- tion between the design of NLP annotation tasks and the characteristics of the annotating popula- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>Annotated data serves as a foundational layer for many NLP tasks. While some annotation tasks only require information from short texts, in many others, we can elicit higher-quality labels by pro- viding annotators with additional contextual infor- mation. However, asking annotators to consider too much information would make their task slow and burdensome.</p><p>In this paper we demonstrate how exposing an- notators to short contextual information leads to better labels and better classification results. How- ever, different contexts lead to results of different quality, and it is not obvious a priori which con- text is best, nor-even given ground truth-how to combine labels produced across contexts to ex- ploit the information present in each. We then pro- pose ConStance, a generalizable model that learns the effects of both individual contexts and individ- ual annotators on the labeling process. The model infers (probability estimates for) ground truth la- bels, plus learns a classifier that can be applied to new instances. We show that this classifier signif- icantly improves classification of political stance compared to the standard practice of training mod- els on majority vote labels. The focus of this work is on improving both the annotation process for nuanced, context- dependent tasks and the use of the resulting labels. While ConStance's label estimation can be used in conjunction with any classification method, this paper does not address the optimization of the classifier itself. Thus, while we consider an as- sortment of contexts and use a rich feature rep- resentation, using additional contexts or different features may lead to better performance on stance detection. Finally, the model is versatile enough we could consider treating different tweets as dif- ferent "contexts" for the same user, augmenting the extensively annotated tweets with other types of data, and, naturally, applying the same frame- work to other annotation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical model for ConStance. Var. Meaning Xi Feature vector of item i Yi Latent true label of item i S c i Latent context-specific label of item i after noise from context c R ca i Label given by annotator a to item i in context c V Set of values for labels and annotations: {−1, 0, 1} N # of items, indexed by i C Set of contexts, indexed by c A Set of annotators, indexed by a M Learned classifier γ c V × V parameter matrix for context c α a V × V parameter matrix for annotator a D All observed data: all values of Xi and R ca i Z All latent variables: all values of Yi and Si θ All model parameters: M, γ, α Ti All latent variables for item i: (Yi, Si) τ i(ys) Current estimate of all latent values for item i: p(Yi = y, Si = s | D, θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>Figure 2 visualizes parameter estimates for γ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Features used in classification.</head><label>2</label><figDesc></figDesc><table>Model 
Agreement Log-Loss Avg F1 
No Context 
0.84 
0.72 
0.61 
Partial Profile 
0.83 
0.71 
0.68 
Full Profile 
0.82 
0.69 
0.62 
Previous Tweets 
0.84 
0.65 
0.71 
Political Tweets 
0.88 
0.61 
0.70 
Political Party 
0.88 
0.63 
0.68 
All Combined 
0.77 
0.62 
0.71 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 : Model variables.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Replication materials for this work, including code for ConStance, are available at https://github.com/ kennyjoseph/constance. The paper&apos;s Supplementary Material can also be accessed there.</note>

			<note place="foot" n="2"> As above, a development set is used for coarse hyperparameter tuning; see the Supplementary Material for details.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Less is more? How demographic sample weights can improve public opinion estimates based on Twitter data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barberá</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Working Paper</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Anchoring and Agreement in Syntactic Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04481</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Demographic dialectal variation in social media: A case study of African-American English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Su Lin Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Connor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminating Gender on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zarrella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1301" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real Men Don&apos;t Say &quot;Cute&quot;: Using Automatic Language Analysis to Isolate Inaccurate Aspects of Stereotypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preot¸iucpreot¸iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucie</forename><surname>Flekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Anneke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Buffone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">E P</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seligman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Psychological and Personality Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fusheng</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">A Comparative Study of Demographic Attribute Inference in Twitter. ICWSM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="590" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Classifying political orientation on Twitter: It&apos;s not easy! In ICWSM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Ruths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">POMDP-based control of workflows for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="52" to="85" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adapting computational text analysis to social science (and vice versa)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Dimaggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Big Data &amp; Society</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Joint Sentiment-Target-Stance Model for Stance Classification in Tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextualized Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Frankenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Computing, Behavioral-Cultural Modeling and Prediction</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08774</idno>
		<title level="m">Who Said What: Modeling Individual Labelers Improves Classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Voters of the Year&quot;: 19 Voters Who Were Unintentional Election Poll Sensors on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wojcik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Girls rule, boys drool: Extracting semantic and affective stereotypes from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM Conference on Computer Supported Cooperative Work</title>
		<imprint>
			<publisher>CSCW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 6: Detecting stance in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation</title>
		<meeting>the International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stance and sentiment in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiritchenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01655</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Replacing mechanical turkers? challenges in the evaluation of models with semantic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data and Information Quality (JDIQ)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic modeling for crowdsourcing partially-subjective ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><forename type="middle">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Conference on Human Computation and Crowdsourcing (HCOMP)</title>
		<meeting>The Conference on Human Computation and Crowdsourcing (HCOMP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How Old Do You Think I Am?&quot;: A Study of Language and Age in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rilana</forename><surname>Gravel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dolf</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Meder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why gender and age prediction from tweets is hard: Lessons from a crowdsourcing experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seza Do˘ Gruöz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rilana</forename><surname>Gravel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariët</forename><surname>Theune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Meder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The benefits of a model of annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="311" to="326" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><forename type="middle">Hermosillo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What I&apos;ve learned about annotating informal text (and why you shouldn&apos;t take my word for it)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 9th Linguistic Annotation Workshop Held in Conjuncion with NAACL 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">152</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A framework for (under) specifying dependency syntax without overloading annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.2091</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards Domain-Specific Semantic Relatedness: A Case Study from Geography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilad</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Horlbeck</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mathers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">Souza</forename><surname>Vonessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><forename type="middle">J</forename><surname>Hecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2362" to="2370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from crowds in the presence of schools of thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian bias mitigation for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Wauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1800" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
