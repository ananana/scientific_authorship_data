<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Resolving Referring Expressions in Conversational Dialogs for Natural User Interfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaleh</forename><surname>Feizollahi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><forename type="middle">Sarikaya</forename><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main">Resolving Referring Expressions in Conversational Dialogs for Natural User Interfaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2094" to="2104"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unlike traditional over-the-phone spoken dialog systems (SDSs), modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system&apos;s response to the user. Visual display of the system&apos;s response not only changes human behavior when interacting with devices, but also creates new research areas in SDSs. On-screen item identification and resolution in utterances is one critical problem to achieve a natural and accurate human-machine communication. We pose the problem as a classification task to correctly identify intended on-screen item(s) from user utterances. Using syntactic, semantic as well as context features from the display screen, our model can resolve different types of referring expressions with up to 90% accuracy. In the experiments we also show that the proposed model is robust to domain and screen layout changes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Todays natural user interfaces (NUI) for applica- tions running on smart devices, e.g, phones (SIRI, Cortana, GoogleNow), consoles (Amazon FireTV, XBOX), tablet, etc., can handle not only simple spoken commands, but also natural conversational utterances. Unlike traditional over-the-phone spo- ken dialog systems (SDSs), user hears and sees the system's response displayed on the screen as an additional modality. Having visual access to the system's response and results changes human be- havior when interacting with the machine, creating new and challenging problems in SDS. <ref type="bibr">[System]</ref>: How can i help you today ? <ref type="bibr">[User]</ref>: Find non-fiction books by Chomsky.</p><p>[System]: (Fetches the following books from database) <ref type="bibr">[User]</ref>: "show details for the oldest production" or "details for the syntax book" or "open the last one" or "i want to see the one on linguistics" or "bring me Jurafsky's text book" Consider a sample dialog in <ref type="table" target="#tab_0">Table 1</ref> between a user and a NUI in the books domain. After the sys- tem displays results on the screen, the user may choose one or more of the on-screen items with natural language utterances as shown in <ref type="table" target="#tab_0">Table 1</ref>. Note that, there are multiple ways of referring to the same item, (e.g. the last book) <ref type="bibr">1</ref> . To achieve a natural and accurate human to machine conversa- tion, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do fo- cus on the impact of the input from multimodal interfaces such as gesture for understanding <ref type="bibr" target="#b4">(Bolt, 1980;</ref><ref type="bibr" target="#b16">Heck et al., 2013;</ref><ref type="bibr" target="#b19">Johnston et al., 2002</ref>), touch for ASR error correction <ref type="bibr" target="#b17">(Huggins-Daines and Rudnicky, 2008)</ref>, or cues from the screen ( <ref type="bibr" target="#b1">Balchandran et al., 2008;</ref><ref type="bibr" target="#b0">Anastasiou et al., 2012</ref>). Most of these systems are engineered for a specific task, making it harder to generalize for different domains or SDSs. In this paper, we investigate a rather generic contextual model for resolving nat- ural language REs for on-screen item selection to improve conversational understanding.</p><p>Our model, which we call FIS (Flexible Item Selection), is able to (1) detect if the user is re- ferring to any item(s) on the screen, and (2) re- solve REs to identify which items are referred to and score each item. FIS is a learning based sys- tem that uses information from pair of user utter- ance and candidate items on the screen to model association between them. We cast the task as a classification problem to determine whether there is a relation between the utterance and the item, representing each instance in the training dataset as relational features.</p><p>In a typical SDS, the spoken language under- standing (SLU) engine maps user utterances into meaning representation by identifying user's in- tent and token level semantic slots via a seman- tic parser <ref type="bibr" target="#b25">(Mori et al., 2008</ref>). The dialog man- ager uses the SLU components to decide on the correct system action. For on-screen item selec- tion SLU alone may not be sufficient. To correctly associate the user's utterance with any of the on- screen items one would need to resolve the rela- tional information between the utterance and the items. For instance, consider the dialog in Ta- ble 1. SLU engine can provide signals to the di- alog model about the selected item, e.g., that "lin- guistics" is a book-genre or content, but may not be enough to indicate which book the user is refer- ring. FIS module provides additional information for the dialog manager by augmenting SLU com- ponents.</p><p>In ยง3, we provide details about our data as well as data collection and annotation steps. In ยง4, we present various syntactic and semantic features to resolve different REs in utterances. In the exper- iments ( ยง6), we evaluate the individual impact of each feature on the FIS model. We analyze the performance of the FIS model per each type of REs. Finally, we empirically investigate the ro- bustness of the FIS model to domain and display screen changes. When tested on a domain that is unseen to the training data or on a device that has a different NUI design, the performance only slightly degrades proving its robustness to domain and design changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Although the problems of modern NUIs on smart devices are fairly new, RE resolution in natural language has been studied by many in NLP com- munity.</p><p>Multimodal systems provide a natural and ef- fective way for users to interact with computers through multiple modalities such as speech, ges- ture, and gaze. Since the first appearance of the Put-That-There system <ref type="bibr" target="#b4">(Bolt, 1980)</ref>, a number of multimodal systems have been built, among which there are systems that combine speech, point- ing <ref type="bibr" target="#b26">(Neal, 1991)</ref>, and gaze ( <ref type="bibr" target="#b21">Koons et al., 1993)</ref>, systems that engage users in an intelligent con- versation ( <ref type="bibr" target="#b14">Gustafson et al., 2000</ref>). Earlier stud- ies have shown that multimodal interfaces enable users to interact with computers naturally and ef- fectively ( <ref type="bibr" target="#b29">Schober and Clark, 1989;</ref><ref type="bibr" target="#b27">Oviatt et al., 1997</ref>). Considered as part of the situated interac- tive frameworks, many work focus on the prob- lem of predicting how the user has resolved REs that is generated by the system, e.g., <ref type="bibr" target="#b7">(Clark and Wilkes-Gibbs, ;</ref><ref type="bibr" target="#b9">Dale and Viethen, 2009;</ref><ref type="bibr" target="#b12">Gieselmann, 2004;</ref><ref type="bibr" target="#b18">Janarthanam and Lemon, 2010;</ref><ref type="bibr" target="#b13">Golland et al., 2010)</ref>. In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action.</p><p>In (Pfleger and J. <ref type="bibr" target="#b28">Alexandersson, 2006</ref>) a refer- ence resolution model is presented for a question- answering system on a mobile, multi-modal inter- face. Their system has several features to parse the posed question and keep history of the dia- log to resolve co-reference issues. Their question- answering model uses gesture as features to re- solve queries such as "what's the name of that [pointing gesture] player?", but they do not re- solve locational referrals such as "the middle one" or "the second harry potter movie". Others such as ( <ref type="bibr" target="#b11">Funakoshi et al., 2012</ref>) resolve anaphoric ("it") or exophoric ("this one") types of expressions in user utterances to identify geometric objects. In this paper, we study several types of REs to build a natural and flexible interaction for the user.</p><p>( <ref type="bibr" target="#b16">Heck et al., 2013)</ref> present an intent prediction model enriched with gesture detector to help dis- ambiguate between different user intents related to the interface. In ( <ref type="bibr" target="#b24">Misu et al., 2014</ref>) a situated in- car dialog model is presented to answer drivers' spoken queries about their surroundings (no dis- play screen). They integrate multi-modal inputs of speech, geo-location and gaze. We investigate a variety of REs for visual interfaces, and analyze automatic resolution in a classification task intro- ducing a wide range of syntactic, semantic and contextual features. We look at how REs change with screen layout comparing different devices. To the best of our knowledge, our work is first to analyze REs from these aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Crowdsourcing services, such as Amazon Me- chanical Turk or CrowdFlower, have been exten- sively used for a variety of NLP tasks <ref type="bibr">(CallisonBurch and Dredze, 2010)</ref>. Here we explain how we collected the raw utterances from Crowd- Flower platform (crowdflower.com).</p><p>For each HITApp (Human Intelligence Task Application), we provide judges with a written ex- planation about our Media App, a SDS built on a device with a large screen which displays items in a grid style layout, and what this particular sys- tem would do, namely search for books, music, tv and movies media result 2 Media App returns results based on the user query using an already implemented speech recognition, SLU and dialog engines. For each HIT, the users are shown a dif- ferent screenshot showing the Media App's search results after a first-turn query is issued (e.g., "find non-fiction books by Chomsky" in <ref type="table" target="#tab_0">Table 1</ref>). Users are asked to provide five different second turn text utterances for each screenshot. We launch several hitapps each with a different prompt to cover dif- ferent REs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HITApp Types and Data Collection</head><p>A grid of media items is shown to the user with a red arrow pointing to the media result we want them to refer to (see <ref type="figure" target="#fig_1">Fig. 1</ref>). They can ask to play (an album or an audio book), select, or ask details about the particular media item. Each item in each grid layout becomes a different HIT or screenshot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Item Layout and Screen Type Variation</head><p>The applications we consider have the following rowรcolumn layouts: 1ร6, 2ร6 and 3ร6, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref> (columns vary depending on the returned item size). By varying the layout, we ex- pect the referent of the last and the bottom layer items to change depending on how many rows, or  columns exist in the grid. In addition, phrases like "middle", or "center" would not appear in the data when there are only one or two rows. Also, we ex- pect that the distribution of types of utterances to vary. For example, in a grid of 1ร6, "the second one" makes sense, but not so much on a 2ร6 grid.</p><p>We expect similar change based on the number of columns.</p><p>We use two kinds of screenshots to collect ut- terances with variations in REs. The first type of screenshots are aimed to bias the users to refer to items 'directly' using (full/partial) titles or 'indi- rectly' using other descriptors, or meta informa- tion such as year the movie is taken, or the au- thor of the book. To collect utterances that indi- rectly referred to items, we need to show screen shots displaying system results with common ti- tles, eventually forcing the user to use other de- scriptors for disambiguation. For example, given the first turn query "find harry potter movies", the system returns all the Harry Potter series, all of which contain the words Harry Potter in the title. The user can either refer in their utterance with the series number, the subtitle (e.g. The prisoners of Azkaban) or the location of the movie in the grid or by date, e.g., "the new one", Because some media items have long titles, or contain foreign names that are not easy to pro- nounce, users may chose to refer these items by their location on the display, such as "top right", "first album", "the movie on the bottom left", etc. The second type of screen shots contains a tem- plate for each layout with no actual media item ( <ref type="figure" target="#fig_1">Fig. 1(d)</ref>) which simply forces user to use loca- tional references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Interface Design Variation</head><p>In order to test our model's robustness to a different screen display on a new device, we employ an additional collection running another application named places, designed for hand- held devices. The places application can assist users in finding local businesses (restaurants, ho- tels, schools, etc.). and by nature of the de- vice size can display fewer media items and ar- ranges them in a list (one column). The num- ber of items on the screen at any given time de- pends on the size of the hand-held device screen. The user can scroll down to see the rest of the results.</p><p>Our collection displays the items in a 3, 4, and 5-rows per 1 column layout as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. We use the same variations in prompts as in ยง3.1. To generate the HitApp screens, we search for nearby places, in the top search engines (Google, Bing) and collect the results to the first turn natural language search queries (e.g.,"find me sushi restaurants near me").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Annotation</head><p>We collect text utterances using our media and places application. Using a similar HitApp we labeled each utterance with a domain, intent and segments in utterance with slot tags (see <ref type="table" target="#tab_2">Table 2</ref>). The annotation agreement, Kappa measure <ref type="bibr" target="#b8">(Cohen, 1960</ref>) is around 85%. Since we are building a relational model between utterances and each item on the screen, we ask the annotators to label each utterance-item as '0' or '1' indicating if the utter- ance is referring to that item or not. '1' means the item is the intended one. '0' indicates the item is not intended one or the utterance is not refer- ring to any item on the screen, e.g., new search query. We also ask the annotators to label each utterance whether they contain locational (spatial) references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Intents (I) &amp; Slots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>movie</head><p>I: find-movie/director/actor,buy-ticket Slots: name, mpaa-rating (g-rated), date,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Types of Observed Referring Expressions</head><p>We observe four main categories of REs in the ut- terances that are collected by varying the prompts and HITApp screens in crowd-sourcing: Explicit Referential : Explicit mentions of whole or portions of the title of the item on the screen, and no other descriptors, e.g.,"show me the details of star wars six" (referring to the item with title "Star wars: Episode VI -Return of the Jedi").</p><p>Implicit Referential : The user refers to the item using distinguishing features other than the title, such as the release or publishing date, writ- ers, actors, image content (describing the item im- age), genre, etc. "how about the one with Kevin Spacey".</p><p>Explicit Locational : The user refers to the item using the grid design, e.g., "i want to pur- chase the e-book on the bottom right corner".</p><p>Implicit Locational : Locational references in relation to other items on the screen, e.g., "the sec- ond of Dan Brown's book" (showing two of the Dan Brown's book on the same row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature Extraction for FIS Model</head><p>Here, provide descriptions of each set of features of FIS model used to resolve each expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Similarity Features (SIM)</head><p>Similarity features represent the lexical overlap between the utterance and the item's title (that is displayed on the user's screen) and are mainly aimed to resolve explicit REs. We represent each utterance u i and item-title t k as sequence of words:</p><formula xml:id="formula_0">u i ={w i (1), . . . , w i (n i )} t k ={w k (1), . . . , w k (m k )}</formula><p>item bigrams &lt;bos&gt; call five guys and fries &lt;eos&gt; &lt;bos&gt; five five guys guys burgers burgers and and fries fries &lt;eos&gt; <ref type="table">Table 3</ref>: Bigram overlap between the item "five guys burg- ers and fries" and utterance"five guys and fries".</p><p>where w i (j) and w k (j) are the jth word in the se- quence. Since inflectional morphology may make a word appear in an utterance in a different form than what occurs in the official title, we use both the word form as it appears in the utterance and in the item title. For example, burger and burgers, or woman and women are considered as four distinct words and all included in the bag-of-words. Us- ing this representation we calculate four different similarity measures:</p><p>Jaccard Similarity: A common feature that can represent the ratio of the intersection to the union of unigrams. Consider, for instance, u i ="call five guys and fries" and the item t k ="five guys burgers and fries" in <ref type="figure" target="#fig_2">Fig 2.</ref> The Jaccard sim- ilarity S(i,k) is:</p><formula xml:id="formula_1">S(i,k)=1-( c(r i โฉ r k )/c(r i โช r k ) )</formula><p>where the r i and r k are unigrams of u i and t k re- spectively. c(r i โฉ r k ) is the number of common words of u i and t k , c(r i โช r k ) is the total unigram vocabulary size between them. In this case, the S(i,k)=0.66.</p><p>Orthographic Distance: Orthographic dis- tance represent similarity of two text and can be as simple as an edit distance (Levenshtein dis- tance) between their graphemes. The Levenshtein distance <ref type="bibr" target="#b23">(Levenshtein, 1965)</ref> counts the insertion, deletion and substitution operations that are re- quired to transform an utterance u i into item's title t k .</p><p>Word Order: This feature represents how sim- ilar are the order of words in two text. Sentences containing the same words but in different orders may result in different meanings. We extend Jac- card similarity by defining bigram word vectors r i and r k and look for overlapping bigrams as in Ta- ble 3. Among 6 bigrams between them, only 2 are overapping, hence the word-order similarity is S(i,k)=0.33.</p><p>Word Vector: This feature is the cosine sim- ilarity between the utterance u i and the item- title t k that measures the cosine of the an- gle between them.</p><p>Here, we use the uni- gram word counts to represent the word vec- tors and the word vector similarity is defined as: S(i, k)=(r i ยท r k )/r i ยท r k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge Graph Features</head><p>This binary feature is used to represent overlap be- tween utterance and the meta information about the item and is mainly aimed to resolve implicit REs.</p><p>First, we obtain the meta information about the on-screen items using Freebase <ref type="bibr" target="#b3">(Bollacker et al., 2008)</ref>, the knowledge graph that contains knowledge about classes (books, movies, ...) and their attributes (title, publisher, year-released, ...). Knowledge is often represented as the attributes of the instances, along with values for those prop- erties. Once we obtain the attribute values of the item from Freebase, we check if any attribute overlaps with part of the utterance. For instance, given an utterance "how about the one with Kevin Spacey", and the item-title "House of Cards", the knowledge graph attributes include year(2013), cast(Kevin Spacey), director(James Foley),... We turn the freebase feature 'on' since the actor at- tribute of that item is contained in the utterance. We also consider partial matches, e.g., last name of the actor attribute.</p><p>This feature is also used to resolve implicit REs, with item descriptions, such as "the messenger boy with bicycle" referring to the media item Ride Like Hell, a movie about a bike messenger. The syn- opsis feature in Freebase fires the freebase meta feature as the synopsis includes the following pas- sage: "... in which real ::::::::: messenger ::::: boys are used as stunts... ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Location Labeler (SLL) Feature</head><p>This feature set captures spatial cues in utterances and is mainly aimed to resolve explicit locational REs. Our goal is to capture the location indicating tokens in utterances and then resolve the referred location on the screen by using an indicator fea- ture. We implement the SLL (Semantic Location Labeler), a sequence labeling model to tag loca- tional cues in utterances using Conditional Ran- dom Fields (CRF) ( <ref type="bibr" target="#b22">Lafferty et al., 2001</ref>).</p><p>We sampled a set of locational utterances from each domain to be used as training set. We asked the annotators to label tokens with four different semantic tags that indicate a location.</p><p>The semantic tags include row and column in- dicator tags, referring to the position or pivotal reference. For instance, in " :::::: second from the :::</p><p>top", "second" is the column-position, and "top" is the row-pivot, indicating the pivotal reference of the row in a multi-row grid dis- play. Also in " :::: third from the ::: last", the "third" is the column-position, and the "last" is the column-pivot, the pivotal reference of the col- umn in a multi-column grid display. The fourth tag, row-position, is used when the specific row is explicitly referred, such as in "the Harry Potter movie in the ::: first row".</p><p>To train our CRF-based SLL model we use three types of features: the current word, window words e.g., previous-word, next-word, etc., using five-window around the current word, and syntac- tic features from the part-of-speech (POS) tagger using the Stanford's parser ( <ref type="bibr" target="#b20">Klein and Manning, 2003)</ref>.</p><p>Row Indicator Feature: This feature sets the relationship between the n-gram in an utterance in- dicated by the row-position or row-pivot tag and the item's row number on the screen. For instance, given SSL output row-pivot('top') and item's location row=1, the value of the feature is set to '1'. If no row tag is found by SLL, this feature is set to '0'. We use regular expressions to parse the numerical indicators, e.g., 'top'='1'.</p><p>Column Indicator Feature: Similarly, this feature indicates if a phrase in utterance indicated by the column-position or column-pivot tag matches the item's col- umn number on the screen. If SLL model tags column-pivot('on the left'), then using the item's column number(=1), the value of this feature is set to '1'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SLU Features</head><p>The SLU (Spoken Language Understanding) fea- tures are used to resolve implicit and explicit REs.</p><p>For our dialog system, we build one SLU model per each domain to extract two sets of semantic at- tributes from utterances: user's intent and seman- tic slots based on a predefined semantic schema (see examples in <ref type="table" target="#tab_2">Table 2</ref>). We use the best in- tent hypothesis as a categorical feature in our FIS model. Although FIS is not an intent detection model, the intent from SLU is an effective seman- tic feature in resolving REs. Consider second turn utterance such as "weather in seattle", which is a 'find' intent that is a new search or not related to any item on the screen. We map SLU intents such as find-book or find-place, to more specific ones, so that the intent feature would have values such as find, filter, check-time, not specific to a domain or device. The intent feature helps us to identify if user's utterance is related to any item on the screen. We also use the best slot hypothesis from the SLU slot model and search if there is full overlap of any recognized slot value with either the item-title or the item meta-information from free- base. In addition, we include the longest slot value n-gram match as an additional feature. We add a binary feature per domain, indicating whether there is a slot value match. Because we are us- ing generic intents as categorical features instead of specific intents, and a slot value match feature instead of domain specific slot types as features, our models are rather domain independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GBDT Classifier</head><p>Among various classifier learning algorithms, we choose the GBDT (gradient boosted decision tree) <ref type="bibr" target="#b10">(Friedman, 2001;</ref><ref type="bibr" target="#b15">Hastie et al., 2009)</ref>, also known as MART (Multiple Additive Regression Trees). GBDT 3 is an efficient algorithm which learns an ensemble of trees. We find the main advantage of the decision tree classifier as opposed to other non-linear classifiers such as SVM (support vec- tor machines) <ref type="bibr">(Vapnik, 1995)</ref> or NN (neural net- works) <ref type="bibr" target="#b2">(Bishop, 1995)</ref> is the interpretability. De- cision trees are "white boxes" in the sense that per- feature gain can be expressed by the magnitude of their weights, while SVM or NN's are gener- ally black boxes, i.e. we cannot read the acquired knowledge in a comprehensible way. Addition- ally, decision trees can easily accept categorical and continuous valued features. We also present the results of the SVM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We investigate several aspects of the SISI model including its robustness in resolving REs for do- main or device variability. We start with the details of the data and model parameters.</p><p>We collect around 16K utterances in the me- dia domains (movies, music, tv, and books) and around 10K utterances in places (businesses and   locations) domain. We also construct additional negative instances from utterance-item pairs us- ing first turn non-selection queries, which mainly indicate a new search or starting over. In total we compile around 250K utterance-item pairs for media domains and 150K utterance-item pairs for the places domain. <ref type="bibr">4</ref> We randomly split each col- lection into 60%-20%-20% parts to construct the train/dev/test datasets. We use the dev set to tune the regularization parameter for the GBDT and SVM using LIBSVM (Chang and Lin, 2011) with linear kernel. We use the training dataset to build the SLU in- tent and slot models for each domain. For the in- tent model, we use the GBDT classifier with n- gram and lexicon features. The lexicon entries are obtained from Freebase and are used as indicator variables, e.g., whether the utterance contains an instance which exists in the lexicon. Similarly, we train a semantic slot tagging model using CRF method. We use n-gram features with up to five- gram window, and lexicon features similar to the intent models. <ref type="table" target="#tab_5">Table 4</ref> shows the accuracy and F- score values of SLU models on the test data. The slot and intent performance is consistent accroess domains. The books domain has only two intents and hence we observe much better intent perfor- mance compared to other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Impact of Individual FIS Features</head><p>In our first experiment, we investigate the impact of individual feature sets on FIS model's perfor- mance. We train a set of FIS models on the entire media dataset to investigate the per-feature gain on the test dataset for each domain. We also train an- other set of FIS models with the same feature sets, this time on the places dataset and present the re- sults on the places test set. <ref type="table" target="#tab_4">Table 5</ref> shows the re- sults. We measure the performance starting with individual feature sets, and then incrementally add each feature set. Note that the SLU feature set includes the categorical intent, binary slot-value match and the longest slot value n-gram match with the item's title or meta information. The SLL feature set includes two features indicating the row and column (see ยง4.3).</p><p>As expected, larger gains in accuracy are ob- served when features that resolve different REs are used. Resolving locational cues in utter- ances with SLL features considerably impacts the performance when used together with similarity (SIM) features. We see a positive impact on per- formance as we add the knowledge graph features, which are used to resolve implicit REs. Using only the predicted SLU features in feature gener- ation without golden values degrades the perfor- mance. Although the results are not statistically significant, the GBDT outperforms the SVM for almost all models, except for a few models, where the results are similar. However, the models which combine different features as apposed to individ- ual feature set (the above the line models versusu below the horizantil line models) are statistically significant (based on the student t-test pยก0.01).</p><p>Next, we illustrate the significance of individual features across domains as well as devices. <ref type="figure" target="#fig_3">Fig. 3</ref> compares the normalized feature weights of me- dia and places domains. Across domains there are similar features with similar weight values such as SLU-intent, some similarity features (SIM-) and even spatial cue features (SLL). It is not surpris- ing to observe that the places domain knowledge- graph meta feature weights are noticeably larger than all media model features. We think that this is due to the way the REs are used when the de- vice changes (places app is on a phone with a smaller screen display). Especially, places appli- cation users refer items related to restaurants, li- braries, etc., not so much by their names, but more so with implicit REs by using: the location (refer- ring to the address: "call the one on 31 street") or cuisine ("Chinese"), or the star-rating ("with the most stars"), etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Resolution Across REs</head><p>We go on to analyze the performance of differ- ent RE types. A particularly interesting set of er- rors we found from the previous experiments are those that involve implicit referrals. <ref type="table" target="#tab_7">Table 6</ref> shows the distribution of different REs in the collected datasets.</p><p>Some noticeable instances with false positives for implicit locational REs include ambiguous cases or item referrals with one of its facets that require further resolution including comparison to other items, e.g., "the nearest one". <ref type="table">Table 7</ref> shows further examples. As might be expected, the lo- cational cues are less common compared to other  "second one over" (incomplete row/col. information) <ref type="table">Table 7</ref>: Display screen as user utters.</p><p>expressions. We also confirm that the handheld (places domain) users implicitly refer to the items more commonly compared to media app, and use the contextual information about the items such as their location, address, star-rating, etc. The mod- els are considerably better at resolving explicit re- ferrals (both non-spatial and spatial) compared to implicit ones. However, for locational referrals, the difference between the accuracy of implicit and explicit REs is significant (75.2% vs. 56.6% in media and 86.2% vs. 56.7% in places). Al- though not very common, we observe negative ex- pressions, e.g., "the one with no reviews", which are harder for the FIS to resolve. They require quantifying over every other item on the screen, namely the context features, which we leave as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">New Domains and Device Independence</head><p>In   els in accuracy on each media test domain. The first row shows the results when all domains are used at training time (same as in <ref type="table" target="#tab_4">Table 5</ref>). The second row represents models where one domain is unseen at training time. We notice that the ac- curacy, although degraded for movies and tv do- mains, is in general not significantly effected by the domain variations. We setup another experi- ment, where we incrementally add utterances from the domain that we are testing the model on. For instance, we incrementally add random samples from movies training utterances on the dataset that does not contain movies utterances and test on all movies test data. The charts in <ref type="figure" target="#fig_4">Fig. 4</ref> show the % improvement in accuracy as in-domain data is incrementally added to the training dataset. The results are interesting in that, using as low as 10- 20% in-domain data is sufficient to build a flexi- ble item selection model given enough utterances from other domains with varying REs. Robustness to a New Device: The difference between the vocabulary and language usage ob- served in the data collected from the two devices Media "only the new movies" ; "second one on the left" "show me the thriller song"; "by Lewis Milestone" "the first harry potter book" Places "directions to Les Schwab tire center" "the closest one" ;"show me a map of ..." "get hours of Peking restaurant"; "call Mike's burgers"  <ref type="table" target="#tab_0">Table 10</ref>: Accuracy of FIS models tested on two separate devices (large screen media, and small screen places) that are unseen at test time.</p><p>is mainly due to changes in: (i) the screen design (places on phone has one column format wheres the media app has multi-column layout); (ii) the domain of the data. <ref type="table" target="#tab_10">Table 9</ref> shows some exam- ples. Here, we add a little bit of complexity, and train one FIS model using the training data col- lected on one device and test the model on a dif- ferent one, which is unseen at training time. <ref type="table" target="#tab_0">Table  10</ref> shows the comparisons for media and phone interfaces. The results are interesting. The perfor- mance of the places domain on phone does not get affected when the models are trained on the media data and tested on the phone device (86.3% down to 85.9% which is statistically insignificant). But when the data is trained on the places and tested on the media, we see a rather larger degradation on the performance (93.7% down to 85.9%). This is due to the fact that the media display screens are much complicated compared to phone result- ing in a larger vocabulary with more variation in REs compared to places domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Conclusion</head><p>We presented a framework for identifying and rec- ognizing referring expressions in user utterances of human-machine conversations in natural user interfaces. We use several on-screen cues to in- terpret whether the user is referring to on-screen items, and if so, which item is being referred to. We investigate the effect of different set of fea- tures on the FIS models performance. We also show that our model is domain and device inde- pendent which is very beneficial when new do-mains are added to the application to cover more scenarios or when FIS is implemented on new de- vices. As a future work, we would like to adapt our model for different languages and include other features from multi modality including gesture or geo-location.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sketches of different HITApp Screens. The red arrows point to the media we want the annotators to refer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A HitApp screen of places app. Items returned by the system regarding the first-turn utterance "burger places near me?"</figDesc><graphic url="image-6.png" coords="4,72.00,248.47,104.31,129.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A sample of normalized feature weights of the GBDT FIS models across domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The accuracy (y-axis) versus the percentage (%) of in-domain utterances used in the training dataset. The dashed vertical line indicates an optimum threshold for the amount of in-domain data to be added to the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>A sample multi-turn dialog. A list of second turn 

utterances referring to the last book (in bold) and a new search 
query (highlighted) are shown. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>A sample of intents and semantic slot tags 
of utterance segments per domain. Examples for 
some slots values are presented in parenthesis as 
italicized. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance of the FIS models on test data using different features. Acc:Accuracy,. SIM: sim-
ilarity features; SLU:Spoken Language Understanding features (intent and slot features); SLL:Semantic 
Locational Labeler features; Gold: using true intent and slot values, Pred.: using predicted intent and 
slot values from the SLU models. 

Model: 
Movies TV Music Books Places 

Intent Acc. 84.5% 87.4% 87.6% 98.1% 89.5% 

Slot F-score 92.1F 89.4F 88.5F 86.6F 88.4F 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The performance of the SLU Engine's 
intent detection models in accuracy (Acc.) and slot 
tagging models in F-Score on the test dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Distribution of referring expressions 
(RE) in the media (large screen like tv) and places 
(handheld device like phone) corpus and the FIS 
accuracies per RE type. 

Utterance 
Displayed on screen 

"the most rated restaurant" 
's next to each item 

"first thomas crown affair" 
original release (vs. remake) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>the series of experiments below, we empirically investigate the FIS model's robustness to when a new domain or device is introduced. Robustness to New Domains: So far we trained media domain FIS models on utterances from all domains. To investigate how FIS models would behave when tested on a new domain, we train additional models by leaving out utterances from one domain and test on the left out domain. We used GBDT with all the feature sets. To set up an upper bound, we also train models on each individual domain and test on the same domain. Table 8 shows the performance of the FIS mod-</figDesc><table>Models tested on: 
Model trained on: Movies TV Music Books 
All domains 
96.2% 95.2% 90.3% 94.6% 
All other domains 
94.6% 92.4% 89.7% % 
Only *this domain 96.4% 96.8% 93.4% % 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 : Accuracy of FIS models tested on domains that are: seen at training time (all domains), unseen at training time (all other domains) and trained on individual domains.</head><label>8</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Sample of utterances collected from media and 

places applications illustrating the differences in language us-
age. 

Trained on 
Tested on Media Tested on Places 

Media 
93.7 % 
85.9% 

Places 
85.9% 
86.3% 

Media+Places 
92.7% 
85.8% 

</table></figure>

			<note place="foot" n="1"> An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen.</note>

			<note place="foot" n="2"> Please e-mail the first author to inquire about the datasets.</note>

			<note place="foot" n="3"> Treenet: http://www.salford-systems.com/products/ treenet is the implementation of the GBDT which is used in this paper.</note>

			<note place="foot" n="4"> In the final version of the paper, we will provide annotated data sets on a web page, which is reserved due to blind review.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech and gesture interaction in an ambient assited living lab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitr</forename><surname>Anastasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desislava</forename><surname>Zhaekova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st Workshop on Speech and Multimodal Interaction in Assitive Environments at ACL</title>
		<meeting>of the 1st Workshop on Speech and Multimodal Interaction in Assitive Environments at ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multi-modal spoken dialog system for interactive tv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Balchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerasimos</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lsadislav</forename><surname>Seredi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th International Conference on Multimodal Interfaces</title>
		<meeting>of the 10th International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural networks for Pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ttim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2008 International Conference on Management of Data</title>
		<meeting>of the 2008 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>SIGMOD-08</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Put-that-there: Voice and gesture at the graphics interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Creating speech and language data with amazons mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deanna</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilkes-Gibbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Referring as colloborative processes</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Referring expression generation through attribute-based heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jette</forename><surname>Viethen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th European Workshop on Natural Language Generation (ENLG)</title>
		<meeting>of the 12th European Workshop on Natural Language Generation (ENLG)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified probabilistic approach to referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Funakoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikio</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takenobu</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Special Interest Group on Discourse and Dialog (SIGDIAL)</title>
		<meeting>of the Special Interest Group on Discourse and Dialog (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reference resolution mechanisms in dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Gieselmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th Workshop on the semantics and pragmatics of dialogues (CATALOG)</title>
		<meeting>of the 8th Workshop on the semantics and pragmatics of dialogues (CATALOG)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adapta multimodal conversational dialogue system in an apartment domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Boye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Granstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Wiren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th International Conference on Spoken Language Processing (ICSLP)</title>
		<meeting>of the 6th International Conference on Spoken Language essing (ICSLP)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="134" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Friedman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chapter 10. Boosting and Additive Trees</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>The Elements of Statistical Learning</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal conversational search and browse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhu</forename><surname>Chinthakunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rukmini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Stifelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Workshop on Speech, Language and Audio in Multimedia</title>
		<meeting>of the IEEE Workshop on Speech, Language and Audio in Multimedia</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive asr error correction for touchscreen devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvaid</forename><surname>Huggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Daines</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Demo session</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive referring expression generation in spoken dialog systems: Evaluation with real users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Janarthanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Match: an architecture for multimodal dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunaranjan</forename><surname>Vasireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ehlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetam</forename><surname>Maloor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL</title>
		<meeting>of the ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integrating simultaneous input from speech, gaze and hand gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Koons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><forename type="middle">J</forename><surname>Sparrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristinn</forename><forename type="middle">R</forename><surname>Thorisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Multimedia Interfaces</title>
		<editor>Maybury, M.</editor>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="257" to="276" />
		</imprint>
	</monogr>
	<note>Proc. of the In</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional random fields: probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlademir Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Doklady Akademii Nauk SSSR</title>
		<meeting>of the Doklady Akademii Nauk SSSR</meeting>
		<imprint>
			<date type="published" when="1965" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Situated language understanding at 25 miles per hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGDIAL-Annual Meeting on Discourse and Dialogue</title>
		<meeting>of the SIGDIAL-Annual Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spoken language understanding: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Bechet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mctear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="50" to="58" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intelligent multimedia interface technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intelligent User Interfaces</title>
		<editor>Sullivan, J., and Tyler, S.</editor>
		<meeting>of the Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="45" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integration and synchronization of input modes during multimodal human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonella</forename><surname>Deangeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Khun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Human Factors in Computing Systems: CHI</title>
		<meeting>of the Human Factors in Computing Systems: CHI</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards resolving referring expressions by implicitly activated referents in practical dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobert</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Workshop on the Semantics and Pragmatics of Dialog</title>
		<meeting>of the 10th Workshop on the Semantics and Pragmatics of Dialog</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>SemDial-10</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding by addressees and overhearers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><forename type="middle">H</forename><surname>Schober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Cognitive Psychology</title>
		<meeting>of the Cognitive Psychology</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="211" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Vlademrr Vapnik. 1995. The nature of statistical learning theory</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
