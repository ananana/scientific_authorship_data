<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sari</forename><surname>Saba-Sadiya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Shukla</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Vision</orgName>
								<orgName type="institution">and Autonomy University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Cognition, Los Angeles</settlement>
									<region>Learning, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhong</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Vision</orgName>
								<orgName type="institution">and Autonomy University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Cognition, Los Angeles</settlement>
									<region>Learning, CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<email>sczhu@stat.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Vision</orgName>
								<orgName type="institution">and Autonomy University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Cognition, Los Angeles</settlement>
									<region>Learning, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1482" to="1492"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To enable language-based communication and collaboration with cognitive robots, this paper presents an approach where an agent can learn task models jointly from language instruction and visual demonstration using an And-Or Graph (AoG) representation. The learned AoG captures a hierarchical task structure where linguistic labels (for language communication) are grounded to corresponding state changes from the physical environment (for perception and action). Our empirical results on a cloth-folding domain have shown that, although state detection through visual processing is full of uncertainties and error prone, by a tight integration with language the agent is able to learn an effective AoG for task representation. The learned AoG can be further applied to infer and interpret ongoing actions from new visual demonstration using linguistic labels at different levels of granularity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given tremendous advances in robotics, computer vision, and natural language processing, a new gen- eration of cognitive robots have emerged that aim to collaborate with humans in joint tasks. To facili- tate natural and efficient communication with these physical agents, natural language processing will need to go beyond traditional symbolic representa- tions, but rather ground language to sensors (e.g., vi- sual perception) and actuators (e.g., lower-level con- trol systems) of physical agents. The internal task * The first two authors contributed equally to this paper. representation will need to capture both higher-level concepts (for language communication) and lower- level visual features (for perception and action).</p><p>To address this need, we have developed an ap- proach on learning procedural tasks jointly from language instruction and visual demonstration. In particular, we use And-Or Graph (AoG), which has been used in many computer vision tasks and robotic applications ( <ref type="bibr" target="#b40">Zhao and Zhu, 2013;</ref><ref type="bibr" target="#b15">Li et al., 2016;</ref>, to represent a hier- archical task model that not only captures symbolic concepts (extracted from language instructions) but also the corresponding visual state changes from the physical environment (detected by computer vision algorithms).</p><p>Different from previous works that ground lan- guage to perception ( <ref type="bibr" target="#b16">Liu et al., 2012;</ref><ref type="bibr" target="#b17">Matuszek et al., 2012;</ref><ref type="bibr" target="#b14">Kollar et al., 2013;</ref><ref type="bibr" target="#b39">Yu and Siskind, 2013;</ref>, a key innovation in our frame- work is that language is no longer grounded just to perceived objects in the environment, but is further grounded to a hierarchical structure of state changes where the states are perceived from the environment during visual demonstration. The state of environ- ment is an important notion in robotic systems as the change of states drives planning for lower-level robotic actions. Thus, connecting language concepts to state changes, our learned AoG provides a unified representation that integrates language and vision to not only support language-based communication but also facilitate robot action planning and execution in the future.</p><p>More specifically, within this AoG framework, we have developed and evaluated our algorithms in the context of learning a cloth-folding task. Although cloth-folding appears simple and intuitive for hu- mans, it represents significant challenges for both vision and robotics systems. Furthermore, although symbolic language processing in this domain is easy due to limited use of vocabulary, grounded language understanding is particularly challenging. A sim- ple phrase (e.g., "fold in half") could have differ- ent grounded meanings (e.g., lower-level represen- tation) given different contexts. Thus, this cloth- folding domain is a good starting point to focus on grounding language to task structures.</p><p>Our empirical results have shown that, although state detection from the physical world can be ex- tremely noisy, our learning algorithm that tightly in- corporates language is capable of acquiring an effec- tive and meaningful task model to compensate the uncertainties in visual processing. Once the AoG for the task is learned, it can be applied by our infer- ence algorithm, for example, to infer on-going ac- tions from new visual demonstration and generate linguistic labels at different levels of granularity to facilitate human-agent communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years have seen an increasing amount of work on grounding language to visual percep- tion ( <ref type="bibr" target="#b16">Liu et al., 2012;</ref><ref type="bibr" target="#b17">Matuszek et al., 2012;</ref><ref type="bibr" target="#b39">Yu and Siskind, 2013;</ref><ref type="bibr" target="#b14">Kollar et al., 2013;</ref><ref type="bibr" target="#b23">Naim et al., 2015;</ref>. Further- more, the robotics community made significant ef- forts to utilize novel grounding techniques to facil- itate task execution given natural language instruc- tions ( <ref type="bibr" target="#b5">Chen et al., 2010;</ref><ref type="bibr" target="#b13">Kollar et al., 2010;</ref><ref type="bibr" target="#b33">Tellex et al., 2011;</ref><ref type="bibr" target="#b18">Misra et al., 2014</ref>) and task learning from demonstration ( <ref type="bibr" target="#b28">Saunders et al., 2006;</ref><ref type="bibr" target="#b6">Chernova and Veloso, 2008)</ref>.</p><p>Research on Learning from Demonstration (LfD) employed various approaches to model the tasks ( <ref type="bibr" target="#b0">Argall et al., 2009</ref>), such as state-to-action mapping , predicate calculus ( <ref type="bibr" target="#b9">Hofmann et al., 2016)</ref>, and Hierarchical Task Networks ( <ref type="bibr" target="#b24">Nejati et al., 2006;</ref><ref type="bibr" target="#b10">Hogg et al., 2009)</ref>. However, aspiring to enable human robot communication, the framework developed in this paper focuses on task representation using language grounded to a structure of state changes detected from the physical world. As demonstrated in recent work ( <ref type="bibr" target="#b30">She et al., 2014a;</ref><ref type="bibr" target="#b19">Misra et al., 2015;</ref><ref type="bibr" target="#b29">She and Chai, 2016)</ref>, explicitly modeling change of states is an important step towards interacting with robots in the physical world.</p><p>Additionally, there has also been an increasing amount of work that learns new tasks either using methods like supervised learning on large corpus of data ( <ref type="bibr" target="#b1">Branavan et al., 2010;</ref><ref type="bibr" target="#b2">Branavan et al., 2012;</ref><ref type="bibr" target="#b34">Tellex et al., 2014;</ref><ref type="bibr" target="#b18">Misra et al., 2014</ref>), or by learn- ing from humans through dialogue <ref type="bibr" target="#b3">(Cantrell et al., 2012;</ref><ref type="bibr" target="#b21">Mohan et al., 2013;</ref><ref type="bibr" target="#b31">She et al., 2014b;</ref><ref type="bibr" target="#b22">Mohseni-Kabir et al., 2015)</ref>. In this pa- per, we focus on jointly learning new tasks through visual demonstration and language instruction. The learned task model is explicitly represented by an AoG, a hierarchical structure consisting of both lin- guistic labels and corresponding changes of states from the physical world. This rich task model will facilitate not only language-based communication, but also lower-level action planning and execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task and Data</head><p>In this paper, we use cloth-folding (e.g., teaching a robot how to fold a T-shirt) as the task to demon- strate and evaluate our joint task learning approach. As mentioned earlier, cloth-folding, although sim- ple for humans, represents a challenging task for the robotics community due to the complex state and ac- tion space. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the setting for our situated task learning. A human teacher can teach a robot how to fold a T-shirt through simultaneous verbal instructions and visual demonstrations. A Microsoft Kinect2 camera is mounted on the robot to record A recorded video of task demonstration and its corresponding verbal instruction become one train- ing example for our task learning system. <ref type="figure" target="#fig_1">Figure 2</ref> shows two examples of such "parallel data". The vi- sual demonstration is processed into a sequence of visual states, where each state is a numeric vector ( v i ) capturing the visual features of the T-shirt at a particular time (see later Section 5.1 for details). The recorded verbal instructions are then aligned with the sequence of visual states based on the timing in- formation.</p><p>During teaching the task, we specifically re- quested the demonstrator to describe and do each step at roughly the same time. This greatly simpli- fied the alignment problem. Since our ultimate goal is to enable humans to teach the robot through nat- ural language dialogue and demonstration, our hy- pothesis is that the alignment issue can be alleviated by certain dialogue mechanism (e.g., ask to repeat the action, ask for step-by-step aligned instructions, etc.). As it is human's best interest that the robot gets the clearest instructions, we also anticipate during dialogue human teachers will be collaborative and provide mostly aligned instructions. Certainly, these hypotheses will need to be validated in the dialogue setting in our future work.</p><p>In our collected data, each change of state, i.e., a transition between two visual states, is caused by one or more physical actions. Some language de- scriptions align with only a single-step change of state. For instance, "fold right sleeve" is aligned with the change ( v 0 → v 1 ) and "fold left sleeve" is aligned with ( v 1 → v 2 ) in Example 1. This kind of single-step change of state is considered as a primitive action. Other language descriptions are aligned with a sequence of multiple state changes. For instance, "fold the two sleeves" in Example 2 is aligned with two consecutive changes:</p><formula xml:id="formula_0">( v 0 , v 1 , v 2 )</formula><p>. This kind of sequence of state changes is considered as a complex action, which can be decomposed into partially ordered primitive actions. A complex ac- tion can also be concisely represented by the change from the initial state to the end state in the sequence, such as ( v 0 → v 2 ) in Example 2. These parallel data are used to train and test our learning and inference algorithms presented later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">And-Or Graph Representation</head><p>We use AoG as the formal model of a procedural task. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example AoG for the cloth- folding task. It is a hierarchical structure that ex- plicitly captures the compositionality and reconfig- urability of a procedural task. The terminal nodes capture state changes associated with primitive ac- tions of this task, and non-terminal nodes capture state changes associated with complex actions which are further composed by lower-level actions.</p><p>In addition to state changes, the learned AoG is also labeled with linguistic information (e.g., verb frames) capturing the causes of the corresponding state changes. The state changes are also considered as grounded meanings of these verb frames. For ex- ample, <ref type="figure" target="#fig_2">Figure 3</ref> shows two "fold the t-shirt" labels at the top layer. Note that although symbolically, these two phrases have the same meaning (e.g., same verb frames), their grounded meanings are different as they correspond to different changes of state. Be- ing able to represent differences or ambiguities in grounded meanings is crucial to connect language to perception and action.</p><p>Formally, an AoG is defined as a 5-tuple G = (S, Σ, N, R, Θ), where</p><p>• S is a root node (or a start symbol) representing a complete task.</p><p>• Σ is a set of terminal nodes, each of which represents a change of state associated with a primitive action.</p><formula xml:id="formula_1">• N = N AN D ∪ N OR is a set of non-terminal</formula><p>nodes, which is divided into two disjoint sub- • R is a "child-parent" relation (many-to-one mapping), i.e., R(n ch ) = n pa (meaning n pa is the parent node of n ch ), where n ch ∈ Σ ∪ N and n pa ∈ N ∪ {S}.</p><p>• Θ is a set of conditional probabili- ties p(n ch |n pa ), where n pa ∈ N OR , n ch ∈ {n | R(n) = n pa }. Namely, for each Or-node, Θ defines a probability distribu- tion over the set of all its children nodes.</p><p>In essence, our AoG model is equivalent to Prob- abilistic Context-Free Grammar (PCFG). An AoG can be converted into a PCFG:</p><p>• Each And-node and its children form a produc- tion rule</p><formula xml:id="formula_2">n AN D → n ch 1 ∧ n ch 2 ∧ . .</formula><p>. that represents the decomposition of a complex action into sequentially ordered sub-actions.</p><p>• Each Or-node and its children form a produc- tion rule</p><formula xml:id="formula_3">n OR → n ch 1 | n ch 2 | . .</formula><p>. that represents all the alternative ways of ac- complishing an action. Each alternative also comes with a probability as specified in Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Vision and Language Processing</head><p>The input data to our AoG learning algorithm con- sist of co-occurring visual demonstrations and lan- guage instructions as described in Section 3. Based on the RGB-D information provided by the Kinect2 sensor, we developed a vision processing system to keep track of human's actions and statuses of the T- shirt object.</p><p>To learn a meaningful task structure, the most im- portant visual information are those key statuses that the object goes through. Therefore, our vision sys- tem processes each visual demonstration into a se- quence of states. Each state v is a multi-dimensional numeric vector that encodes the geometric infor- mation of the detected T-shirt, such as its smallest bounding rectangle and largest inscribed contour- fitting rectangle. These key states are detected by tracking the human's folding actions. Namely, whenever a folding action is detected 1 , we append the new state caused by the action to the sequence of observed states, till the end of the demonstration.</p><p>The verbal instructions given by the demonstra- tors were mainly verb phrases such as "fold which- part", "fold to which-position", or "fold in-what- manner". A semantic parser 2 is applied to parse each instruction text into a canonical verb-frame representation, such as Through the vision and language processing, each task demonstration becomes two parallel sequences, i.e., a sequence of extracted visual states and a se- quence of parsed language instructions. The align- 1 The vision system keeps track of human's hands, and de- tects a folding action as a gripping action followed by moving and releasing the hand(s). <ref type="bibr">2</ref> We use the CMU's Phoenix parser: http://wiki.speech.cs.cmu.edu/olympus/index.php/Phoenix ment between these two sequences is also extracted from their co-occurrence timing information. Thus, an instance of a task demonstration is formally rep- resented as a 3-tuple x = (D, L, ∆), where D = { v 1 , v 2 , . . . , v M } is the sequence of visual states, L = {l 1 , l 2 , . . . , l K } is the sequence of linguis- tic verb-frames, and ∆(k) = (i, j) is an "align- ment function" specifying the correspondence be- tween a linguistic verb-frame l k and a single or a sub-sequence of visual state(s) { v i , . . . , v j } (i ≤ j).</p><p>Then, given a dataset X of such task demonstra- tions, our AoG learning algorithm learns an AoG G as defined in Section 4. The next section describes our learning algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">AoG Learning Algorithm</head><p>Learning an AoG G = (S, Σ, N, R, Θ) is carried out in two stages. Firstly, we learn a set of termi- nal nodes Σ to represent the primitive actions (i.e., the actions that can be preformed in a single step). This is done through clustering the observed visual states. Secondly, the hierarchical structure (i.e., N and R) and parameters Θ of the AoG is learned us- ing an iterative grammar induction algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Learning Terminal Nodes</head><p>A terminal node in the AoG represents a primitive action, which causes the object to directly change from one state to another. Thus we represent a ter- minal node as a 2-tuple of states (or a "change of state"). Since the visual states detected by computer vision are numeric vectors with continuous values, we first apply a clustering algorithm to form a fi- nite set of discrete state representations. Each clus- ter then represents a unique situation that one can encounter in a task. Since when learning a new task we usually do not know how many unique situa- tions exist, here we employ a greedy clustering algo- rithm ( <ref type="bibr" target="#b25">Ng and Cardie, 2002</ref>), which does not assume a fixed number of clusters.</p><p>As the greedy clustering algorithm relies on the pairwise similarities of all the visual states, we also train an SVM classifier on a separate dataset of 22 T- shirt folding videos and use its classification output to measure the similarity between two visual states. The SVM classifier takes two numeric vectors as an input, and predicts whether these two vectors rep- resent the same status of a T-shirt. We then apply this SVM classifier on each pair of detected visual states in our new dataset (i.e., the dataset for learn- ing the AoG), and use the SVM's output class la- bel (1 or −1) multiplies its classification confidence score as the similarity measurement between two vi- sual states.</p><p>After clustering all the observed visual states in the data, we then replace each numeric vec- tor state representation with the cluster "ID" it be- <ref type="figure" target="#fig_2">s 3 )</ref>, . . . , (s M −1 , s M )}, in which each change of state essentially represents a primi- tive action in this task. These change of state pairs then form the set of terminal nodes Σ.</p><note type="other">longs to. Thus each visual demonstration now be- comes a sequence of symbolic values, denoted as D = {s 1 , s 2 , . . . , s M }. And we further trans- form it into an equivalent change of state sequence C = {(s 1 , s 2 ), (s 2 ,</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Learning the Structure and Parameters</head><p>With the sequences of numeric vector states re- placed by the "symbolic" change of state sequences in the first stage, we can further learn the structure and parameters of an AoG. Namely, to learn N , R, and Θ that maximize the posterior probability:  To solve the first term, we use greedy or beam search with a heuristic function similar to <ref type="bibr" target="#b32">(Solan et al., 2005</ref>). To solve the second term, we estimate the probability of each branch of an Or-node by comput- ing the frequency of that branch, which is essentially a maximum likelihood estimation similar to ( <ref type="bibr" target="#b26">Pei et al., 2013)</ref>.</p><p>In detail, the learning procedure first initializes empty N , R, and Θ, then iterates through the follow- ing two steps until no further update can be made.</p><p>Step (1): search for new And-nodes.</p><p>This step searches for new And-node candidates from Σ∪N , and update N and R with the top-ranked candidates. Specifically, we denote an And-node candidate to be searched as A = (s l → s m → s r ).</p><p>Here s l is the initial state of an existing node, whose end state is s m . And s r is the end state of another ex- isting node, whose initial state is s m . Thus an And- node candidate always has two child nodes, and rep- resents a pattern of sub-sequences which starts from state s l , ends at s r , and has s m occurred somewhere in the middle.</p><p>Using the above notation, the heuristic function for ranking And-node candidates is defined as</p><formula xml:id="formula_4">h(A) = (1 − λ)P state (A) + λP label (A)</formula><p>where P state (A) captures the prevalence of a partic- ular And-node candidate based on the observed state change sequences: We specially define two AoG learning settings based on the role that language plays:</p><formula xml:id="formula_5">P state (A) = P R (A) + P L (A</formula><p>• Tight language integration: incorporate heuris- tics on linguistic labels (i.e., λ = 0.5). In this setting, the learned AoG prefers And-nodes that not only happen frequently, but also can be described by a linguistic label.</p><p>• Loose language integration: without incorpo- rating the heuristics on linguistic labels (λ = 0). Each And-node is learned only based on the frequency of its state change pattern. The learned node can still acquire a linguistic label if there happen to be a co-occurring one, but the chance is lower than the "tight" setting.</p><p>Step <ref type="formula">(2)</ref>: search for new Or-nodes or new branches of existing Or-nodes, then update Θ.</p><p>Once new And-nodes are added by the previous step, the next step is to search for Or-nodes that can be created or updated. An Or-node in the AoG essentially represents the set of all And-nodes that share the same initial and end states, denoted as (s l → s r ) here (s l and s r are the common initial and end states, respectively). Suppose (s l → s m → s r ) is a newly added And-node, it is then assigned as a child of the Or-node (s l → s r ). To further update Θ, the branching probability is computes as the ra- tio between the number of times (s l → s m → s r ) appears and the number of times (s l → s r ) appears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Inference Using AoG</head><p>Once a task AoG is learned, it can be applied to in- terpret and explain new visual demonstrations using linguistic labels. Due to the noises and uncertainties from computer vision processing, one key challenge in interpreting the visual demonstration is to reliably identify the different states of the T-shirt.</p><p>To tackle this issue, we formulate a joint infer- ence problem. Namely, given a task demonstration video, we first process it into a sequence of numeric vector states D = { v 1 , v 2 , . . . , v M } as described in Section 5.1. Then the goal of inference is to find the most-likely parse tree T and a sequence of "sym- bolic states" D = {s 1 , s 2 , . . . , s M } based on the AoG G and the input D:</p><formula xml:id="formula_6">(T * , D * ) = arg max T,D P (T, D | G, D)</formula><p>We apply a chart parsing algorithm <ref type="bibr" target="#b12">(Klein and Manning, 2001</ref>) to efficiently solve this problem. Furthermore, to accommodate the ambiguities in mapping a numeric vector state v m to a symbolic state, we take into consideration the top-k hypothe- ses measured by the similarity between v m and a symbolic state s k . <ref type="bibr">4</ref> For each state mapping hypoth- esis, we add a completed edge between indices m and m + 1 in the chart, with s k as its symbol and a probability p based on the similarity between v m and s k . Based on the given AoG, the chart pars- ing algorithm then uses Dynamic Programming to search the best parse tree that maximizes the joint probability of P (T, D |G, D). <ref type="figure" target="#fig_7">Figure 4</ref> illustrates the input and output of our in- ference algorithm. As illustrated by this example, the parse tree represents a hierarchical structure un- derlying the observed task procedure, and the lin- guistic labels associated with the nodes can be used to describe the primitive and complex actions in- volved in the procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>Using the setting as described in Section 3, we collected 45 T-shirt folding demonstrations from 6 people to evaluate our AoG learning and inference methods. More specifically, we conducted a 5-fold cross validation. In each fold, 36 demonstrations were used for training to learn a task AoG. Then the remaining 9 demonstrations were used for testing, in which the learned AoG is further applied to process each of the testing visual demonstrations.</p><p>Motivated by earlier work on plan/activity recog- nition using CFG-based models <ref type="bibr" target="#b4">(Carberry, 1990;</ref><ref type="bibr" target="#b27">Pynadath and Wellman, 2000</ref>), we use an extrin- sic task that automatically assigns linguistic labels to new demonstrations to evaluate the quality of the learned AoG and the effectiveness of the inference algorithm. This involves three steps: (1) parse the video using the learned AoG; (2) identify linguistic labels associated with terminal or nonterminal nodes in the parse tree; and (3) compare the identified lin- guistic labels with the manually annotated labels.</p><p>We conduct the evaluation at two levels:</p><p>• Primitive actions: use linguistic labels associ- ated with terminal nodes to describe the primi- tive actions in each video. This level provides detailed descriptions on how the observed task procedure is performed step-by-step.</p><p>• Complex actions: use linguistic labels associ- ated with nonterminal nodes to describe com- plex actions. This provides a high-level "sum- mary" of the detailed low-level actions.</p><p>The capability to recognize fine-grained primitive actions as well as high-level complex actions in a task procedure and to communicate those in lan- guage is important for many real-world AI applica- tions such as human-robot collaboration <ref type="bibr">(MohseniKabir et al., 2015)</ref> and visual question answer- ing ( <ref type="bibr" target="#b36">Tu et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Primitive Actions</head><p>We first compare the performance of interpreting primitive actions using the learned AoG with a base- line. The baseline applies a memory-based (or similarity-base) approach. Given a testing video, it extracts all the different visual states and maps each state to the nearest cluster learned from the train- ing data (see Section 5.2.1). It then pairs each two consecutive states as a change of state instance, and uses the linguistic label corresponding to the identi- cal change of state found in the training data as the label of a primitive action.</p><p>We measure the primitive action recognition per- formance in terms of the normalized Minimal Edit Distance (MED). Namely, for each testing demon- stration we calculate the MED between the ground- truth sequence of primitive action labels and the au- tomatically generated sequence of labels, and divide the MED value by the length of the ground-truth sequence to produce a normalized score (a smaller score indicates better performance in recognizing the primitive actions). The performances of the baseline and our AoG- based approach are shown in <ref type="figure" target="#fig_8">Figure 5</ref>. For the AoG-based approach, <ref type="figure" target="#fig_8">Figure 5</ref> also shows the per- formances of incorporating different number of state mapping hypotheses (i.e., k = 1, 5, 10, 15, 20) into the inference algorithm (Section 5.3). Here we only report the performance of using AoG learned with the tight language integration (see Section 5.2.2), since there is no difference in performance between the tight and loose language integration settings in recognizing primitive actions <ref type="bibr">5</ref> .</p><p>As <ref type="figure" target="#fig_8">Figure 5</ref> shows, the baseline performance is rather weak (i.e., high MED scores). This is largely due to the noise in state clustering and mapping from vision. After manually inspecting the collected demonstration videos, we found 18 unique statuses associated with folding a T-shirt. However the com- puter vision based clustering on average produces more than 30 clusters when all the 36 training ex- amples are used. This makes it difficult to directly match the state changes as in the baseline. For our AoG-based method, when the inference algorithm only takes the single best state mapping hypothesis into consideration (i.e., k = 1), it yields a very weak performance because the observed state change se- quence often cannot be parsed using the learned AoG.</p><p>However, the performance of the AoG-based <ref type="bibr">5</ref> Because the linguistic labels generated for primitive actions are all from terminal nodes, and the two different AoG learning settings only affect nonterminal nodes. method is significantly improved when multiple state mapping hypotheses are incorporated into the inference process. When the top-5 (k = 5) state mapping hypotheses are incorporated into the AoG- based inference, its MED score has already outper- formed the baseline by a 0.3 gap (p &lt; 0.001 using the Wilcoxon signed-rank test). When k = 20, the MED score has dropped by more than 0.6 compared to k = 1 (p &lt; 0.001).</p><p>These results indicate that our AoG-based method is capable of learning useful task structure from small data. When multiple hypotheses of visual state mapping are incorporated, the learned AoG can compensate the uncertainties in vision processing and identify highly reliable primitive actions from unseen demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Complex Actions</head><p>We further evaluate the performance of interpreting complex actions using the learned AoG. The base- line for comparison is similar to the one used in the previous section. It first converts a test video into a sequence of "symbolic" states by mapping each detected visual state to its nearest cluster. It then enumerates all the possible segments that consist of more than two consecutive states and search for the identical segments in the training data. If a matching segment is found, then the corresponding linguistic label (if any) is used as the label for a complex ac- tion. Since complex actions correspond to nontermi- nal nodes in the parse tree generated by AoG-based inference, and some of them may have linguistic la- bels while others may not. We use precision, re- call, and F-score to measure how well the generated linguistic labels match the manually segmented and annotated complex actions in testing videos. <ref type="figure" target="#fig_9">Figure 6</ref> shows the F-scores of recognizing com- plex actions using the AoG learned from the loose and the tight language integration, respectively. In this figure, results are based on k = 20 state map- ping hypotheses incorporated into the inference al- gorithm. As shown here, performances from both settings are significantly better than the baseline (p &lt; 0.001). The AoG learned based on the tight integration with language yields significantly bet- ter performance than the loose integration (over 0.2 gain on F-score, p &lt; 0.001).</p><p>This result indicates that the tight integration of language during AoG learning favors And-node pat- terns that are more likely to be described by natural language (or more consistent with human conceptu- alization of the task structure) <ref type="bibr">6</ref> . Such an AoG repre- sentation can lead to recognition of video segments that can be better explained or summarized by hu- man language. This capability of learning explicit and language-oriented task representations is impor- tant to link language and vision for enabling situated human-agent communication/collaboration. <ref type="table">Table 6</ref>.2 further shows the results from different numbers of state mapping hypotheses that are incor- porated into the inference algorithm. As shown here, the trend of performance improvement with the in- crease in k is again observed. When multiple state mapping hypotheses are incorporated in inference, the learned AoG is capable of compensating uncer- tainties in vision processing and producing better parses for unseen visual demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>This paper presents an approach on task learning where an agent can learn a grounded task model from human demonstrations and language instruc- tions. A key innovation of this work is grounding language to a perceived structure of state changes <ref type="bibr">6</ref> By further investigating the learned AoG under the two dif- ferent settings, we found that the nonterminal nodes learned from the tight language integration setting is more likely to acquire a linguistic label (33%) than the nonterminal nodes learned from the loose setting (18%). based on AoG representation. Once the task model is acquired, it can be used as a basis to support col- laboration and communication between humans and agents/robots. Using cloth-folding as an example, our empirical results have demonstrated that tightly integrating language with vision can effectively pro- duce task structures in AoG that can generalize well to new demonstrations. Although we have only made an initial attempt on a small task, our approach can be naturally ex- tended to more complex tasks such like assembling and cooking. Both the AoG representation and the task learning approach are general and applicable to different domains. What needs to be adapted is the representation of the visual states and computer vi- sion algorithms to detect these states for a specific task.</p><p>Grounding language to a structure of perceived state changes will provide an important stepping stone towards integrating language, perception, and action for human-robot communication and collabo- ration. Currently, our algorithms learn the task struc- tures based on offline parallel data. Our future work will explore incremental learning through human- agent dialogue to acquire grounded task structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The setting of our situated task learning where a human teacher teaches the robot how to fold a T-shirt through both task demonstrations and language instructions.</figDesc><graphic url="image-1.png" coords="2,358.05,57.83,137.10,103.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of our parallel data where language instructions are paired with a sequence of visual states detected from the video.</figDesc><graphic url="image-2.png" coords="3,75.42,57.83,219.96,110.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of the learned AoG</figDesc><graphic url="image-3.png" coords="4,79.51,57.82,453.00,160.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc>OLD : [P ART : lef t sleeve] [P OSIT ION : middle].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>P</head><label></label><figDesc>(N, R|X , Σ) P (Θ|X , Σ, N, R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Following the iterative grammar induction paradigm (Tu et al., 2013; Xiong et al., 2016), we employ an iterative procedure that al- ternatively solves arg max N,R P (N, R|X , Σ) and arg max Θ P (Θ|X , Σ, N, R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>) 2</head><label>2</label><figDesc>and P R (A) is the ratio between the number of times (s l → s m → s r ) appears and the number of times (s l → s m ) appears, and P L (A) is the ratio between the number of times (s l → s m → s r ) appears and the number of times (s m → s r ) appears. The component P label (A) captures the prevalence of linguistic labels associated with the sequential state change patterns. It is computed as the ratio between the number of times (s l → s m → s r ) co-occurs with a linguistic instruction 3 and the to- tal number of times (s l → s m → s r ) appears.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An illustration of the input and output of our AoG-based inference algorithm.</figDesc><graphic url="image-4.png" coords="7,90.17,57.83,431.66,146.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of interpreting primitive actions. Different number of state mapping hypotheses (k) are used in the inference algorithm. The x-axis is the number of training examples used for learning the AoG.</figDesc><graphic url="image-5.png" coords="8,100.36,57.83,170.08,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performances (F-score) of recognizing complex actions. The lowest curve shows the performance from the baseline. Two other curves represent the performance using the AoG learned from the loose integration and the tight integration with language respectively (where k = 20 is used in inference).</figDesc><graphic url="image-6.png" coords="9,100.36,57.83,170.07,141.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Performance of recognizing complex actions using the AoG learned from the loose and tight integration of language as described in Section 5.2. Different (k) number of state mapping hypotheses are used in the inference algorithm. k=1 k=5 k=10 k=15 k=20</head><label>1</label><figDesc></figDesc><table>Precision 
Loose 0.34 0.76 
0.79 
0.84 
0.84 
Tight 
0.34 
0.8 
0.86 
0.89 
0.9 

Recall 
Loose 0.12 0.33 
0.35 
0.38 
0.38 
Tight 
0.12 0.51 
0.59 
0.64 
0.65 

F-Score 
Loose 0.17 0.46 
0.49 
0.52 
0.52 
Tight 
0.18 0.63 
0.70 
0.74 
0.75 

</table></figure>

			<note place="foot" n="3"> Such information is encoded in the ∆ function as mentioned in Section 5.1.</note>

			<note place="foot" n="4"> A symbolic state is represented by a cluster of numeric vector states learned from the training data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to Sarah Fillwock and James Finch for their help on data collection and processing, to Mun Wai Lee for his helpful discus-sions, and to anonymous reviewers for their valu-able comments and suggestions. This work was supported in part by N66001-15-C-4035 from the DARPA SIMPLEX program, and IIS-1208390 and IIS-1617682 from the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey of robot learning from demonstration. Robotics and autonomous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenna</forename><forename type="middle">D</forename><surname>Argall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Browning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="469" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading between the lines: Learning to map high-level instructions to commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning high-level planning from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tell me when and why to do it! runtime planner model updates via natural language instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rehj</forename><surname>Cantrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Schermerhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Scheutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Plan recognition in natural language dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Carberry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training a multilingual sportscaster: Using perceptual context to learn language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="397" to="436" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching multi-robot coordination using demonstration of communication and state sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems</title>
		<meeting>the 7th international joint conference on Autonomous agents and multiagent systems</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1183" to="1186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive policy learning through confidence-based autonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Physical causality of action verbs in grounded language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1814" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Continual planning in golog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Till</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Niemueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Claßen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Lakemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning hierarchical task networks for nondeterministic planning domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Kuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor</forename><surname>Muñoz-Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFirst International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the TwentyFirst International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1708" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning task formulations through situated interactive instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Annual Conference on Advances in Cognitive Systems (ACS)</title>
		<meeting>the Second Annual Conference on Advances in Cognitive Systems (ACS)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page">236</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An o(n 3 ) agenda-based chart parser for arbitrary probabilistic context-free grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">Stanford Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward understanding natural language directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM/IEEE International Conference on Human-robot Interaction (HRI)</title>
		<meeting>the 5th ACM/IEEE International Conference on Human-robot Interaction (HRI)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward interactive grounded language acqusition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><forename type="middle">P</forename><surname>Strimel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing car fluents from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 29th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards mediating shared perceptual basis in situated dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tell me dave: Contextsensitive grounding of natural language to manipulation instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyong</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Environment-driven lexicon induction for high-level instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejia</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A computational model for situated task learning with interactive instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwali</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Cognitive Modeling (ICCM)</title>
		<meeting>the 12th International Conference on Cognitive Modeling (ICCM)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive hierarchical task learning from a single demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anahita</forename><surname>Mohseni-Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Annual ACM/IEEE International Conference on HumanRobot Interaction (HRI)</title>
		<meeting>the Tenth Annual ACM/IEEE International Conference on HumanRobot Interaction (HRI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised alignment of natural language instructions with corresponding video segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iftekhar</forename><surname>Naim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Chol</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning hierarchical task networks by observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negin</forename><surname>Nejati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Konik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning (ICML)</title>
		<meeting>the 23rd international conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="665" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and parsing video events with goal and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangzhang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songchun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1369" to="1383" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic state-dependent grammars for plan recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Pynadath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Sixteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Teaching robots by moulding behavior and scaffolding the environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrystopher</forename><forename type="middle">L</forename><surname>Nehaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerstin</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM SIGCHI/SIGART conference on Humanrobot interaction (HRI)</title>
		<meeting>the 1st ACM SIGCHI/SIGART conference on Humanrobot interaction (HRI)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Incremental acquisition of verb hypothesis space towards physical world interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Teaching robots new actions through natural language instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd IEEE International Symposium on Robot and Human Interactive Communication</title>
		<meeting>the 23rd IEEE International Symposium on Robot and Human Interactive Communication</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="868" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Back to the blocks world: Learning new actions through situated human-robot dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="11629" to="11634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashis</forename><forename type="middle">Gopal</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">J</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning perceptually grounded word meanings from unaligned parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratiksha</forename><surname>Thaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="151" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised structure learning of stochastic and-or grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pavlovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1322" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint video and text parsing for understanding events and answering queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mun</forename><forename type="middle">Wai</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><forename type="middle">Eun</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="70" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robot learning with a spatial, temporal, and causal and-or graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the 2016 IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grounded semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grounded language learning from video described with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3119" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
