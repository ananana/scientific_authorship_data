<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@uic.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="553" to="563"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
					<note>1 Noah&apos;s Ark Lab, Huawei Technologies 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although many sentiment lexicons in different languages exist, most are not comprehensive. In a recent sentiment analysis application, we used a large Chinese sentiment lexicon and found that it missed a large number of sentiment words used in social media. This prompted us to make a new attempt to study sentiment lexicon expansion. This paper first formulates the problem as a PU learning problem. It then proposes a new PU learning method suitable for the problem based on a neural network. The results are further enhanced with a new dictionary lookup technique and a novel polarity classification algorithm. Experimental results show that the proposed approach greatly outper-forms baseline methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment lexicons contain words (such as good, beautiful, bad, and awful) that convey positive or negative sentiments. They are instrumental for many sentiment analysis applications. So far many algorithms have been proposed to generate such lexicons ( <ref type="bibr" target="#b6">Liu, 2012)</ref>. These algorithms are either dictionary-based or corpus-based. In the dictionary-based approach, one exploits synonym and antonym relations in the dictionary to boot- strap a given seed set of sentiment words ( <ref type="bibr">Hu and Liu, 2004;</ref><ref type="bibr">Kim and Hovy, 2004;</ref><ref type="bibr">Kamps et al., 2004</ref>), or learns a classifier to classify the gloss of each word in the dictionary <ref type="bibr">(Esuli and Sebastiani, 2005</ref>). The corpus-based approach uses var- ious linguistic rules or patterns <ref type="bibr">(Hatzivassiloglou and McKeown, 1997;</ref><ref type="bibr">Kanayama and Nasukawa, 2006;</ref><ref type="bibr" target="#b12">Qiu et al., 2011;</ref><ref type="bibr" target="#b16">Tang et al., 2014</ref>). We will discuss these and other existing methods in the re- lated work section.</p><p>Despite many existing studies, the problem is far from being solved. In a recent application, we used a popular Chinese sentiment lexicon for sentiment classification of Weibo posts (similar to Twitter), and found that it missed a large num- ber of sentiment words. As the lexicon was com- piled using formal text such as news, it misses a large number of sentiment words used in social media. Due to the informal nature, many "low class" words are used in social media but seldom used in formal media. New words are also created constantly. Note that new words in Chinese are easier to create from individual characters than in other languages. Thus many of these words are not in the dictionary. All these prompted us to make a new attempt to study sentiment lexicon expansion.</p><p>In this paper, we solve the problem in two steps: (1) identify sentiment words from a given corpus, and (2) classify their polarity. We formulate Step 1 as a PU learning problem (learning from positive unlabeled examples). To our knowledge, this is the first such formulation. This is important because it gives us a formal model to tackle the problem. PU learning is stated as follows ( <ref type="bibr" target="#b7">Liu et al., 2002</ref>): given a set P of examples of a particular class (we also use P to denote the class) and a set U of un- labeled examples which contains hidden instances from both classes P and not-P (called N ), we want to build a classifier using P and U to classify the data in U or future test data into the two classes, i.e., P and N (or not-P). In our case, P is the exist- ing sentiment lexicon, and U is a set of candidate words from a social media corpus. We identify those words in U that are also sentiment words.</p><p>A typical PU learning algorithm works by first identifying a small set of reliable N class exam- ples (RN) from the unlabeled set U and then run- ning a supervised learning method (e.g., SVM) it-eratively to add more and more data to the RN set to finally build a classifier ( <ref type="bibr" target="#b5">Liu, 2011)</ref>.</p><p>In this work, we first adapt a popular such approach to an augmented multilayer perceptron (AMP) method and use it to replace SVM, and show that using SVM as the learning method is inferior to using AMP. However, we can do much better. We then propose a new PU learn- ing method, called SE-AMP (Spy-based Elimina- tion of P class instances using AMP), which can better exploit the specific nature of our problem. SE-AMP goes in the opposite direction to the ex- isting approach. It starts by treating U as the class N (not-P) data, and then runs the AMP learning method on P and U iteratively to gradually re- move potential P class instances from U to purify U so that as iterations progress, fewer and fewer P class instances are still in U . We detail the method in Sec. 3.3. Note that SE-AMP is general and not limited to our task of sentiment lexicon expansion.</p><p>We also propose a new method based on dic- tionary lookup, called Double dictionary Lookup (DL), to enhance the results from the proposed PU learning method. The DL method is also in the framework of PU learning. Our final proposed method for Step 1 is called SE-AMP-DL.</p><p>For polarity classification (Step 2, after sen- timent words are found), we propose a novel method that is based on polarity association of in- dividual (Chinese) characters in each word.</p><p>In summary, this paper has several innovations:</p><p>1. It formulates Step 1 of sentiment lexicon ex- pansion as a PU learning problem. To the best of our knowledge, this is the first such formulation.</p><p>2. It proposes a new neural learning method AMP and shows that AMP outperforms the traditional SVM based PU learning approach.</p><p>3. It further proposes a new and general PU learning strategy that works in the opposite direction to the popular existing approach to suit our task.</p><p>4. It also proposes a double dictionary lookup technique to improve the result further.</p><p>5. It proposes a novel polarity classification method to classify the polarity of each word.</p><p>Experimental results show that the proposed ap- proach makes considerable improvement over ex- isting baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are two main approaches for sentiment lex- icon generation <ref type="bibr" target="#b6">(Liu, 2012)</ref>: the dictionary-based approach and the corpus-based approach. Under the dictionary-based approach, one method is to use synonym and antonym relations and Word- Net graph in the dictionary to bootstrap a set of given seed sentiment words. There are numer- ous variations of and enhancements to this ap- proach ( <ref type="bibr">Hu and Liu, 2004;</ref><ref type="bibr" target="#b18">Valitutti et al., 2004;</ref><ref type="bibr">Kim and Hovy, 2004;</ref><ref type="bibr" target="#b15">Takamura et al., 2007;</ref><ref type="bibr" target="#b0">Andreevskaia and Bergler, 2006;</ref><ref type="bibr">Kaji and Kitsuregawa, 2007;</ref><ref type="bibr" target="#b1">Blair-Goldensohn et al., 2008;</ref><ref type="bibr" target="#b3">Cambria et al., 2016;</ref><ref type="bibr" target="#b13">Rao and Ravichandran, 2009;</ref><ref type="bibr" target="#b11">Perez-Rosas et al., 2012</ref>). For example, ( <ref type="bibr" target="#b18">Valitutti et al., 2004;</ref><ref type="bibr">Kim and Hovy, 2004</ref>) tried to remove error words and assign a sentiment strength to each word. <ref type="bibr" target="#b9">Mohammad et al. (2009)</ref> exploited many antonym-generating affix patterns, <ref type="bibr">Kamps et al. (2004)</ref> used a WordNet distance, and <ref type="bibr">Hassan and Radev (2010)</ref> used a Markov random walk model over a word relatedness graph. <ref type="bibr">Dragut et al. (2010)</ref> used a set of inference rules to determine word sentiment polarity through a deductive process, and <ref type="bibr" target="#b14">Schneider and Dragut (2015)</ref> employed a lin- ear programming approach. Another method is to build a supervised sentiment classifier to classify the gloss text of each word in the dictionary <ref type="bibr">(Esuli and</ref><ref type="bibr">Sebastiani, 2005, 2006</ref>). <ref type="bibr" target="#b23">Xu et al. (2010a)</ref> integrated both dictionaries and corpora to find emotion words based on label-propagation. <ref type="bibr">PerezRosas et al. (2012)</ref> also worked on cross lingual lexicon construction.</p><p>In the corpus-based approach, one key idea is to exploit some linguistic conventions on connec- tives such as AND and OR <ref type="bibr">Hatzivassiloglou and McKeown (1997)</ref>. For example, in the sentence "This car is beautiful and spacious," if "beauti- ful" is known to be positive, it can be inferred that "spacious" is also positive. <ref type="bibr">Kanayama and Nasukawa (2006)</ref> extended the idea to the sentence level by exploiting adversative expressions such as "but" and "however." <ref type="bibr" target="#b12">Qiu et al. (2011)</ref> proposed a double propagation (DP) method that uses both sentiment and target relation and various connec- tives to extract sentiment words. ( <ref type="bibr" target="#b20">Wang and Wang, 2008</ref>) did similar works. <ref type="bibr">Huang et al. (2014)</ref> de- tected new sentiment words using lexical patterns. <ref type="bibr" target="#b22">Wilson et al. (2005)</ref>, <ref type="bibr">Ding et al. (2008)</ref>, <ref type="bibr">Choi and Cardie (2008)</ref> and Zhang and Liu (2011) studied contextual sentiments at the phrase or expression level. We do not study contextual sentiments.</p><p>Another idea for the corpus-based approach is to use word co-occurrences. <ref type="bibr" target="#b17">Turney (2002)</ref> com- puted the Pointwise Mutual Information (PMI) be- tween the target word and seed words to decide its sentiment polarity. This method was extended in ( <ref type="bibr" target="#b10">Mohammad et al., 2013;</ref><ref type="bibr" target="#b26">Yang et al., 2014)</ref>. ( <ref type="bibr">Hamilton et al., 2016;</ref><ref type="bibr" target="#b24">Xu et al., 2010b</ref>) con- structed domain lexicons using lexical graphs.</p><p>Recent works also exploited neural networks and word embedding, and treated lexicon gener- ation as a classification problem like us. <ref type="bibr" target="#b16">Tang et al. (2014)</ref> expanded a sentiment lexicon using a softmax classifier with the proposed sentiment- specific embedding. <ref type="bibr" target="#b19">Vo and Zhang (2016)</ref> ob- tained sentiment attribute scores of each word through a neural network model to predict tweets emoticons. <ref type="bibr" target="#b2">Bravo-Marquez et al. (2015)</ref> classi- fied words using manual features and emoticon- annotated tweets. However, all these works re- quire different kinds of labeled data. We do not rely on emoticons or other forms of annotations.</p><p>The problem of adapting a general lexicon to a domain specific one was studied in <ref type="bibr">(Choi and Cardie, 2009;</ref><ref type="bibr">Jijkoun et al., 2010;</ref><ref type="bibr">Du et al., 2010)</ref>. <ref type="bibr">Feng et al. (2011)</ref> also generated a connotation lexicon. These are clearly different from our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head><p>As mentioned in Sec. 1, our problem is solved in two steps: (1) identify sentiment words and (2) classify their polarity. In the following, we first introduce a traditional PU learning method using SVM (Sec. 3.1), and the augmented multi- layer perceptron (AMP) method (Sec. 3.2) to set the background for the proposed technique. The proposed PU learning algorithm is presented in Sec. 3.3 for performing the task of step 1, which uses AMP. After that, a dictionary based method called Double Dictionary Lookup (Sec. 3.4) is pre- sented to further improve the result of the first step. The proposed polarity classification method for the second step is discussed in (Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Traditional PU Learning</head><p>PU learning has been investigated by several re- searchers ( <ref type="bibr" target="#b7">Liu et al., 2002;</ref><ref type="bibr">Denis et al., 2002;</ref><ref type="bibr" target="#b4">Li and Liu, 2003;</ref><ref type="bibr">Elkan and Noto, 2008;</ref><ref type="bibr">Hsieh et al., 2015)</ref>. A popular approach follows a two-stage strategy: (i) identifying a set of reliable N class instances RN from the unla- beled set U ; and (ii) building a classifier using P (P class) and RN (N class) and Q = U − RN (unlabeled) by applying a learning algorithm (e.g., SVM) iteratively.</p><p>To understand the difference between the pro- posed algorithm and the above two-stage ap- proach, we give more details to an existing algo- rithm ( <ref type="bibr" target="#b5">Liu, 2011</ref>). In the first stage, a Spy tech- nique is used to identify the set of reliable N class instances (or examples) RN from U . It works as follows: 10% of P class instances (in our cases, they are words) is first randomly selected as a spy set S and put in the unlabeled set U . Then SVM is run using the set P − S as the P class training data and U ∪S as the N class training data. After train- ing, the resulting classifier assigns a probability to each instance in S to decide a probability thresh- old th. Instances in U that has a lower probability than th are selected as RN . As suggested in <ref type="bibr" target="#b5">(Liu, 2011)</ref>, th is set to the probability that separates the last 15% instances in S. We also experimented with 10% and 20%, but the results are similar.</p><p>In the second stage, we run SVM iteratively. In each iteration, a classifier trained using P and RN is used to classify the instances in Q = U − RN . Those instances assigned the N class in Q are re- moved and added to RN . Iterations stop when no instance in Q is classified to the N class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Augmented Multilayer Perceptron</head><p>We now present the proposed AMP (Augmented Multilayer Perceptron) method, which we will use to replace SVM in PU learning as AMP produces better results. AMP has three layers ( <ref type="figure">Figure 1</ref>). The first layer takes the word vector of each word as input with an output of 50 dimensions. The sec- ond layer takes the output of the first layer as in- put to produce an output of 2 dimensions. Both the first and second layers use the RELU activa- tion function. The 2 dimension output of the sec- ond layer concatenates with 5 POS features (see Sec. 4.1.3) to form a 7 dimension feature vector as input of the third layer, which is the final layer with the activation function of Sigmoid.</p><p>We note that instead of putting POS features in the first layer, we first reduce the dimension of word vector from 200 to 2 with two layers, then compose a vector with POS features as the input to the third layer. This enables POS features to play a more important role. This method gives better results than combining the word vector and POS <ref type="figure">Figure 1</ref>: Aumented Multilayer Perceptron tag features as the input in the first layer. Our ex- perimental results also show that using AMP to re- place SVM above in PU learning produces much better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed New PU Learning Algorithm</head><p>We now present the proposed new PU learning strategy, which has only one stage that runs a supervised learning algorithm iteratively using P and U by treating U as the N class data. This strategy is more suitable for our task as we will explain in Sec. 4.1.4. The general idea is to re- move words from U that are likely to belong to the P class until some stopping criterion is satis- fied. The proposed algorithm is called SE-AMP (Spy-based Elimination of P class instances using AMP), which is given in Algorithm 1. Note that in the algorithm, we use +1 to denote class P (line 1) and −1 to denote class N (line 5). We don't use SVM any more as AMP performs much better. We now detail the working of the algorithm.</p><p>The algorithm still uses Spies and also works iteratively, but in a very different way. It first ran- domly sample a small proportion γ of words from set P to form the spy set S (line 2), which is added to the current U set to form U s (line 4). The U set is updated in each iteration. An AMP classifier is trained using set P (with the class label +1) and set U s (regarded as class N with the class label −1) (line 6). The resulting classifier or model M is used to score or assign a probability to each in- stance in U and in S (line 7). Now we come to the crucial steps of the pro- posed algorithm. It tries to remove likely P class instances from U . U is essentially regarded as an noisy N class data, i.e., it has a lot of errors (which are hidden P class instances). Thus this step ef- fectively aims to purify the N class set. In line 8, the algorithm determines a threshold θ to remove some likely P class instances from U . We will discuss the function for determining θ below.</p><p>Based on the probability threshold θ, the algo- rithm removes those instances in U with greater probability than θ (lines 9-13) and those instances in the spy set S (line 14-18).</p><p>The algorithm stops when the stopping crite- ria is met (lines 19 and 20); otherwise, it goes to the next iteration with the updated U and S. We determine the threshold δ using a validation set (Sec. 4.1.1).</p><p>Determine θ: In this new algorithm, each iter- ation removes instances that are likely to be of P class from the unlabeled set U . One simple way is to remove the top k% of U based on the classifier result of each iteration. However, this method is undesirable because we cannot control the proba- bilities of the eliminated instances. We propose to use Gaussian fitting to determine the threshold θ.</p><p>After each iteration, every spy word w i ∈ S is assigned a probability x i (= P r(P |w i )) to be in class P by the classifier M , a Gaussian fit- ting is done on these probabilities. Parameters of Gaussian distribution are obtained using Maxi- mum Likelihood Estimate (MLE) as follows: µ = 1 n n i=1 x i , and σ 2 = 1 n n i=1 (x i −µ) 2 . We set the threshold θ = µ + σ. Thus those words have prob- abilities higher than θ are considered very likely to belong to the P class. This threshold is very con- servative and only allows those very likely P class instances to be removed from U .</p><p>We will discus why the proposed SE-AMP is better than S-AMP in Sec. 4.1.4 after we see the experiment results. Note that S-AMP uses the tra- ditional PU-learning strategy discussed in Sec. 3.1 but it replaces SVM with AMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Double Dictionary Lookup</head><p>To improve PU learning of SE-AMP further, we propose to employ a dictionary. Using a dictionary is natural because human beings always use dictio- naries to understand a word. The proposed Double Lookup (DL) technique is given in Algorithm 2. Why double lookup is needed will be clear shortly.</p><p>The algorithm works as follows. Let the set of given sentiment lexicon be P , the given dictionary be D, and the set of candidate words be U (in this case, it is the test set). For each candidate word w ∈ U , we first look w up in the dictionary D (lines 1-2). If w can be found in D (line 2), we use a lexicon-based sentiment classifier (C) (see be- low) to classify the gloss or explanation note (G w ) of w (lines 3-4). The function classify returns a set of sentiment words O 1 from G w (line 4), which are also in P . If O 1 is not empty, it means that G w Algorithm 1 SE-AMP(P, U ) 1: Every instance in P is assigned the class label +1 2: S = Sample(P, γ); // γ = 0.1 in this experiment; instances in S are spies 3: loop 4:</p><formula xml:id="formula_0">U s = U ∪ S 5:</formula><p>Every instance u in U s is assigned the class label −1 // −1 denotes the N class 6:</p><formula xml:id="formula_1">M = AMP(P , U s ) // Build a binary AMP classifier M 7:</formula><p>score(M, U, S) // Score each instance i in U and S using M to give each i a probability score 8: θ = DetermineThreshold(S) // decide a probability threshold θ using S;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>for each instance u ∈ U do 10:</p><p>if its probability P r(+1|u) &gt; θ then 11:</p><formula xml:id="formula_2">U = U − {u} 12:</formula><p>end if exit-loop <ref type="bibr">21:</ref> end if 22: end loop is classified to the sentiment class. If it is empty, it is classified to the non-sentiment class.</p><p>This one dictionary lookup is not safe to deter- mine whether word w is a sentiment word or not because some sentiment words in the lexicon P are noisy and don't have clear sentiments. This gives us too low precision. That is why we per- form the second dictionary lookup, which is on the words in O 1 to ensure that at least one word in O 1 is very likely to be a true sentiment word because a noisy sentiment word in P is unlikely to be explained by another word in P . But a true sentiment word in P is very likely to be explained by another sentiment word in P .</p><p>Line 7 looks up each word o ∈ O 1 in D and finds its gloss or explanation note G o . G o is then classified in line 8, which returns a list of senti- ment words O 2 from G o , also in P . If O 2 is not empty, meaning that G o is a sentiment sentence (line 9), we return w as a sentiment word (line 10).</p><p>Lexicon-based sentiment classifier (C): C is a binary classifier with two classes sentiment or non-sentiment. Given a sentence s (e.g., the expla- nation note of a word in the dictionary), it simply finds sentiment words in s that are also in the given sentiment lexicon P . If some sentiment words are found, it returns them in a set indicating the sen- tence s is a sentiment sentence. Although simple, this works quite well because the explanation note of a word in a dictionary is usually quite simple.</p><p>Integrating SE-AMP and DL: Our final pro- posed method (SE-AMP-DL) combines SE-AMP and DL. As we will see that DL has high precision but low recall because most of the new words can- not be found in the dictionary, we use the results of DL to correct the results of the SE-AMP algo- rithm. Words that are classified as belong to the N class by SE-AMP are moved to the P class if the DL method believes them to be sentiment words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Polarity Classification</head><p>After sentiment words are discovered, this step de- termines the polarity (positive or negative) of each discovered sentiment word. We propose a new classification method exploiting the fact that new Chinese words are created with 2 or more Chi- nese characters. The meaning of a Chinese word is closely related to the meaning of each individual character of the word. So the polarity of a Chinese word is strongly related to the polarity of the char- acters that form the word, which is the motivation of the new classification method. We use the po- larity association of the characters in each word to predict the polarity of the word based on super-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 DL(P, D, U, C)</head><p>1: for each candidate word w ∈ U do if w can be found in D then // First dictionary lookup; D is the dictionary 3:</p><p>Let G w be the gloss or explanation of word w in D 4: end if 15: end for vised learning. We call the method CPA (Char- acter Polarity Association). This method is very useful for languages whose words are constructed by characters such as Chinese and Japanese.</p><formula xml:id="formula_3">O 1 = classify(G w , C) //</formula><p>Feature Vector: For a character, we calcu- late the percentages of it appearing in the posi- tive words and negative words in the existing lex- icon P to form a 2-dimensional polarity vector. For example, the polarity vector (0.9, 0.1) means that 90% of the words in the existing lexicon that contains the character are positive and the other 10% are negative. Thus, for a word with 2 char- acters, which covers most cases in Chinese, a 4- dimensional vector is formed. For those words with more than 2 characters, we choose 2 char- acters with the strongest polarity to form a 4- dimensional vector.</p><p>Classifier Building: Using all positive and neg- ative words in the existing lexicon P as the train- ing sample, each word represented as a vector of four features, we apply a Naive Bayesian Classi- fier to build a polarity classifier. For testing, a word is represented in the same way. If a test word contains charters that don't exist in the lexicon, we give each character (0.5, 0.5) as the polarity vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We now evaluate the proposed technique SE- AMP-DL to expand an existing Chinese sentiment lexicon, DUTIR (Dalian University of Technol- ogy, Information Retrieve Lab) Affective Lexicon Ontology ( <ref type="bibr" target="#b25">Xu et al., 2008)</ref>. DUTIR lexicon is per- haps the largest Chinese sentiment lexicon with 27466 words. Although large, since it was built based on formal text such as news, essays, and novels, it does not contain many sentiment words often used in social media as we will see later. We will also see that many new sentiment words that we discovered are not even in an authoritative Chi- nese language dictionary. Thus, compiling a com- prehensive sentiment lexicon is needed. Below, we first evaluate sentiment words discovery (Step 1) and then polarity classification (Step 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Words Discovery</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data and Parameter Settings</head><p>We use a large Chinese Weibo corpus (Chinese version of Twitter) from ( <ref type="bibr" target="#b21">Wang et al., 2013</ref>) for our lexicon expansion, which has about 4.4 mil- lion pairs of post and response messages. Al- though it was originally used to study natural lan- guage conversations, it is quite suitable for our purpose as online conversations are sentiment rich.</p><p>Word embedding: We first used the Stanford Chinese word segmenter to split sentences into se- quences of words (the POS-tag of each word is also obtained in the process). For word embed- ding, we used word2vec ( <ref type="bibr" target="#b8">Mikolov et al., 2013)</ref>. Each word vector has 200 dimensions.</p><p>P set, U set, validation set, and test set: We randomly sampled 200K messages, and used all 54303 words contained in them as our experiment dataset. Out of the 54303 words (stopwords have been removed), 4957 of them also appear in the DUTIR lexicon and are thus sentiment words. The remaining 49346 words are unlabeled.</p><p>Validation set: The validation set consists of randomly selected 300 words from the 4957 sen-timent words and 700 words from the 46742 un- labeled words. Although the 700 words are unla- beled, they are treated as N class words.</p><p>Test set: 1000 words are randomly sampled from 48646 (= 49346 − 700) unlabeled words as the test set, which is labeled manually. Two native speakers labeled the 1000 test words. The Kappa agreement score is 0.695. For any word with dis- agreement, the annotators discussed to come to a final decision. The annotated test set has 199 sen- timent words, 78 positive and 121 negative words.</p><p>P set and U set: The remaining 4657 (= 4957 − 300 sentiment words serve as the P set and the remaining 45042 (= 46742 − 700 − 1000) words serve as the U set for learning.</p><p>Parameter settings: As indicated earlier, 10% (γ) of the P class examples are randomly selected as spies S (465). The probability threshold θ is set using the Gaussian fitting (Sec. 3.3). The iter- ation stopping criterion of SE-AMP (Sec. 3.3) is decided using the validation set (δ = 30%).</p><p>Evaluation measures: We use the classic precision, recall, and F score for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Compared Systems</head><p>We compare the following seven (7) techniques: DP: The Double Propagation (DP) Method in ( <ref type="bibr" target="#b12">Qiu et al., 2011</ref>). This method uses dependency patterns for extraction.</p><p>PMI: The classic PMI method <ref type="bibr" target="#b17">(Turney, 2002</ref>) using the full Weibo corpus and 100 positive and 100 negative words in the DUTIR sentiment lex- icon as the reference words. These words appear most frequently in the Weibo corpus. In <ref type="bibr" target="#b17">(Turney, 2002)</ref>, only 1 positive and 1 negative reference words are used. We also tried to use 1, 50, 150, 200, and all words in the positive and negative classes as reference words, respectively. However, they give poorer results. Since the PMI method can only determine the polarity, but cannot decide whether a test word is a sentiment word or not. We make that decision by using the mean score the PMI method of all positive words in the lexi- con as the positive threshold and the mean score of all negative words as the negative threshold.</p><p>S-SVM: The traditional PU learning method described in Sec. 3.1 using SVM.</p><p>S-AMP: Same as S-SVM, but SVM is replaced with AMP.</p><p>SE-AMP: The proposed PU learning method without DL. DL: Only the double dictionary lookup method, using the Contemporary Chinese Dictionary. SE-AMP-DL: This is our final proposed method, which combines SE-AMP and DL.</p><p>We could not compare with recent approaches <ref type="bibr" target="#b16">Tang et al. (2014)</ref>; <ref type="bibr" target="#b19">Vo and Zhang (2016)</ref>; <ref type="bibr">BravoMarquez et al. (2015)</ref> as they all require some su- pervised information on the data. Our method is unsupervised except the use of the lexicon. These methods were also evaluated indirectly based on the social media post sentiment or emotion classi- fication results. None reported precision, recall, or F score of the discovered sentiment words.</p><p>We do not compare with a dictionary-based ap- proach because most of the test words are not even in the dictionary. Only 87 of 199 sentiment words in the test set can be found in the Contemporary Chinese dictionary. Note that in Chinese, one can form a word using characters fairly easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Features</head><p>For both SVM and AMP, the feature vector for each word is the word vector and POS tags. POS tags are divided into 5 classes (noun, verb, adjec- tive, adverb, others) and form a 5 dimension bi- nary vector (e.g. [1, 0, 0, 0, 0] for noun). In the SVM approach, POS tags are concatenated to the word embedding features to form a 205 dimension feature vector (5 POS tags and 200 word embed- ding features). We used the RBF kernel, which gives the best result as compared to other kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Experiment Results</head><p>We now present and discuss the results. The syn- tactic pattern based DP approach performed very poorly because social media posts are brief and the use of patterns to link sentiment words are quite infrequent. Thus, we will not include its results. Below, we first compare PMI, S-SVM and S-AMP, and then the results of the proposed PU learning method SE-AMP (without using DL). The results of incorporating DL are discussed last.</p><p>Existing Approach -PMI vs. S-SVM vs. S- AMP: S-SVM and S-AMP use the traditional PU learning approach described in Sec. 3.1 for model building. 10% of P class examples are sampled as spies to produce the RN set. <ref type="table" target="#tab_0">Table 1</ref> shows the results of S-SVM and S-AMP iterations. We observe that the AMP version per- forms much better than the SVM version. The first three iterations improve the results. But af- ter that, the results deteriorate for both systems.</p><p>Thus the table only shows the first few iterations since the results keep deteriorating after iteration 2. The best results are obtained by S-AMP, which is 0.548 in F score. S-SVM's best F score is only 0.509, which is poorer. We also see that the pre- cision and recall of S-AMP are both consistently better than those of S-SVM.</p><p>We note that these two algorithms cannot catch the best results when they stop following the algo- rithm in <ref type="bibr" target="#b5">(Liu, 2011</ref>). We did not use the validation set here to find the best stopping criterion because even their best results are poorer than those of SE- AMP. PMI does similarly to S-AMP.</p><p>Proposed Approach -SE-AMP: <ref type="table" target="#tab_2">Table 2</ref> shows the results of the proposed PU learning ap- proach (SE-AMP). As noted above, the iteration stopping criterion δ is determined using the val- idation set. Iteration 0 means the classifier uses all unlabeled examples in U as the N set. Com- pared with iteration 0 of the traditional strategy (S- AMP), the F score of SE-AMP improves slightly (from 52.0% to 54.8%), but both are low. This is because the reliable N set RN for the traditional approach is too small (not representative of all N class examples) while for the proposed approach, the N set has too many hidden P class instances. The traditional PU learning tries to include more and more likely N examples iteratively to move to the P direction while the proposed approach doing the opposite, eliminating likely P instances from the unlabeled set U . As the table shows, the results of SE-AMP gets better and better after each itera- tion (it stops when the stopping condition is met). Precision, recall and F score all improve consis- tently, which result in improvements of 12.0%, 8.5% and 9.6% respectively. Compared with the best result of S-AMP, the precision of SE-AMP improves from 55.4% to 67.4%, recall improves from 51.8% to 59.3% and the F score improves from 53.5% to 63.1%.</p><p>We now explain why SE-AMP is better than S- AMP. We believe that the main reason is the high level of noise in P , i.e., many words in P don't have clear sentiments. The traditional PU learning (S-AMP) tries to add classified N class instances into the RN set in each iteration. This works fine for the first few iterations but then goes wrong be- cause the noise in P resulted in a lot of hidden P instances added into the RN set. Then the results deteriorate as more and more wrong instances are added as iterations progress. In contrast, the pro-  posed SE-AMP removes likely P instances (in- cluding those noisy ones) from the U set to obtain a purer and purer N set. Due to the very conser- vative setting of the θ parameter (see Sec. 3.3), the number of words removed from U in each itera- tion is small, so is the number of spy words re- moved from S as we can see in <ref type="table" target="#tab_2">Table 2</ref>. Thus, U becomes purer and purer slowly, and the validation set helps find a good stopping iteration. Incorporating Double Dictionary Lookup (DL) -SE-AMP-DL: The double dictionary lookup (DL) method improves the results further. DL uses the most authoritative Chinese dictionary: The Contemporary Chinese Dictionary. However, only 379 out of the 1000 test words are in the dic- tionary, among which 87 are sentiment words. As <ref type="table" target="#tab_4">Table 3</ref> shows, the DL method alone has a high precision but low recall as a lot of sentiment words are not in the dictionary. After correction of the re- sults from SE-AMP by DL, the F score improves from 63.1 of SE-AMP to 65.6 of SE-AMP-DL.</p><p>Note: We also tried to clean up the lexicon first using DL to reduce the noise in the P set   before performing the proposed algorithms. But after cleaning, only 1968 sentiment words out of 4957 remained. We inspected the result and found that the cleaning removed a lot of true sentiment words, making the P set too small for our algo- rithms and thus produced much poorer results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Polarity Classification</head><p>Here we use the well-known PMI method in <ref type="bibr" target="#b17">(Turney, 2002</ref>) as the baseline, which was designed for polarity classification. Again, for PMI computa- tion, we use 100 positive and 100 negative words in our lexicon that appear most frequently in the Weibo corpus as the reference words, and compute the PMI scores between the candidate words and the references. Using many other numbers of sen- timent words in the lexicon as the reference words give poorer results (see also Sec. 4.1.2). A word is considered as a positive sentiment word if its score is positive, or negative if the score is negative. We apply the proposed CPA method and PMI to all 199 true sentiment words in the test set (199 set), and the sentiment words identified by SE- AMP-DL (identified set), which has many errors, i.e., non-sentiment words, Experimental results in <ref type="table" target="#tab_5">Table 4</ref> show that CPA outperforms PMI greatly in both cases. Those non-sentiment words are con- sidered wrong in the "identified set" case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper made a new attempt to study sentiment lexicon expansion based on a social media corpus. It first showed that the problem can be formulated as PU learning. It then proposed an augmented multilayer perceptron method to give PU learn- ing an neural network solution. It then proposed a new PU learning method, which outperforms a classic existing PU learning method. To improve the results further, it proposed a double dictionary lookup technique. Additionally, a novel polarity classification method was also designed. Exper- imental results demonstrated the effectiveness of these proposed methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>for each instance s ∈ S do 15: if its probability P r(+1|s) &gt; θ then 16: S = S − {s} 17: end if 18: end for 19: if |S| ≤ δ(γ|P |) then // Stopping criterion; γ|P | is actually the original size of S in line 2 20:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>O 1 is the set of sentiment words in G w and O 1 ⊆ P (lexicon) 5: if O 1 = ∅ then // G w is classified to the sentiment class</head><label>1</label><figDesc></figDesc><table>6: 

for each word o ∈ O 1 (⊆ P ) do 

7: 

Let G o be the gloss or explanation of word o in D // Second dictionary lookup 

8: 

O 2 = classify(G o , C) // O 2 ⊆ P (lexicon) and C is the sentiment classifier 

9: 

if O 2 = ∅ then // G o is classified as a sentiment sentence 

10: 

w is a new sentiment word 

11: 

end if 

12: 

end for 

13: 

end if 

14: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Results of the proposed SE-AMP.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results with double dictionary lookup. 

560 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>PMI and CPA classification results. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Bing Liu's work was supported in part by National Science Foundation (NSF) under grant no. IIS-1407927 and IIS-1650900. We would like to ex-press our gratitude to Feng Liu, Jiansheng Wei and Youliang Yan from Noah's Ark Lab, Huawei Technologies for their help and excellent advice in this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining wordnet for a fuzzy sentiment: Sentiment tag extraction from wordnet glosses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Andreevskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building a sentiment summarizer for local service reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerry</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Neylon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>George A Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW workshop on NLP in the information explosion era</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Positive, negative, or neutral: Learning an expanded opinion lexicon from emoticon-annotated tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2015</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1229" to="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Senticnet 4: A semantic resource for sentiment analysis based on conceptual primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting><address><addrLine>Osaka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to classify texts using positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="587" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Web data mining: exploring hyperlinks, contents, and usage data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sentiment analysis and data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Partially supervised classification of text documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Mach ine Learning (ICML2002)</title>
		<meeting>the Nineteenth International Conference on Mach ine Learning (ICML2002)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.6242</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning sentiment lexicons in spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Perez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised polarity lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delip</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards debugging sentiment lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">C</forename><surname>Dragut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting semantic orientations of phrases from dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building large-scale twitter-specific sentiment lexicon: A representation learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Developing affective lexical resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Valitutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliviero</forename><surname>Stock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PsychNology Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="83" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dont count, predict! an automatic approach to learning sentiment lexicons for short text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="289" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing. Association for Computational Linguistics</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Build chinese emotion lexicons using a graph-based algorithm and multiple resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1209" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Expanding chinese sentiment dictionaries from large scale unlabeled corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjian</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constructing the affective lexicon ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the China Society for Scientific and Technical Information</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A topic model for building fine-grained domain-specific emotion lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingju</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Pui</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pebl: positive example based learning for web page classification using svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identifying noun product features that imply opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="575" to="580" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
