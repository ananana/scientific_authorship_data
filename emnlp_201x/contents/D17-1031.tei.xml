<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Sanu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<email>quanliu@mail.ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of EEIS</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<addrLine>4700 Keele Street</addrLine>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="310" to="315"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose to learn word embeddings based on the recent fixed-size ordinally forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence into a fixed-size representation. We use FOFE to fully encode the left and right context of each word in a corpus to construct a novel word-context matrix, which is further weighted and factorized using truncated SVD to generate low-dimension word embedding vectors. We have evaluated this alternative method in encoding word-context statistics and show the new FOFE method has a notable effect on the resulting word embeddings. Experimental results on several popular word similarity tasks have demonstrated that the proposed method outperforms many recently popular neural prediction methods as well as the conventional SVD models that use canonical count based techniques to generate word context matrices.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Low dimensional vectors as word representations are very popular in NLP tasks such as inferring semantic similarity and relatedness. Most of these representations are based on either matrix factor- ization or context sampling described by ( <ref type="bibr" target="#b0">Baroni et al., 2014</ref>) as count or predict models. The ba- sis for both models is the distributional hypoth- esis <ref type="bibr" target="#b6">(Harris, 1954)</ref>, which states that words that appear in similar contexts have similar meaning. Traditional context representations have been ob- tained by capturing co-occurrences of words from a fixed-size window relative to the focus word. This representation however does not encompass the entirety of the context surrounding the focus word. Therefore, the distributional hypothesis is not being taken advantage of to the fullest extent. In this work, we seek to capture these contexts through the fixed-size ordinally forgetting encod- ing (FOFE) method, recently proposed in ( <ref type="bibr" target="#b22">Zhang et al., 2015b</ref>). In addition to just capturing word co-occurrences, we attempt to use the FOFE to encode the full contexts of each focus word, in- cluding the order information of the context se- quences. We believe the full encoding of con- texts can enhance the resulting word embedding vectors, derived by factoring the corresponding word-context matrix. As argued in ( <ref type="bibr" target="#b22">Zhang et al., 2015b</ref>), the FOFE method can almost uniquely en- code discrete sequences of varying lengths into a fixed-size code, and this encoding method was used to address the challenges of a limited size window when using deep neural networks for lan- guage modeling. The resulting algorithm fulfills the needs of keeping long term dependency while being fast. The word order in a sequence is mod- eled by FOFE using an ordinally-forgetting mech- anism which encodes each position of every word in the sequence.</p><p>In this paper, we elaborate how to use the FOFE to fully encode context information of each focus word in text corpora, and present a new method to construct the word-context matrix for word em- bedding, which may be weighted and factorized as in traditional vector space models <ref type="bibr" target="#b20">(Turney and Pantel, 2010)</ref>. Next, we report our experimental results on several popular word similarity tasks, which demonstrate that the proposed FOFE-based approach leads to significantly better performance in these tasks, comparing with the conventional vector space models as well as the popular neu- ral prediction methods, such as word2vec, GloVe and more recent Swivel. Finally, this paper will conclude with the analysis and prospects of com- 310 bining this approach with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been some debate as to what the opti- mal length of a text should be for measuring word similarity. Word occurrences from a fixed con- text window of words can be used to represent a context <ref type="bibr" target="#b11">(Lund and Burgess, 1996)</ref>. The word co-occurrence frequencies are based on fixed win- dows spanning in both directions from the focus word. This is then used to create a word-context matrix from which row vectors can be used to measure word similarity. A weighting step is usu- ally applied to highlight words with close associa- tion in the co-occurrence matrix, and the truncated SVD is used to factorize the weighted matrix to generate low-dimension word vectors. Recently, ( <ref type="bibr" target="#b13">Mikolov et al., 2013a</ref>) has introduced an alterna- tive way to generate word embeddings using the skipgram model trained with stochastic gradient descent and negative sampling, named as SGNS. SGNS tries to maximize the dot product between w · c where both a word w and a context c are obtained from observed word-context pairs, and meanwhile it also tries to minimize the dot product between w · c where c is a negative sample repre- senting some contexts that are not observed in the corpus. More recently, ( <ref type="bibr" target="#b9">Levy and Goldberg, 2014)</ref> has showed that the objective function of SGNS is essentially seeking to minimize the difference between the models estimate and the log of co- occurrence count. Their finding has shown that the optimal solution is a weighted factorization of a pointwise mutual information matrix shifted by the log of the number of negative samples.</p><p>SGNS and GloVe ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>) se- lect a fixed window of usually 5 words or less around a focus word to encode its context and the word order information within the window is com- pletely ignored. Other attempts to fully capture the contexts have been successful with the use of re- current neural networks (RNNs) but these methods are much more expensive to run over large corpora when comparing with the proposed FOFE method in this paper. Some previous approaches to en- code order information, such as such as BEAGLE ( <ref type="bibr" target="#b8">Jones and Mewhort, 2007)</ref> and Random Permu- tations ( <ref type="bibr" target="#b18">Sahlgren et al., 2008)</ref>, typically require the use of expensive operations such as convolu- tion and permutation to process all n-grams within a context window to memorize order information for a given word. On the contrary, the FOFE meth- ods only use a simple recursion to process a sen- tence once to memorize both context and order in- formation for all words in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FOFE based Embedding</head><p>To capture the full essence of the distributional hy- pothesis, we need to fully encode the left and right context of each focus word in the text, and fur- ther take into accounts that words closer to the fo- cus word should play a bigger role in representing the context relevant to the focus word than other words locating much farther away. Traditional co- occurrence word-context matrixes fail to address these concerns of context representation.</p><p>In this work, we propose to make use of the fixed-size ordinally-forgetting encoding (FOFE) method, proposed in ( <ref type="bibr" target="#b22">Zhang et al., 2015b</ref>) as a unique encoding method for any variable-length sequence of discrete words.</p><p>Given a vocabulary of size K, FOFE uses 1-of- K one-hot representation to represent each word. To encode any variable-length sequence of words, FOFE generates the code using a simple recursive formula from the first word (w 1 ) to the last one (w T ) of the sequence: (assume z 0 = 0)</p><formula xml:id="formula_0">z t = α · z t−1 + e t (1 ≤ t ≤ T )<label>(1)</label></formula><p>where z t denotes the FOFE code for the partial sequence up to word w t , α is a constant forget- ting factor, and e t denotes the one-hot vector rep- resentation of word w t . In this case, the code z T may be viewed as a fixed-size representation of any sequence of {w 1 , w 2 , · · · , w T }. For ex- ample, assume we have three symbols in vocabu- lary, e.g., A, B, C, whose 1-of-K codes are </p><formula xml:id="formula_1">{ABCBC} is [α 4 , α + α 3 , 1 + α 2 ].</formula><p>The uniqueness of the FOFE code is made evi- dent if the original sequence can be unequivocally recovered from the given FOFE code. According to <ref type="bibr" target="#b22">(Zhang et al., 2015b</ref>), FOFE codes have some nice theoretical properties to ensure the unique- ness, as exemplified by the following two theo- rems 1 :</p><p>Theorem 1 If the forgetting factor α satisfies 0 &lt; α ≤ 0.5, FOFE is unique for any K and T . Theorem 2 For 0.5 &lt; α &lt; 1, given any finite values of K and T , FOFE is almost unique every- where for α ∈ (0.5, 1.0), except only a finite set of countable choices of α.</p><p>Finally, for alpha values less than or equal to 0.5 and greater than 0, the FOFE is unique for any sequence. For alpha values greater than 0.5, the chance of collision is extremely low and the FOFE is unique in almost all cases. Too find more about the theoretical correctness of FOFE, please refer to ( <ref type="bibr" target="#b22">Zhang et al., 2015b</ref>). In other words, the FOFE codes can almost uniquely encode any se- quences, serving as a fixed-size but theoretically lossless representation for any variable-length se- quences.</p><p>In this work, we propose to use FOFE to encode the full context where each focus word appears in text. As shown in <ref type="figure" target="#fig_2">Figure 1</ref>, the left context of a fo- cus word, i.e., bank, may be viewed as a sequence and encoded as a FOFE code L from the left to right while its right context is encoded as another FOFE code R from right to left. When a proper forgetter factor α is chosen, the two FOFE codes can almost fully represent the context of the focus word. If the focus word appears multiple times in text, a pair of FOFE codes <ref type="bibr">[L, R]</ref> is generated for each occurrence. Next, a mean vector is calcu- lated for each word from all of its occurrences in text. Finally, as shown in <ref type="figure" target="#fig_2">Figure 1</ref>, we may line up these mean vectors (one word per row) to form a new word-context matrix, called the FOFE matrix here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PMI-based Weighting and SVD-based Matrix Factorization</head><p>We further weight the above FOFE matrix using the standard positive pointwise mutual informa- tion (PMI) <ref type="bibr" target="#b3">(Church and Hanks, 1990</ref>) which has been shown to be of benefit for regular word- context matrices ( <ref type="bibr" target="#b15">Pantel and Lin, 2002</ref>). PMI is used as a measure of association between a word and a context. PMI tries to compute the asso- ciation probabilities based on co-occurrence fre- quencies. Positive pointwise mutual information is a commonly adopted approach where all neg- ative values in the PMI matrix are replaced with zero. The PMI-based weighting function is critical here since it helps to highlight the more surprising events in original word-context matrix. There are significant benefits in working with low-dimensional dense vectors, as noted by <ref type="bibr" target="#b4">(Deerwester et al., 1990</ref>) with the use of truncated sin- gular value decomposition (SVD). Here, we also use truncated SVD to factorize the above weighted FOFE matrix as the product of three dense matri- ces U, Σ, V T , where U and V T have orthonormal columns and Σ is a diagonal matrix consisting of singular values. If we select Σ to be of rank d, its diagonal values represent the top d singular val- ues, and U d can be used to represent all word em- beddings with d dimensions where each row rep- resents a word vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conducted experiments on several popular word similarity data sets and compare our FOFE method with other existing word embedding mod- els in these tasks. In this work, we opt to use five data sets: WordSim353 ( For our training data, we use the standard en- wiki9 corpus which contains 130 million words. The pre-processing stage includes discarding ex- tremely long sentences, tokenizing, lowercasing and splitting each sentence as a context. Our vo- cabulary size is chosen to be 80,000 for the most frequent words in the corpus. All words not in the vocabulary are replaced with the token &lt;unk&gt;. In this work, we use a python-based library, called scipy 2 , to perform truncated SVD to factorize all word-context matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Our first baseline is the conventional vector space model (VSM) <ref type="bibr" target="#b20">(Turney and Pantel, 2010)</ref>, relying on the PMI-weighted co-occurrence matrix with dimensionality reduction performed using trun- cated SVD. The dimension of word vectors is cho- sen to be 300 and this number is kept the same for all models examined in this paper. Our main goal is to outperform VSM as the model proposed in this paper also uses SVD based matrix factor- ization. This allows for appropriate comparisons between the different word encoding methods. left FOFE code L <ref type="bibr">w1</ref> right FOFE code R <ref type="bibr">w1</ref> left FOFE code L <ref type="bibr">w2</ref> right FOFE code R <ref type="bibr">w2</ref> left  For the purpose of completeness, the other non- SVD based embedding models, mainly the more recent neural prediction methods, are also com- pared in our experiments. As a result, we build the second baseline using the skip-gram model pro- vided by the word2vec software package ( <ref type="bibr" target="#b13">Mikolov et al., 2013a</ref>), denoted as SGNS. The word em- beddings are generated using the recommended hyper-parameters from ( <ref type="bibr" target="#b10">Levy et al., 2015)</ref>. Their findings show a larger number of negative sam- ples is preferable and increments on the window size have minimal improvements on word similar- ity tasks. In our experiments the number of nega- tive samples is set to 5 and the window size is set to 5. In addition, we set the subsampling rate to 10 −4 and run 3 iterations for training. In adition to SGNS, we also obtained results for CBOW, <ref type="bibr">GloVe (Pennington et al., 2014</ref>) and <ref type="bibr">Swivel (Shazeer et al., 2016</ref>) models using similar recommended settings. While the window size has a fixed limit in the baseline models, our model does not have a window size parameter as the entire sentence is fully captured as well as distinctions between left and right contexts when generating the FOFE codes. The impact of closer context words is fur- ther highlighted by the use of the forgetting factor which is unique to the FOFE based word embed- ding.</p><p>Finally, we use the FOFE codes to construct the word-context matrix and generate word embed- ding as described in sections 3 and 4. Throughout our experiments, we have chosen to use a constant forgetting factor α = 0.7. There is no significant difference in word similarity scores after experi- menting with different α values between [0.6, 0.9] when generating FOFE codes.</p><p>We have applied the same hyperparameters to both VSM and FOFE methods and fine-tune them based on the recommended settings provided in ( <ref type="bibr" target="#b10">Levy et al., 2015)</ref>. Although it has been previ- ously reported that context distribution smoothing ( <ref type="bibr" target="#b14">Mikolov et al., 2013b</ref>) can provide a net posi- tive effect, it did not yield significant gains in our experiments. On the other hand, the eigenvalue  <ref type="bibr" target="#b2">(Caron, 2001</ref>) proved to be incredibly effective for some datasets but in- effectual in others. The net benefit however is pal- pable and we include it for both VSM and FOFE methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>The best results of all word embedding models are summarized in <ref type="table" target="#tab_1">Table 1</ref> for all five examined data sets, which include the the traditional count based VSM with SVD alongside SGNS using <ref type="bibr">word2vec</ref> and our proposed FOFE word embeddings. The most discernible piece of information from the ta- ble is that the FOFE method significantly outper- forms the traditional count based VSM method on most of these word similarity tasks. The results in <ref type="table" target="#tab_1">Table 1</ref> show that substantial gains are obtained by FOFE in WordSim353, MEN and Rare Words data sets. The MEN dataset shows a 7% relative improvement over the conventional VSM. Among all of these five data sets, the proposed FOFE word embedding significantly outperforms VSM in four tasks while yielding similar perfor- mance as VSM in the last data set, i.e. SimLex- 999. FOFE also outperforms all the other models except Swivel in the Mech Turk dataset. It is im- portant to note that this paper does not state that SVD is obligatory to obtain the best model. The FOFE method can be complemented with other models such as Swivel in place of count based en- coding methods. It is also theoretically guaranteed that the original sentence is perfectly recoverable from this FOFE code. This theoretical guarantee is clearly missing in previous methods to encode word order information, such as both BEAGLE and Random Permutations. It is evident that over- all the FOFE encoding method does achieve sig- nificant gains in performance in these word sim- ilarity tests over the traditional VSM method that applies the same factorization method. This is sub- stantial as ( <ref type="bibr" target="#b10">Levy et al., 2015</ref>) demonstrates that larger window sizes when using SVD does not payoff and the optimal context window is 2. We establish that we can indeed encode more infor- mation into our embedding with the FOFE codes.</p><p>In summary, our experimental results show great promise in using the FOFE encoding to rep- resent word contexts for traditional matrix factor- ization methods. As for future work, the FOFE en- coding method may be combined with other pop- ular algorithms, such as Swivel, to replace the co- occurrence statistics based on a fixed window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The ability to capture the full context without restriction can play a crucial factor in generat- ing superior word embeddings that excel in NLP tasks. The fixed-size ordinally forgetting encod- ing (FOFE) has the ability to seize large contexts while discriminating contexts that are farther away as being less significant. Conventional embed- dings are derived from ambiguous co-occurrence statistics that fail to adequately discriminate con- texts words even within the fixed-size window. The FOFE encoding technique trumps other ap- proaches in its ability to procure the state of the art results in several word similarity tasks when combined with prominent factorization practices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[1, 0, 0], [0, 1, 0] and [0, 0, 1] respectively. When calculat- ing from left to right, the FOFE code for the se- quence {ABC} is [α 2 , α, 1], and that of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Finkelstein et al., 2001), MEN (Bruni et al., 2012), Mechanical Turk (Radinsky et al., 2011), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015). The word similarity performance is evaluated based on the Spearman rank correlation coefficient obtained by comparing cosine distance between word vec- tors and human assigned similarity scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: i) encoding left and right contexts of each focus word with FOFE and ii) forming the FOFE word-context matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>day, we had an entire bank of computers devoted to this problem. left</head><label></label><figDesc></figDesc><table>FOFE code L wK 
right FOFE code R wK 

w 1 
w 2 

w K 

K x 2K 

Back in the FOFE code L 
right FOFE code R 

i) encoding left and right context for one occurrence of the focus word, i.e. bank 

ii) forming the FOFE word-context matrix for all words 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : The best achieved performance of various word embedding models on all five</head><label>1</label><figDesc></figDesc><table>examined word 
</table></figure>

			<note place="foot" n="1"> See (Zhang et al., 2015a) for the proof of these two theorems.</note>

			<note place="foot" n="2"> See http://docs.scipy.org/doc/scipy/ reference/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by a Discovery Grant from Natural Sciences and Engineering Re-search Council (NSERC) of Canada, and a re-search donation from iFLYTEK Co., Hefei, China.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. 314 context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Experiments with lsa scoring: Optimal rank and basis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM Computational Information Retrieval Workshop</title>
		<meeting>the SIAM Computational Information Retrieval Workshop</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="157" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Indexing by latent semantic analysis. Journal of the American society for information science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Representing word meaning and order information in a composite holographic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mewhort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior Research Methods, Instruments, and Computers</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discovering word senses from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A word at a time: computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Permutations as a means to encode order in word space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Holst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanerva</forename><surname>Pentti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 30th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1300" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Waterson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02215</idno>
		<title level="m">Swivel: Improving embeddings by noticing whats missing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A fixed-size encoding method for variable-length sequences with its application to neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01504</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The fixed-size ordinallyforgetting encoding method for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="495" to="500" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
