<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subgoal Discovery for Hierarchical Dialogue Policy Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2298</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<settlement>Kirkland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<settlement>Kirkland</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
						</author>
						<title level="a" type="main">Subgoal Discovery for Hierarchical Dialogue Policy Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2298" to="2309"/>
							<date type="published">October 31-November 4, 2018. 2018. 2298</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Developing agents to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient policy learning. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these subgoals to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our method by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined sub-goals. Moreover, we show that the learned subgoals are often human comprehensible.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider we want to plan a trip to a distant city using a dialogue agent. The agent must make choices at each leg, e.g., whether to fly or to drive, whether to book a hotel. Each of these steps in turn involves making a sequence of decisions all the way down to lower-level actions. For exam- ple, to book a hotel involves identifying the loca- tion, specifying the check-in date and time, and negotiating the price etc.</p><p>The above process of the agent has a natural hierarchy: a top-level process selects which sub- goal to complete, and a low-level process chooses primitive actions to accomplish the selected sub- goal. Within the reinforcement learning (RL) paradigm, such a hierarchical decision making process can be formulated in the options frame- work ( <ref type="bibr" target="#b36">Sutton et al., 1999</ref>), where subgoals with their own reward functions are used to learn poli- cies for achieving these subgoals. These learned policies are then used as temporally extended ac- tions, or options, for solving the entire task.</p><p>Based on the options framework, researchers have developed dialogue agents for complex tasks, such as travel planning, using hierarchical re- inforcement learning (HRL) <ref type="bibr" target="#b6">(Cuayáhuitl et al., 2010)</ref>. Recently, <ref type="bibr" target="#b29">Peng et al. (2017b)</ref> showed that the use of subgoals mitigates the reward sparsity and leads to more effective exploration for dia- logue policy learning. However, these subgoals need to be human-defined which limits the appli- cability of the approach in practice because the do- main knowledge required to properly define sub- goals is often not available in many cases.</p><p>In this paper, we propose a simple yet effective Subgoal Discovery Network (SDN) that discovers useful subgoals automatically for an RL-based di- alogue agent. The SDN takes as input a collection of successful conversations, and identifies "hub" states as subgoals. Intuitively, a hub state is a re- gion in the agent's state space that the agent tends to visit frequently on successful paths to a goal but not on unsuccessful paths. Given the discovered subgoals, HRL can be applied to learn a hierar- chical dialogue policy which consists of (1) a top- level policy that selects among subgoals, and (2) a low-level policy that chooses primitive actions to achieve selected subgoals.</p><p>We present the first study of learning dialogue agents with automatically discovered subgoals. We demonstrate the effectiveness of our approach by building a composite task-completion dialogue agent for travel planning. Experiments with both simulated and real users show that an agent learned with discovered subgoals performs com- petitively against an agent learned using expert- defined subgoals, and significantly outperforms an agent learned without subgoals. We also find that the subgoals discovered by SDN are often human comprehensible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>A goal-oriented dialogue can be formulated as a Markov decision process, or MDP ( <ref type="bibr" target="#b16">Levin et al., 2000</ref>), in which the agent interacts with its en- vironment over a sequence of discrete steps. At each step t ∈ {0, 1, . . .}, the agent observes the current state s t of the conversation <ref type="bibr" target="#b13">(Henderson, 2015;</ref><ref type="bibr" target="#b26">Mrkši´Mrkši´c et al., 2017;</ref>, and chooses action a t according to a policy π. Here, the action may be a natural-language sentence or a speech act, among others. Then, the agent re- ceives a numerical reward r t and switches to next state s t+1 . The process repeats until the dialogue terminates. The agent is to learn to choose op- timal actions {a t } t=1,2,... so as to maximize the total discounted reward r 0 + γr 1 + γ 2 r 2 + · · · , where γ ∈ [0, 1] is a discount factor. This learning paradigm is known as reinforcement learning, or RL ( <ref type="bibr" target="#b35">Sutton and Barto, 1998)</ref>.</p><p>When facing a complex task, it is often more efficient to divide it into multiple simpler sub- tasks, solve them, and combine the partial solu- tions into a full solution for the original task. Such an approach may be formalized as hierarchical RL (HRL) in the options framework <ref type="bibr" target="#b36">(Sutton et al., 1999</ref>). An option can be understood as a subgoal, which consists of an initiation condition (when the subgoal can be triggered), an option policy to solve the subgoal, and a termination condition (when the subgoal is considered finished).</p><p>When subgoals are given, there exist effective RL algorithms to learn a hierarchical policy. A major open challenge is the automatic discovery of subgoals from data, the main innovation of this work is covered in the next section. <ref type="figure">Figure 1</ref> shows the overall workflow of our pro- posed method of using automatic subgoal discov- ery for HRL. First a dialogue session is divided into several segments. Then at the end of those segments (subgoals), we equip an intrinsic or ex- trinsic reward for the HRL algorithm to learn a hi- erarchical dialogue policy. Note that only the last segment has an extrinsic reward. The details of the segmentation algorithm and how to use sub- goals for HRL are presented in Section 3.1 and Section 3.3.  <ref type="figure">Figure 1</ref>: The workflow for HRL with subgoal discov- ery. In addition to the extrinsic reward at the end of the dialogue session, HRL also uses intrinsic rewards induced by the subgoals (or the ends of dialogue seg- ments). Section 3.2 details the reward design for HRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Subgoal Discovery for HRL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subgoal Discovery Network</head><p>Assume that we have collected a set of successful state trajectories of a task, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. We want to find subgoal states, such as the three red states s 4 , s 9 and s 13 , which form the "hubs" of these trajectories. These hub states indicate the subgoals, and thus divide a state trajectory into several segments, each for an option <ref type="bibr">1</ref>  Thus, discovering subgoals by identifying hubs in state trajectories is equivalent to segmenting state trajectories into options. In this work, we for- mulate subgoal discovery as a state trajectory seg- mentation problem, and address it using the Sub- goal Discovery Network (SDN), inspired by the sequence segmentation model ( <ref type="bibr" target="#b40">Wang et al., 2017</ref>).</p><p>The SDN architecture. SDN repeats a two- stage process of generating a state trajectory seg- ment, until a trajectory termination symbol is gen- erated: first it uses an initial segment hidden state <ref type="bibr">1</ref> There are many ways of creating a new option I, π, β for a discovered subgoal state. For example, when a subgoal state is identified at time step t, we add to I the set of states visited by the agent from time t − n to t, where n is a pre-set parameter. I is therefore the union of all such states over all the state trajectories. The termination condition β is set to 1 when the subgoal is reached or when the agent is no longer in I, and to 0 otherwise. In the deep RL setting where states are represented by continuous vectors, β is a probability whose value is proportional to the vector distance e.g., between cur- rent state and subgoal state. to start a new segment, or a trajectory termina- tion symbol to terminate the trajectory, given all previous states; if the trajectory is not terminated, then keep generating the next state in this trajec- tory segment given previous states until a segment termination symbol is generated. We illustrated this process in <ref type="figure">Figure 3</ref>. We model the likelihood of each segment using an RNN, denoted as RNN1. During the training, at each time step, RNN1 predicts the next state with the current state as input, until it reaches the op- tion termination symbol #. Since different options are under different conditions, it is not plausible to apply a fixed initial input to each segment. There- fore, we use another RNN (RNN2) to encode all previous states to provide relevant information and we transform these information to low dimen- sional representations as the initial inputs for the RNN1 instances. This is based on the causality as- sumption of the options framework ( <ref type="bibr" target="#b36">Sutton et al., 1999</ref>) -the agent should be able to determine the next option given all previous information, and this should not depend on information related to any later state. The low dimensional representa- tions are obtained via a global subgoal embedding matrix M ∈ R d×D , where d and D are the di- mensionality of RNN1's input layer and RNN2's output layer, respectively. Mathematically, if the output of RNN2 at time step t is o t , then from time t the RNN1 instance has M · softmax(o t ) as its initial input 2 . D is the number of subgoals we aim to learn. Ideally, the vector softmax(o t ) in a well-trained SDN is close to an one-hot vector. Therefore, M · softmax(o t ) should be close to one column in M and we can view that M provides at most D different "embedding vectors" for RNN1 as inputs, indicating at most D different subgoals. Even in the case where softmax(o t ) is not close to any one-hot vector, choosing a small D helps avoid overfitting.</p><p>Segmentation likelihood. Given the state tra- jectory (s 0 , . . . , s 5 ), assuming that s 2 , s 4 and s 5 are the discovered subgoal states, we model the conditional likelihood of a proposed segmen-</p><formula xml:id="formula_0">tation σ = ((s 0 , s 1 , s 2 ), (s 2 , s 3 , s 4 ), (s 4 , s 5 )) as p(σ|s 0 ) = p((s 0 , s 1 , s 2 )|s 0 ) · p((s 2 , s 3 , s 4 )|s 0:2 ) · p((s 4 , s 5 )|s 0:4 )</formula><p>, where each probability term p(·|s 0:i ) is based on an RNN1 instance. And for the whole trajectory (s 0 , . . . , s 5 ), its likelihood is the sum over all possible segmentations.</p><p>Generally, for state trajectory s = (s 0 , . . . , s T ), we model its likelihood as follows <ref type="bibr">3</ref> :</p><formula xml:id="formula_1">L S (s) = σ⊆S(s),length(σ)≤S length(σ) i=1 p(σ i |τ (σ 1:i )),</formula><p>(1) where S(s) is the set of all possible segmentations for the trajectory s, σ i denotes the i th segment in the segmentation σ, and τ is the concatenation op- erator. S is an upper limit on the maximal num- ber of segments. This parameter is important for learning subgoals in our setting since we usually prefer a small number of subgoals. This is differ- ent from <ref type="bibr" target="#b40">Wang et al. (2017)</ref>, where a maximum segment length is enforced.</p><p>We use maximum likelihood estimation with Eq. (1) for training. However, the number of pos- sible segmentations is exponential in S(s) and the naive enumeration is intractable. Here, dy- namic programming is employed to compute the likelihood in Eq. (1) efficiently: for a trajectory s = (s 0 , . . . , s T ), if we denote the sub-trajectory (s i , . . . , s t ) of s as s i:t , then its likelihood follows</p><formula xml:id="formula_2">2 softmax(ot)i = exp(ot,i)/ D i =1 exp(o t,i ) ∈ R D for ot = (ot,1, . . . , ot,D).</formula><p>3 For notation convenience, we include s0 into the obser- vational sequence, though s0 is always conditioned upon. the below recursion:</p><formula xml:id="formula_3">L m (s 0:t ) =      t−1 i=0 L m−1 (s 0:i )p(s i:t |s 0:i ), m &gt; 0, I[t = 0], m = 0.</formula><p>Here, L m (s 0:t ) denotes the likelihood of sub- trajectory s 0:t with no more than m segments and I <ref type="bibr">[·]</ref> is an indicator function. p(s i:t |s 0:i ) is the likelihood segment s i:t given the previous history, where RNN1 models the segment and RNN2 mod- els the history as shown in <ref type="figure">Figure 3</ref>. With this re- cursion, we can compute the likelihood</p><formula xml:id="formula_4">L S (s) for the trajectory s = (s 0 , . . . , s T ) in O(ST 2 ) time.</formula><p>Learning algorithm. We denote θ s as the model parameter including the parameters of the em- bedding matrix M , RNN1 and RNN2. We then parameterize the segment likelihood function as p(s i:t |s 0:i ) = p(s i:t |s 0:i ; θ s ), and the trajectory likelihood function as</p><formula xml:id="formula_5">L m (s 0:t ) = L m (s 0:t ; θ s ).</formula><p>Given a set of N state trajectories (s <ref type="bibr">(1)</ref> , . . . , s (N ) ), we optimize θ s by mini- mizing the negative mean log-likelihood with L 2 regularization term 1 2 λ||θ s || 2 where λ &gt; 0, using stochastic gradient descent:</p><formula xml:id="formula_6">LS(θ s , λ) = − 1 N N i=1 log LS(s (i) , θ s ) + 1 2 λ||θ s || 2 . (2)</formula><p>Algorithm 1 outlines the training procedure for SDN using stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning SDN</head><p>Input: A set of state trajectories (s1, . . . sN ), the number of segments limit S, initial learning rate η &gt; 0. 1: Initialize the SDN parameter θ s . 2: while not converged do 3:</p><p>Compute the gradient θ s LS(θ s , λ) of the loss</p><formula xml:id="formula_7">LS(θ s , λ) as in Eq. (2). 4: Update θ s ← θ s − η θ s LS(θ s , λ). 5:</formula><p>Update the learning rate η. 6: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Dialogue Policy Learning</head><p>Before describing how we use a trained SDN model for HRL, we first present a short review of HRL for a task-oriented dialogue system. Follow- ing the options framework ( <ref type="bibr" target="#b36">Sutton et al., 1999</ref>), assume that we have a state set S, an option set G and a finite primitive action set A.</p><p>The HRL approach we take learns two Q- functions ( <ref type="bibr" target="#b29">Peng et al., 2017b</ref>), parameterized by θ e and θ i , respectively:</p><p>• The top-level Q * (s, g; θ e ) measures the maxi- mum total discounted extrinsic reward received by choosing subgoal g in state s and then fol- lowing an optimal policy. These extrinsic re- wards are the objective to be maximized by the entire dialogue policy.</p><p>• The low-level Q * (s, a, g; θ i ) measures the max- imum total discounted intrinsic reward received to achieve a given subgoal g, by choosing action a in state s and then following an optimal option policy. These intrinsic rewards are used to learn an option policy to achieve a given subgoal.</p><p>Suppose we have a dialogue session of T turns: τ = (s 0 , a 0 , r 0 , . . . , s T ), which is segmented into a sequence of subgoals g 0 , g 1 , . . . ∈ G. Consider one of these subgoals g which starts and ends in steps t 0 and t 1 , respectively.</p><p>The top-level Q-function is learned using Q- learning, by treating subgoals as temporally ex- tended actions:</p><formula xml:id="formula_8">θ e ← θ e + α · (q − Q(s t , g; θ e )) · θe Q(s t , g; θ e ) , where q = t 1 −1 t=t 0 γ t−t 0 r e t + γ t 1 −t 0 max g ∈G Q(s t 1 , g ; θ e ) ,</formula><p>and α is the step-size parameter, γ ∈ [0, 1] is a discount factor. In the above expression of q, the first term refers to the total discounted reward dur- ing fulfillment of subgoal g, and the second to the maximum total discounted after g is fulfilled.</p><p>The low-level Q-function is learned in a sim- ilar way, and follows the standard Q-learning up- date, except that intrinsic rewards for subgoal g are used. Specifically, for t = t 0 , t 0 + 1, . . . , t 1 − 1:</p><formula xml:id="formula_9">θi ← θi + α · (qt − Q(st, at, g; θe)) · θ i Q(st, at, g; θi) , where q t = r i t + γ max a ∈A Q(s t+1 , a , g; θ i ) .</formula><p>Here, the intrinsic reward r i t is provided by the in- ternal critic of dialogue manager. More details are in Appendix A.</p><p>In hierarchical policy learning, the combination of the extrinsic and intrinsic rewards is expected to help the agent to successfully accomplish a com- posite task as fast as possible while trying to avoid unnecessary subtask switches. Hence, we define the extrinsic and intrinsic rewards as follows:</p><p>Extrinsic Reward. Let L be the maximum number of turns of a dialogue, and K the number of subgoals. At the end of a dialogue, the agent re- ceives a positive extrinsic reward of 2L for a suc- cess dialogue, or −L for a failure dialogue; for each turn, the agent receives an extrinsic reward of −1 to encourage shorter dialogues.</p><p>Intrinsic Reward. When a subgoal terminates, the agent receives a positive intrinsic reward of 2L/K if a subgoal is completed successfully, or a negative intrinsic reward of −1 otherwise; for each turn, the agent receives an intrinsic reward −1 to encourage shorter dialogues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Policy Learning with SDN</head><p>We use a trained SDN in HRL as follows. The agent starts from the initial state s 0 , keeps sam- pling the output from the distribution related to the top-level RNN (RNN1) until a termination symbol # is generated, which indicates the agent reaches a subgoal. In this process, intrinsic rewards are gen- erated as specified in the previous subsection. Af- ter # is generated, the agent selects a new option, and repeats this process.</p><p>This type of naive sampling may allow the op- tion to terminate at some places with a low proba- bility. To stabilize the HRL training, we introduce a threshold p ∈ (0, 1), which directs the agent to terminate an option if and only if the probability of outputting # is at least p. We found this modi- fication leads to better behavior of the HRL agent than the naive sampling method, since it normally has a smaller variance.</p><p>In the HRL training, the agent only uses the probability of outputting # to decide subgoal ter- mination. Algorithm 2 outlines the full proce- dure of one episode for hierarchical dialogue poli- cies with a trained SDN in the composite task- completion dialogue system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We evaluate the proposed model on a travel plan- ning scenario for composite task-oriented dia- logues ( <ref type="bibr" target="#b29">Peng et al., 2017b</ref>). Over the exchange of a conversation, the agent gathers information about the user's intent before booking a trip. The environment then assesses a binary outcome (suc- cess or failure) at the end of the conversation, based on (1) whether a trip is booked, and <ref type="formula">(2)</ref> whether the trip satisfies the user's constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 HRL episode with a trained SDN</head><p>Input: A trained SDN M, initial state s0 of an episode, threshold p, the HRL agent A. 1: Initialize an RNN2 instance R2 with parameters from M and s0 as the initial input. 2: Initialize an RNN1 instance R1 with parameters from M and M · softmax(o RNN2 0 ) as the initial input, where M is the embedding matrix (from M) and o RNN2 0 is the initial output of R2. 3: Current state s ← s0. 4: Select an option o using the agent A. 5: while Not reached the final goal do 6:</p><p>Select an action a according to s and o using the agent A. Get the reward r and the next state s from the environment. 7:</p><p>Place s to R2, denote o RNN2 t as R2's latest output and take M · softmax(o RNN2 t ) as the R1's new input. Let p s be the probability of outputting the termination symbol #. 8: if p s ≥ p then 9:</p><p>Select a new option o using the agent A. 10:</p><p>Re-initialize R1 using the latest output from R2 and the embedding matrix M . 11:</p><p>end if 12: end while Dataset. The raw dataset in our experiments is from a publicly available multi-domain dialogue corpus <ref type="bibr" target="#b8">(El Asri et al., 2017)</ref>. Following <ref type="bibr" target="#b29">Peng et al. (2017b)</ref>, a few changes were made to in- troduce dependencies among subtasks. For exam- ple, the hotel check-in date should be the same with the departure flight arrival date. The data was mainly used to create simulated users, and to build the knowledge bases for the subtasks of booking flights and reserving hotels.</p><p>User Simulator. In order to learn good policies, RL algorithms typically need an environment to interact with. In the dialogue research community, it is common to use simulated users for this pur- pose ( <ref type="bibr" target="#b31">Schatzmann et al., 2007;</ref><ref type="bibr" target="#b20">Liu and Lane, 2017)</ref>. In this work, we adapted a pub- licly available user simulator ( <ref type="bibr" target="#b17">Li et al., 2016)</ref> to the composite task-completion dialogue setting with the dataset described above. During training, the simulator provides the agent with an (extrinsic) re- ward signal at the end of the dialogue. A dialogue is considered to be successful only when a travel plan is booked successfully, and the information provided by the agent satisfies user's constraints.</p><p>Baseline Agents. We benchmarked the pro- posed agent (referred to as the m-HRL Agent) against three baseline agents:</p><p>• A Rule Agent uses a sophisticated, hand-crafted dialogue policy, which requests and informs a hand-picked subset of necessary slots, and then confirms with the user about the reserved trip before booking the flight and hotel.</p><p>• A flat RL Agent is trained with a standard deep reinforcement learning method, <ref type="bibr">DQN (Mnih et al., 2015)</ref>, which learns a flat dialogue pol- icy using extrinsic rewards only.</p><p>• A h-HRL Agent is trained with hierarchical deep reinforcement learning (HDQN), which learns a hierarchical dialogue policy based on human- defined subgoals <ref type="figure" target="#fig_2">(Peng et al., 2017b)</ref>.</p><p>Collecting State Trajectories. Recall that our subgoal discovery approach takes as input a set of state trajectories which lead to successful out- comes. In practice, one can collect a large set of successful state trajectories, either by asking hu- man experts to demonstrate (e.g., in a call center), or by rolling out a reasonably good policy (e.g., a policy designed by human experts). In this paper, we obtain dialogue state trajectories from a rule- based agent which is handcrafted by a domain ex- pert, the performance of this rule-based agent can achieve success rate of 32.2% as shown in <ref type="figure" target="#fig_0">Figure 4</ref> and <ref type="table">Table 1</ref>. We only collect the successful dia- logue sessions from the roll-outs of the rule-based agent, and try to learn the subgoals from these di- alogue state trajectories.</p><p>Experiment Settings. To train SDN, we use RMSProp ( <ref type="bibr" target="#b38">Tieleman and Hinton, 2012</ref>) to opti- mize the model parameters. For both RNN1 and RNN2, we use LSTM (Hochreiter and Schmid- huber, 1997) as hidden units and set the hidden size to 50. We set embedding matrix M with D = 4 columns. As we discussed in Section 3.1, D captures the maximum number of subgoals that the model is expected to learn. Again, to avoid SDN from learning many unnecessary subgoals, we only allow segmentation with at most S = 4 segments during subgoal training. The values for D and S are usually set to be a little bit larger than the expected number of subgoals (e.g., 2 or 3 for this task) since we expect a great proportion of the subgoals that SDN learns are useful, but not nec- essary for all of them. As long as SDN discovers useful subgoals that guide the agent to learn poli- cies faster, it is beneficial for HRL training, even if some non-perfect subgoals are found. During the HRL training, we use the learned SDN to pro- pose subgoal-completion queries. In our experi- ment, we set the maximum turn L = 60. We collected N = 1634 successful, but imper-  fect, dialogue episodes from the rule-based agent in <ref type="table">Table 1</ref> and randomly choose 80% of these di- alogue state trajectories for training SDN. The re- maining 20% were used as a validation set. As illustrated in Section 3.3, SDN starts a new RNN1 instance and issues a subgoal-completion query when the probability of outputting the ter- mination symbol # is above a certain threshold p (as in Algorithm 2). In our experiment, p is set to be 0.2, which was manually picked according to the termination probability during SDN training.</p><p>In dialogue policy learning, for the baseline RL agent, we set the size of the hidden layer to 80. For the HRL agents, both top-level and low-level dialogue policies have a hidden layer size of 80. RMSprop was applied to optimize the parameters. We set the batch size to be 16. During train- ing, we used -greedy strategy for exploration with annealing and set γ = 0.95. For each simula- tion epoch, we simulated 100 dialogues and stored these state transition tuples in the experience re- play buffers. At the end of each simulation epoch, the model was updated with all the transition tu- ples in the buffers in a batch manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulated User Evaluation</head><p>In the composite task-completion dialogue sce- nario, we compared the proposed m-HRL agent <ref type="figure">Figure 5</ref>: Performance of three agents tested with real users: success rate, number of dialogues and p-value are indicated on each bar (difference in mean is signif- icant with p &lt; 0.05).</p><p>with three baseline agents in terms of three met- rics: success rate 4 , average rewards and average turns per dialogue session. <ref type="figure" target="#fig_0">Figure 4</ref> shows the learning curves of all four agents trained against the simulated user. Each learning curve was averaged over 5 runs. <ref type="table">Table 1</ref> shows the test performance where each number was averaged over 5 runs and each run gener- ated 2000 simulated dialogues. We find that the HRL agents generated higher success rates and needed fewer conversation turns to achieve the users' goals than the rule-based agent and the flat RL agent. The performance of the m-HRL agent is tied with that of the h-HRL agent, even though the latter requires high-quality subgoals designed by human experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Evaluation</head><p>We further evaluated the agents that were trained on simulated users against real users, who were recruited from the authors' organization. We con- ducted a study using the one RL agent and two HRL agents {RL, h-HRL, m-HRL}, and com- pared two pairs: {RL, m-HRL} and {h-HRL, m- HRL}. In each dialogue session, one agent was randomly selected from the pool to interact with a user. The user was not aware of which agent was selected to avoid systematic bias. The user was presented with a goal sampled from a user- goal corpus, then was instructed to converse with the agent to complete the given task. At the end of each dialogue session, the user was asked to give a rating on a scale from 1 to 5 based on the natural- <ref type="figure">Figure 6</ref>: Distribution of user ratings for three agents in human evaluation ness and coherence of the dialogue; here, 1 is the worst rating and 5 the best. In total, we collected 196 dialogue sessions from 10 human users. <ref type="figure">Figure 5</ref> summarizes the performances of these agents against real users in terms of success rate. <ref type="figure">Figure 6</ref> shows the distribution of user ratings for each agent. For these two metrics, both HRL agents were significantly better than the flat RL agent. Another interesting observation is that the m-HRL agent performs similarly to the h-HRL agent in terms of success rate in the real user study as shown on <ref type="figure">Figure 5</ref>. Meanwhile in <ref type="figure">Fig- ure 6</ref>, the h-HRL agent is significantly better than m-HRL agent in terms of real user ratings. This may be caused by the probabilistic termination of subgoals: we used a threshold strategy to decide whether to terminate a subgoal. This could intro- duce variance so the agent might not behave rea- sonably compared with human-defined subgoals which terminate deterministically. <ref type="table" target="#tab_5">Table 2</ref> shows the subgoals discovered by SDN in a sample dialogue by a rule-based agent interact- ing with the simulated user. The rule-based agent is equipped with a human-defined subtask struc- ture, which always solves subtask flight (turns 1- 15) before hotel (turns 16-23), as shown in the first column. At turn 10, the user starts to talk about hotel while the rule-based agent is still working on the pre-defined, unfinished flight subtask until subtask flight is finished at turn 15. At turn 16, the user switches to hotel, and so does the rule-based agent until the end of the dialogue. For this rule- based agent, the human-defined subgoal (flight) terminates at turn 15. Meanwhile, our SDN model detected two subgoals (except for the final goal):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Subgoal Visualization</head><p>h- <ref type="table">Task  ID  Text   Flight   1</ref> agt: Hi, may I know your departure city? 2 usr: I depart from Fort Lauderdale. 3</p><p>agt: And what is your destination city? 4</p><p>usr: I need economy seat to Pittsburgh. 5 agt: What is your departure date? 6</p><p>usr: <ref type="table" target="#tab_5">2016-09-12, what time is available for the flight?  7</ref> agt: There is one flight at 21:00:00. 8 usr: What is the price for that flight? 9</p><p>agt: $423.  one terminating at turn 9 (Subgoal 1), and another terminating at turn 15 (Subgoal 2). Subgoal 2 is consistent with the human-defined subgoal. Sub- goal 1 is also reasonable since the user tries to switch to hotel at turn 10. In Appendix B, <ref type="table">Table 3</ref> shows a sample dialogue session by m-HRL agent interacting with a real user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Task-completion dialogue systems have attracted numerous research efforts, and there is grow- ing interest in leveraging reinforcement learn- ing for policy learning. (2017b) presented a composite task-completion dialogue system. Unlike multi-domain dialogue systems, composite tasks introduce inter-subtask constraints. As a result, the completion of a set of individual subtasks does not guarantee the so- lution of the entire task. <ref type="bibr" target="#b6">Cuayáhuitl et al. (2010)</ref> applied HRL to di- alogue policy learning, although they focus on problems with a small state space.</p><p>Later, <ref type="bibr" target="#b5">Budzianowski et al. (2017)</ref> used HRL in multi- domain dialogue systems. <ref type="bibr" target="#b29">Peng et al. (2017b)</ref> first presented an HRL agent with a global state tracker to learn the dialogue policy in the composite task- completion dialogue systems. All these works are built based on subgoals that were pre-defined with human domain knowledge for the specific tasks. The only job of the policy learner is to learn a hier- archical dialogue policy, which leaves the subgoal discovery problem unsolved. In addition to the applications in dialogue systems, subgoal is also widely studied in the linguistics research commu- nity <ref type="bibr" target="#b1">(Allwood, 2000;</ref><ref type="bibr" target="#b19">Linell, 2009</ref>).</p><p>In the literature, researchers have proposed al- gorithms to automatically discovery subgoals for hierarchical RL. One large body of work is based on analyzing the spatial structure of the state tran- sition graphs, by identifying bottleneck states or clusters, among others ( <ref type="bibr" target="#b33">Stolle and Precup, 2002;</ref><ref type="bibr" target="#b24">McGovern and Barto, 2001;</ref><ref type="bibr" target="#b23">Mannor et al., 2004;</ref><ref type="bibr" target="#b32">S ¸ ims¸ekims¸ek et al., 2005;</ref><ref type="bibr" target="#b9">Entezari et al., 2011;</ref><ref type="bibr" target="#b2">Bacon, 2013)</ref>. Another family of algorithms identifies commonalities of policies and extracts these par- tial policies as useful skills <ref type="bibr" target="#b37">(Thrun and Schwartz, 1994;</ref><ref type="bibr" target="#b30">Pickett and Barto, 2002;</ref><ref type="bibr" target="#b4">Brunskill and Li, 2014</ref>). While similar in spirit to ours, these meth- ods do not easily scale to continuous problems as in dialogue systems. More recently, researchers have proposed deep learning models to discover subgoals in continuous-state MDPs ( <ref type="bibr" target="#b3">Bacon et al., 2017;</ref><ref type="bibr" target="#b22">Machado et al., 2017;</ref><ref type="bibr" target="#b39">Vezhnevets et al., 2017)</ref>. It would be interesting to see how effec- tive they are for dialogue management.</p><p>Segmental structures are common in human languages. In the NLP community, some related research on segmentation includes word segmen- tation ( <ref type="bibr" target="#b10">Gao et al., 2005;</ref><ref type="bibr">Zhang et al., 2016)</ref> to divide the words into meaningful units. Alterna- tively, topic detection and tracking <ref type="bibr" target="#b0">(Allan et al., 1998;</ref><ref type="bibr" target="#b34">Sun et al., 2007</ref>) segment a stream of data and identify stories or events in news or social text. In this work, we formulate subgoal discovery as a trajectory segmentation problem. Section 3.1 presents our approach to subgoal discovery which is inspired by a probabilistic sequence segmenta- tion model ( <ref type="bibr" target="#b40">Wang et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusion</head><p>We have proposed the Subgoal Discovery Net- work to learn subgoals automatically in an unsu- pervised fashion without human domain knowl- edge. Based on the discovered subgoals, we learn the dialogue policy for complex task-completion dialogue agents using HRL. Our experiments with both simulated and real users on a composite task of travel planning, show that an agent trained with automatically discovered subgoals performs com- petitively against an agent with human-defined subgoals, and significantly outperforms an agent without subgoals. Through visualization, we find that SDN discovers reasonable, comprehensible subgoals given only a small amount of suboptimal but successful dialogue state trajectories.</p><p>These promising results suggest several direc- tions for future research. First, we want to inte- grate subgoal discovery into dialogue policy learn- ing rather than treat them as two separate pro- cesses. Second, we would like to extend SDN to identify multi-level hierarchical structures among subgoals so that we can handle more complex tasks than those studied in this paper. Third, we would like to generalize SDN to a wide range of complex goal-oriented tasks beyond dialogue, such as the particularly challenging Atari game of Montezuma's Revenge ( <ref type="bibr" target="#b15">Kulkarni et al., 2016</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hierarchical Dialogue Policy Learning</head><p>This section provides more algorithmic details for Section 3. Suppose an HRL agent segments the trajectory into a sequence of subgoals as g 0 , g 1 , . . . ∈ G, and the corresponding subgoal termination time steps as t g 0 , t g 1 , . . . ∈ N * . Furthermore, denote the intrinsic reward at time step t by r i t . The top- level and low-level Q-functions satisfy the follow- ing Bellman equations: |s t = s, g j = g, a t = a, t ∈ [t g j , t g j+1 ) .</p><p>Here γ ∈ [0, 1] is a discount factor, and the ex- pectations are taken over the randomness of the reward and the state transition, We use deep neural networks to approximate the two Q-value functions as Q * (s, a, g) ≈ Q(s, a, g; θ i ) and Q * (s, g) ≈ Q(s, g; θ e ). The pa- rameters θ i and θ e are optimized to minimize the following quadratic loss functions:  Optimization of parameters θ i and θ e can be done by stochastic gradient descent on the two loss functions in Equations <ref type="formula">(3)</ref>  To avoid overfitting, we also add L 2 -regularization to the objective functions above.</p><formula xml:id="formula_10">L i (θ i ) = 1 2|D i | (</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning curves of agents under simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>One line of research is on single-domain task-completion dialogues with flat deep reinforcement learning algorithms such as DQN (Zhao and Eskenazi, 2016; Li et al., 2017; Peng et al., 2018), actor-critic (Peng et al., 2017a; Liu and Lane, 2017) and policy gradi- ents (Williams et al., 2017; Liu et al., 2017). An- other line of research addresses multi-domain di- alogues where each domain is handled by a sepa- rate agent (Gaši´Gaši´c et al., 2015; Gaši´Gaši´c et al., 2015; Cuayáhuitl et al., 2016). Recently, Peng et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 .</head><label>2</label><figDesc>Again, assume a conversation of length T : τ = (s 0 , a 0 , r 0 , . . . , s T −1 , a T −1 , r T −1 , s T ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Q</head><label></label><figDesc>* (s, g) = E tg j+1 −1 i=tg j γ i−tg j r e t+1 + γ tg j+1 −tg j · max g ∈G Q * (s tg j+1 , g ) |s tg j = s, g j = g and Q * (s, a, g) = E r i t + γ · max a ∈A Q * i (s t+1 , g j , a )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>s,a,g,s ,r i )∈D i [(y i − Q(s, a, g; θ i )) 2 ] y i =r i + γ · max a ∈A Q i (s , a , g; θ i ) (3) and L e (θ e ) = 1 2|D e | (s,g,s ,r e )∈D e [(y e − Q(s, g; θ e )) 2 ] y e =r e + γ · max g ∈G Q(s , g ; θ e ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>D e , D i are the replay buffers storing dia- logue experience for training top-level and low- level policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>and ( 4 )</head><label>4</label><figDesc>. The gradients of the two loss functions w.r.t their parameters are θ i L i = 1 |D i | (s,a,g,s ,r i )∈D i θ i Q(s, a, g; θ i )· (y i − Q i (s, a, g; θ i )) and θe L e = 1 |D e | (s,g,s ,r e )∈D e θe Q(s, g; θ e )· (y e − Q e (s, g; θ e )) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Discovered subgoals (except for the final goal) 
in a sample dialogue by a rule-based agent interacting 
with user simulator. The left column (h-Task) shows 
the human-defined subtasks for the rule-based agent. 
SDN detects two subgoals that terminate at turn 9 and 
15 respectively. (h-Task: human-defined subtask, ID: 
turn ID, agt: Agent, usr: User) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Jason D Williams, Kavosh Asadi, and Geoffrey Zweig. 
2017. Hybrid code networks: Practical and efficient 
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational 
Linguistics (ACL). 

Meishan Zhang, Yue Zhang, and Guohong Fu. 2016. 
Transition-based neural word segmentation. In ACL 
(1). 

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking 
and management using deep reinforcement learning. 
arXiv preprint arXiv:1606.02560. 

</table></figure>

			<note place="foot" n="4"> Success rate is the fraction of dialogues which accomplished the task successfully within the maximum turns.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers, members of the xlab at the University of Washing-ton, and Chris Brockett, Michel Galley for their insightful comments on the work. Most of this work was done while DT, CW &amp; LL were with Microsoft.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sample Dialogue</head> <ref type="table">Table 3</ref><p>: Sample dialogue by the m-HRL agent interact- ing with real user: bolded slots are the joint constraints between two subtasks. (agt: Agent, usr: User)</p><p>User Goal reserve-hotel subtask: { "request slots": { "inform slots": { "hotel price": "?"</p><p>"hotel date checkin":"2016-09-22" "hotel date checkout": "?" "hotel city": "Curitiba" "hotel name": "?" "hotel numberofpeople": "4" "hotel amenity wifi": "?" } } } book-flight-ticket subtask: { "request slots": { "inform slots": { "price": "?" "or city": "Lima", "return time dep": "?" "dst city": "Curitiba", "return date dep": "?" "numberofpeople": "4", "depart time dep": "?" "depart date dep":"2016-09-22" "seat": "?" </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Topic detection and tracking pilot study final report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Yamron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An activity based approach to pragmatics. Abduction, belief and context in dialogue: Studies in computational pragmatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><forename type="middle">Allwood</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="47" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the bottleneck concept for options discovery: Theoretical underpinnings and extension in continuous state spaces. Master&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>McGill University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The option-critic architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1726" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PAC-inspired option discovery in lifelong reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="316" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Subdomain modelling for dialogue management with hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06210</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation of a hierarchical reinforcement learning spoken dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="395" to="429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for multi-domain dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Carse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08675</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Frames: A corpus for adding memory to goaloriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emery</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00057</idno>
		<ptr target="https://datasets.maluuba.com/Frames" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Subgoal discovery in reinforcement learning using local graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negin</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Ebrahim</forename><surname>Shiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><surname>Moradi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chinese word segmentation and named entity recognition: A pragmatic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="574" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed dialogue policies for multi-domain statistical dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5371" to="5375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Policy committee for adaptation in multidomain spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="806" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine learning for dialog state tracking: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Machine Learning in Spoken Language Processing</title>
		<meeting>the 1st International Workshop on Machine Learning in Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Tejas D Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3675" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A stochastic model of human-machine interaction for learning dialog strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="23" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A user simulator for task-completion dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end task-completion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01008</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rethinking Language, Mind, and World Dialogically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per</forename><surname>Linell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Information Age Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06136</idno>
		<title level="m">Iterative policy learning in end-to-end trainable task-oriented neural dialog models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">End-to-end optimization of task-oriented dialogue model with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10712</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Laplacian framework for option discovery in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">H</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2295" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic abstraction in reinforcement learning via clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishai</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Hoze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine learning (ICML)</title>
		<meeting>the 21st International Conference on Machine learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="560" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic discovery of subgoals in reinforcement learning using diverse density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1777" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adversarial advantage actor-critic model for taskcompletion dialogue policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06176</idno>
		<title level="m">Integrating planning for task-completion dialogue policy learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2221" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PolicyBlocks: An algorithm for creating useful macroactions in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Machine Learning (ICML)</title>
		<meeting>the 19th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Agenda-based user simulation for bootstrapping a pomdp dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2007; Companion Volume, Short Papers</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifying useful subgoals in reinforcement learning by local graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">¨</forename><surname>Ozgür S ¸ Ims¸ekims¸ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><forename type="middle">P</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 22nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="816" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning options in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Stolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Abstraction, Reformulation, and Approximation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Topic segmentation with shared topic detection and alignment of multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2007</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finding structure in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 7 (NIPS)</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FeUdal Networks for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3540" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07463</idno>
		<title level="m">Sequence modeling via segmentations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
