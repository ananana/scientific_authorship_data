<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<email>douwe.kiela@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge Computer Laboratory</orgName>
								<orgName type="institution" key="instit2">Microsoft Research New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Cambridge Computer Laboratory</orgName>
								<orgName type="institution" key="instit2">Microsoft Research New York</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="36" to="45"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent works have shown that multi-modal se- mantic representation models outperform uni- modal linguistic models on a variety of tasks, in- cluding modeling semantic relatedness and pre- dicting compositionality <ref type="bibr" target="#b13">(Feng and Lapata, 2010;</ref><ref type="bibr" target="#b23">Leong and Mihalcea, 2011;</ref><ref type="bibr" target="#b6">Bruni et al., 2012;</ref><ref type="bibr" target="#b30">Roller and Schulte im Walde, 2013;</ref>. These results were obtained by combin- ing linguistic feature representations with robust visual features extracted from a set of images as- sociated with the concept in question. This extrac- tion of visual features usually follows the popular computer vision approach consisting of comput- ing local features, such as SIFT features <ref type="bibr" target="#b26">(Lowe, 1999)</ref>, and aggregating them as bags of visual words <ref type="bibr" target="#b34">(Sivic and Zisserman, 2003)</ref>.</p><p>Meanwhile, deep transfer learning techniques have gained considerable attention in the com- puter vision community. First, a deep convolu- tional neural network (CNN) is trained on a large * This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. labeled dataset ( <ref type="bibr" target="#b20">Krizhevsky et al., 2012</ref>). The convolutional layers are then used as mid-level feature extractors on a variety of computer vi- sion tasks ( <ref type="bibr" target="#b28">Oquab et al., 2014;</ref><ref type="bibr" target="#b17">Girshick et al., 2013;</ref><ref type="bibr" target="#b40">Zeiler and Fergus, 2013;</ref><ref type="bibr" target="#b10">Donahue et al., 2014)</ref>. Although transferring convolutional net- work features is not a new idea <ref type="bibr" target="#b11">(Driancourt and Bottou, 1990)</ref>, the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: "SIFT and HOG descriptors pro- duced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough" <ref type="bibr" target="#b29">(Razavian et al., 2014)</ref>.</p><p>This work reports on results obtained by using CNN-extracted features in multi-modal semantic representation models. These results are interest- ing in several respects. First, these superior fea- tures provide the opportunity to increase the per- formance gap achieved by augmenting linguistic features with multi-modal features. Second, this increased performance confirms that the multi- modal performance improvement results from the information contained in the images and not the information used to select which images to use to represent a concept. Third, our evaluation re- veals an intriguing property of the CNN-extracted features. Finally, since we use the skip-gram ap- proach of  to generate our linguistic features, we believe that this work rep- resents the first approach to multimodal distribu- tional semantics that exclusively relies on deep learning for both its linguistic and visual compo- nents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Multi-Modal Distributional Semantics</head><p>Multi-modal models are motivated by parallels with human concept acquisition. Standard se-mantic space models extract meanings solely from linguistic data, even though we know that hu- man semantic knowledge relies heavily on percep- tual information <ref type="bibr" target="#b25">(Louwerse, 2011)</ref>. That is, there exists substantial evidence that many concepts are grounded in the perceptual system <ref type="bibr" target="#b2">(Barsalou, 2008)</ref>. One way to do this grounding in the context of distributional semantics is to obtain represen- tations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments <ref type="bibr" target="#b32">(Silberer and Lapata, 2012;</ref><ref type="bibr" target="#b30">Roller and Schulte im Walde, 2013)</ref> or from processing and extracting features from images <ref type="bibr" target="#b13">(Feng and Lapata, 2010;</ref><ref type="bibr" target="#b23">Leong and Mihalcea, 2011;</ref><ref type="bibr" target="#b6">Bruni et al., 2012)</ref>. This approach has met with quite some success ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-modal Deep Learning</head><p>Other examples that apply multi-modal deep learning use restricted Boltzmann machines <ref type="bibr" target="#b36">(Srivastava and Salakhutdinov, 2012;</ref><ref type="bibr" target="#b14">Feng et al., 2013)</ref>, auto-encoders ( <ref type="bibr" target="#b39">Wu et al., 2013</ref>) or recur- sive neural networks <ref type="bibr" target="#b35">(Socher et al., 2014</ref>). Multi- modal models with deep learning components have also successfully been employed in cross- modal tasks ( <ref type="bibr" target="#b21">Lazaridou et al., 2014</ref>). Work that is closely related in spirit to ours is by <ref type="bibr" target="#b33">Silberer and Lapata (2014)</ref>. They use a stacked auto-encoder to learn combined embeddings of textual and vi- sual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In con- trast, our work keeps the modalities separate and follows the standard multi-modal approach of con- catenating linguistic and visual representations in a single semantic space model. This has the advan- tage that it allows for separate data sources for the individual modalities. We also learn visual repre- sentations directly from the images (i.e., we apply deep learning directly to the images), as opposed to taking a higher-level representation as a start- ing point. <ref type="bibr" target="#b16">Frome et al. (2013)</ref> jointly learn multi- modal representations as well, but apply them to a visual object recognition task instead of concept meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Convolutional Neural Networks</head><p>A flurry of recent results indicates that image de- scriptors extracted from deep convolutional neu- ral networks (CNNs) are very powerful and con- sistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks ( <ref type="bibr" target="#b29">Razavian et al., 2014</ref>). Embeddings from state- of-the-art CNNs (such as <ref type="bibr" target="#b20">Krizhevsky et al. (2012)</ref>) have been applied successfully to a number of problems in computer vision ( <ref type="bibr" target="#b17">Girshick et al., 2013;</ref><ref type="bibr" target="#b40">Zeiler and Fergus, 2013;</ref><ref type="bibr" target="#b10">Donahue et al., 2014</ref>). This contribution follows the approach de- scribed by <ref type="bibr" target="#b28">Oquab et al. (2014)</ref>: they train a CNN on 1512 ImageNet synsets <ref type="bibr" target="#b9">(Deng et al., 2009)</ref>, use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Improving Multi-Modal Representations</head><p>Figure 1 illustrates how our system computes multi-modal semantic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perceptual Representations</head><p>The perceptual component of standard multi- modal models that rely on visual data is often an instance of the bag-of-visual-words (BOVW) representation <ref type="bibr" target="#b34">(Sivic and Zisserman, 2003)</ref>. This approach takes a collection of images associated with words or tags representing the concept in question. For each image, keypoints are laid out as a dense grid. Each keypoint is represented by a vector of robust local visual features such as SIFT (Lowe, 1999), SURF ( <ref type="bibr" target="#b3">Bay et al., 2008)</ref> and HOG ( <ref type="bibr" target="#b8">Dalal and Triggs, 2005</ref>), as well as pyra- midal variants of these descriptors such as PHOW ( <ref type="bibr" target="#b5">Bosch et al., 2007</ref>). These descriptors are sub- sequently clustered into a discrete set of "visual words" using a standard clustering algorithm like k-means and quantized into vector representations by comparing the local descriptors with the cluster centroids. Visual representations are obtained by taking the average of the BOVW vectors for the images that correspond to a given word. We use BOVW as a baseline. Our approach similarly makes use of a collec- tion of images associated with words or tags rep- resenting a particular concept. Each image is pro- cessed by the first seven layers of the convolu- tional network defined by <ref type="bibr" target="#b20">Krizhevsky et al. (2012)</ref> and adapted by <ref type="bibr" target="#b28">Oquab et al. (2014)</ref>  <ref type="bibr">1</ref> . This net- work takes 224 Ã 224 pixel RGB images and ap- plies five successive convolutional layers followed by three fully connected layers. Its eighth and last</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training linguistic features (after Mikolov et al., 2013)</head><p>C1-C2-C3-C4-C5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training visual features (after Oquab et al., 2014)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional layers</head><p>Fully-connected layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6144-dim feature vector</head><p>African elephant</p><p>Wall clock</p><p>Imagenet labels</p><formula xml:id="formula_0">â¦ FC6 FC7 FC8</formula><p>100-dim word projections layer produces a vector of 1512 scores associated with 1000 categories of the ILSVRC-2012 chal- lenge and the 512 additional categories selected by <ref type="bibr" target="#b28">Oquab et al. (2014)</ref>. This network was trained us- ing about 1.6 million ImageNet images associated with these 1512 categories. We then freeze the trained parameters, chop the last network layer, and use the remaining seventh layer as a filter to compute a 6144-dimensional feature vector on ar- bitrary 224 Ã 224 input images.</p><formula xml:id="formula_1">w(t) w(t+1) w(t+2) w(t-2) w(t-2) C1-C2-C3-C4-C5 FC7</formula><p>We consider two ways to aggregate the feature vectors representing each image.</p><p>1. The first method (CNN-Mean) simply com- putes the average of all feature vectors.</p><p>2. The second method (CNN-Max) computes the component-wise maximum of all feature vectors. This approach makes sense because the feature vectors extracted from this par- ticular network are quite sparse (about 22% non-zero coefficients) and can be interpreted as bags of visual properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linguistic representations</head><p>For our linguistic representations we extract 100- dimensional continuous vector representations us- ing the log-linear skip-gram model of  trained on a corpus consisting of the 400M word Text8 corpus of Wikipedia text 2 together with the 100M word British National Corpus ( <ref type="bibr" target="#b22">Leech et al., 1994)</ref>. We also experi- mented with dependency-based skip-grams ( <ref type="bibr" target="#b24">Levy and Goldberg, 2014</ref>) but this did not improve re- sults. The skip-gram model learns high quality se- mantic representations based on the distributional properties of words in text, and outperforms stan- dard distributional models on a variety of semantic similarity and relatedness tasks. However we note that  have recently reported an even better performance for their linguistic com- ponent using a standard distributional model, al- though this may have been tuned to the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-modal Representations</head><formula xml:id="formula_2">v concept = Î± Ã v ling || (1 â Î±) Ã v vis ,<label>(1)</label></formula><p>where || denotes the concatenation operator and Î± is an optional tuning parameter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We carried out experiments using visual repre- sentations computed using two canonical image datasets. The resulting multi-modal concept rep- resentations were evaluated using two well-known semantic relatedness datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Data</head><p>We carried out experiments using two distinct sources of images to compute the visual represen- tations.</p><p>The ImageNet dataset ( <ref type="bibr" target="#b9">Deng et al., 2009</ref>) is a large-scale ontology of images organized ac- cording to the hierarchy of WordNet <ref type="bibr" target="#b12">(Fellbaum, 1999</ref>). The dataset was constructed by manually re-labelling candidate images collected using web searches for each WordNet synset. The images tend to be of high quality with the designated ob- ject roughly centered in the image. Our copy of ImageNet contains about 12.5 million images or- ganized in 22K synsets. This implies that Ima- geNet covers only a small fraction of the existing 117K WordNet synsets.</p><p>The ESP Game dataset (Von Ahn and Dabbish, 2004) was famously collected as a "game with a purpose", in which two players must indepen- dently and rapidly agree on a correct word label for randomly selected images. Once a word label has been used sufficiently frequently for a given image, that word is added to the image's tags. This dataset contains 100K images, but with every im- age having on average 14 tags, that amounts to a coverage of 20,515 words. Since players are en- couraged to produce as many terms per image, the dataset's increased coverage is at the expense of accuracy in the word-to-image mapping: a dog in a field with a house in the background might be a golden retriever in ImageNet and could have tags dog, golden retriever, grass, field, house, door in the ESP Dataset. In other words, images in the ESP dataset do not make a distinction between ob- jects in the foreground and in the background, or between the relative size of the objects (tags for images are provided in a random order, so the top tag is not necessarily the best one).</p><p>Figures 2 and 3 show typical examples of im- ages belonging to these datasets. Both datasets have attractive properties. On the one hand, Ima- geNet has higher quality images with better labels. On the other hand, the ESP dataset has an interest- ing coverage because the MEN task (see section 4.4) was specifically designed to be covered by the ESP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Selection</head><p>Since ImageNet follows the WordNet hierarchy, we would have to include almost all images in the dataset to obtain representations for high-level concepts such as entity, object and animal. Doing so is both computationally expensive and unlikely to improve the results. For this reason, we ran- domly sample up to N distinct images from the subtree associated with each concept. When this returns less than N images, we attempt to increase coverage by sampling images from the subtree of the concept's hypernym instead. In order to allow for a fair comparison, we apply the same method of sampling up to N on the ESP Game dataset. In all following experiments, N = 1.000. We used the WordNet lemmatizer from NLTK ( <ref type="bibr" target="#b4">Bird et al., 2009</ref>) to lemmatize tags and concept words so as to further improve the dataset's coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Processing</head><p>The ImageNet images were preprocessed as de- scribed by <ref type="bibr" target="#b20">(Krizhevsky et al., 2012</ref></p><note type="other">). The largest centered square contained in each image is resam-pled to form a 256 Ã 256 image. The CNN input is then formed by cropping 16 pixels off each bor- der and subtracting 128 to the image components. The ESP Game images were preprocessed slightly differently because we do not expect the objects to be centered. Each image was rescaled to fit in- side a 224 Ã 224 rectangle. The CNN input is then formed by centering this image into the 224 Ã 224 input field, subtracting 128 to the image compo- nents, and zero padding.</note><p>The BOVW features were obtained by comput- ing DSIFT descriptors using VLFeat <ref type="bibr" target="#b37">(Vedaldi and Fulkerson, 2008)</ref>. These descriptors were subse- quently clustered using mini-batch k-means (Scul- ley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual con- cept representations by taking their mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics ( <ref type="bibr" target="#b0">Agirre et al., 2009;</ref><ref type="bibr" target="#b13">Feng and Lapata, 2010;</ref><ref type="bibr" target="#b6">Bruni et al., 2012;</ref>).</p><p>WordSim353 ( <ref type="bibr" target="#b15">Finkelstein et al., 2001</ref>) is a se- lection of 353 concept pairs with a similarity rat- ing provided by human annotators. Since this is probably the most widely used evaluation dataset for distributional semantics, we include it for com- parison with other approaches. WordSim353 has some known idiosyncracies: it includes named en- tities, such as OPEC, Arafat, and Maradona, as well as abstract words, such as antecedent and credibility, for which it may be hard to find cor- responding images. Multi-modal representations are often evaluated on an unspecified subset of <ref type="bibr">WordSim353 (Feng and Lapata, 2010;</ref><ref type="bibr" target="#b6">Bruni et al., 2012;</ref>), making it impossi- ble to compare the reported scores. In this work, we report scores on the full WordSim353 dataset (W353) by setting the visual vector v vis to zero for concepts without images. We also report scores on the subset (W353-Relevant) of pairs for which both concepts have both ImageNet and ESP Game images using the aforementioned selection proce- dure.</p><p>MEN ( <ref type="bibr" target="#b6">Bruni et al., 2012</ref>) was in part designed to alleviate the WordSim353 problems. It was con- structed in such a way that only frequent words with at least 50 images in the ESP Game dataset were included in the evaluation pairs. The MEN dataset has been found to mirror the aggregate score over a variety of tasks and similarity datasets . It is also much larger, with 3000 words pairs consisting of 751 individual words. Although MEN was constructed so as to have at least a minimum amount of images avail- able in the ESP Game dataset for each concept, this is not the case for ImageNet. Hence, simi- larly to WordSim353, we also evaluate on a subset (MEN-Relevant) for which images are available in both datasets.</p><p>We evaluate the models in terms of their Spear- man Ï correlation with the human relatedness rat- ings. The similarity between the representations associated with a pair of words is calculated using the cosine similarity:</p><formula xml:id="formula_3">cos(v 1 , v 2 ) = v 1 Â· v 2 v 1 v 2<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluate on the two semantic relatedness datasets using solely linguistic, solely visual and multi-modal representations. In the case of MEN- Relevant and W353-Relevant, we report scores for BOVW, CNN-Mean and CNN-Max visual repre- sentations. For all datasets we report the scores obtained by BOVW, CNN-Mean and CNN-Max multi-modal representations. Since we have full coverage with the ESP Game dataset on MEN, we are able to report visual representation scores for the entire dataset as well. The results can be seen in <ref type="table">Table 1</ref>.</p><p>There are a number of questions to ask. First of all, do CNNs yield better visual representa- tions? Second, do CNNs yield better multi-modal representations? And third, is there a difference between the high-quality low-coverage ImageNet and the low-quality higher-coverage ESP Game dataset representations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual Representations</head><p>In all cases, CNN-generated visual representations perform better or as good as BOVW representa- tions (we report results for BOVW-Mean, which performs slightly better than taking the element- wise maximum). This confirms the motivation outlined in the introduction: by applying state-of- the-art approaches from computer vision to multi- modal semantics, we obtain a signficant perfor- mance increase over standard multi-modal mod- els.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-modal Representations</head><p>Higher-quality perceptual input leads to better- performing multi-modal representations. In all cases multi-modal models with CNNs outperform multi-modal models with BOVW, occasionally by quite a margin. In all cases, multi-modal rep- resentations outperform purely linguistic vectors that were obtained using a state-of-the-art system. This re-affirms the importance of multi-modal rep- resentations for distributional semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Contribution of Images</head><p>Since the ESP Game images come with a multi- tude of word labels, one could question whether a performance increase of multi-modal models based on that dataset comes from the images them- selves, or from overlapping word labels. It might also be possible that similar concepts are more likely to occur in the same image, which encodes relatedness information without necessarily tak- ing the image data itself into account. In short, it is a natural question to ask whether the perfor- mance gain is due to image data or due to word label associations? We conclusively show that the image data matters in two ways: (a) using a dif- ferent dataset (ImageNet) also results in a perfor- mance boost, and (b) using higher-quality image features on the ESP game images increases the performance boost without changing the associa- tion between word labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Image Datasets</head><p>It is important to ask whether the source im- age dataset has a large impact on performance.</p><p>Although the scores for the visual representa- tion in some cases differ, performance of multi- modal representations remains close for both im- age datasets. This implies that our method is ro- bust over different datasets. It also suggests that it is beneficial to train on high-quality datasets like ImageNet and to subsequently generate embed- dings for other sets of images like the ESP Game dataset that are more noisy but have better cover- age. The results show the benefit of transfering convolutional network features, corroborating re- cent results in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Semantic Similarity/Relatedness Datasets</head><p>There is an interesting discrepancy between the two types of network with respect to dataset per- formance: CNN-Mean multi-modal models tend to perform best on MEN and MEN-Relevant, while CNN-Max multi-modal models perform better on W353 and W353-Relevant. There also appears to be some interplay between the source corpus, the evaluation dataset and the best per- forming CNN: the performance leap on W353- Relevant for CNN-Max is much larger using ESP Game images than with ImageNet images. We speculate that this is because CNN-Max per- forms better than CNN-Mean on a somewhat dif- ferent type of similarity. It has been noted <ref type="bibr" target="#b0">(Agirre et al., 2009</ref>) that WordSim353 captures both sim- ilarity (as in tiger-cat, with a score of 7.35) as well as relatedness (as in Maradona-football, with a score of 8.62). MEN, however, is explicitly de- signed to capture semantic relatedness only ( <ref type="bibr" target="#b6">Bruni et al., 2012)</ref>. CNN-Max using sparse feature vec- tors means that we treat the dominant components as definitive of the concept class, which is more suited to similarity. CNN-Mean averages over all the feature components, and as such might be more suited to relatedness. We conjecture that the performance increase on WordSim353 is due to increased performance on the similarity subset of that dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Tuning</head><p>The concatenation scheme in Equation 1 allows for a tuning parameter Î± to weight the relative contribution of the respective modalities. Previous work on MEN has found that the optimal param- eter for that dataset is close to 0.5 ( ). We have found that this is indeed the case. On WordSim353, however, we have found the pa- rameter for optimal performance to be shifted to the right, meaning that optimal performance is achieved when we include less of the visual input compared to the linguistic input. <ref type="figure" target="#fig_4">Figure 4</ref> shows what happens when we vary alpha over the four datasets. There are a number of observations to be made here.</p><p>First of all, we can see that the performance peak for the MEN datastes is much higher than for the WordSim353 ones, and that its peak is rel- atively higher as well. This indicates that MEN is in a sense a more balanced dataset. There are two possible explanations: as indicated earlier, Word- Sim353 contains slightly idiosyncratic word pairs which may have a detrimental effect on perfor- mance; or, WordSim353 was not constructed with multi-modal semantics in mind, and contains a substantial amount of abstract words that would not benefit at all from including visual informa- tion.</p><p>Due to the nature of the datasets and the tasks at hand, it is arguably much more important that CNNs beat standard bag-of-visual-words repre- sentations on MEN than on W353, and indeed we see that there exists no Î± for which BOVW would beat any of the CNN networks.  <ref type="table" target="#tab_1">Table 2</ref>: The top 5 best and top 5 worst scoring pairs with respect to the gold standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head><p>Mean multi-modal vectors. The most accurate pairs are consistently the same across the two im- age datasets. There are some clear differences between the least accurate pairs, however. The MEN words potatoes and tomato probably have low quality ImageNet-derived representations, be- cause they occur often in the bottom pairs for that dataset. The MEN words dessert, bread and fruit occur in the bottom 5 for both image datasets, which implies that their linguistic representations are probably not very good. For WordSim353, the bottom pairs on ImageNet could be said to be sim- ilarity mistakes; while the ESP Game dataset con- tains more relatedness mistakes (king and queen would evaluate similarity, while stock and market would evaluate relatedness). It is difficult to say anything conclusive about this discrepancy, but it is clearly a direction for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Image embeddings</head><p>To facilitate further research on image embed- dings and multi-modal semantics, we publicly re- lease embeddings for all the image labels occur- ring in the ESP Game dataset. Please see the fol- lowing web page: http://www.cl.cam.ac. uk/ Ë dk427/imgembed.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a novel approach to improving multi-modal representations using deep convo- lutional neural network-extracted features. We reported high results on two well-known and widely-used semantic relatedness benchmarks, with increased performance both in the separate visual representations and in the combined multi- modal representations. Our results indicate that such multi-modal representations outperform both linguistic and standard bag-of-visual-words multi- modal representations. We have shown that our approach is robust and that CNN-extracted fea- tures from separate image datasets can succesfully be applied to semantic relatedness. In addition to improving multi-modal represen- tations, we have shown that the source of this im- provement is due to image data and is not simply a result of word label associations. We have shown this by obtaining performance improvements on two different image datasets, and by obtaining higher performance with higher-quality image fea- tures on the ESP game images, without changing the association between word labels.</p><p>In future work, we will investigate whether our system can be further improved by including con- creteness information or a substitute metric such as image dispersion, as has been suggested by other work on multi-modal semantics ( ). Furthermore, a logical next step to increase performance would be to jointly learn multi-modal representations or to learn weighting parameters. Another interesting possibility would be to exam- ine multi-modal distributional compositional se- mantics, where multi-modal representations are composed to obtain phrasal representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Computing word feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Following</head><label></label><figDesc>Bruni et al. (2014), we construct multi- modal semantic representations by concatenating the centered and L 2 -normalized linguistic and per- ceptual feature vectors v ling and v vis ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of dog in the ESP Game dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of golden retriever in ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Varying the Î± parameter for MEN, MEN-Relevant, WordSim353 and WordSim353-Relevant, respectively.</figDesc><graphic url="image-10.png" coords="7,72.00,62.80,453.55,242.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Dataset Linguistic Visual Multi-modal BOVW CNN-Mean CNN-Max BOVW CNN-Mean CNN-Max</head><label></label><figDesc></figDesc><table>ImageNet visual features 

MEN 
0.64 
-
-
-
0.64 
0.70 
0.67 

MEN-Relevant 
0.62 
0.40 
0.64 
0.63 
0.64 
0.72 
0.71 

W353 
0.57 
-
-
-
0.58 
0.59 
0.60 

W353-Relevant 
0.51 
0.30 
0.32 
0.30 
0.55 
0.56 
0.57 

ESP game visual features 

MEN 
0.64 
0.17 
0.51 
0.20 
0.64 
0.71 
0.65 

MEN-Relevant 
0.62 
0.35 
0.58 
0.57 
0.63 
0.69 
0.70 

W353 
0.57 
-
-
-
0.58 
0.59 
0.60 

W353-Relevant 
0.51 
0.38 
0.44 
0.56 
0.52 
0.55 
0.61 

Table 1: Results (see sections 4 and 5). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows the top 5 best and top 5 worst scor- ing word pairs for the two datasets using CNN-</head><label>2</label><figDesc></figDesc><table>W353-Relevant 
ImageNet 
ESP Game 

word1 
word2 
system score gold standard 
word1 
word2 
system score gold standard 
tiger 
tiger 
1.00 
1.00 
tiger 
tiger 
1.00 
1.00 
man 
governor 
0.53 
0.53 
man 
governor 0.53 
0.53 
stock 
phone 
0.15 
0.16 
stock 
phone 
0.15 
0.16 
football 
tennis 
0.68 
0.66 
football tennis 
0.68 
0.66 
man 
woman 
0.85 
0.83 
man 
woman 
0.85 
0.83 
cell 
phone 
0.27 
0.78 
law 
lawyer 
0.33 
0.84 
discovery space 
0.10 
0.63 
monk 
slave 
0.58 
0.09 
closet 
clothes 
0.22 
0.80 
gem 
jewel 
0.41 
0.90 
king 
queen 
0.26 
0.86 
stock 
market 
0.33 
0.81 
wood 
forest 
0.13 
0.77 
planet 
space 
0.32 
0.79 
MEN-Relevant 
ImageNet 
ESP Game 

word1 
word2 
system score gold standard 
word1 
word2 
system score gold standard 
beef 
potatoes 
0.35 
0.35 
beef 
potatoes 
0.35 
0.35 
art 
work 
0.35 
0.35 
art 
work 
0.35 
0.35 
grass 
stop 
0.06 
0.06 
grass 
stop 
0.06 
0.06 
shade 
tree 
0.45 
0.45 
shade 
tree 
0.45 
0.45 
blonde 
rock 
0.07 
0.07 
blonde 
rock 
0.07 
0.07 
bread 
potatoes 
0.88 
0.34 
bread 
dessert 
0.78 
0.24 
fruit 
potatoes 
0.80 
0.26 
jacket 
shirt 
0.89 
0.34 
dessert 
sandwich 0.76 
0.23 
fruit 
nuts 
0.88 
0.33 
pepper 
tomato 
0.79 
0.27 
dinner 
lunch 
0.93 
0.37 
dessert 
tomato 
0.66 
0.14 
dessert 
soup 
0.81 
0.23 

</table></figure>

			<note place="foot" n="1"> http://www.di.ens.fr/willow/research/cnn/</note>

			<note place="foot" n="2"> http://mattmahoney.net/dc/textdata.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Maxime Oquab for pro-viding the feature extraction code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies</title>
		<meeting>Human Language Technologies</meeting>
		<imprint>
			<publisher>The</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Marius PasÂ¸caPasÂ¸ca, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grounded cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">W</forename><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="617" to="845" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SURF: Speeded Up Robust Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image classification using random forests and ferns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>CVPR &apos;05</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TDNNextracted features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Driancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neuro Nimes 90</title>
		<meeting>Neuro Nimes 90<address><addrLine>Nimes, France. EC2</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual information in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Constructing hierarchical image-tags bimodal representations for word tags alternative choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeViSE: A Deep VisualSemantic Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">MarcÂ´aureliomarcÂ´</forename><surname>MarcÂ´aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>arXiv preprint:1311.2524</idno>
		<imprint>
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Systematic Study of Semantic Vector Space Model Parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2014, Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<meeting>EACL 2014, Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Claws4: the tagging of the British National Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th conference on Computational linguistics</title>
		<meeting>the 15th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="622" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going Beyond Text: A Hybrid Image-Text Approach for Measuring Word Relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint International Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>Joint International Conference on Natural Language Processing (IJCNLP)<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Symbol interdependency in symbolic and embodied cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Louwerse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TopiCS in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="273" to="302" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference of Learning Representations</title>
		<meeting>International Conference of Learning Representations<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>arXiv preprint:1403.6382</idno>
		<title level="m">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multimodal LDA model integrating textual, cognitive and visual modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1146" to="1157" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1177" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grounded models of semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1423" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Grounded Meaning Representations with Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2014</title>
		<meeting>ACL 2014<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth IEEE International Conference on Computer Vision</title>
		<meeting>the Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grounded Compositional Semantics for Finding and Describing Images with Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Online multimodal deep similarity learning with application to image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia, MM &apos;13</title>
		<meeting>the 21st ACM International Conference on Multimedia, MM &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
