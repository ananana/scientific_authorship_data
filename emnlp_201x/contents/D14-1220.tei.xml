<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Deep Models for Discourse Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiweil@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
							<email>alicerumeng@foxmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>ehovy@andrew.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Deep Models for Discourse Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2061" to="2069"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Text-level discourse parsing remains a challenge: most approaches employ features that fail to capture the intentional, semantic , and syntactic aspects that govern discourse coherence. In this paper, we propose a recursive model for discourse parsing that jointly models distributed representations for clauses, sentences, and entire discourses. The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically,. The proposed framework obtains comparable performance regarding standard discoursing parsing evaluations when compared against current state-of-art systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a coherent text, units (clauses, sentences, and larger multi-clause groupings) are tightly con- nected semantically, syntactically, and logically. <ref type="bibr" target="#b23">Mann and Thompson (1988)</ref> define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Dis- course parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which mul- tiple NLP tasks can benefit, including text sum- marization ( <ref type="bibr" target="#b21">Louis et al., 2010)</ref>, sentence compres- sion <ref type="bibr" target="#b36">(Sporleder and Lapata, 2005</ref>) or question- answering ( <ref type="bibr" target="#b38">Verberne et al., 2007)</ref>.</p><p>Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., <ref type="bibr" target="#b9">(Fisher and Roark, 2007;</ref><ref type="bibr" target="#b16">Joty et al., 2012;</ref><ref type="bibr" target="#b35">Soricut and Marcu, 2003)</ref>, document-level discourse parsing remains a significant challenge. Recent attempts (e.g., <ref type="bibr" target="#b12">(Hernault et al., 2010b;</ref><ref type="bibr" target="#b7">Feng and Hirst, 2012;</ref><ref type="bibr" target="#b17">Joty et al., 2013)</ref>) are still consid- erably inferior when compared to human gold- standard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level 'parts of dis- course' analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how dis- course units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, taken recursively all the way up for the entire text. Manually developed features relating to words and other syntax-related cues, used in most of the re- cent prevailing approaches (e.g., <ref type="bibr" target="#b7">(Feng and Hirst, 2012;</ref><ref type="bibr" target="#b12">Hernault et al., 2010b)</ref>), are insufficient for capturing such nested intentionality.</p><p>Recently, deep learning architectures have been applied to various natural language processing tasks (for details see Section 2) and have shown the advantages to capture the relevant semantic and syntactic aspects of units in context. As word distributions are composed to form the meanings of clauses, the goal is to extend distributed clause- level representations to the single-and multi- sentence (discourse) levels, and produce the hier- archical structure of entire texts.</p><p>Inspired by this idea, we introduce in this pa- per a deep learning approach for discourse pars- ing. The proposed parsing algorithm relies on a recursive neural network to decide (1) whether two discourse units are connected and if so <ref type="formula" target="#formula_3">(2)</ref> by what relation they are connected. Concretely, the parsing algorithm takes as input a document of any length, and first obtains the distributed repre- sentation for each of its sentences using recursive convolution based on the sentence parse tree. It then proceeds bottom-up, applying a binary clas- sifier to determine the probability of two adjacent discourse units being merged to form a new sub- tree followed by a multi-class classifier to select the appropriate discourse relation label, and cal- culates the distributed representation for the sub- tree so formed, gradually unifying subtrees un- til a single overall tree spans the entire sentence. The compositional distributed representation en- ables the parser to make accurate parsing decisions and capture relations between different sentences and units. The binary and multi-class classifiers, along with parameters involved in convolution, are jointly trained from a collection of gold-standard discourse structures.</p><p>The rest of this paper is organized as follows. We present related work in Section 2 and de- scribe the RST Discourse Treebank in Section 3. The sentence convolution approach is illustrated in Section 4 and the discourse parser model in Sec- tion 5. We report experimental results in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Discourse Analysis and Parsing</head><p>The basis of discourse structure lies in the recog- nition that discourse units (minimally, clauses) are related to one another in principled ways, and that the juxtaposition of two units creates a joint mean- ing larger than either unit's meaning alone. In a coherent text this juxtaposition is never random, but serves the speaker's communicative goals.</p><p>Considerable work on linguistic and computa- tional discourse processing in the 1970s and 80s led to the development of several proposals for re- lations that combine units; for a compilation see <ref type="bibr" target="#b14">(Hovy and Maier, 1997)</ref>. Of these the most influ- ential is Rhetorical Structure Theory RST ( <ref type="bibr" target="#b23">Mann and Thompson, 1988</ref>) that defines about 25 rela- tions, each containing semantic constraints on its component parts plus a description of the overall functional/semantic effect produced as a unit when the parts have been appropriately connected in the text. For example, the SOLUTIONHOOD relation connects one unit describing a problem situation with another describing its solution, using phrases such as "the answer is"; in successful communi- cation the reader will understand that a problem is described and its solution is given.</p><p>Since there is no syntactic definition of a prob- lem or solution (they can each be stated in a sin- gle clause, a paragraph, or an entire text), one has to characterize discourse units by their commu- nicative (rhetorical) function. The functions are reflected in text as signals of the author's inten- tions, and take various forms (including expres- sions such as "therefore", "for example", "the an- swer is", and so on; patterns of tense or pronoun usage; syntactic forms; etc.). The signals govern discourse blocks ranging from a clause to an en- tire text , each one associated with some discourse relation.</p><p>In order to build a text's hierarchical structure, a discourse parser needs to recognize these signals and use them to appropriately compose the rela- tionship and nesting. Early approaches <ref type="bibr" target="#b24">(Marcu, 2000a;</ref><ref type="bibr" target="#b19">LeThanh et al., 2004</ref>) rely mainly on overt discourse markers (or cue words) and use hand- coded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs. . . . Since a hierarchical discourse tree structure is analo- gous to a constituency based syntactic tree, mod- ern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on mul- tiple text-level or sentence-level features <ref type="bibr" target="#b35">(Soricut and Marcu, 2003;</ref><ref type="bibr" target="#b28">Reitter, 2003;</ref><ref type="bibr" target="#b0">Baldridge and Lascarides, 2005;</ref><ref type="bibr" target="#b37">Subba and Di Eugenio, 2009;</ref><ref type="bibr" target="#b20">Lin et al., 2009;</ref><ref type="bibr" target="#b22">Luong et al., 2014)</ref>.</p><p>A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary struc- ture classifier for determining whether two adja- cent text units should be merged to form a new subtree, followed by a multi-class relation classi- fier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues <ref type="bibr" target="#b6">(Duverle and Prendinger, 2009;</ref><ref type="bibr" target="#b11">Hernault et al., 2010a</ref>) and followed by other work using more sophisticated features <ref type="bibr" target="#b7">(Feng and Hirst, 2012;</ref><ref type="bibr" target="#b12">Hernault et al., 2010b)</ref>. Current state-of-art performance for re- lation identification is achieved by the recent rep- resentation learning approach proposed by <ref type="bibr" target="#b15">(Ji and Eisenstein, 2014</ref>). The proposed framework pre- sented in this paper is similar to <ref type="bibr" target="#b15">(Ji and Eisenstein, 2014</ref>) for transforming the discourse units to the abstract representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recursive Deep Learning</head><p>Recursive neural networks constitute one type of deep learning frameworks which was first pro- posed in <ref type="bibr" target="#b10">(Goller and Kuchler, 1996)</ref>. The recur- sive framework relies and operates on structured inputs (e.g., a parse tree) and computes the rep- resentation for each parent based on its children iteratively in a bottom-up fashion. A series of vari- ations of RNN has been proposed to tailor differ- ent task-specific requirements, including <ref type="bibr">MatrixVector RNN (Socher et al., 2012</ref>) that represents every word as both a vector and a matrix, or Recur- sive Neural Tensor Network ( <ref type="bibr" target="#b34">Socher et al., 2013</ref>) that allows the model to have greater interactions between the input vectors. Many tasks have ben- efited from the recursive framework, including parsing ( <ref type="bibr" target="#b32">Socher et al., 2011b</ref>), sentiment analysis ( <ref type="bibr" target="#b34">Socher et al., 2013)</ref>, textual entailment <ref type="bibr" target="#b1">(Bowman, 2013)</ref>, segmentation ( <ref type="bibr" target="#b39">Wang and Mansur, 2013;</ref>, and paraphrase detection <ref type="bibr" target="#b31">(Socher et al., 2011a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The RST Discourse Treebank</head><p>There are today two primary alternative discourse treebanks suitable for training data: the Rhetor- ical Structure Theory Discourse Treebank RST- DT ( <ref type="bibr" target="#b2">Carlson et al., 2003</ref>) and the Penn Discourse Treebank ( <ref type="bibr" target="#b27">Prasad et al., 2008)</ref>. In this paper, we select the former. In RST ( <ref type="bibr" target="#b23">Mann and Thompson, 1988</ref>), a coherent context or a document is repre- sented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs). Adjacent nodes (siblings in the tree) are linked with discourse relations that are either binary (hypotactic) or multi-child (parat- actic). One child of each hypotactic relation is al- ways more salient (called the NUCLEUS); its sib- ling (the SATELLITE) is less salient compared and may be omitted in summarization. Multi-nuclear relations (e.g., CONJUNCTION) exhibit no distinc- tion of salience between the units.</p><p>The RST Discourse Treebank contains 385 an- notated documents (347 for training and 38 for testing) from the Wall Street Journal. A total of 110 fine-grained relations defined in (Marcu, 2000b) are used for tagging relations in RST-DT. They are subtypes of 18 original high-level RST categories. For fair comparison with existing sys- tems, we use in this work the 18 coarse-grained re- lation classes, which with nuclearity attached form a set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching bi- nary relations.</p><p>Conventionally, discourse parsing in RST-DT involves the following sub-tasks: (1) EDU seg- mentation to segment the raw text into EDUs, (2) tree-building. Since the segmentation task is es- sentially clause delimitation and hence relatively easy (with state-of-art accuracy at most 95%), we focus on the latter problem. We assume that the gold-standard EDU segmentations are already given, as assumed in other past work <ref type="bibr" target="#b7">(Feng and Hirst, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EDU Model</head><p>In this section, we describe how we compute the distributed representation for a given sentence based on its parse tree structure and contained words. Our implementation is based on <ref type="bibr" target="#b34">(Socher et al., 2013)</ref>. As the details can easily be found there, we omit them for brevity.</p><p>Let s denote any given sentence, comprised of a sequence of tokens s = {w 1 , w 2 , ..., w ns }, where n s denotes the number of tokens in s. Each to- ken w is associated with a specific vector embed- ding e w = {e 1 w , e 2 w , ..., e K w }, where K denotes the dimension of the word embedding. We wish to compute the vector representation h s for current sentence, where</p><formula xml:id="formula_0">h s = {h 1 s , h 2 s , ..., h K s }.</formula><p>Parse trees are obtained using the Stanford Parser 1 , and each clause is treated as an EDU. For a given parent p in the tree and its two children c 1 (associated with vector representation h c 1 ) and c 2 (associated with vector representation h c 2 ), stan- dard recursive networks calculate the vector for parent p as follows:</p><formula xml:id="formula_1">h p = f (W · [h c 1 , h c 2 ] + b)<label>(1)</label></formula><p>where [h c 1 , h c 2 ] denotes the concatenating vector for children representations h c 1 and h c 2 ; W is a K × 2K matrix and b is the 1 × K bias vector; and f (·) is the function tanh. Recursive neural models compute parent vectors iteratively until the root node's representation is obtained, and use the root embedding to represent the whole sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discourse Parsing</head><p>Since recent work <ref type="bibr" target="#b7">(Feng and Hirst, 2012;</ref><ref type="bibr" target="#b12">Hernault et al., 2010b</ref>) has demonstrated the advantage of combining the binary structure classifier (deter- mining whether two adjacent text units should be merged to form a new subtree) with the multi-class classifier (determining which discourse relation la- bel to assign to the new subtree) over the older single multi-class classifier with the additional la- bel NO-REL, our approach follows the modern strategy but trains binary and multi-class classi- fiers jointly based on the discourse structure tree. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the structure of a discourse parse tree. Each node e in the tree is associated with a distributed vector h e . e 1 , e 2 , e 3 and e 6 constitute the leaves of trees, the distributed vec- tor representations of which are assumed to be al- ready obtained from convolution in Section 4. Let N r denote the number of relations and we have N r = 41.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Binary (Structure) Classification</head><p>In this subsection, we train a binary (structure) classifier, which aims to decide whether two EDUs or spans should be merged during discourse tree reconstruction.</p><p>Let t binary (e i , e j ) be the binary valued variable indicating whether e i and e j are related, or in other words, whether a certain type of discourse rela- tions holds between e i and e j . According to <ref type="figure" target="#fig_1">Fig- ure 2</ref>, the following pairs constitute the training data for binary classification: t binary (e 1 , e 2 ) = 1, t binary (e 3 , e 4 ) = 1, t binary (e 2 , e 3 ) = 0, t binary (e 3 , e 6 ) = 0, t binary (e 5 , e 6 ) = 1</p><p>To train the binary classifier, we adopt a three- layer neural network structure, i.e., input layer, hidden layer, and output layer. Let H = [h e i , h e j ] denote the concatenating vector for two spans e i and e j . We first project the concatenating vector H to the hidden layer with N binary hidden neurons. The hidden layer convolutes the input with non- linear tanh function as follows:</p><formula xml:id="formula_2">L binary (e i ,e j ) = f (G binary * [h e i , h e j ] + b binary )</formula><p>where G binary is an N binary * 2K convolution ma- trix and b binary denotes the bias vector.</p><p>The output layer takes as input L binary (e i ,e j ) and gen- erates a scalar using the linear function U binary · L binary (e i ,e j ) + b. A sigmod function is then adopted to project the value to a [0,1] probability space. The execution at the output layer can be summarized as:</p><formula xml:id="formula_3">p[t binary (e i , e j ) = 1] = g(U binary ·L binary (e i ,e j ) +b * binary )<label>(2)</label></formula><p>where U binary is an N binary × 1 vector and b * binary denotes the bias. g(·) is the sigmod function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-class Relation Classification</head><p>If t binary (e i , e j ) is determined to be 1, we next use variable r(e i , e j ) to denote the index of rela- tion that holds between e i and e j . A multi-class classifier is train based on a three-layer neural net- work, in the similar way as binary classification in Section 5.1. Concretely, a matrix G Multi and bias vector b Multi are first adopted to convolute the con- catenating node vectors to the hidden layer vector</p><formula xml:id="formula_4">L multi (e i ,e j ) : L multi (e i ,e j ) = f (G multi * [h e i , h e j ] + b multi )<label>(3)</label></formula><p>We then compute the posterior probability over labels given the hidden layer vector L using the softmax and obtain the N r dimensional probabil- ity vector P (e 1 ,e 2 ) for each EDU pair as follows:</p><formula xml:id="formula_5">S (e i ,e j ) = U multi · L multi (e i ,e j ) (4) P (e 1 ,e 2 ) (i) = exp(S (e 1 ,e 2 ) (i)) k exp(S (e 1 ,e 2 ) )(k)<label>(5)</label></formula><p>where U multi is the N r × 2K matrix. The i th ele- ment in P (e 1 ,e 2 ) denotes the probability that i t h re- lation holds between e i and e j . To note, binary and multi-class classifiers are trained independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Distributed Vector for Spans</head><p>What is missing in the previous two subsections are the distributed vectors for non-leaf nodes (i.e., e 4 and e 5 in <ref type="figure" target="#fig_0">Figure 1</ref>), which serve as structure and relation classification. Again, we turn to recursive deep learning network to obtain the distributed vector for each node in the tree in a bottom-up fashion.</p><p>Similar as for sentence parse-tree level compo- sitionally, we extend a standard recursive neural network by associating each type of relations r with one specific K × 2K convolution matrix W r . The representation for each node within the tree is calculated based on the representations for its chil- dren in a bottom-up fashion. Concretely, for a par- ent node p, given the distributed representation h e i for left child, h e j for right child, and the relation r(e 1 , e 2 ), its distributed vector h p is calculated as follows:</p><formula xml:id="formula_6">h p = f (W r(e 1 ,e 2 ) · [h e i , h e j ] + b r(e 1 ,e 2 ) ) (6)</formula><p>where b r(e 1 ,e 2 ) is the bias vector and f (·) is the non-linear tanh function.</p><p>To note, our approach does not make any dis- tinction between within-sentence text spans and cross-sentence text spans, different from <ref type="bibr" target="#b7">(Feng and Hirst, 2012;</ref><ref type="bibr" target="#b17">Joty et al., 2013)</ref>  Suppose we have M 1 binary training samples and M 2 multi-class training examples (M 2 equals the number of positive examples in M 1 , which is also the non-leaf nodes within the training dis- course trees). The cost function for our framework with regularization on the training set is given by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cost Function</head><formula xml:id="formula_7">J(Θ binary ) = (e i ,e j )∈{binary} J binary (e i , e j ) + Q binary · θ∈Θ binary θ 2 (7) J(Θ multi ) = (e i ,e j )∈{multi} J multi (e i , e j ) + Q multi · θ∈Θ multi θ 2 (8) where</formula><formula xml:id="formula_8">J binary (e i , e j ) = −t(e i , e j ) log p(t(e i , e j ) = 1) − (1 − t(e i , e j )) log[1 − p(t(e i , e j ) = 1)] J multi (e i , e j ) = − log[p(r(e i , e j ) = r)]<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Backward Propagation</head><p>The derivative for parameters involved is com- puted through backward propagation. Here we illustrate how we compute the derivative of J multi (e i , e j ) with respect to different parameters. For each pair of nodes (e i , e j ) ∈ multi, we associate it with a N r dimensional binary vector R(e i , e j ), which denotes the ground truth vector with a 1 at the correct label r(e i , e j ) and all other entries 0. Integrating softmax error vector, for any parameter θ, the derivative of J multi (e i , e j ) with re- spect to θ is given by:</p><formula xml:id="formula_9">∂J multi (e i , e j ) ∂θ = [P (e i ,e j ) − R (e i ,e j ) ] ⊗ ∂S (e i ,e j ) ∂θ<label>(10</label></formula><p>) where ⊗ denotes the Hadamard product between the two vectors. Each training pair recursively backpropagates its error to some node in the dis- course tree through [{W r }, {b r }], and then to nodes in sentence parse tree through <ref type="bibr">[W, b]</ref>, and the derivatives can be obtained according to stan- dard backpropagation <ref type="bibr" target="#b10">(Goller and Kuchler, 1996;</ref><ref type="bibr" target="#b29">Socher et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Additional Features</head><p>When determining the structure/multi relation be- tween individual EDUs, additional features are also considered, the usefulness of which has been illustrated in a bunch of existing work <ref type="bibr" target="#b7">(Feng and Hirst, 2012;</ref><ref type="bibr" target="#b12">Hernault et al., 2010b;</ref><ref type="bibr" target="#b16">Joty et al., 2012)</ref>. We consider the following simple text-level features:</p><p>• Tokens at the beginning and end of the EDUs.</p><p>• POS at the beginning and end of the EDUs.</p><p>• Whether two EDUs are in the same sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Optimization</head><p>We use the diagonal variant of AdaGrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>) with minibatches, which is widely ap- plied in deep learning literature (e.g., <ref type="bibr" target="#b31">(Socher et al., 2011a;</ref><ref type="bibr" target="#b26">Pei et al., 2014)</ref>). The learning rate in AdaGrad is adapted differently for different pa- rameters at different steps. Concretely, let g i τ de- note the subgradient at time step t for parameter θ i obtained from backpropagation, the parameter update at time step t is given by:</p><formula xml:id="formula_10">θ τ = θ τ −1 − α τ t=0 g i2 τ g i τ<label>(11)</label></formula><p>where α denotes the learning rate and is set to 0.01 in our approach. Elements in {W r }, W , G binary , G multi , U binary , U multi are initialized by randomly drawing from the uniform distribution <ref type="bibr">[−, ]</ref>, where is calcu- lated as suggested in ). All bias vectors are initialized with 0. Word embed- dings {e} are borrowed from <ref type="bibr">Senna (Collobert et al., 2011;</ref><ref type="bibr" target="#b4">Collobert, 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Inference</head><p>For inference, the goal is to find the most proba- ble discourse tree given the EDUs within the doc- ument. Existing inference approach basically in- clude the approach adopted in <ref type="bibr" target="#b7">(Feng and Hirst, 2012;</ref><ref type="bibr" target="#b12">Hernault et al., 2010b</ref>) that merges the most likely spans at each step and SPADE <ref type="bibr" target="#b9">(Fisher and Roark, 2007</ref>) that first finds the tree structure that is globally optimal, then assigns the most probable relations to the internal nodes.</p><p>In this paper, we implement a probabilistic CKY-like bottom-up algorithm for computing the most likely parse tree using dynamic program- ming as are adopted in ( <ref type="bibr" target="#b16">Joty et al., 2012;</ref><ref type="bibr" target="#b17">Joty et al., 2013;</ref><ref type="bibr" target="#b18">Jurafsky and Martin, 2000</ref>) for the search of global optimum. For a document with n EDUs, as different relations are characterized with different compositions (thus leading to dif- ferent vectors), we use a N r × n × n dynamic pro- gramming table P r, the cell P r[r, i, j] of which represents the span contained EDUs from i to j and stores the probability that relation r holds be- tween the two spans within i to j. P r[r, i, j] is computed as follows:</p><formula xml:id="formula_11">P r[r, i, j] =max r 1 ,r 2 ,k P r[r 1 , i, k] · P r[r 2 , k, j] ×P (t binary (e [i,k] , e [k,j] ) = 1) ×P (r(e [i,k] , e [k,j] ) = 1)</formula><p>(12) At each merging step, a distributed vector for the merged point is calculated according to Eq. 13 for different relations. The CKY-like algorithms finds the global optimal. To note, the worst-case run- ning time of our inference algorithm is O(N 2 r n 3 ), where n denotes the number of sentences within the document, which is much slower than the greedy search. In this work, for simplification, we simplify the framework by maintaining the top 10 options at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>A measure of the performance of the system is realized by comparing the structure and labeling of the RS-tree produced by our algorithm to gold- standard annotations.</p><p>Standard evaluation of discourse parsing output computes the ratio of the number of identical tree constituents shared in the generated RS-trees and the gold-standard trees against the total number of constituents in the generated discourse trees 2 , which is further divided to three matrices: Span (on the blank tree structure), nuclearity (on the tree structure with nuclearity indication), and rela- tion (on the tree structure with rhetorical relation indication but no nuclearity indication).</p><p>The nuclearity and relation decisions are made based on the multi-class output labels from the deep learning framework. As we do not consider nuclearity when classifying different discourse re- lations, the two labels attribute <ref type="bibr">[N]</ref>[S] and at- tribute <ref type="bibr">[S]</ref>[N] made by multi-class classifier will be treated as the same relation label ATTRIBUTE.   <ref type="table">Table 1</ref>: Performances for different approaches. Performances for baselines are reprinted from <ref type="bibr" target="#b17">(Joty et al., 2013;</ref><ref type="bibr" target="#b8">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b15">Ji and Eisenstein, 2014</ref>).</p><p>Also, we do not train a separate classifier for NU- CLEUS and SATELLITE identification. The nucle- arity decision is made based on the relation type produced by the multi-class classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Parameter Tuning</head><p>The regularization parameter Q constitutes the only parameter to tune in our framework. We tune it on the 347 training documents. Concretely, we employ a five-fold cross validation on the RST dataset and tune Q on 5 different values: 0.01, 0.1, 0.5, 1.5, 2.5. The final model was tested on the testing set after parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>We compare our model against the following currently prevailing discourse parsing baselines:</p><p>HILDA A discourse parser based on support vector machine classification introduced by <ref type="bibr" target="#b12">Hernault et al. (Hernault et al., 2010b</ref>). HILDA uses the binary and multi-class classifier to reconstruct the tree structure in a greedy way, where the most likely nodes are merged at each step. The results for HILDA are obtained by running the system with default settings on the same inputs we provided to our system. Feng and Hirst The linear-time discourse parser introduced in <ref type="bibr" target="#b8">(Feng and Hirst, 2014</ref>) which relies on two linear-chain CRFs to obtain a se- quence of discourse constituents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ji and Eisenstein</head><p>The shift-reduce discourse parser introduced in <ref type="bibr" target="#b15">(Ji and Eisenstein, 2014)</ref> which parses document by relying on the dis- tributed representations obtained from deep learn- ing framework. Additionally, we implemented a simplified ver- sion of our model called unified where we use a unified convolutional function with unified pa- rameters [W sen , b sen ] for span vector computation. Concretely, for a parent node p, given the dis- tributed representation h e i for left child, h e j for right child, and the relation r(e 1 , e 2 ), rather than taking the inter relation between two children, its distributed vector h p is calculated:</p><formula xml:id="formula_12">h p = f (W sen · [h e i , h e j ] + b sen )<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance</head><p>Performances for different models approaches re- ported in <ref type="table">Table 1</ref>. And as we can observe, al- though the proposed framework obtains compa- rable result compared with existing state-of-state performances regarding all evaluating parameters for discourse parsing. Specifically, as for the three measures, no system achieves top performance on all three, though some systems outperform all oth- ers for one of the measures. The proposed system achieves high overall performance on all three, al- though it does not achieve top score on any mea- sure. The system gets a little bit performance boost by considering text-level features illustrated in Section 5.6. The simplified version of the orig- inal model underperforms against the original ap- proach due to lack of expressive power in convo- lution. Performance plummets when different re- lations are uniformly treated, which illustrates the importance of taking into consideration different types of relations in the span convolution proce- dure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we describe an RST-style text-level discourse parser based on a neural network model. The incorporation of sentence-level distributed vectors for discourse analysis obtains compara- ble performance compared with current state-of- art discourse parsing system. Our future work will focus on extending discourse-level distributed presentations to related tasks, such as implicit discourse relation identifi- cation or dialogue analysis. Further, once the tree structure for a document can be determined, the vector for the entire document can be obtained in bottom-up fashion, as in this paper. One can now investigate whether the discourse parse tree is useful for acquiring a single document-level vector representation, which would benefit mul- tiple tasks, such as document classification or macro-sentiment analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RST Discourse Tree Structure.</figDesc><graphic url="image-1.png" coords="4,94.74,62.81,172.80,114.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System Overview.</figDesc><graphic url="image-2.png" coords="5,104.37,62.81,388.81,184.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Approach</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Joty et al The discourse parser introduced by Joty et al. (Joty et al., 2013). It relies on CRF and combines intra-sentential and multi-sentential parsers in two different ways. Joty et al. adopt the global optimal inference as in our work. We reported the performance from their paper (Joty et al., 2013).</figDesc></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/software/ lex-parser.shtml</note>

			<note place="foot" n="2"> Conventionally, evaluation matrices involve precision, recall and F-score in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors want to thank Vanessa Wei Feng and Shafiq Joty for helpful discussions regarding RST dataset. We also want to thank Richard Socher, Zhengyan He and Pradeep Dasigi for the clarifica-tion of deep learning techniques.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic head-driven parsing for discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6192</idno>
		<title level="m">Can recursive neural tensor networks learn logical reasoning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno>number EPFL- CONF-192374</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel discourse parser based on support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prendinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Textlevel discourse parsing with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A linear time bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The utility of parse-derived features for automatic discourse segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seeger</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">488</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="399" to="409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hilda: a discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving chinese word segmentation on microblog using rich punctuations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Houfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parsimonious or profligate: How many and which discourse structure relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discourse Processes</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel discriminative framework for sentence-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="904" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the association for computational linguistics (ACL)</title>
		<meeting>the 51st annual meeting of the association for computational linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<title level="m">Speech &amp; Language Processing</title>
		<imprint>
			<publisher>Pearson Education India</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating discourse structures for written texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huong</forename><surname>Lethanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geetha</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Huyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">329</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the penn discourse treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discourse indicators for content selection in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The rhetorical parsing of unrestricted texts: A surface-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="448" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The theory and practice of discourse parsing and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Baobao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LDV Forum</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="38" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sentence level discourse parsing using syntactic and lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discourse chunking and its application to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An effective discourse parser that uses rich linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="566" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating discoursebased answer extraction for why-question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter-Arno</forename><surname>Coppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="735" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Exploring representations from unlabeled data with co-training for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu Sun Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
