<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yau-Shian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Taiwan University</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Taiwan University</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4187" to="4195"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4187</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Auto-encoders compress input data into a latent-space representation and reconstruct the original data from the representation. This latent representation is not easily interpreted by humans. In this paper, we propose training an auto-encoder that encodes input text into human-readable sentences, and unpaired ab-stractive summarization is thereby achieved. The auto-encoder is composed of a generator and a reconstructor. The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the generator input from the generator output. To make the generator output human-readable, a discriminator restricts the output of the generator to resemble human-written sentences. By taking the generator output as the summary of the input text, abstractive summarization is achieved without document-summary pairs as training data. Promising results are shown on both En-glish and Chinese corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When it comes to learning data representations, a popular approach involves the auto-encoder ar- chitecture, which compresses the data into a la- tent representation without supervision. In this paper we focus on learning text representations. Because text is a sequence of words, to encode a sequence, a sequence-to-sequence (seq2seq) auto- encoder ( <ref type="bibr" target="#b7">Li et al., 2015;</ref><ref type="bibr" target="#b6">Kiros et al., 2015</ref>) is usu- ally used, in which a RNN is used to encode the input sequence into a fixed-length representation, after which another RNN is used to decode the original input sequence given this representation.</p><p>Although the latent representation learned by the seq2seq auto-encoder can be used in down- stream applications, it is usually not human- readable. A human-readable representation should comply the rule of human grammar and can be comprehended by human. Therefore, in this work, we use comprehensible natural language as a la- tent representation of the input source text in an auto-encoder architecture. This human-readable latent representation is shorter than the source text; in order to reconstruct the source text, it must re- flect the core idea of the source text. Intuitively, the latent representation can be considered a sum- mary of the text, so unpaired abstractive summa- rization is thereby achieved.</p><p>The idea that using human comprehensible lan- guage as a latent representation has been ex- plored on text summarization, but only in a semi- supervised scenario. Previous work <ref type="bibr" target="#b11">(Miao and Blunsom, 2016</ref>) uses a prior distribution from a pre-trained language model to constrain the gen- erated sequence to natural language. However, to teach the compressor network to generate text summaries, the model is trained using labeled data. In contrast, in this work we need no labeled data to learn the representation.</p><p>As shown in <ref type="figure">Fig. 1</ref>, the proposed model is com- posed of three components: a generator, a discrim- inator, and a reconstructor. Together, the generator and reconstructor form a text auto-encoder. The generator acts as an encoder in generating the la- tent representation from the input text. Instead of using a vector as latent representation, however, the generator generates a word sequence much shorter than the input text. From the shorter text, the reconstructor reconstructs the original input of the generator. By minimizing the reconstruc- tion loss, the generator learns to generate short text segments that contain the main information in the original input. We use the seq2seq model in modeling the generator and reconstructor because both have input and output sequences with differ- ent lengths. However, it is very possible that the gener- ator's output word sequence can only be pro- cessed and recognized by the reconstructor but is not readable by humans. Here, instead of reg- ularizing the generator output with a pre-trained language model <ref type="bibr" target="#b11">(Miao and Blunsom, 2016)</ref>, we borrow from adversarial auto-encoders ( <ref type="bibr" target="#b10">Makhzani et al., 2015</ref>) and cycle GAN ( <ref type="bibr" target="#b23">Zhu et al., 2017)</ref> and introduce a third component -the discrimina- tor -to regularize the generator's output word se- quence. The discriminator and the generator form a generative adversarial network (GAN) <ref type="bibr" target="#b4">(Goodfellow et al., 2014</ref>). The discriminator discrim- inates between the generator output and human- written sentences, and the generator produces out- put as similar as possible to human-written sen- tences to confuse the discriminator. With the GAN framework, the discriminator teaches the genera- tor how to create human-like summary sentences as a latent representation. However, due to the non-differential property of discrete distributions, generating discrete distributions by GAN is chal- lenging. To tackle this problem, in this work, we proposed a new kind of method on language gen- eration by GAN.</p><p>By achieving unpaired abstractive text summa- rization, machine is able to unsupervisedly extract the core idea of the documents. This approach has many potential applications. For example, the output of the generator can be used for the down- stream tasks like document classification and sen- timent classification. In this study, we evaluate the results on an abstractive text summarization task. The output word sequence of the generator is re- garded as the summaries of the input text. The model is learned from a set of documents with- out summaries. As most documents are not paired with summaries, for example the movie reviews or lecture recordings, this technique makes it possi- ble to learn summarizer to generate summaries for these documents. The results show that the gener- ator generates summaries with reasonable quality on both English and Chinese corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Abstractive Text Summarization Recent model architectures for abstractive text summarization basically use the sequence-to- sequence ( <ref type="bibr" target="#b18">Sutskever et al., 2014</ref>) framework in combination with various novel mechanisms. One popular mechanism is attention ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>, which has been shown helpful for summa- rization ( <ref type="bibr" target="#b12">Nallapati et al., 2016;</ref><ref type="bibr" target="#b16">Rush et al., 2015;</ref>. It is also possible to directly optimize evaluation metrics such as ROUGE (Lin, <ref type="figure">Figure 1</ref>: Proposed model. Given long text, the generator produces a shorter text as a summary. The generator is learned by minimizing the recon- struction loss together with the reconstructor and making discriminator regard its output as human- written text. 2004) with reinforcement learning ( <ref type="bibr" target="#b14">Ranzato et al., 2016;</ref><ref type="bibr" target="#b13">Paulus et al., 2017;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2016</ref>). The hybrid pointer-generator network ( <ref type="bibr" target="#b17">See et al., 2017</ref>) selects words from the original text with a pointer ( <ref type="bibr" target="#b20">Vinyals et al., 2015)</ref> or from the whole vocabulary with a trained weight. In order to elim- inate repetition, a coverage vector ( <ref type="bibr" target="#b19">Tu et al., 2016)</ref> can be used to keep track of attended words, and coverage loss ( <ref type="bibr" target="#b17">See et al., 2017)</ref> can be used to encourage model focus on diverse words. While most papers focus on supervised learning with novel mechanisms, in this paper, we explore un- supervised training models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN for Language Generation</head><p>In this paper, we borrow the idea of GAN to make the generator output human-readable. The major challenge in applying GAN to sentence genera- tion is the discrete nature of natural language. To generate a word sequence, the generator usually has non-differential parts such as argmax or other sample functions which cause the original GAN to fail.</p><p>In <ref type="bibr" target="#b5">(Gulrajani et al., 2017)</ref>, instead of feeding a discrete word sequence, the authors directly feed the generator output layer to the discriminator. This method works because they use the earth mover's distance on GAN as proposed in , which is able to evaluate the distance between a discrete and a continuous dis- tribution. <ref type="bibr">SeqGAN (Yu et al., 2017</ref>) tackles the sequence generation problem with reinforcement learning. Here, we refer to this approach as ad- versarial REINFORCE. However, the discrimina- tor only measures the quality of whole sequence, and thus the rewards are extremely sparse and the rewards assigned to all the generation steps are all the same. MC search ( <ref type="bibr" target="#b21">Yu et al., 2017</ref>) is proposed to evaluate the approximate reward at each time step, but this method suffers from high time com- plexity. Following this idea, ( <ref type="bibr" target="#b8">Li et al., 2017</ref>) pro- poses partial evaluation approach to evaluate the expected reward at each time step. In this pa- per, we propose the self-critical adversarial RE- INFORCE algorithm as another way to evaluate the expected reward at each time step. The per- formance between original WGAN and proposed adversarial REINFORCE is compared in experi- ment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>The overview of the proposed model is shown in <ref type="figure">Fig. 2</ref>. The model is composed of three com- ponents: generator G, discriminator D, and re- constructor R. Both G and R are seq2seq hy- brid pointer-generator networks ( <ref type="bibr" target="#b17">See et al., 2017)</ref> which can decide to copy words from encoder in- put text via pointing or generate from vocabulary. They both take a word sequence as input and out- put a sequence of word distributions. Discrimina- tor D, on the other hand, takes a sequence as input and outputs a scalar. The model is learned from a set of documents x and human-written sentences y real .</p><p>To train the model, a training document x = {x 1 , x 2 , ..., x t , ..., x T }, where x t rep- resents a word, is fed to G, which outputs a sequence of word distributions G(x) = {y 1 , y 2 , ..., y n , ..., y N }, where y n is a distribution over all words in the lexicon. Then we randomly sample a word y s n from each distribution y n , and a word sequence y s = {y s 1 , y s 2 , ..., y s N } is obtained according to <ref type="bibr">G(x)</ref>. We feed the sampled word se- quence y s to reconstructor R, which outputs an- other sequence of word distributionsˆxdistributionsˆ distributionsˆx. The re- constructor R reconstructs the original text x from y s . That is, we seek an output of reconstructorˆx reconstructorˆ reconstructorˆx that is as close to the original text x as possible; hence the loss for training the reconstructor, R loss , is defined as</p><formula xml:id="formula_0">R loss = K � k=1 l s (x, ˆ x),<label>(1)</label></formula><p>where the reconstruction loss l s (x, ˆ x) is the cross- entropy loss computed between the reconstructor output sequencê</p><p>x and the source text x, or the negative conditional log-likelihood of source text x given word sequence y s sampled from G(x). The reconstructor output sequencê x is teacher- forced by source text</p><formula xml:id="formula_1">x. The subscript s in l s (x, ˆ x)</formula><p>indicates thatˆxthatˆ thatˆx is reconstructed from y s . K is the number of training documents, and <ref type="formula" target="#formula_0">(1)</ref> is the sum- mation of the cross-entropy loss over all the train- ing documents x. In the proposed model, the generator G and re- constructor R form an auto-encoder. However, the reconstructor R does not directly take the genera- tor output distribution G(x) as input <ref type="bibr">1</ref> . Instead, the reconstructor takes a sampled discrete sequence y s as input. Due to the non-differentiable property of discrete sequences, we apply the REINFORCE al- gorithm, which is described in Section 4.</p><p>In addition to reconstruction, we need the dis- criminator D to discriminate between the real se- quence y real and the generated sequence y s to reg- ularize the generated sequence satisfying the sum- mary distribution. D learns to give y real higher scores while giving y s lower scores. The loss for training the discriminator D is denoted as D loss ; this is further described in Section 5.</p><p>G learns to minimize the reconstruction loss R loss , while maximizing the loss of the discrimi- nator D by generating a summary sequence y s that cannot be differentiated by D from the real thing. The loss for the generator G loss is</p><formula xml:id="formula_2">G loss = αR loss − D � loss (2)</formula><p>where D � loss is highly related to D loss -but not necessary the same 2 -and α is a hyper-parameter. After obtaining the optimal generator by minimiz- ing (2), we use it to generate summaries.</p><p>Generator G and discriminator D together form a GAN. We use two different adversarial training methods to train D and G; as shown in <ref type="figure">Fig. 2</ref>, these two methods have their own discriminators 1 and 2. Discriminator 1 takes the generator out- put layer G(x) as input, whereas discriminator 2 takes the sampled discrete word sequence y s as input. The two methods are described respectively in Sections 5.1 and 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Minimizing Reconstruction Loss</head><p>Because discrete sequences are non-differentiable, we use the REINFORCE algorithm. The gener- ator is seen as an agent whose reward given the source text x is −l s (x, ˆ x). Maximizing the re- ward is equivalent to minimizing the reconstruc- tion loss R loss in (1). However, the reconstruction <ref type="figure">Figure 2</ref>: Architecture of proposed model. The generator network and reconstructor network are a seq2seq hybrid pointer-generator network, but for simplicity, we omit the pointer and the attention parts. loss varies widely from sample to sample, and thus the rewards to the generator are not stable either. Hence we add a baseline to reduce their difference. We apply self-critical sequence training ( <ref type="bibr" target="#b15">Rennie et al., 2017)</ref>; the modified reward r R (x, ˆ x) from reconstructor R with the baseline for the genera- tor is</p><formula xml:id="formula_3">r R (x, ˆ x) = −l s (x, ˆ x) − (−l a (x, ˆ x) − b) (3)</formula><p>where</p><formula xml:id="formula_4">−l a (x, ˆ x) − b is the baseline. l a (x, ˆ x)</formula><p>is also the same cross-entropy reconstruction loss as l s (x, ˆ x), except thatˆxthatˆ thatˆx is obtained from y a instead of y s .</p><p>y a is a word sequence {y a 1 , y a 2 , ..., y a n , ..., y a N }, where y a n is selected using the argmax function from the output distribution of generator y n . As in the early training stage, the sequence y s barely yields higher reward than sequence y a , to encourage exploration we intro- duce the second baseline score b, which gradu- ally decreases to zero. Then, the generator is up- dated using the REINFORCE algorithm with re- ward r R (x, ˆ x) to minimize R loss .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GAN Training</head><p>With adversarial training, the generator learns to produce sentences as similar to the human-written sentences as possible. Here, we conduct experi- ments on two kinds of methods of language gen- eration with GAN. In Section 5.1 we directly feed the generator output probability distributions to the discriminator and use a Wasserstein GAN (WGAN) with a gradient penalty. In Section 5.2, we explore adversarial REINFORCE, which feeds sampled discrete word sequences to the discrim- inator and evaluates the quality of the sequence from the discriminator for use as a reward signal to the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Method 1: Wasserstein GAN</head><p>In the lower left of <ref type="figure">Fig. 2</ref> </p><formula xml:id="formula_5">D loss = 1 K K � k=1 D 1 (G(x (k) )) − 1 K K � k=1 D 1 (y real(k) ) +β 1 1 K K � k=1 (Δ y i(k) D 1 (y i(k) ) − 1) 2 ,</formula><p>where K denotes the number of training exam- ples in a batch, and k denotes the k-th exam- ple. The last term is the gradient penalty <ref type="bibr" target="#b5">(Gulrajani et al., 2017)</ref>. We interpolate the genera- tor output layer G(x) and the real sample y real , and apply the gradient penalty to the interpolated sequence y i . β 1 determines the gradient penalty scale. In Equation <ref type="formula">(2)</ref>, for WGAN, the generator maximizes D � loss :</p><formula xml:id="formula_6">D � loss = 1 K K � k=1 D 1 (G(x (k) )).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Method 2: Self-Critic Adversarial REINFORCE</head><p>In this section, we describe in detail the pro- posed adversarial REINFORCE method. The core idea is we use the LSTM discriminator to evalu- ate the current quality of the generated sequence {y s 1 , y s 2 , ..., y s i } at each time step i. The generator knows that compared to the last time step, as the generated sentence either improves or worsens, it can easily find the problematic generation step in a long sequence, and thus fix the problem easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Discriminator 2</head><p>As shown in <ref type="figure">Fig. 2</ref>, the discriminator2 D 2 is a unidirectional LSTM network which takes a dis- crete word sequence as input. At time step i, given input word y s i it predicts the current score s i based on the sequence {y 1 , y 2 , ..., y i }. The score is viewed as the quality of the current sequence. An example of discriminator regularized by weight clipping( ) is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. In order to compute the discriminator loss D loss , we sum the scores {s 1 , s 2 , ..., s N } of the whole sequence y s to yield</p><formula xml:id="formula_7">D 2 (y s ) = 1 N N � n=1 s n .</formula><p>where N denotes the generated sequence length. Then, the loss of discriminator is</p><formula xml:id="formula_8">D loss = 1 K K � k=1 D 2 (y s(k) ) − 1 K K � k=1 D 2 (y real(k) ) +β 2 1 K K � k=1 (Δ y i(k) D 2 (y i(k) ) − 1) 2 ,</formula><p>Similar to previous section, the last term is gra- dient penalty term. With the loss mentioned above, the discriminator attempts to quickly deter- mine whether the current sequence is real or fake. The earlier the timestep discriminator determines whether the current sequence is real or fake, the lower its loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Self-Critical Generator</head><p>Since we feed a discrete sequence y s to the dis- criminator, the gradient from the discriminator cannot directly back-propagate to the generator. Here, we use the policy gradient method. At timestep i, we use the i − 1 timestep score s i−1 from the discriminator as its self-critical baseline. The reward r D i evaluates whether the quality of se- quence in timestep i is better or worse than that in timestep i − 1. The generator reward r D i from D 2 is</p><formula xml:id="formula_9">r D i = � s i if i = 1 s i − s i−1 otherwise.</formula><p>However, some sentences may be judged as bad sentences at the previous timestep, but at later timesteps judged as good sentences, and vice versa. Hence we use the discounted expected re- ward d with discount factor γ to calculate the dis- counted reward d i at time step i as</p><formula xml:id="formula_10">d i = N � j=i γ j−i r D j .</formula><p>To maximize the expected discounted reward d i , the loss of generator is:</p><formula xml:id="formula_11">G � loss = −E y s i ∼p G (y s i |y s 1 ,...,y s i−1 ,x) [d i ].</formula><p>(5) We use the likelihood ratio trick to approximate the gradient to minimize (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>Our model was evaluated on the English/Chinese Gigaword datasets and CNN/Daily Mail dataset. In Section 6.1,6.2 and 6.4, the experiments were conducted on English Gigaword, while the experi- ments were conducted on CNN/Daily Mail dataset and Chinese Gigaword dataset respectively in Sec- tions 6.3 and 6.6. We used ROUGE <ref type="bibr" target="#b9">(Lin, 2004</ref>) as our evaluation metric. <ref type="bibr">3</ref> During testing, when us- ing the generator to generate summaries, we used beam search with beam size=5, and we eliminated repetition. We provide the details of the imple- mentation and corpus re-processing respectively in Appendix A and B.</p><p>Before jointly training the whole model, we pre-trained the three major components -gener- ator, discriminator, and reconstructor -separately. First, we pre-trained the generator in an unsuper- vised manner so that the generator would be able to somewhat grasp the semantic meaning of the source text. The details of the pre-training are in Appendix C. We pre-trained the discriminator and reconstructor respectively with the pre-trained generator's output to ensure that these two critic networks provide good feedback to the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Labeled  In part (A), the model was trained supervisedly. In row (B-1), we select the article's first eight words as its summary. Part (C) are the results obtained without paired data. In part (D), we trained our model with few labeled data. In part (E), we pre-trained generator on CNN/Diary and used the summaries from CNN/Diary as real data for the discriminator.</p><formula xml:id="formula_12">Methods R-1 R-2 R-L (A)Supervised</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">English Gigaword</head><p>The English Gigaword is a sentence summariza- tion dataset which contains the first sentence of each article and its corresponding headlines. The preprocessed corpus contains 3.8M training pairs and 400K validation pairs. We trained our model on part of or fully unparalleled data on 3.8M train- ing set. To have fair comparison with previous works, the following experiments were evaluated on the 2K testing set same as ( <ref type="bibr" target="#b16">Rush et al., 2015;</ref><ref type="bibr" target="#b11">Miao and Blunsom, 2016)</ref>. We used the sentences in article headlines as real data for discriminator 4 . As shown in the following experiments, the head- lines can even come from another set of docu- ments not related to the training documents. The results on English Gigaword are shown in <ref type="table" target="#tab_2">Table 1</ref>. WGAN and adversarial REINFORCE refer to the adversarial training methods men- tioned in Sections 5.1 and 5.2 respectively. Re- sults trained by full labeled data are in part (A). In row (A-1), We trained our generator by su-pervised training. Compared with the previous work ( <ref type="bibr" target="#b22">Zhou et al., 2017)</ref>, we used simpler model and smaller vocabulary size. We did not try to achieve the state-of-the-art results because the fo- cus of this work is unsupervised learning, and the proposed approach is independent to the summa- rization models used. In row (B-1), we simply took the first eight words in a document as its sum- mary.</p><p>The results for the pre-trained generator with method mentioned in Appendix.C is shown in row (C-1). In part (C), we directly took the sentences in the summaries of Gigaword as the training data of discriminator. Compared with the pre-trained generator and the trivial baseline , the proposed approach (rows (C-2) and (C-3)) showed good im- provement. <ref type="figure" target="#fig_1">In Fig. 4</ref>, we provide a real example. More examples can be found in the Appendix.D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Semi-Supervised Learning</head><p>In semi-supervised training, generator was pre- trained with few available labeled data. During training, we conducted teacher-forcing with la- beled data on generator after several updates with- out labeled data. With 10K, 500K and 1M la-beled data, the teacher-forcing was conducted ev- ery 25, 5 and 3 updates without paired data, re- spectively. In teacher-forcing, given source text as input, the generator was teacher-forced to pre- dict the human-written summary of source text. Teacher-forcing can be regarded as regularization of unpaired training that prevents generator from producing unreasonable summaries of source text. We found that if we teacher-forced generator too frequently, generator would overfit on training data since we only used very few labeled data on semi-supervised training.</p><p>The performance of semi-supervised model in English Gigaword regarding available labeled data is shown in <ref type="table" target="#tab_2">Table 1</ref> part (D). We compared our results with <ref type="bibr" target="#b11">(Miao and Blunsom, 2016)</ref> which was the previous state-of-the-art method on semi- supervised summarization task under the same amount of labeled data. With both 500K and 1M labeled data, our method performed better. Fur- thermore, with only 1M labeled data, using ad- versarial REINFORCE even outperformed super- vised training in <ref type="table" target="#tab_2">Table 1</ref> (A-1) with the whole 3.8M labeled data.  <ref type="table" target="#tab_2">Table 1</ref>. The proposed methods generated sum- maries that grasped the core idea of the articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">CNN/Daily Mail dataset</head><p>The CNN/Daily Mail dataset is a long text sum- marization dataset which is composed of news ar- ticles paired with summaries. We evaluated our model on this dataset because it's a popular bench- mark dataset, and we want to know whether the proposed model works on long input and long output sequences. The details of corpus pre- processing can be found in Appendix.B . In un- paired training, to prevent the model from directly matching the input articles to its corresponding summaries, we split the training pairs into two equal sets, one set only supplied articles and the other set only supplied summaries.</p><p>The results are shown in <ref type="table" target="#tab_4">Table 2</ref>. For super- vised approaches in part (A), although our seq2seq model was similar to <ref type="bibr" target="#b17">(See et al., 2017)</ref>, due to the smaller vocabulary size (we didn't tackle out- of-vocabulary words), simpler model architecture, shorter output length of generated summaries, there was a performance gap between our model and the scores reported in ( <ref type="bibr" target="#b17">See et al., 2017)</ref>. Com- pared to the lead-3 baseline in part (B) which took the first three sentences of articles as summaries, the seq2seq models fell behind. That was be- cause news writers often put the most important information in the first few sentences, and thus even the best abstractive summarization model only slightly beat the lead-3 baseline on ROUGE scores. However, during pre-training or training we didn't make assumption that the most impor- tant sentences are in first few sentences.</p><p>We observed that our unpaired model yielded decent ROUGE-1 score, but it yielded lower ROUGE-2 and ROUGE-L score. That was proba- bly because the length of our generated sequence was shorter than ground truth, and our vocabu- lary size was small. Another reason was that the generator was good at selecting the most impor- tant words from the articles, but sometimes failed to combine them into reasonable sentences be- cause it's still difficult for GAN to generate long sequence. In addition, since the reconstructor only evaluated the reconstruction loss of whole se- quence, as the generated sequence became long, the reconstruction reward for generator became extremely sparse. However, compared to pre- trained generator (rows (C-2), (C-3) v.s. (C-1)), our model still enhanced the ROUGE score. An real example of generated summary can be found at Appendix.D <ref type="figure">Fig.11</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Transfer Learning</head><p>The experiments conducted up to this point re- quired headlines unpaired to the documents but in the same domain to train discriminator. In this subsection, we generated the summaries from English Gigaword (target domain), but the sum- maries for discriminator were from CNN/Daily Mail dataset (source domain).</p><p>The results of transfer learning are shown in Ta- ble. 1 part (E).   trained generator and the poor pre-training result indicates that the data distributions of two datasets are quite different. We find that using sentences from another dataset yields lower ROUGE scores on the target testing set (parts (E) v.s. (C)) due to the mismatch word distributions between the sum- maries of the source and target domains. How- ever, the discriminator still regularizes the gener- ated word sequence. After unpaired training, the model enhanced the ROUGE scores of the pre- trained model (rows (E-2), (E-3) v.s. (E-1)) and it also surpassed the trivial baselines in part (B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">GAN Training</head><p>In this section, we discuss the performance of two GAN training methods. As shown in the <ref type="table" target="#tab_2">Table 1</ref>, in English Gigaword, our proposed ad- versarial REINFORCE method performed better than WGAN. However, in <ref type="table" target="#tab_4">Table 2</ref>, our proposed method slightly outperformed by WGAN. In addi- tion, we find that when training with WGAN, con- vergence is faster. Because WGAN directly eval- uates the distance between the continuous distri- bution from generator and the discrete distribution from real data, the distribution was sharpened at an early stage in training. This caused generator to converge to a relatively poor place. On the other hand, when training with REINFORCE, genera- tor keeps seeking the network parameters that can better fool discriminator. We believe that training GAN on language generation with this method is worth exploring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Chinese Gigaword</head><p>The Chinese Gigaword is a long text summariza- tion dataset composed of paired headlines and news. Unlike the input news in English Gigaword, the news in Chinese Gigaword consists of sev- eral sentences. The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Row (A) lists the results using 1.1M document- summary pairs to directly train the generator with- out the reconstructor and discriminator: this is the upper bound of the proposed approach. In row (B), we simply took the first fifteen words in a docu- ment as its summary. The number of words was chosen to optimize the evaluation metrics. Part (C) are the results obtained in the scenario with- out paired data. The discriminator took the sum- maries in the training set as real data. We show the results of the pre-trained generator in row (C- 1); rows (C-2) and (C-3) are the results for the two GAN training methods respectively. We find that despite the performance gap between the un- paired and supervised methods (rows (C-2), (C- 3) v.s. (A)), the proposed method yielded much better performance than the trivial baselines (rows (C-2), (C-3) v.s. (B)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>Using GAN, we propose a model that encodes text as a human-readable summary, learned with- out document-summary pairs. In future work, we hope to use extra discriminators to control the style and sentiment of the generated summaries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: When the second arrested appears, as the sentence becomes ungrammatical, the discriminator determines that this example comes from the generator. Hence, after this time-step, it outputs low scores.</figDesc><graphic url="image-3.png" coords="5,82.91,269.43,196.44,65.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Real examples with methods referred in Table 1. The proposed methods generated summaries that grasped the core idea of the articles.</figDesc><graphic url="image-4.png" coords="7,72.00,394.99,218.27,170.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Average F1 ROUGE scores on English Gigaword. R-1, R-2 and R-L refers to ROUGE 1, 
ROUGE 2 and ROUGE L respectively. Results marked with ✝ are obtained from corresponding papers. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 (E-1) is the result of pre-</head><label>1</label><figDesc></figDesc><table>Methods 
R-1 
R-2 
R-L 

(A)Supervised 
(A-1)Supervised training on our generator 38.89 13.74 29.42 
(A-2) (See et al., 2017)✝ 
39.53 17.28 36.38 
(B)Lead-3 baseline (See et al., 2017)✝ 
40.34 17.70 36.57 

(C) Unpaired 

(C-1) Pre-trained generator 
29.86 5.14 14.66 
(C-2) WGAN 
35.14 9.43 21.04 
(C-3) Adversarial REINFORCE 
35.51 9.38 20.98 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F1 ROUGE scores on CNN/Diary Mail dataset. In row (B), the first three sentences were taken as 
summaries. Part (C) are the results obtained without paired data. The results with symbol ✝ are directly 
obtained from corresponding papers. 

Methods 
R-1 
R-2 
R-L 
(A) Training with paired data (supervised) 
49.62 34.10 46.42 
(B)Lead-15 baseline 
30.08 18.24 27.74 

(C) Unpaired 

(C-1) Pre-trained generator 
28.36 16.73 26.48 
(C-2) WGAN 
38.15 24.60 35.27 
(C-3) Adversarial REINFORCE 41.25 26.54 37.76 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>F1 ROUGE scores on Chinese Gigaword. In row (B), we selected the article's first fifteen words as its summary. Part (C) are the results obtained without paired data.</figDesc><table></table></figure>

			<note place="foot" n="1"> We found that if the reconstructor R directly takes G(x) as input, the generator G learns to put the information about the input text in the distribution of G(x), making it difficult to sample meaningful sentences from G(x). 2 D � loss has different formulations in different approaches. This will be clear in Sections 5.1 and 5.2.</note>

			<note place="foot" n="3"> We used pyrouge package with option-m-n 2-w 1.2 to compute ROUGE score for all experiments.</note>

			<note place="foot" n="4"> Instead of using general sentences as real data for discriminator, we chose sentences from headlines because they have their own unique distribution.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>HLT-NAAC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vincent Dumoulin, and Aaron Courville</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
	</analytic>
	<monogr>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sbastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06547</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: ACL workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-tosequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
	<note>Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>NIPS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Pointer networks. NIPS</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
