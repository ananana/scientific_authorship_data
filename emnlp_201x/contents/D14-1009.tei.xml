<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Strongly Incremental Repair Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
							<email>julian.hough@uni-bielefeld.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Dialogue Systems Group Faculty of Linguistics and Literature Bielefeld University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Cognitive Science Research Group School of Electronic Engineering and Computer Science Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
							<email>m.purver@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Cognitive Science Research Group School of Electronic Engineering and Computer Science Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Strongly Incremental Repair Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="78" to="89"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present STIR (STrongly Incremen-tal Repair detection), a system that detects speech repairs and edit terms on transcripts incrementally with minimal la-tency. STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs. Results on the Switchboard dis-fluency tagged corpus show utterance-final accuracy on a par with state-of-the-art in-cremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead. We evaluate its performance using incremental metrics and propose new repair processing evaluation standards.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-repairs in spontaneous speech are annotated according to a well established three-phase struc- ture from <ref type="bibr" target="#b29">(Shriberg, 1994)</ref> onwards, and as de- scribed in <ref type="bibr" target="#b20">Meteer et al. (1995)</ref> From a dialogue systems perspective, detecting re- pairs and assigning them the appropriate structure is vital for robust natural language understanding (NLU) in interactive systems. Downgrading the commitment of reparandum phases and assigning appropriate interregnum and repair phases permits computation of the user's intended meaning.</p><p>Furthermore, the recent focus on incremental dialogue systems (see e.g. ( <ref type="bibr" target="#b25">Rieser and Schlangen, 2011)</ref>) means that repair detection should oper- ate without unnecessary processing overhead, and function efficiently within an incremental frame- work. However, such left-to-right operability on its own is not sufficient: in line with the princi- ple of strong incremental interpretation <ref type="bibr" target="#b22">(Milward, 1991)</ref>, a repair detector should give the best re- sults possible as early as possible. With one ex- ception ( <ref type="bibr" target="#b31">Zwarts et al., 2010)</ref>, there has been no focus on evaluating or improving the incremental performance of repair detection.</p><p>In this paper we present STIR (Strongly In- cremental Repair detection), a system which ad- dresses the challenges of incremental accuracy, computational complexity and latency in self- repair detection, by making local decisions based on relatively simple measures of fluency and sim- ilarity. Section 2 reviews state-of-the-art methods; Section 3 summarizes the challenges and explains our general approach; Section 4 explains STIR in detail; Section 5 explains our experimental set-up and novel evaluation metrics; Section 6 presents and discusses our results and Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous work</head><p>Qian and <ref type="bibr" target="#b23">Liu (2013)</ref> achieve the state of the art in Switchboard corpus self-repair detection, with an F-score for detecting reparandum words of 0.841 using a three-step weighted Max-Margin Markov network approach. Similarly, <ref type="bibr" target="#b6">Georgila (2009)</ref> uses Integer Linear Programming post-processing of a CRF to achieve F-scores over 0.8 for reparan- dum start and repair start detection. However nei- ther approach can operate incrementally.</p><p>Recently, there has been increased interest in left-to-right repair detection: <ref type="bibr" target="#b24">Rasooli and Tetreault (2014)</ref> and <ref type="bibr" target="#b12">Honnibal and Johnson (2014)</ref> present dependency parsing systems with reparan- dum detection which perform similarly, the latter equalling Qian and Liu (2013)'s F-score at 0.841. However, while operating left-to-right, these sys- tems are not designed or evaluated for their incre- mental performance. The use of beam search over different repair hypotheses in <ref type="bibr" target="#b12">(Honnibal and Johnson, 2014</ref>) is likely to lead to unstable repair label sequences, and they report repair hypothesis 'jit- ter'. Both of these systems use a non-monotonic dependency parsing approach that immediately re- moves the reparandum from the linguistic anal- ysis of the utterance in terms of its dependency structure and repair-reparandum correspondence, which from a downstream NLU module's perspec- tive is undesirable. <ref type="bibr" target="#b11">Heeman and Allen (1999)</ref> and <ref type="bibr" target="#b21">Miller and Schuler (2008)</ref> present earlier left-to- right operational detectors which are less accu- rate and again give no indication of the incremen- tal performance of their systems. While Heeman and Allen (1999) rely on repair structure template detection coupled with a multi-knowledge-source language model, the rarity of the tail of repair structures is likely to be the reason for lower per- formance: <ref type="bibr" target="#b13">Hough and Purver (2013)</ref> show that only 39% of repair alignment structures appear at least twice in Switchboard, supported by the 29% reported by Heeman and Allen (1999) on the smaller TRAINS corpus. <ref type="bibr" target="#b21">Miller and Schuler (2008)</ref>'s encoding of repairs into a grammar also causes sparsity in training: repair is a general pro- cessing strategy not restricted to certain lexical items or POS tag sequences.</p><p>The model we consider most suitable for in- cremental dialogue systems so far is <ref type="bibr" target="#b31">Zwarts et al. (2010)</ref>'s incremental version of <ref type="bibr" target="#b15">Johnson and Charniak (2004)</ref>'s noisy channel repair detector, as it incrementally applies structural repair anal- yses (rather than just identifying reparanda) and is evaluated for its incremental properties. Fol- lowing ( <ref type="bibr" target="#b15">Johnson and Charniak, 2004</ref>), their sys- tem uses an n-gram language model trained on roughly 100K utterances of reparandum-excised ('cleaned') Switchboard data. Its channel model is a statistically-trained S-TAG parser whose gram- mar has simple reparandum-repair alignment rule categories for its non-terminals (copy, delete, in- sert, substitute) and words for its terminals. The parser hypothesises all possible repair structures for the string consumed so far in a chart, before pruning the unlikely ones. It performs equally well to the non-incremental model by the end of each utterance (F-score = 0.778), and can make detections early via the addition of a speculative next-word repair completion category to their S- TAG non-terminals. In terms of incremental per- formance, they report the novel evaluation met- ric of time-to-detection for correctly identified re- pairs, achieving an average of 7.5 words from the start of the reparandum and 4.6 from the start of the repair phase. They also introduce delayed ac- curacy, a word-by-word evaluation against gold- standard disfluency tags up to the word before the current word being consumed (in their terms, the prefix boundary), giving a measure of the stability of the repair hypotheses. They report an F-score of 0.578 at one word back from the current prefix boundary, increasing word-by-word until 6 words back where it reaches 0.770. These results are the point-of-departure for our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Challenges and Approach</head><p>In this section we summarize the challenges for incremental repair detection: computational com- plexity, repair hypothesis stability, latency of de- tection and repair structure identification. In 3.1 we explain how we address these.</p><p>Computational complexity Approaches to de- tecting repair structures often use chart storage ( <ref type="bibr" target="#b31">Zwarts et al., 2010;</ref><ref type="bibr" target="#b15">Johnson and Charniak, 2004;</ref><ref type="bibr" target="#b11">Heeman and Allen, 1999</ref>), which poses a com- putational overhead: if considering all possible boundary points for a repair structure's 3 phases beginning on any word, for prefixes of length n the number of hypotheses can grow in the order O(n 4 ). Exploring a subset of this space is nec- essary for assigning entire repair structures as in (1) above, rather than just detecting reparanda: the ( <ref type="bibr" target="#b15">Johnson and Charniak, 2004;</ref><ref type="bibr" target="#b31">Zwarts et al., 2010)</ref> noisy-channel detector is the only system that applies such structures but the potential run- time complexity in decoding these with their S- TAG repair parser is O(n 5 ). In their approach, complexity is mitigated by imposing a maximum repair length (12 words), and also by using beam search with re-ranking ( <ref type="bibr" target="#b18">Lease et al., 2006;</ref><ref type="bibr" target="#b30">Zwarts and Johnson, 2011</ref>). If we wish to include full decoding of the repair's structure (as argued by <ref type="bibr" target="#b13">Hough and Purver (2013)</ref> as necessary for full in- terpretation) whilst taking a strictly incremental and time-critical perspective, reducing this com- plexity by minimizing the size of this search space is crucial.</p><p>Stability of repair hypotheses and latency Us- ing a beam search of n-best hypotheses on a word- by-word basis can cause 'jitter' in the detector's output. While utterance-final accuracy is desired, for a truly incremental system good intermedi- ate results are equally important. <ref type="bibr" target="#b31">Zwarts et al. (2010)</ref>'s time-to-detection results show their sys- tem is only certain about a detection after process- ing the entire repair. This may be due to the string alignment-inspired S-TAG that matches repair and reparanda: a 'rough copy' dependency only be- comes likely once the entire repair has been con- sumed. The latency of 4.6 words to detection and a relatively slow rise to utterance-final accuracy up to 6 words back is undesirable given repairs have a mean reparandum length of ≈1.5 words ( <ref type="bibr" target="#b13">Hough and Purver, 2013;</ref><ref type="bibr" target="#b28">Shriberg and Stolcke, 1998</ref>).</p><p>Structural identification Classifying repairs has been ignored in repair processing, despite the presence of distinct categories (e.g. repeats, sub- stitutions, deletes) with different pragmatic effects <ref type="bibr" target="#b13">(Hough and Purver, 2013)</ref>. 1 This is perhaps due to lack of clarity in definition: even for human anno- tators, verbatim repeats withstanding, agreement is often poor <ref type="bibr" target="#b13">(Hough and Purver, 2013;</ref><ref type="bibr" target="#b29">Shriberg, 1994)</ref>. Assigning and evaluating repair (not just reparandum) structures will allow repair interpre- tation in future; however, work to date evaluates only reparandum detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our approach</head><p>To address the above, we propose an alternative to ( <ref type="bibr" target="#b15">Johnson and Charniak, 2004;</ref><ref type="bibr" target="#b31">Zwarts et al., 2010)</ref>'s noisy channel model. While the model elegantly captures intuitions about parallelism in repairs and modelling fluency, it relies on string- matching, motivated in a similar way to automatic spelling correction <ref type="bibr" target="#b3">(Brill and Moore, 2000</ref>): it as- sumes a speaker chooses to utter fluent utterance X according to some prior distribution P (X), but a noisy channel causes them instead to utter a noisy Y according to channel model P (Y | X). Estimating P (Y |X) directly from observed data is difficult due to sparsity of repair instances, so a transducer is trained on the rough copy alignments between reparandum and repair. This approach succeeds because repetition and simple substitu- tion repairs are very common; but repair as a psy- chological process is not driven by string align- ment, and deletes, restarts and rarer substitution forms are not captured. Furthermore, the noisy channel model assumes an inherently utterance- global process for generating (and therefore find-ing) an underlying 'clean' string -much as sim- ilar spelling correction models are word-global - we instead take a very local perspective here.</p><p>In accordance with psycholinguistic evidence <ref type="bibr" target="#b2">(Brennan and Schober, 2001</ref>), we assume charac- teristics of the repair onset allow hearers to detect it very quickly and solve the continuation prob- lem <ref type="bibr" target="#b19">(Levelt, 1983)</ref> of integrating the repair into their linguistic context immediately, before pro- cessing or even hearing the end of the repair phase. While repair onsets may take the form of inter- regna, this is not a reliable signal, occurring in only ≈15% of repairs <ref type="bibr" target="#b13">(Hough and Purver, 2013;</ref><ref type="bibr" target="#b11">Heeman and Allen, 1999</ref>). Our repair onset de- tection is therefore driven by departures from flu- ency, via information-theoretic features derived incrementally from a language model in line with recent psycholinguistic accounts of incremental parsing -see <ref type="bibr" target="#b16">(Keller, 2004;</ref><ref type="bibr" target="#b14">Jaeger and Tily, 2011)</ref>.</p><p>Considering the time-linear way a repair is pro- cessed and the fact speakers are exponentially less likely to trace one word further back in repair as utterance length increases <ref type="bibr" target="#b28">(Shriberg and Stolcke, 1998)</ref>, backwards search seems to be the most ef- ficient reparandum extent detection method. <ref type="bibr">2</ref> Fea- tures determining the detection of the reparan- dum extent in the backwards search can also be information-theoretic: entropy measures of dis- tributional parallelism can characterize not only rough copy dependencies, but distributionally sim- ilar or dissimilar correspondences between se- quences. Finally, when detecting the repair end and structure, distributional information allows computation of the similarity between reparan- dum and repair. We argue a local-detection- with-backtracking approach is more cognitively plausible than string-based left-to-right repair la- belling, and using this insight should allow an im- provement in incremental accuracy, stability and time-to-detection over string-alignment driven ap- proaches in repair detection. "likes"</p><formula xml:id="formula_0">S 0 S 1 S 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T0</head><p>"John" "likes" "uh"</p><formula xml:id="formula_1">ed S 0 S 1 S 2 S 3 ed T1 "John" "likes" "uh" ed "loves" rp start S 0 S 1 S 2 S 3 ed ? S 4 rp start T2</formula><p>"John" "likes"</p><formula xml:id="formula_2">rm start rm end "uh" ed "loves" rp start S 0 S 1 S 2 rm start rm end S 3 ed S 4</formula><p>rp start</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T3</head><p>"John" "likes"</p><formula xml:id="formula_3">rm start rm end "uh" ed "loves" rp start rp sub end S 0 S 1 S 2 rm start rm end S 3 ed S 4 rp start rp sub end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T4</head><p>"John" "likes"</p><formula xml:id="formula_4">rm start rm end "uh" ed "loves" rp start rp sub end "Mary" S 0 S 1 S 2 rm start rm end S 3 ed S 4</formula><p>rp start rp sub end S 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5</head><p>Figure 1: Strongly Incremental Repair Detection proach to detecting repairs and isolated edit terms, assigning words the structures in (2). We in- clude interregnum recognition in the process, due to the inclusion of interregnum vocabulary within edit term vocabulary <ref type="bibr" target="#b9">(Ginzburg, 2012;</ref><ref type="bibr" target="#b13">Hough and Purver, 2013</ref>), a useful feature for repair detection ( <ref type="bibr" target="#b18">Lease et al., 2006;</ref><ref type="bibr" target="#b23">Qian and Liu, 2013</ref> </p><p>Rather than detecting the repair structure in its left-to-right string order as above, STIR functions as in <ref type="figure">Figure 1</ref>: first detecting edit terms (possibly interregna) at step T1; then detecting repair onsets rp start at T2; if one is found, backwards searching to find rm start at T3; then finally finding the re- pair end rp end at T4.</p><p>Step T1 relies mainly on lexical probabilities from an edit term language model; T2 exploits features of divergence from a fluent language model; T3 uses fluency of hypoth- esised repairs; and T4 the similarity between dis- tributions after reparandum and repair. However, each stage integrates these basic insights via mul- tiple related features in a statistical classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Enriched incremental language models</head><p>We derive the basic information-theoretic features required using n-gram language models, as they have a long history of information theoretic anal- ysis <ref type="bibr" target="#b27">(Shannon, 1948)</ref> and provide reproducible re- sults without forcing commitment to one partic- ular grammar formalism. Following recent work on modelling grammaticality judgements <ref type="bibr" target="#b4">(Clark et al., 2013)</ref>, we implement several modifications to standard language models to develop our basic measures of fluency and uncertainty. For our main fluent language models we train a trigram model with Kneser-Ney smoothing <ref type="bibr" target="#b17">(Kneser and Ney, 1995)</ref> on the words and POS tags of the standard Switchboard training data (all files with conversation numbers beginning sw2*,sw3* in the Penn Treebank III release), con- sisting of ≈100K utterances, ≈600K words. We follow ( <ref type="bibr" target="#b15">Johnson and Charniak, 2004</ref>) by clean- ing the data of disfluencies (i.e. edit terms and reparanda), to approximate a 'fluent' language model. We call these probabilities p lex kn , p pos kn be- low. <ref type="bibr">3</ref> We then derive surprisal as our principal default lexical uncertainty measurement s (equation 3) in both models; and, following <ref type="bibr" target="#b4">(Clark et al., 2013)</ref>, the (unigram) Weighted Mean Log trigram prob- ability (WML, eq. 4)-the trigram logprob of the sequence divided by the inverse summed logprob of the component unigrams (apart from the first two words in the sequence, which serve as the first trigram history). As here we use a local ap- proach we restrict the WML measures to single trigrams (weighted by the inverse logprob of the final word). While use of standard n-gram prob- ability conflates syntactic with lexical probability, WML gives us an approximation to incremental syntactic probability by factoring out lexical fre- quency.</p><formula xml:id="formula_6">s(wi−2 . . . wi) = − log 2 p kn (wi | wi−2, wi−1)<label>(3)</label></formula><formula xml:id="formula_7">WML(w0 . . . wn) = i=n i=2 log 2 p kn (wi | wi−2, wi−1) − n j=2 log 2 p kn (wj )<label>(4)</label></formula><p>Distributional measures To approximate un- certainty, we also derive the entropy H(w | c) of the possible word continuations w given a context c, from p(w i | c) for all words w i in the vocabu- lary -see (5). Calculating distributions over the entire lexicon incrementally is costly, so we ap- proximate this by constraining the calculation to words which are observed at least once in context c in training, w c = {w|count(c, w) ≥ 1} , assum- ing a uniform distribution over the unseen suffixes by using the appropriate smoothing constant, and subtracting the latter from the former -see eq. (6). Manual inspection showed this approximation to be very close, and the trie structure of our n- gram models allows efficient calculation. We also make use of the Zipfian distribution of n-grams in corpora by storing entropy values for the 20% most common trigram contexts observed in train- ing, leaving entropy values of rare or unseen con- texts to be computed at decoding time with little search cost due to their small or empty w c sets.</p><formula xml:id="formula_8">H(w | c) = − w∈V ocab p kn (w | c) log 2 p kn (w | c) (5) H(w | c) ≈ − w∈wc p kn (w | c) log 2 p kn (w | c) − [n × λ log 2 λ]</formula><p>where n = |V ocab| − |wc|</p><formula xml:id="formula_9">and λ = 1 − w∈wc p kn (w | c) n<label>(6)</label></formula><p>Given entropy estimates, we can also sim- ilarly approximate the Kullback-Leibler (KL) divergence (relative entropy) between distribu- tions in two different contexts c 1 and c 2 , i.e. θ(w|c 1 ) and θ(w|c 2 ), by pair-wise computing p(w|c 1 ) log 2 ( p(w|c 1 <ref type="figure">Figure 1)</ref>, exploiting the Markov assumption of n-gram models to allow ef- ficient calculation by avoiding re-computation.</p><note type="other">) p(w|c 2 ) ) only for words w ∈ wc 1 ∩ wc 2 , then approximating unseen values by assum- ing uniform distributions. Using p kn smoothed es- timates rather than raw maximum likelihood es- timations avoids infinite KL divergence values. Again, we found this approximation sufficiently close to the real values for our purposes. All such probability and distribution values are stored in incrementally constructed directed acyclic graph (DAG) structures (see</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Individual classifiers</head><p>This section details the features used by the 4 indi- vidual classifiers. To investigate the utility of the features used in each classifier we obtain values on the standard Switchboard heldout data (PTB III files sw4[5-9]*: 6.4K utterances, 49K words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Edit term detection</head><p>In the first component, we utilise the well-known observation that edit terms have a distinctive vocabulary <ref type="bibr" target="#b9">(Ginzburg, 2012)</ref>, training a bigram model on a corpus of all edit words annotated in Switchboard's training data. The classifier simply uses the surprisal s lex from this edit word model, and the trigram surprisal s lex from the standard fluent model of Section 4.1. At the current position w n , one, both or none of words w n and w n−1 are classified as edits. We found this simple approach effective and stable, although some delayed deci- sions occur in cases where s lex and WML lex are high in both models before the end of the edit, e.g. "I like" → "I {like} want...". Words classified as ed are removed from the incremental processing graph (indicated by the dotted line transition in <ref type="figure">Figure 1</ref>) and the stack updated if repair hypothe- ses are cancelled due to a delayed edit hypothesis of w n−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Repair start detection</head><p>Repair onset detection is arguably the most crucial component: the greater its accuracy, the better the input for downstream components and the lesser the overhead of filtering false positives required. We use Section 4.1's information-theoretic fea- tures s, WML, H for words and POS, and intro- duce 5 additional information-theoretic features: ∆WML is the difference between the WML val- ues at w n−1 and w n ; ∆H is the difference in en- tropy between w n−1 and w n ; InformationGain is the difference between expected entropy at w n−1 and observed s at w n , a measure that factors out the effect of naturally high entropy contexts; BestEntropyReduce is the best reduc- tion in entropy possible by an early rough hy- pothesis of reparandum onsets within 3 words; and BestWMLBoost similarly speculates on the best improvement of WML possible by positing rm start positions up to 3 words back. We also in- clude simple alignment features: binary features which indicate if the word w i−x is identical to the current word w i for x ∈ {1, 2, 3}. With 6 align- ment features, 16 N-gram features and a single logical feature edit which indicates the presence of an edit word at position w i−1 , rp start detection uses 23 features-see <ref type="table">Table 1</ref>.</p><p>We hypothesised repair onsets rp start would have significantly lower p lex (lower lexical- syntactic probability) and WML lex (lower syntac- tic probability) than other fluent trigrams. This was the case in the Switchboard heldout data for both measures, with the biggest difference obtained for WML lex (non-repair-onsets: -0. To compare n-gram measures against other lo- cal features, we ranked the features by Informa- tion Gain using 10-fold cross validation over the Switchboard heldout data-see <ref type="table">Table 1</ref>. The lan- guage model features are far more discriminative than the alignment features, showing the potential of a general information-theoretic approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Reparandum start detection</head><p>In detecting rm start positions given a hypothe- sised rp start (stage T3 in <ref type="figure">Figure 1)</ref>, we use the noisy channel intuition that removing the reparan- dum (from rm start to rp start ) increases fluency of the utterance, expressed here as WMLboost as described above. When using gold standard in- put we found this was the case on the heldout data, with a mean WMLboost of 0.223 (sd=0.267) for reparandum onsets and -0.058 (sd=0.224) for other words in the 6-word history-the negative boost for non-reparandum words captures the in- tuition that backtracking from those points would make the utterance less grammatical, and con- versely the boost afforded by the correct rm start detection helps solve the continuation problem for the listener (and our detector).</p><p>Parallelism in the onsets of rp start and rm start can also help solve the continuation problem, and in fact the KL divergence be- tween θ pos (w | rm start , rm start−1 ) and θ pos (w | rp start , rp start−1 ) is the second most useful fea- ture with average merit 0.429 (+-0.010) in cross-validation. The highest ranked feature is ∆WML (0.437 (+-0.003)) which here encodes the drop in the WMLboost from one backtracked position to the next. In ranking the 32 features we use, again information-theoretic ones are higher ranked than the logical features.  <ref type="table">Table 1</ref>: Feature ranker (Information Gain) for rp start detection-10-fold x-validation on Switch- board heldout data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Repair end detection and structure classification</head><p>For rp end detection, using the notion of paral- lelism, we hypothesise an effect of divergence be- tween θ lex at the reparandum-final word rm end and the repair-final word rp end : for repetition re- pairs, KL divergence will trivially be 0; for substi- tutions, it will be higher; for deletes, even higher. Upon inspection of our feature ranking this KL measure ranked 5th out of 23 features (merit= 0.258 (+-0.002)).</p><p>We introduce another feature encoding paral- lelism ReparandumRepairDifference : the differ- ence in probability between an utterance cleaned of the reparandum and the utterance with its repair phase substituting its reparandum. In both the POS (merit=0.366 (+-0.003)) and word (merit=0.352 (+-0.002)) LMs, this was the most discriminative feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classifier pipeline</head><p>STIR effects a pipeline of classifiers as in <ref type="figure" target="#fig_5">Fig- ure 3</ref>, where the ed classifier only permits non ed words to be passed on to rp start classification and for rp end classification of the active repair hypotheses, maintained in a stack. The rp start classifier passes positive repair hypotheses to the rm start classifier, which backwards searches up to 7 words back in the utterance. If a rm start is classified, the output is passed on for rp end clas- sification at the end of the pipeline, and if not re- jected this is pushed onto the repair stack. Repair hypotheses are are popped off when the string is 7 words beyond its rp start position. Putting limits on the stack's storage space is a way of controlling for processing overhead and complexity. Embed- ded repairs whose rm start coincide with another's rp start are easily dealt with as they are added to the stack as separate hypotheses. 4</p><p>Classifiers Classifiers are implemented using Random Forests <ref type="bibr" target="#b1">(Breiman, 2001</ref>) and we use dif- ferent error functions for each stage using Meta- Cost <ref type="bibr" target="#b5">(Domingos, 1999</ref>). The flexibility afforded by implementing adjustable error functions in a pipelined incremental processor allows control of the trade-off of immediate accuracy against run- time and stability of the sequence classification.</p><p>Processing complexity This pipeline avoids an exhaustive search all repair hypotheses. If we limit the search to within the rm start , rp start possibil- ities, this number of repairs grows approximately in the triangular number series-i.e. n(n+1) 2 , a nested loop over previous words as n gets incre- mented -which in terms of a complexity class is a quadratic O(n 2 ). If we allow more than one rm start , rp start hypothesis per word, the com- plexity goes up to O(n 3 ), however in the tests that we describe below, we are able to achieve good de- tection results without permitting this extra search space. Under our assumption that reparandum on- set detection is only triggered after repair onset de- tection, and repair extent detection is dependent on positive reparandum onset detection, a pipeline with accurate components will allow us to limit processing to a small subset of this search space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental set-up</head><p>We train STIR on the Switchboard data described above, and test it on the standard Switchboard test data (PTB III files 4[0-1]*). In order to avoid over- fitting of classifiers to the basic language models, we use a cross-fold training approach: we divide the corpus into 10 folds and use language mod- els trained on 9 folds to obtain feature values for the 10th fold, repeating for all 10. Classifiers are then trained as standard on the resulting feature- annotated corpus. This resulted in better feature utility for n-grams and better F-score results for detection in all components in the order of 5-6%. 5</p><p>Training the classifiers Each Random Forest classifier was limited to 20 trees of maximum depth 4 nodes, putting a ceiling on decoding time. In making the classifiers cost-sensitive, MetaCost resamples the data in accordance with the cost functions: we found using 10 iterations over a re- sample of 25% of the training data gave the most effective trade-off between training time and accu- racy. <ref type="bibr">6</ref> We use 8 different cost functions in rp start with differing costs for false negatives and posi- tives of the form below, where R is a repair ele- ment word and F is a fluent onset:</p><formula xml:id="formula_10">R hyp F hyp R gold 0 2 F gold 1 0</formula><p>We adopt a similar technique in rm start using 5 different cost functions and in rp end using 8 dif- ferent settings, which when combined gives a to- tal of 320 different cost function configurations. We hypothesise that higher recall permitted in the pipeline's first components would result in better overall accuracy as these hypotheses become re- fined, though at the cost of the stability of the hy-potheses of the sequence and extra downstream processing in pruning false positives.</p><p>We also experiment with the number of repair hypotheses permitted per word, using limits of 1- best and 2-best hypotheses. We expect that allow- ing 2 hypotheses to be explored per rp start should allow greater final accuracy, but with the trade-off of greater decoding and training complexity, and possible incremental instability.</p><p>As we wish to explore the incrementality versus final accuracy trade-off that STIR can achieve we now describe the evaluation metrics we employ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Incremental evaluation metrics</head><p>Following ( <ref type="bibr" target="#b0">Baumann et al., 2011</ref>) we divide our evaluation metrics into similarity metrics (mea- sures of equality with or similarity to a gold stan- dard), timing metrics (measures of the timing of relevant phenomena detected from the gold stan- dard) and diachronic metrics (evolution of incre- mental hypotheses over time).</p><p>Similarity metrics For direct comparison to previous approaches we use the standard measure of overall accuracy, the F-score over reparandum words, which we abbreviate F rm (see 7):</p><formula xml:id="formula_11">precision = rm correct rm hyp recall = rm correct rm gold Frm = 2 × precision × recall precision + recall (7)</formula><p>We are also interested in repair structural clas- sification, we also measure F-score over all repair components (rm words, ed words as interregna and rp words), a metric we abbreviate F s . This is not measured in standard repair detection on Switchboard. To investigate incremental accuracy we evaluate the delayed accuracy (DA) introduced by ( <ref type="bibr" target="#b31">Zwarts et al., 2010)</ref>, as described in section 2 against the utterance-final gold standard disflu- ency annotations, and use the mean of the 6 word F-scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>unnecessary edits</head><p>Timing and resource metrics Again for com- parative purposes we use Zwarts et al's time-to- detection metrics, that is the two average distances (in numbers of words) consumed before first de- tection of gold standard repairs, one from rm start , TD rm and one from rp start , TD rp . In our 1-best detection system, before evaluation we know a pri- ori TD rp will be 1 token, and TD rm will be 1 more than the average length of rm start − rp start repair spans correctly detected. However when we in- troduce a beam where multiple rm start s are pos- sible per rp start with the most likely hypothesis committed as the current output, the latency may begin to increase: the initially most probable hy- pothesis may not be the correct one. In addition to output timing metrics, we account for intrinsic processing complexity with the metric processing overhead (PO), which is the number of classifica- tions made by all components per word of input.</p><p>Diachronic metrics To measure stability of re- pair hypotheses over time we use ( <ref type="bibr" target="#b0">Baumann et al., 2011</ref>)'s edit overhead (EO) metric. EO measures the proportion of edits (add, revoke, substitute) ap- plied to a processor's output structure that are un- necessary. STIR's output is the repair label se- quence shown in <ref type="figure">Figure 1</ref>, however rather than evaluating its EO against the current gold stan- dard labels, we use a new mark-up we term the in- cremental repair gold standard: this does not pe- nalise lack of detection of a reparandum word rm as a bad edit until the corresponding rp start of that rm has been consumed. While F rm , F s and DA evaluate against what <ref type="bibr" target="#b0">Baumann et al. (2011)</ref> call the current gold standard, the incremental gold standard reflects the repair processing approach we set out in 3. An example of a repaired utterance with an EO of 44% <ref type="formula" target="#formula_7">( 4  9</ref> ) can be seen in <ref type="figure" target="#fig_6">Figure 4</ref>: of the 9 edits (7 repair annotations and 2 correct flu- ent words), 4 are unnecessary (bracketed). Note </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>We evaluate on the Switchboard test data; <ref type="table" target="#tab_2">Ta- ble 2</ref> shows results of the best performing settings for each of the metrics described above, together with the setting achieving the highest total score (TS)-the average % achieved of the best per- forming system's result in each metric. <ref type="bibr">7</ref> The set- tings found to achieve the highest F rm (the metric standardly used in disfluency detection), and that found to achieve the highest TS for each stage in the pipeline are shown in <ref type="figure" target="#fig_9">Figure 5</ref>.</p><p>Our experiments showed that different system settings perform better in different metrics, and no individual setting achieved the best result in all of them. Our best utterance-final F rm reaches 0.779, marginally though not significantly exceed- ing ( <ref type="bibr" target="#b31">Zwarts et al., 2010)</ref>'s measure and STIR achieves 0.736 on the previously unevaluated F s . The setting with the best DA improves on ( <ref type="bibr" target="#b31">Zwarts et al., 2010</ref>)'s result significantly in terms of mean values (0.718 vs. 0.694), and also in terms of the steepness of the curves ( <ref type="figure" target="#fig_7">Figure 6</ref>). The fastest av- erage time to detection is 1 word for TD rp and 2.6 words for TD rm <ref type="table" target="#tab_3">(Table 3)</ref>, improving dramatically on the noisy channel model's 4.6 and 7.5 words.</p><p>Incrementality versus accuracy trade-off We aimed to investigate how well a system could do in terms of achieving both good final accuracy and incremental performance, and while the best F rm setting had a large PO and relatively slow DA in- crease, we find STIR can find a good trade-off set-     ting: the highest TS scoring setting achieves an F rm of 0.754 whilst also exhibiting a very good DA (0.711) -over 98% of the best recorded score -and low PO and EO rates -over 96% of the best recorded scores. See the bottom row of <ref type="table" target="#tab_2">Table 2</ref>.</p><p>As can be seen in <ref type="figure" target="#fig_9">Figure 5</ref>, the cost functions for these winning settings are different in nature. The best non-incremental F rm measure setting requires high recall for the rest of the pipeline to work on, using the highest cost, 64, for false negative rp start words and the highest stack depth of 2 (similar to a wider beam); but the best overall TS scoring sys- tem uses a less permissive setting to increase in- cremental performance.</p><p>We make a preliminary investigation into the effect of increasing the stack capacity by com- paring stacks with 1-best rm start hypotheses per rp start and 2-best stacks. The average differences between the two conditions is shown in <ref type="table" target="#tab_3">Table 3</ref>. Moving to the 2-stack condition results in gain in overall accuracy in F rm and F s , but at the cost of EO and also time-to-detection scores TD rm and TD rp . The extent to which the stack can be in- creased without increasing jitter, latency and com- plexity will be investigated in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented STIR, an incremental repair detector that can be used to experiment with in- cremental performance and accuracy trade-offs. In future work we plan to include probabilistic and distributional features from a top-down incremen- tal parser e.g. <ref type="bibr" target="#b26">Roark et al. (2009)</ref>, and use STIR's distributional features to classify repair type.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"</head><label></label><figDesc>John"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: WML lex values for trigrams for a repaired utterance exhibiting the drop at the repair onset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>736 (sd=0.359); repair onsets: -1.457 (sd=0.359)). In the POS model, entropy of continuation H pos was the strongest feature (non-repair-onsets: 3.141 (sd=0.769); repair onsets: 3.444 (sd=0.899)). The trigram WML lex measure for the repaired utter- ance "I haven't had any [ good + really very good ] experience with child care" can be seen in Fig- ure 2. The steep drop at the repair onset shows the usefulness of WML features for fluency measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classifier pipeline</figDesc><graphic url="image-114.png" coords="8,159.86,53.11,244.42,95.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Edit Overhead-4 unnecessary edits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Delayed Accuracy Curves</figDesc><graphic url="image-187.png" coords="9,311.54,62.75,209.94,127.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The cost function settings for the MetaCost classifiers for each component, for the best F rm setting (top row) and best total score (TS) setting (bottom row) F rm F s DA EO PO Best Final rm F-score (F rm ) 0.779 0.735 0.698 3.946 1.733 Best Final repair structure F-score (F s ) 0.772 0.736 0.707 4.477 1.659 Best Delayed Accuracy of rm (DA) 0.767 0.721 0.718 1.483 1.689 Best (lowest) Edit Overhead (EO) 0.718 0.674 0.675 0.864 1.230 Best (lowest) Processing Overhead (PO) 0.716 0.671 0.673 0.875 1.229 Best Total Score (mean % of best scores) (TS) 0.754 0.708 0.711 0.931 1.255</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Comparison of the best performing system settings using different measures</head><label>2</label><figDesc></figDesc><table>F rm 
F s 
DA 
EO 
PO 
TD rp TD rm 
1-best rm start 0.745 0.707 0.699 3.780 1.650 
1.0 
2.6 
2-best rm start 0.758 0.721 0.701 4.319 1.665 
1.1 
2.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison of performance of systems with different stack capacities</figDesc><table></table></figure>

			<note place="foot" n="1"> Though see (Germesin et al., 2008) for one approach, albeit using idiosyncratic repair categories.</note>

			<note place="foot" n="4"> STIR: Strongly Incremental Repair detection Our system, STIR (Strongly Incremental Repair detection), therefore takes a local incremental ap2 We acknowledge a purely position-based model for reparandum extent detection under-estimates prepositions, which speakers favour as the retrace start and over-estimates verbs, which speakers tend to avoid retracing back to, preferring to begin the utterance again, as (Healey et al., 2011)&apos;s experiments also demonstrate.</note>

			<note place="foot" n="3"> We suppress the pos and lex superscripts below where we refer to measures from either model.</note>

			<note place="foot" n="4"> We constrain the problem not to include embedded deletes which may share their rpstart word with another repair-these are in practice very rare.</note>

			<note place="foot" n="5"> Zwarts and Johnson (2011) take a similar approach on Switchboard data to train a re-ranker of repair analyses. 6 As (Domingos, 1999) demonstrated, there are only relatively small accuracy gains when using more than this, with training time increasing in the order of the re-sample size.</note>

			<note place="foot" n="7"> We do not include time-to-detection scores in TS as it did not vary enough between settings to be significant, however there was a difference in this measure between the 1-best stack condition and the 2-best stack condition-see below.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the three anonymous EMNLP review-ers for their helpful comments. Hough is sup-ported by the DUEL project, financially supported by the Agence Nationale de la Research (grant number ANR-13-FRAL-0001) and the Deutsche Forschungsgemainschaft. Much of the work was carried out with support from an EPSRC DTA scholarship at Queen Mary University of Lon-don. Purver is partly supported by ConCreTe: the project ConCreTe acknowledges the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Com-mission, under FET grant number 611733.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation and optimisation of incremental processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="141" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How listeners compensate for disfluencies in spontaneous speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Schober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="296" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An improved error model for noisy channel spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical representation of grammaticality judgements: the limits of n-gram models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Giorgolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL)</title>
		<meeting>the Fourth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Metacost: A general method for making classifiers cost-sensitive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the fifth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using integer linear programming for detecting speech disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hybrid multi-step disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Germesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Poller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Machine Learning for Multimodal Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="185" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Interactive Stance: Meaning for Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ginzburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making a contribution: Processing clarification requests in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G T</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Meeting of the Society for Text and Discourse</title>
		<meeting>the 21st Annual Meeting of the Society for Text and Discourse<address><addrLine>Poitiers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech repairs, intonational phrases, and discourse markers: modeling speakers&apos; utterances in spoken dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Heeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="527" to="571" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint incremental disfluency detection and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linugistics (TACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelling expectation in the self-repair processing of annotat-, um, listeners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th SemDial Workshop on the Semantics and Pragmatics of Dialogue (DialDam)</title>
		<meeting>the 17th SemDial Workshop on the Semantics and Pragmatics of Dialogue (DialDam)<address><addrLine>Amsterdam, December</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On language utility: Processing complexity and communicative efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Tily</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A TAGbased noisy channel model of speech repairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing disfluencies in conversational speech. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1566" to="1573" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monitoring and self-repair in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J M</forename><surname>Levelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="104" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Disfluency annotation stylebook for the switchboard corpus. ms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meteer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A syntactic time-series model for parsing fluent and disfluent speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Axiomatic Grammar, NonConstituent Coordination and Incremental Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disfluency detection using multi-step stacked learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="820" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Non-monotonic parsing of fluent umm I mean disfluent sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on incremental processing in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Pallier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
		<respStmt>
			<orgName>AT &amp; T Bell Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How far do speakers back up in repairs? A quantitative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Language Processing</title>
		<meeting>the International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2183" to="2186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Preliminaries to a Theory of Speech Disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The impact of language models and loss functions on repair disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Zwarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting speech repairs incrementally using a noisy channel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Zwarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1371" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
