<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory-enhanced Decoder for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Memory-enhanced Decoder for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="278" to="286"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN de-coder is called MEMDEC. At each time during decoding, MEMDEC will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous work(Bahdanau et al., 2014) to store the representation of source sentence, the memory in MEMDEC is a matrix with predetermined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by 4.8 BLEU upon Groundhog and 5.3 BLEU upon on Moses, yielding the best performance achieved with the same training set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The introduction of external memory has greatly expanded the representational capability of neu- ral network-based model on modeling se- quences( <ref type="bibr">Graves et al., 2014</ref>), by providing flex- ible ways of storing and accessing information. More specifically, in neural machine translation, one great improvement came from using an array of vectors to represent the source in a sentence- level memory and dynamically accessing relevant segments of them (alignment) ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) through content-based addressing ( <ref type="bibr">Graves et al., 2014</ref>). The success of RNNsearch demon- strated the advantage of saving the entire sen- tence of arbitrary length in an unbounded mem- ory for operations of next stage (e.g., decoding).</p><p>In this paper, we show that an external memory can be used to facilitate the decoding/generation process thorough a memory-enhanced RNN de- coder, called MEMDEC.</p><p>The memory in MEMDEC is a direct extension to the state in the decoding, therefore functionally closer to the memory cell in LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>). It takes the form of a matrix with pre-determined size, each column ("a memory cell") can be accessed by the decoding RNN with content-based addressing for both reading and writing during the decoding process. This mem- ory is designed to provide a more flexible way to select, represent and synthesize the informa- tion of source sentence and previously generated words of target relevant to the decoding. This is in contrast to the set of hidden states of the entire source sentence (which can viewed as an- other form of memory) in ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) for attentive read, but can be combined with it to greatly improve the performance of neural ma- chine translator. We apply our model on English- Chinese translation tasks, achieving performance superior to any published results, SMT or NMT, on the same training data <ref type="bibr" target="#b6">(Xie et al., 2011;</ref><ref type="bibr">Meng et al., 2015;</ref><ref type="bibr">Tu et al., 2016;</ref><ref type="bibr" target="#b3">Hu et al., 2015)</ref> Our contributions are mainly two-folds</p><p>• we propose a memory-enhanced decoder for neural machine translator which naturally extends the RNN with vector state.</p><p>• our empirical study on Chinese-English translation tasks show the efficacy of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roadmap</head><p>In the remainder of this paper, we will first give a brief introduction to attention- based neural machine translation in Section 2, presented from the view of encoder-decoder, which treats the hidden states of source as an unbounded memory and the attention model as a content-based reading. In Section 3, we will elaborate on the memory-enhanced decoder MEMDEC. In Section 4, we will apply NMT with MEMDEC to a Chinese-English task. Then in Section 5 and 6, we will give related work and conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural machine translation with attention</head><p>Our work is built on attention-based NMT( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), which repre- sents the source sentence as a sequence of vectors after being processed by RNN or bi- directional RNNs, and then conducts dynamic alignment and generation of the target sentence with another RNN simultaneously. Attention-based NMT, with RNNsearch as its most popular representative, generalizes the con- ventional notion of encoder-decoder in using a unbounded memory for the intermediate repre- sentation of source sentence and content-based addressing read in decoding, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. More specifically, at time step t, RNNsearch first get context vector c t after read- ing from the source representation M S , which is then used to update the state, and generate the word y t (along with the current hidden state s t , and the previously generated word y i−1 ).</p><p>Formally, given an input sequence x = [x 1 , x 2 , . . . , x Tx ] and the previously generated sequence y &lt;t = [y 1 , y 2 , . . . , y t−1 ], the probabil- ity of next word y t is p(y t |y &lt;t ; x) = f (c t , y t−1 , s t ),</p><p>(1) where s t is state of decoder RNN at time step t calculated as</p><formula xml:id="formula_0">s t = g(s t−1 , y t−1 , c t ).<label>(2)</label></formula><p>where g(·) can be an be any activation function, here we adopt a more sophisticated dynamic op- erator as in Gated Recurrent Unit (GRU, ( <ref type="bibr" target="#b0">Cho et al., 2014)</ref>). In the remainder of the paper, we will also use GRU to stand for the operator. The read- ing c t is calculated as</p><formula xml:id="formula_1">c t = j=Tx j=1 α t,j h j ,<label>(3)</label></formula><p>where h j is the j th cell in memory M S . More formally,</p><formula xml:id="formula_2">h j = [ ← − h j , − → h j ]</formula><p>is the annotations of x j and contains information about the whole input sequence with a strong focus on the parts surrounding x j , which is computed by a bidirec- tional RNN. The weight α t,j is computed by</p><formula xml:id="formula_3">α t,j = exp(e t,j ) k=Tx k=1 exp(e t,k )</formula><p>. where e i,j = v T a tanh(W a s t−1 + U a h j ) scores how well s t−1 and the memory cell h j match. This is called automatic alignment ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) or attention model ( <ref type="bibr" target="#b4">Luong et al., 2015</ref>), but it is essentially reading with content-based addressing defined in ( <ref type="bibr">Graves et al., 2014</ref>). With this addressing strategy the decoder can attend to the source representation that is most relevant to the stage of decoding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Improved Attention Model</head><p>The alignment model α t,j scores how well the output at position t matches the inputs around po- sition j based on s t−1 and h j . It is intuitively beneficial to exploit the information of y t−1 when reading from M S , which is missing from the im- plementation of attention-based NMT in <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>. In this work, we build a more effective alignment path by feeding both previous hidden state s t−1 and the context word y t−1 to the attention model, inspired by the re- cent implementation of attention-based NMT 1 . Formally, the calculation of e t,j becomes</p><formula xml:id="formula_4">e t,j = v T a tanh(W a ˜ s t−1 + U a h j ),</formula><p>where</p><formula xml:id="formula_5">• ˜ s t−1 = H(s t−1 , e y t−1 )</formula><p>is an intermediate state tailored for reading from M S with the information of y t−1 (its word embedding be- ing e y t−1 ) added;</p><p>• H is a nonlinear function, which can be as simple as tanh or as complex as GRU.</p><p>In our preliminary experiments, we found GRU works slightly better than tanh func- tion, but we chose the latter for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoder with External Memory</head><p>In this section we will elaborate on the proposed memory-enhanced decoder MEMDEC. In ad- dition to the source memory M S , MEMDEC is equipped with a buffer memory M B as an ex- tension to the conventional state vector.  In the remainder of the paper, we will refer to the conventional state as vector-state (denoted s t ) and its memory extension as memory-state (de- noted as M B t ). Both states are updated at each time step in a interweaving fashion, while the out- put symbol y t is predicted based solely on vector- state s t (along with c t and y t−1 ). The diagram of this memory-enhanced decoder is given in <ref type="figure" target="#fig_1">Figure  2</ref>.</p><p>Vector-State Update At time t, the vector-state s t is first used to read M B</p><formula xml:id="formula_6">r t−1 = read B (s t−1 , M B t−1 )<label>(4)</label></formula><p>which then meets the previous prediction y t−1 to form an "intermediate" state-vector</p><formula xml:id="formula_7">s t = tanh(W r r t−1 + W y e y t−1 ).<label>(5)</label></formula><p>where e y t−1 is the word-embedding associated with the previous prediction y t−1 . This pre-state s t is used to read the source memory</p><formula xml:id="formula_8">M S c t = read S ( s t , M S ).<label>(6)</label></formula><p>Both readings in Eq. <ref type="formula" target="#formula_6">(4)</ref> &amp; <ref type="formula" target="#formula_8">(6)</ref> follow content- based addressing( <ref type="bibr">Graves et al., 2014</ref>) (details later in Section 3.1). After that, r t−1 is combined with output symbol y t−1 and c t to update the new vector-state</p><formula xml:id="formula_9">s t = GRU(r t−1 , y t−1 , c t )<label>(7)</label></formula><p>The update of vector-state is illustrated in  Memory-State Update As illustrated in <ref type="figure" target="#fig_6">Fig- ure 5</ref>, the update for memory-state is simple after the update of vector-state: with the vector-state s t+1 the updated memory-state will be</p><formula xml:id="formula_10">M B t = write(s t , M B t−1 )<label>(8)</label></formula><p>The writing to the memory-state is also content- based, with same forgetting mechanism sug- gested in ( <ref type="bibr">Graves et al., 2014</ref>), which we will elaborate with more details later in this section. Prediction As illustrated in <ref type="figure" target="#fig_7">Figure 6</ref>, the pre- diction model is same as in ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>, where the score for word y is given by</p><formula xml:id="formula_11">score(y) = DNN([s t , c t , e y t−1 ]) ω y (9)</formula><p>where ω y is the parameters associated with the word y. The probability of generating word y at time t is then given by a softmax over the scores p(y|s t , c t , y t−1 ) = exp(score(y)) y exp(score(y ))</p><p>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reading Memory-State</head><p>Formally M B t ∈ R n×m is the memory-state at time t after the memory-state update, where n is the number of memory cells and m is the dimen- sion of vector in each cell. Before the vector-state update at time t, the output of reading r t is given by</p><formula xml:id="formula_12">r t = j=n j=1 w R t (j)M B t−1 (j)</formula><p>where w R t ∈ R n specifies the normalized weights assigned to the cells in M B t . Similar with the reading from M S ( a.k.a. attention model), we use content-based addressing in determining w R t .</p><p>More specifically, w R t is also updated from the one from previous time w R t−1 as</p><formula xml:id="formula_13">w R t = g R t w R t−1 + (1 − g R t ) w R t ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_14">• g R t = σ(w R g s t )</formula><p>is the gate function, with pa- rameters w R g ∈ R m ;</p><p>• w t gives the contribution based on the cur- rent vector-state s t w R t = softmax(a R t )</p><formula xml:id="formula_15">a R t (i) = v (W R a M B t−1 (i) + U R a s t−1 ),<label>(11)</label></formula><p>with parameters W R a , U R a ∈ R m×m and v ∈ R m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Writing to Memory-State</head><p>There are two types of operation on writing to memory-state: ERASE and ADD. Erasion is simi- lar to the forget gate in LSTM or GRU, which de- termines the content to be remove from memory cells. More specifically, the vector µ ERS t ∈ R m specifies the values to be removed on each dimen- sion in memory cells, which is than assigned to each cell through normalized weights w W t . For- mally, the memory-state after ERASE is given by</p><formula xml:id="formula_17">M B t (i) = M B t−1 (i)(1 − w W t (i) · µ ERS t )<label>(13)</label></formula><p>i = 1, · · · , n where</p><formula xml:id="formula_18">• µ ERS t = σ(W ERS s t ) is parametrized with W ERS ∈ R m×m ;</formula><p>• w W t (i) specifies the weight associated with the i th cell in the same parametric form as in Eq. (10)-(12) with generally different pa- rameters.</p><p>ADD operation is similar with the update gate in LSTM or GRU, deciding how much current in- formation should be written to the memory.</p><formula xml:id="formula_19">M B t (i) = M B t (i) + w W t (i)µ ADD t µ ADD t = σ(W ADD s t )</formula><p>where µ ADD t ∈ R m and W ADD ∈ R m×m .</p><p>In our experiments, we have a peculiar but in- teresting observation: it is often beneficial to use the same weights for both reading (i.e., w R t in Section 3.1) and writing (i.e., w W t in Section 3.2 ) for the same vector-state s t . We conjecture that this acts like a regularization mechanism to en- courage the content of reading and writing to be similar to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Some Analysis</head><p>The writing operation in Eq. <ref type="formula" target="#formula_1">(13)</ref> at time t can be viewed as an nonlinear way to combine the previous memory-state M B t−1 and the newly updated vector-state s t , where the nonlinearity comes from both the content-based addressing and the gating. This is in a way similar to the update of states in regular RNN, while we con- jecture that the addressing strategy in MEMDEC makes it easier to selectively change some con- tent updated (e.g., the relatively short-term con- tent) while keeping other content less modified (e.g., the relatively long-term content).</p><p>The reading operation in Eq. <ref type="formula" target="#formula_13">(10)</ref> can "extract" the content from M B t relevant to the alignment (reading from M S ) and prediction task at time t. This is in contrast with the regular RNN decoder including its gated variants, which takes the en- tire state vector to for this purpose. As one ad- vantage, although only part of the information in M B t is used at t, the entire memory-state, which may store other information useful for later, will be carry over to time t + 1 for memory-state up- date (writing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Chinese-English Translation</head><p>We test the memory-enhanced decoder to task of Chinese-to-English translation, where MEMDEC is put on the top of encoder same as in ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation metrics</head><p>Our training data for the translation task con- sists of 1.25M sentence pairs extracted from LDC corpora 2 , with 27.9M Chinese words and 34.5M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) and 2006 (MT06) datasets as our test</head><p>sets. We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric as our eval- uation metric ( <ref type="bibr" target="#b5">Papineni et al., 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment settings</head><p>Hyper parameters In training of the neural networks, we limit the source and target vocab- ularies to the most frequent 30K words in both Chinese and English, covering approximately 97.7% and 99.3% of the two corpora respectively. The dimensions of word embedding is 512 and the size of the hidden layer is 1024. The dimem- sion of each cell in M B is set to 1024 and the number of cells n is set to 8.</p><p>Training details We initialize the recurrent weight matrices as random orthogonal matrices. All the bias vectors were initialize to zero. For other parameters, we initialize them by sampling each element from the Gaussian distribution of mean 0 and variance 0.01 2 . Parameter optimiza- tion is performed using stochastic gradient de- scent. Adadelta <ref type="bibr" target="#b7">(Zeiler, 2012</ref>) is used to auto- matically adapt the learning rate of each param- eter ( = 10 −6 and ρ = 0.95). To avoid gra- dients explosion, the gradients of the cost func- tion which had 2 norm larger than a predefined threshold 1.0 was normalized to the threshold ( <ref type="bibr">Pascanu et al., 2013)</ref>. Each SGD is of a mini- batch of 80 sentences. We train our NMT model with the sentences of length up to 50 words in training data, while for moses system we use the full training data.</p><p>Memory Initialization Each memory cell is initialized with the source sentence hidden state computed as <ref type="formula" target="#formula_7">(15)</ref> LDC2004T08 and LDC2005T06.</p><formula xml:id="formula_20">M B (i) = m + ν i (14) m = σ(W INI i=Tx i=0 h i )/T x</formula><p>where W INI ∈ R m×2·m ; σ is tanh function. m makes a nonlinear transformation of the source sentence information. ν i is a random vector sam- pled from N (0, 0.1).</p><p>Dropout we also use dropout for our NMT baseline model and MEMDEC to avoid over- fitting ( <ref type="bibr" target="#b2">Hinton et al., 2012</ref>). The key idea is to randomly drop units (along with their connec- tions) from the neural network during training. This prevents units from co-adapting too much.</p><p>In the simplest case, each unit is omitted with a fixed probability p, namely dropout rate. In our experiments, dropout was applied only on the output layer and the dropout rate is set to 0.5. We also try other strategy such as dropout at word embeddings or RNN hidden states but fail to get further improvements.</p><p>Pre-training For MEMDEC, the objective function is a highly non-convex function of the parameters with more complicated land- scape than that for decoder without exter- nal memory, rendering direct optimization over all the parameters rather difficult. Inspired by the effort on easing the training of very deep architectures <ref type="bibr" target="#b1">(Hinton and Salakhutdinov, 2006</ref>), we propose a simple pre-training strategyFirst we train a regular attention-based NMT model without external memory. Then we use the trained NMT model to initialize the parameters of encoder and parameters of MEMDEC, except those related to memory-state</p><formula xml:id="formula_21">(i.e., {W R a , U R a , v, w R g , W ERS , W ADD }).</formula><p>After that, we fine-tune all the parameters of NMT with MEMDEC decoder, including the parame- ters initialized with pre-training and those associ- ated with accessing memory-state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison systems</head><p>We compare our method with three state-of-the- art systems:</p><p>• Moses: an open source phrase-based trans- lation system 3 : with default configuration and a 4-gram language model trained on the target portion of training data.  • RNNSearch: an attention-based NMT model with default settings. We use the open source system GroundHog as our NMT baseline 4 .</p><p>• Coverage model: a state-of-the-art variant of attention-based NMT model ( <ref type="bibr">Tu et al., 2016</ref>) which improves the attention mecha- nism through modelling a soft coverage on the source representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>The main results of different models are given in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model selection</head><p>Pre-training plays an important role in optimiz- ing the memory model. As can be seen in Tab.2, pre-training improves upon our baseline +1.11 BLEU score on average, but even without pre- training our model still gains +1.04 BLEU score on average. Our model is rather robust to the memory size: with merely four cells, our model will be over 2 BLEU higher than RNNsearch . This further verifies our conjecture the the exter- nal memory is mostly used to store part of the source and history of target sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case study</head><p>We show in <ref type="table">Table 5</ref> sample translations from Chinese to English, comparing mainly MEMDEC src :"(200311), "</p><p>ref "All parties that signed the (November 2003 ceasefire) accord should finish the cantoning of their fighters by January 5, 2004, at the latest," Ndayizeye said.</p><p>MEMDEC UNK said, " the parties involved in the ceasefire agreement on November 2003 will have to be completed by January 5, 2004. "</p><p>base "The signing of the agreement (UNK-fire) agreement in the November 2003 ceasefire must be completed by <ref type="bibr">January 5, 2004.</ref> src ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ref</head><p>Members of the delegation told US Today that the Bush administration had approved the US delegation' s visit to North Korea from January 6 to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEMDEC</head><p>The delegation told the US today that the Bush administration has approved the US delegation's visit to north Korea from 6 to 10 january .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base</head><p>The delegation told the US that the Bush administration has approved the US to begin his visit to north Korea from 6 to 10 January. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There is a long thread of work aiming to im- prove the ability of RNN in remembering long se- quences, with the long short-term memory RNN (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) be- ing the most salient examples and GRU <ref type="bibr" target="#b0">(Cho et al., 2014</ref>) being the most recent one. Those works focus on designing the dynamics of the RNN through new dynamic operators and appropri- ate gating, while still keeping vector form RNN states. MEMDEC, on top of the gated RNN, ex- plicitly adds matrix-form memory equipped with content-based addressing to the system, hence greatly improving the power of the decoder RNN in representing the information important for the translation task. MEMDEC is obviously related to the recent ef- fort on attaching an external memory to neural networks, with two most salient examples be- ing Neural Turing Machine (NTM) ( <ref type="bibr">Graves et al., 2014</ref>) and Memory Network ( <ref type="bibr">Weston et al., 2014</ref>). In fact MEMDEC can be viewed as a special case of NTM, with specifically designed reading (from two different types of memory) and writing mechanism for the translation task. Quite remarkably MEMDEC is among the rare instances of NTM which significantly improves upon state-of-the-arts on a real-world NLP task with large training corpus.</p><p>Our work is also related to the recent work on machine reading ( <ref type="bibr" target="#b0">Cheng et al., 2016)</ref>, in which the machine reader is equipped with a memory tape, enabling the model to directly read all the previous hidden state with an attention mecha- nism. Different from their work, we use an ex- ternal bounded memory and make an abstraction of previous information. In ( <ref type="bibr">Meng et al., 2015)</ref>, Meng et. al. also proposed a deep architecture for sequence-to-sequence learning with stacked lay- ers of memory to store the intermediate represen- tations, while our external memory was applied within a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose to enhance the RNN decoder in a neural machine translator (NMT) with exter- nal memory. Our empirical study on Chinese- English translation shows that it can significantly improve the performance of NMT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RNNsearch in the encoder-decoder view.</figDesc><graphic url="image-1.png" coords="2,331.70,56.69,176.76,166.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagram of the proposed decoder MEMDEC with details.</figDesc><graphic url="image-2.png" coords="3,129.23,56.69,353.54,172.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig- ure 3 contrasts MEMDEC with the decoder in RNNsearch (Figure 1) on a high level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: High level digram of MEMDEC.</figDesc><graphic url="image-3.png" coords="3,331.70,415.96,176.77,111.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Vector-state update at time t.</figDesc><graphic url="image-4.png" coords="4,92.49,420.57,198.86,76.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Memory-state update at time t.</figDesc><graphic url="image-5.png" coords="4,353.79,56.69,132.58,74.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Prediction at time t.</figDesc><graphic url="image-6.png" coords="4,375.89,360.15,88.38,93.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>MEMDEC performances of different memory size.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Clearly MEMDEC leads to remark-
able improvement over Moses (+5.28 BLEU) and 
Groundhog (+4.78 BLEU). The feedback atten-
tion gains +1.06 BLEU score on top of Ground-
hog on average, while together with dropout adds 
another +0.83 BLEU score, which constitute the 
1.89 BLEU gain of RNNsearch over Ground-
hog. Compared to RNNsearch MEMDEC is 
+2.89 BLEU score higher, showing the model-
ing power gained from the external memory. Fi-nally, we also compare MEMDEC with the state-
of-the-art attention-based NMT with COVERAGE 
mechanism(Tu et al., 2016), which is about 2 
BLEU over than the published result after adding 
fast attention and dropout. In this comparison 
MEMDEC wins with big margin (+1.46 BLEU 
score). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Sample translations-for each example, we show the source(src), the human translation (ref),the translation from 

our memory model MEMDEC and the translation from RNNsearch(equipped with fast attention and dropout).We italicise 

some correct translation segments and highlight a few wrong ones in bold. 

and the RNNsearch model for its pre-training. It 
is appealing to observe that MEMDEC can pro-
duce more fluent translation results and better 
grasp the semantic information of the sentence. 

</table></figure>

			<note place="foot" n="1"> github.com/nyu-dl/dl4mt-tutorial/ tree/master/session2</note>

			<note place="foot" n="2"> The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07,</note>

			<note place="foot" n="3"> http://www.statmt.org/moses/</note>

			<note place="foot" n="4"> https://github.com/lisa-groundhog/ GroundHog</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<idno>arXiv:1410.5401</idno>
	</analytic>
	<monogr>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salakhutdinov2006] Geoffrey E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Context-dependent translation selection using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<editor>Meng et al.2015] Fandong Meng, Zhengdong Lu, Zhaopeng Tu, Hang Li, and Qun Liu</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papineni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06442</idno>
		<idno>arXiv:1410.3916</idno>
	</analytic>
	<monogr>
		<title level="m">Kyunghyun Cho, and Yoshua Bengio. 2013. How to construct deep recurrent neural networks</title>
		<editor>Tu et al.2016] Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li</editor>
		<meeting><address><addrLine>Weston</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
<note type="report_type">ArXiv eprints</note>
	<note>Modeling coverage for neural machine translation. Weston et al.2014. Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A novel dependency-to-string model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
