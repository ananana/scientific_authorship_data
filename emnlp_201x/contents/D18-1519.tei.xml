<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-You</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computer Science &amp; Information Engineering</orgName>
								<orgName type="department" key="dep2">Dept of Computer Science &amp;</orgName>
								<orgName type="laboratory">Information Engineering National Taiwan University Research Center for Information Technology</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country>Academia Sinica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Syuan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computer Science &amp; Information Engineering</orgName>
								<orgName type="department" key="dep2">Dept of Computer Science &amp;</orgName>
								<orgName type="laboratory">Information Engineering National Taiwan University Research Center for Information Technology</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country>Academia Sinica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Te</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computer Science &amp; Information Engineering</orgName>
								<orgName type="department" key="dep2">Dept of Computer Science &amp;</orgName>
								<orgName type="laboratory">Information Engineering National Taiwan University Research Center for Information Technology</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country>Academia Sinica</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
							<email>sdlin@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computer Science &amp; Information Engineering</orgName>
								<orgName type="department" key="dep2">Dept of Computer Science &amp;</orgName>
								<orgName type="laboratory">Information Engineering National Taiwan University Research Center for Information Technology</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country>Academia Sinica</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4834" to="4839"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4834</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Lexicon relation extraction given distribu-tional representation of words is an important topic in NLP. We observe that the state-of-the-art projection-based methods cannot be generalized to handle unseen hypernyms. We propose to analyze it in the perspective of pollution , that is, the predicted hypernyms are limited to those appeared in training set. We propose a word relation autoencoder (WRAE) model to address the challenge and construct the corresponding indicator to measure the pollution. Experiments on several hypernym-like lexicon datasets show that our model out-performs the competitors significantly.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper discusses the inference of relations be- tween words. For the hypernym beer IsA drink, , denoted as IsA(x, y), beer is the hyponym x and drink serves as the hypernym y. Relation lexi- cons are precious resource for NLP systems, while constructing the semantic graphs such as Word- Net <ref type="bibr" target="#b1">(Fellbaum, 1998)</ref> and <ref type="bibr">ConceptNet (Speer and Havasi, 2012</ref>) requires expensive human efforts for labeling.</p><p>Recently, researchers have started working on extracting word relations based on pre-trained word embedding without the need of an exist- ing corpus, thanks to the success of distributional word representation models such as GloVe <ref type="bibr" target="#b11">(Pennington et al., 2014)</ref>.</p><p>Comparing with hypernym classification mod- els ( <ref type="bibr" target="#b8">Lenci and Benotto, 2012;</ref><ref type="bibr" target="#b17">Weeds et al., 2014;</ref><ref type="bibr" target="#b9">Levy et al., 2015;</ref><ref type="bibr" target="#b16">Vylomova et al., 2016</ref>) that take a pair of entities (x,y) as inputs and output a binary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Answer beef → meat ≈ crab → ? seafood tiger → zoo ≈ dolphin → ? aquarium paint → artist ≈ book → ? writer japan → asia ≈ italy → ? europe decision about the existence of relation, there has been less work focusing on hypernym extraction task. It is a challenging task to automatically ex- tract all possible hypernyms of a given hyponym query, especially the unlabeled ones, from the vo- cabularies.</p><p>Classification-based models are not applicable for this task because the complexity of inference is O(V ), where V is the size of vocabulary that often scales to billions.</p><p>Among the existing solutions, projection-based methods ( <ref type="bibr" target="#b3">Fu et al., 2014;</ref><ref type="bibr" target="#b18">Yamane et al., 2016;</ref><ref type="bibr" target="#b0">Espinosa-Anke et al., 2016;</ref><ref type="bibr" target="#b15">Ustalov et al., 2017)</ref> emphasize on hypernym extraction which intu- itively represent a relation as y − x according to the linear structure of word embedding. By di- rectly learning a linear mapping Φ between two words such that xΦ = y, the predictionˆypredictionˆ predictionˆy can be obtained with nearest neighbor search for xΦ in the word embedding space. Moreover, the poten- tial candidates of y are not required to be seen in advance so that the method can be used to predict unseen hypernym directly. <ref type="bibr" target="#b3">Fu et al. (2014)</ref> further observe the existence of cluster structures in relation representation y − x and propose to learn a piecewise linear mapping such that xΦ k = y for each cluster C k . Their ex- periments show that domain clustering on training offset is very useful for hypernym identification. However, we observe that each cluster contains very few distinct hypernyms. For instance, about 83% of the clusters contain fewer than 5 hyper- nyms for ConceptNet-IsA in our experiments. Hy- pernyms can be seen as the collections of related word pairs, e.g., IsA(dog, animal), IsA(cat, ani- mal), IsA(horse, animal), ... etc. The piecewise projection matrices can hardly learn the inference between hyponyms and hypernyms but only mem- orize some words which serve as the hypernyms in the training data. Inevitably, the state-of-the-art models using piecewise projection learning face generalization problem and fail to predict unseen hypernyms correctly.</p><p>We design a novel Word Relation Autoen- coder (WRAE) framework, which adopts the con- ditional autoencoder structure (x → r → x ) that encodes hyponyms and reconstructs itself by de- coding from r = y − x. The weights of encoder are further tied with decoder which is imposed to learn how to separate the hypernym and the hy- ponym from the relation vectors and extract the hyponym x with the intention to optimize recon- struction loss, thus effectively mitigates the men- tioned generalization problem.</p><p>We summarize our main contributions as fol- lows: (1) We propose a novel, yet more general scenario for relation extraction to handle unseen hypernyms. <ref type="formula" target="#formula_1">(2)</ref> We propose an intuitive pollu- tion indicator that allows us to empirically mea- sure whether the model learns the inference be- tween a relation pair or not. <ref type="formula" target="#formula_2">(3)</ref> We propose a novel Word Relation Autoencoder (WRAE) which can effectively reduce pollution. We conduct thor- ough experiments to show that our model outper- forms the competitors, and can be applied to other hypernym-like relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fu et al. (2014) first apply projection learning for generalized hypernym extraction by learning a lin- ear transformation from a hyponym word embed- ding to the corresponding hypernym word vector. They further conduct piecewise projection learn- ing, i.e., learning a projection matrix for each clus- ter and harvest significant improvements by first applying k-means clustering. They perform train- ing with stochastic gradient descent methods, im- plying good potential for attaching different reg- ularizers for optimization. Several recent works also follow the schema as the one proposed by <ref type="bibr" target="#b0">Espinosa-Anke et al. (2016)</ref> and operate the sim- ilar model at the sense level and took advan- tage of domain clustering to discover hypernyms through domain adaption between different top- ics. <ref type="bibr" target="#b18">Yamane et al. (2016)</ref> focus on improving the performance through better cluster assignments by learning clustering and the projections jointly. <ref type="bibr" target="#b15">Ustalov et al. (2017)</ref> propose several regulariza- tion terms in addition to the original loss function ( <ref type="bibr" target="#b3">Fu et al., 2014</ref>) using extra synonym pairs or the asymmetric property of hypernym. <ref type="bibr" target="#b10">Nayak (2015)</ref> provides detailed technical studies on piecewise projection models.</p><p>Our work differs from all of them, as we empha- size on the setting that all hyponyms and hyper- nyms in testing vocabulary are not seen in train- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Piecewise Projection</head><p>Piecewise projection learning ( <ref type="bibr" target="#b3">Fu et al., 2014</ref>) serves as our baseline. The objective is to learn a relation transform from x to y on training pairs (x, y). Piecewise projection matrix Φ k is learned separately for each cluster, after applying k-means clustering on the offset of training using y − x be- tween each pair.</p><formula xml:id="formula_0">min Φ k 1 |C k | (x,y)∈C k xΦ k − y 2 2 ,<label>(1)</label></formula><p>where C k represents the size of the k th cluster. In addition, we also examine a simple solution of L2-penalized projection learning model which imposes a L2 constraint on Φ in Equation 1, i.e., α Φ k 2 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Relation Autoencoder (WRAE)</head><p>Our model takes the form of an autoencoder. As shown in Equation 2,</p><formula xml:id="formula_1">min Φ k 1 |C k | (x,y)∈C k x − xΦ k Φ * k 2 2 ,<label>(2)</label></formula><p>where xΦ k = y − x. Here we adopt the simplify- ing trick ( <ref type="bibr" target="#b5">Kodirov et al., 2017a</ref>) to tie with the con- straint ( <ref type="bibr" target="#b12">Ranzato et al., 2008</ref>) Φ * = Φ T . Note that the L2-norm regularization term is not necessary for WRAE to avoid overfitting since the constraint of Φ * = Φ T guarantees Φ 2 2 cannot be large oth- erwise the reconstruction loss will be bad. Also, the learning process is more efficient.</p><p>To release the constraint of xΦ k = y − x, the objective can be further split into two terms:</p><formula xml:id="formula_2">min Φ k 1 |C k | (x,y)∈C k (xΦ k − (y − x) 2 2 +λ (y − x)Φ T k − x 2 2 ),<label>(3)</label></formula><p>where λ is a weighting constant. We find that learning relation mapping from x → (y−x) instead of x → y effectively mitigates the pollution problem ( <ref type="bibr" target="#b7">Lazaridou et al., 2015)</ref>. A prediction is said to be polluted if the nearest neighbor of predictedˆypredictedˆ predictedˆy matches a hypernym ap- peared in training set. The operation fundamen- tally solves the cause of pollution since each pair of input and output becomes (x, y − x) instead of (x, y). Unlike projecting to a small number of tar- get y, the target y−x obviously differs from pair to pair thus avoiding simply overfitting the lexicons.</p><p>Conceptually, WRAE learns to extract the hy- ponym x from the relation vectors r = y − x to optimize on the reconstruction loss. By encourag- ing the projection to learn the relationship between a word pair, WRAE effectively mitigates the men- tioned generalized problem.</p><p>Our model is related to Semantic Autoencoder (SAE) ( <ref type="bibr" target="#b6">Kodirov et al., 2017b</ref>). With the latent re- lation directly associates with input x, WRAE can be regarded as a special conditional SAE where the condition is the input itself and is incorporated into the middle layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Different from the experimental setup in previous works ( <ref type="bibr" target="#b3">Fu et al., 2014;</ref><ref type="bibr" target="#b15">Ustalov et al., 2017;</ref><ref type="bibr" target="#b18">Yamane et al., 2016</ref>) that do not assume the candi- date hypernyms are unseen, in our experiments the vocabulary sets for training and testing are com- pletely disjoint, i.e., all vocabularies in testing are not seen in training at all. To further examine the generality of our model, we collect several hypernym-like relations listed in <ref type="table" target="#tab_2">Table 2</ref> from ConceptNet semantic graph. Con- sidering the property of these relations, we treat the head and tail words of a pair as the x and y for our models similar to hyponym and hypernym, respectively. Examples are in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We split the datasets with ratios 0.7, 0.2, and 0.1 for training, testing, and validation, respec- tively. For all results, we report the mean of 30 random splits. We test two different settings, one uses k-means clustering and one does not (k = 1). We tune the number of cluster k unsupervisedly with the Silhouette score <ref type="bibr" target="#b13">(Rousseeuw, 1987)</ref> on validation. The projection matrices are optimized with the Adam method ( <ref type="bibr" target="#b4">Kingma and Ba, 2014</ref>) with learning rate = 1e −3 . We adopt the GloVe ( <ref type="bibr" target="#b11">Pennington et al., 2014</ref>) 300d pre-trained word embeddings 1 which are trained on 6B token cor- pus (Wikipedia 2014 + Gigaword 5) with 400,000 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hit Rate</head><p>To evaluate the precision of returned hypernyms, we follow <ref type="bibr" target="#b15">Ustalov et al. (2017)</ref> and <ref type="bibr" target="#b5">Kodirov et al. (2017a)</ref> using the hit rate measure ( <ref type="bibr" target="#b2">Frome et al., 2013</ref>). We also adopt area under curve (AUC) measure which computes the averaged area under the l − 1 trapezoids of hit@l to take the ranks of ground truth into consideration:</p><formula xml:id="formula_3">AU C l = 1 2(l − 1) l−1 i=1 (hit@i + hit@(i + 1)),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Pollution</head><p>To evaluate the degree of pollution of the extracted hypernym, we adopt a metric similar to <ref type="bibr" target="#b7">Lazaridou et al. (2015)</ref>. A prediction is said to be polluted if the nearest neighbor of predictedˆypredictedˆ predictedˆy matches a hypernym appears in the training set, noted as a binary function pol 1 (ˆ y).</p><p>However, it is possible that ground truth unseen hypernyms are be very close to some seen hyper- nyms in Y train in real cases. We take ground truths  into consideration:</p><formula xml:id="formula_4">pol sof t l (ˆ y, y) = ρ · pol 1 (ˆ y), y ∈ Y test , ρ = 1, if N N l (y) ∩ Y train = φ, 2 n−1 l−1 − 1, otherwise, (5)</formula><p>where n is for the top n nearest neighbors (from 1 to l) of y that appears in Y train and ρ is a factor term exponentially decreases from 1 to 0 along with the increase of n therefore provides a smoother estimation. With pollution indications, one can understand to what degree the model suf- fers from overfitting on the seen examples. φ is the empty set. Note that it is reasonable to set l equal for both hit rate and soft pollution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results: Unseen General Hypernym-Like Relation Extraction</head><p>We report two sets of results for all models, one with clustering and one without (k = 1). As shown in <ref type="table" target="#tab_4">Table 3</ref>, WRAE outperforms the com- petitors significantly with and without clustering. The naive application of Equation 2 which set xΦ k = y, denoted as WRAE-Y, consistently ranks second. The y − x operation is crucial to avoid pollution thus guarantees the generalization power of the mapping. Apply simple L2-norm regular- ization Equation 1 for Proj., denoted as Proj.+L2, only slightly improves the performance. The re- sults in <ref type="table" target="#tab_4">Table 3</ref> supports our hypothesis that Proj. models deteriorate significantly for larger k, due to lack of training examples for hypernyms in each cluster. We prove that WRAE is effective against pollution. The role of regularizer is important for decoders to optimize towards better objective.</p><p>The negative effects derived from pollution im- pact accuracy. We observe severe pollution prob- lem in simple projection learning. Take IsA as example, in k = 1 group the pol 1 is about 71% for Proj., which implies about two of out of three returned predictions are data points from training data. Our WRAE reduces the pollution pol 1 to 60% and 12% after clustering. The improvement on accuracy supports that pollution indication re- flects the inherent overfitting problem. In general, results are consistent with our claims that pollution can be viewed as valid negative indicators.</p><p>Across the board, the performance should ben-efit from domain clustering if pollution is handled properly as the experiments showed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present an unseen hypernym extraction frame- work and analyze the pollution problem with this setup. Consequently we argue that only by us- ing unseen candidates in evaluation can truly test whether the model learns the true relation repre- sentation, instead of being polluted by the seen training examples. Future work includes relation discovery, which is to identify new relations be- sides hypernyms in an unsupervised manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Unseen relation extraction examples for IsA, 
AtLocation, CreatedBy, and PartOf (top row to bottom 
row) in ConceptNet. The answers are not appeared in 
training. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Relations from ConceptNet. For a relation pair 
x → y, x is the head and y is the tail. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance on ConceptNet relation dataset.  †: all results pass the hypothesis test against the other 
models with p &lt; 0.01. *: for IsA, we report hit@5 and hit@10. For hit rates, the higher the better. For pollution, 
the lower the better. 

</table></figure>

			<note place="foot" n="1"> https://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by Taiwan Ministry of Science and Technology (MOST) under grant number 106-2218-E-002-042 (machine learning related).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supervised Distributional Hypernym Discovery via Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Bradford Book. Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gs</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic Autoencoder for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition. Computer Vision Foundation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval &apos;12</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation, SemEval &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning hypernymy over word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Nayak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Stanford Univ</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse Feature Learning for Deep Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lan Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter J Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Representing General Relational Knowledge in ConceptNet 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on language resources and evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3679" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Negative Sampling Improves Hypernymy Extraction Based on Projection Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Arefyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1671" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to Distinguish Hypernyms and Co-Hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributional hypernym generation by jointly learning clusters and projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josuke</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Takatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 26th International Conference on Computational Linguistics (COLING 2016)</title>
		<meeting>26th International Conference on Computational Linguistics (COLING 2016)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1871" to="1879" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
