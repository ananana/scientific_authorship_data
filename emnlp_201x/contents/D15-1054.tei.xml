<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Embedding of Query and Ad by Leveraging Implicit Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Labs</orgName>
								<address>
									<addrLine>229 West 43rd Street</addrLine>
									<postCode>10036</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Labs</orgName>
								<address>
									<addrLine>229 West 43rd Street</addrLine>
									<postCode>10036</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Embedding of Query and Ad by Leveraging Implicit Feedback</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sponsored search is at the center of a multibil-lion dollar market established by search technology. Accurate ad click prediction is a key component for this market to function since the pricing mechanism heavily relies on the estimation of click probabilities. Lexical features derived from the text of both the query and ads play a significant role, complementing features based on historical click information. The purpose of this paper is to explore the use of word embedding techniques to generate effective text features that can capture not only lexical similarity between query and ads but also the latent user intents. We identify several potential weaknesses of the plain application of conventional word embedding methodolo-gies for ad click prediction. These observations motivated us to propose a set of novel joint word embedding methods by leveraging implicit click feedback. We verify the effectiveness of these new word embedding models by adding features derived from the new models to the click prediction system of a commercial search engine. Our evaluation results clearly demonstrate the effectiveness of the proposed methods. To the best of our knowledge this work is the first successful application of word embedding techniques for the task of click prediction in sponsored search.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sponsored search is a multibillion dollar market <ref type="bibr" target="#b7">(Easley and Kleinberg, 2010</ref>) that makes most search engine revenue and is one of the most successful ways for advertisers to reach their intended audiences. When search engines deliver results to a user, sponsored ad- vertisement impressions (ads) are shown alongside the organic search results <ref type="figure">(Figure 1</ref>). Typically the adver- tiser pays the search engine based on the pay-per-click model. In this model the advertiser pays only if the im- pression that accompanies the search results is clicked. The price is usually set by a generalized second-price (GSP) auction <ref type="bibr" target="#b8">(Edelman et al., 2005</ref>) that encourages advertisers to bid truthfully. An advertiser wins if the expected revenue for this advertiser, which is the bid <ref type="figure">Figure 1</ref>: Sponsored ads when "pizza" was searched at Yahoo! (www.yahoo.com).</p><p>price times the expected click probability (also know as click through rate, or CTR), is ranked the highest. The price the advertiser pays, known as cost-per-click (CPC), is the bid price for the second ranked advertiser times the ratio of the expected CTR between the sec- ond and first ranked advertisers. From this discussion it should be clear that CTR plays a key role in deciding both the ranking and the pricing of the ads. Therefore it is very important to predict CTR accurately.</p><p>The state of the art search engine typically uses a machine learning model to predict CTR by exploiting various features that have been found useful in prac- tice. These include historical click performance fea- tures such as historical click probability for the query, the ad, the user, and a combination of these; contextual features such as temporal and geographical informa- tion; and text-based features such as query keywords or ad title and description. Among these, historical click performance features often have the most predictive power for queries, ads and users that have registered many impressions. For queries, ads and users that have not registered many impressions, however, historical CTR may have too high a variance to be useful. <ref type="bibr" target="#b12">Hillard et al. (2011)</ref> observed that the number of impressions and clicks recorded on query-ad pairs have a very long tail: only 61% of queries has greater than three clicks. They also reported a drastic drop in the accuracy of the click prediction model when fewer historical observa- tions are available. Furthermore, fine-grained historical CTR information takes a huge amount of space, which makes it costly to maintain. On the other hand, text features are always readily available, and thus are par- ticularly useful for those cases for which there is insuf- ficient historical information.</p><p>Multiple researchers, for example <ref type="bibr" target="#b23">(Richardson, 2007;</ref><ref type="bibr" target="#b4">Cheng and Cant√∫-Paz, 2010)</ref>, reported the us- age of text features including simple lexical similarity scores between the query and ads, word or phrase over- laps and the number of overlapping words and charac- ters. Such features rely on the assumption that query-ad overlap is correlated with perceived relevance. While this is true to a certain extent, the use of simple lexical similarity cannot capture semantic information such as synonyms, entities of the same type and strong rela- tionships between entities (e.g. CEO-company, brand- model, part-of). Recently a host of studies on word embedding have been conducted; all map words into a vector space such that semantically relevant words are placed near each other in the space ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b20">Pennington et al., 2014;</ref><ref type="bibr" target="#b1">Baroni et al., 2014</ref>). The use of continuous word vectors has been shown to be helpful for a wide range of NLP tasks by better capturing both syntactic and semantic information than simple lexical features <ref type="bibr" target="#b24">(Socher et al., 2012a)</ref>.</p><p>No previous research on sponsored search has suc- cessfully used word embeddings to generate text fea- tures. In this paper, we explore the use of word em- beddings for click prediction. However, it is clear that conventional word embeddings (which solely rely on word co-occurrence in a context window) can only of- fer limited discriminative power because queries and ad text are typically very short. In addition, conven- tional word embeddings cannot capture user intents, preferences and desires. <ref type="bibr" target="#b32">Wang et al. (2013)</ref> showed that specific frequently occurring lexical patterns, e.g., x% off, guaranteed return in x days and official site, are effective in triggering users desires, and thus lead to significant differences in CTR. Conventional word em- beddings cannot capture these phenomena since they do not incorporate the implicit feedback users provide through clicks and non-clicks. These observations nat- urally lead us to leverage click feedback to infuse users' intentions and desires into the vector space.</p><p>The simplest way to harness click feedback is to train conventional word embedding models on a cor- pus that only includes clicked impressions, where each "sentence" is constructed by mixing the query and ad text. Having trained a word embedding model, we sim- ply take the average of word vectors of the query and ads respectively to obtain sentence (or paragraph) vec- tors, which in turn are used to compute the similarity scores between the query and ads. Our experiments show that this method does improve click prediction performance. However, this method has several po- tential weaknesses. First, the use of only clicked im- pressions ignores the large amount of negative signals contained in the non-clicked ad impressions. Second, the use of indirect signals (word co-occurrences) can be noisy or even harmful to our ultimate goal (accurate click prediction) when it is combined with direct sig- nals (impressions with click feedback). Third, without explicit consideration about the averaging step in the training process of word embedding models, a simple averaging scheme across word vectors may be a subop- timal. We therefore propose several joint word embed- ding models; all of these aim to put query vectors close to relevant ad vectors by explicitly utilizing both posi- tive and negative click feedback. We evaluate all these models against a large sponsored search data set from a commercial search engine, and demonstrate that our proposed models significantly improve click prediction performance.</p><p>The rest of this paper is organized as follows. In Sec- tion 2 we present a brief summary of related work. In Section 3 we give some background information on ad click prediction in sponsored search. In Section 4 we describe our methods. In Section 5 we discuss our ex- periments. We finish with some conclusions and future directions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Text features for predicting click probability There have been many studies on the use of text features for click prediction. For example, <ref type="bibr" target="#b5">Dembczynski et al. (2008)</ref> used a decision rule-based approach involv- ing such lexical features as the number of words in ad title and description, the number of segments and length of the ad URL, and individual words and terms in ad title and description. <ref type="bibr" target="#b4">Cheng et al. (2010)</ref> used a logistic regression model that used both historical click performance features and simple lexical features such as word or phrase overlap between query and ad title and description. <ref type="bibr" target="#b31">Trofimov et al. (2012)</ref> used a variant of boosted decision trees with similar features. <ref type="bibr" target="#b23">Richardson et al. (2007)</ref> specifically considered new ads (which lack historical click prediction data) and proposed to use the CTR for ad terms, the frequency of certain unigrams (e.g., dollar signs) and general En- glish usage patterns, and simple lexical distance be- tween the query and ads. In all this previous work, text features consisted only of surface-level text fea- tures. To the best of our knowledge, there is no previ- ous work adopting semantic-level text features for the purpose of click prediction, in particular word embed- dings to measure query-ad relevance. In a similar vein of research, <ref type="bibr" target="#b11">Grbovic et al. (2015)</ref> adopted word embed- dings to the task of query rewriting for a better match between queries and keywords that advertisers entered into an auction. Using the embeddings, semantically similar queries are mapped into vectors close in the em- bedding space, which allows expansion of a query via K-nearest neighbor search. Word embeddings for language processing Recently many NLP systems have obtained improved perfor- mance with less human engineering by adopting dis- tributed word representations <ref type="bibr" target="#b24">(Socher et al., 2012a</ref>).</p><p>In particular, neural word embedding techniques are now known to be effective in capturing syntactic and semantic relationships, and more computationally ef- ficient than many other competitors ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b20">Pennington et al., 2014;</ref><ref type="bibr" target="#b1">Baroni et al., 2014</ref>). On top of word embeddings, a new stream of research on sentence and paragraph embeddings has also emerged to tackle higher level tasks such as sentiment analy- sis, machine translation and semantic relatedness tasks. Recursive Neural Networks (RNNs) have been used for syntactic parsing <ref type="bibr" target="#b25">(Socher et al., 2012b;</ref><ref type="bibr" target="#b26">Socher et al., 2013;</ref><ref type="bibr" target="#b20">Socher et al., 2014;</ref><ref type="bibr" target="#b13">Irsoy and Cardie, 2014)</ref>. Long Short-Term Memory (LSTM) networks have been applied to machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b29">Sutskever et al., 2014</ref>) and semantic processing <ref type="bibr" target="#b30">(Tai et al., 2015)</ref>. Interestingly Convolutional Neural Net- works (CNNs), widely used for image processing, have recently emerged as a strong class of models for NLP tasks <ref type="bibr" target="#b15">(Kim, 2014;</ref><ref type="bibr">Blunsom et al., 2014)</ref>. As apposed to the models above, <ref type="bibr">PhraseVector (Le and Mikolov, 2014</ref>) takes a less structured but unsupervised approach by treating a piece of text as a token and performing word embedding-like training with an unlimited con- text window. None of this previous work exactly fits the click prediction task. Since queries and ads are much less structured than usual text, it is not attractive to use models with complex structures, such as RNNs, at the cost of speed and scalability. PhraseVector is less structured but it does not support compositionality, suf- fering from sparseness or requiring to train new vectors for each unseen query and ad. Interestingly, as reported in <ref type="bibr" target="#b30">(Tai et al., 2015</ref>), a simple averaging scheme (mean vector) was found to be very competitive to more com- plex models for high level semantic tasks despite its simplicity. These observations lead us to one of our models that aims to improve the mean vector method by directly optimizing mean vectors instead of word vectors.</p><p>Joint embedding to bridge multiple views. Multi- ple studies have explored the task of bringing multiple views into the same vector space. For example, there is now a large body of research on joint modeling of text and image information ( <ref type="bibr" target="#b10">Frome et al., 2013;</ref><ref type="bibr" target="#b14">Karpathy and Fei-Fei, 2014;</ref><ref type="bibr" target="#b20">Socher et al., 2014</ref>). The mul- timodal embedding space helps find appropriate align- ments between image regions and corresponding pieces of text description. Joint embedding has also been ap- plied to question answering ( <ref type="bibr" target="#b33">Wang et al., 2014</ref>) and se- mantic understanding ( <ref type="bibr" target="#b34">Yang et al., 2014</ref>). In contrast to the tasks above, there is no natural component-wise correspondence between queries and ads; instead the relationship is more implicit and pragmatic. Because of this, our methods rely on global rather than component- level signals for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Click Prediction Model</head><p>We first present a high-level description of sponsored search. The process consists of several stages. First, given a user query, a list of candidate ads are retrieved, either by exactly matching query terms to the bid terms of the advertiser, or by first using query term expansion to obtain a longer list of matched ads. Some candi- date ads may be filtered out based on metrics such as ad quality. Then, a click prediction model scores the candidate ads to estimate how likely it is that each will be clicked. This click probability serves a crucial role both in the user experience and in the revenue for the search engine. The ads with the highest click probabil- ities are placed in the search results page. The price- per-click for each ad shown is determined based on the click probabilities and the GSP auction.</p><p>Our baseline click prediction model is formulated as a supervised learning problem. Specifically we use Lo- gistic Regression (LR) since LR is well suited for prob- ability estimation. Given a variety of features, the prob- ability of a click is expressed as:</p><formula xml:id="formula_0">p(c|q, a, u) = 1 1 + exp i w i f i (q, a, u) ,<label>(1)</label></formula><p>where c ‚àà {1, 0} is the label (1: click or 0: non-click), f i (q, a, u) is the ith feature derived for query-ad-user triple (q, a, u) and w i is the associated weight. The model is trained using a stochastic gradient descent al- gorithm on a per impression basis with l 1 regularization to avoid overfitting. An accurate LR model relies greatly on the effective- ness of its features. Our baseline model is furnished with a rich set of features that are typically used in commercial search engines. The first feature type is based on the historical CTR of user, query, ad triples (if there is enough historical information on this). We use two groups of features of this type: COEC based features and user factor features. The second feature type is based on query and ad text.</p><p>Due to the significant decrease of CTR depending on the ad position, it has become common practice to use position-normalized CTR (a.k.a. Clicks Over Expected Clicks):</p><formula xml:id="formula_1">COEC = p c p p i p * CT R p ,<label>(2)</label></formula><p>where the numerator is the total number of clicks re- ceived by the configuration of interest; the denomi- nator is the expected clicks (ECs) that an average ad would receive after i p times impressions at position p, and CT R p is the average CTR at position p, cal- culated over all queries, ads and users. We use user- independent features derived from COEC statistics for specific query-ad pairs. However, many impressions are needed for these statistics to be reliable and there- fore data for specific query-ad pairs can be sparse and noisy (only around 70% of queries and about 50% of query-URL, query-bid term pairs have historical CTR).</p><p>To alleviate this problem, additional COEC statistics over aggregations of queries or ads are also used. The exact description of these aggregations is beyond the scope of this paper, but briefly we exploit the catego- rization of the ads in ad groups, campaigns, and ac- counts defined by advertisers.</p><p>Since it is well known that personalization features are crucial to obtain accurate click prediction mod- els <ref type="bibr" target="#b4">(Cheng and Cant√∫-Paz, 2010)</ref>, we also use features that measure user factors relating to CTR. The user click feedback features capture the inclination of indi- vidual users to click on ads in general. The user-query click feedback features indicate the propensity of users to click for certain queries or groups of queries. Fi- nally user-ad features dictate the user preferences on certain ads or advertisers. However the data sparseness problem becomes even more serious when it comes to user-specific features (we use a threshold of 100 for statistical confidence). For example, only about 5% of user-URL pairs and 1% of user, query, URL triples have historical CTR information. Therefore a set of segment-level features can be extracted as back-off fea- tures where users, queries and ads are clustered into groups, and group level historical CTRs are collected.</p><p>Our second major feature type involves the lexical similarity between query and ad text. These text fea- tures assume that users are more likely to click on ads that seem to be relevant to the query, and that per- ceived relevance is correlated with the degree of query- ad overlap. These features include the number of over- lapping words and characters in query-ad URL, query- ad title, and query-ad description, and the number of words and characters in the query. The discrimination power of simple lexical features is relatively limited be- cause query and ad text are typically very short.</p><p>Finally, we use other contextual features that are helpful in predicting the click probabilities.; for exam- ple, time of day, day of week, and geographic infor- mation. To model interactions among features, some features are selected by domain knowledge to be con- joined. All together, our baseline model utilize a com- prehensive set of historical CTR, lexical, and contex- tual features (over a hundred features in total). The fact that this baseline model is highly optimized makes the subsequent performance improvement from our pro- posed algorithm meaningful. This baseline model is used in production in part of a major search platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint Embedding for Click Prediction</head><p>We now describe several methods that jointly embed words in both the query and the ads into the same vec- tor space. In our experiments, we incorporate these methods as features in our click prediction model. We start by defining the notation used in this section. A sponsored search dataset D is a set of tuples for each ad impression (q, t, d, y) where q ‚â° {q j } is a multi- word query string, t ‚â° {t k }, d ‚â° {d l } are multiword ad title and description strings, and y is a binary in- dicator for whether the ad is clicked. We have two choices in defining the vocabulary V from which words are drawn: we can use a unified vocabulary for both query and ads or define a separate vocabulary for each -V ‚â° V q ‚äï V a . In our initial experiments the uni- fied vocabulary constantly yielded better performances, thus we always use the unified vocabulary here. We use bold letters q j , t k , d l to denote the corresponding em- bedding representations of {q j , t k , d l }. Finally we use W to represent the vocabulary matrix; in W each col- umn is a word vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exploiting word2vec embedding</head><p>Typically word embeddings are learned from a given text corpus through implicit supervision of predicting the current word given a window of its surrounding text or predicting each word in the window of the current word. The former approach is known as continuous bag-of-words (CBOW) and the latter Skip-gram. For simplicity's sake we use negative-sampling for training word embedding models ( <ref type="bibr" target="#b19">Mikolov et al., 2013b</ref>). More formally we define the binary conditional probability for a pair of words (v, w):</p><formula xml:id="formula_2">1 p(v, w) = 1 1 + exp(‚àív T w) ,<label>(3)</label></formula><p>The CBOW algorithm learns word embeddings by minimizing the following logloss of each impression i with regard to W :</p><formula xml:id="formula_3">CB i (W ) = ‚àí log p(w i , ¬µ C ) ‚àí v‚ààN (wi) log (1 ‚àí p(v, ¬µ C )) ,<label>(4)</label></formula><p>where the context C of a word w i comes from a win- dow of size k around the word in a sentence of n words w 1 ,. . .,w n : C = w i‚àík ,. . .,w i‚àí1 ,w i+1 ,. . .,w i+k . ¬µ C is the averaged context vector of w i ; ¬µ C = 1 |C| v‚ààC v. N (w i ) is the set of negative examples which is drawn according to the unigram distribution of the corpus raised to the 3/4th power. Similarly to ( <ref type="bibr" target="#b19">Mikolov et al., 2013b</ref>), we adopt a dynamic window size -for each word the actual window size is sampled uniformly from 1, . . . , k.</p><p>Similarly the Skip-gram algorithm minimizes the logloss of each impression i with regard to W :</p><formula xml:id="formula_4">SK i (W ) = ‚àí v‚ààC log p(w i , v) ‚àí v‚ààN (wi) log (1 ‚àí p(w i , v)) .<label>(5)</label></formula><p>In our first word embedding model that incorporates click feedback, we construct a corpus by taking only clicked impressions from D and then mixing (q, t, d) of each impression into a sentence. Then we simply train CBOW and Skip-gram models on the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Joint word embedding using click feedback</head><p>Although the CBOW and Skip-gram models trained on a specially constructed corpus can capture signals from both click feedback and word co-occurrence, they have a couple of drawbacks. First, by ingesting only clicked impressions we "waste" the large amount of negative signals contained in the non-clicked impressions. Sec- ond, the incorporation of indirect signals such as word co-occurrences can be rather harmful for achieving ac- curate click prediction as these are very noisy com- pared to direct signals such as click feedback.</p><p>For our second word embedding model that incorpo- rates click feedback, we define a joint word embedding model that minimizes the following weighted logloss:</p><formula xml:id="formula_5">JW i (W ) = Œ∑(y i ) l(y i , q i , t i ) + l(y i , q i , d i ) ,<label>(6)</label></formula><p>where the component loss function l(¬∑, ¬∑, ¬∑) is defined as follows:</p><formula xml:id="formula_6">l(y, a, b) = |a| k |b| l ‚àí y log p(a k , b l ) ‚àí (1 ‚àí y) log (1 ‚àí p(a k , b l )) .<label>(7)</label></formula><p>Here Œ∑(y i ) is a function that returns a small weight Œ∑ only to negative examples:</p><formula xml:id="formula_7">Œ∑(y i ) = Œ∑, if y i = 0, 1, otherwise.<label>(8)</label></formula><p>The fact that the user did not click on an ad does not necessarily mean that the ad is not what the user wanted; it often means that the ad is just less favored than the clicked ads ( <ref type="bibr" target="#b22">Rendle et al., 2009)</ref>. Since the scope of this work is restricted to estimating click prob- ability on a per-impression basis, we adopt a weighting scheme rather than optimizing rank-based loss over a set of related impressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint mean vector optimization</head><p>The joint word embedding models defined in the pre- vious sections do not define how to aggregate a vari- able length sequence of word vectors into a sentence (or paragraph) vector to facilitate the computation of sentence-level similarity scores. One approach to this aggregation task is mean vector: simply average the word-level embeddings across the sentence or para- graph. As noted by <ref type="bibr" target="#b30">(Tai et al., 2015)</ref>, this approach is a strong competitor to more complex models such as RNNs or LSTMs despite its simple composition method. However, this method may generate subop- timal sentence vectors. With weight logloss, we aim to optimize sentence vectors instead of individual word vectors:</p><formula xml:id="formula_8">JM i (W ) = Œ∑(y i ) ¬Ø l(y i , q i , t i ) + ¬Ø l(y i , q i , d i ) ,<label>(9)</label></formula><p>where the component loss function ¬Ø l(¬∑, ¬∑, ¬∑) is defined as follows:</p><p>¬Ø l(y, a, b) = ‚àí y log p(¬µ a , ¬µ b )</p><formula xml:id="formula_9">‚àí (1 ‚àí y) log (1 ‚àí p(¬µ a , ¬µ b )) ,<label>(10)</label></formula><p>where ¬µ s returns the average vector for the multiword string s, i.e. ¬µ s = 1 |s| |s| k s k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Data The data used in our experiments were collected from a random bucket of the Yahoo! sponsored search traffic logs for a period of 4 weeks in October 2014.</p><p>In the data there are approximately 65 million unique users, 150 million unique queries and 12 million unique ads. There are approximately 985 million ad impres- sion events in total. We split the data into 3 partitions with respect to time: the first 14 days' data are used for training word embedding models, the next 7 days' data for training click prediction models, and the last 7 days' data for testing. <ref type="table">Table 1</ref> presents more detailed statistics for the data. Models We are interested in evaluating the usefulness of different word embedding models as features for click prediction, and already have a very good baseline system for this task. Consequently, in all experiments below, we used the same personalized historical CTRs, contextual and lexical features as the baseline system described in Section 3. We tested models with the fol- lowing additional features derived from word embed- dings:</p><p>1. cosine similarity between the mean vectors of the query and ad title 2. cosine similarity between the mean vectors of the query and ad description 3. sum of 1 and 2 4. sigmoid function value for the dot product of the mean vectors of the query and ad title 5. sigmoid function value for the dot product of the query and ad description 6. sum of 4 and 5</p><p>All these continuous features are quantized into 50 bins. We compared five different word embedding al- gorithms:</p><p>1. Skip-gram trained on Wikipedia (SK-WIKI)</p><p>2. Skip-gram trained on clicked impressions (SK- CI), see Section 4.1 <ref type="table" target="#tab_1">Embedding Train   Test  Total  Number of impressions  489M  252M  244M  985M  Number of unique queries  82M  46M  45M  150M  Number of unique ads  8M  6M  6M  12M  Number of unique users  40M  25M  25M  65M  Number of unique words in query  6757K  6039K 5774K 16028K  Number of unique words in ad title  3371K  2586K 2557K</ref>  We used the skip-gram mode of word2vec 2 with a win- dow size of 5 and negative sampling to train the SK- WIKI model. For all other models we used in-house implementation which employs AdagradRDA (Duchi et al., 2011) to minimize the loss functions introduced in Section 4. The dimension of a word vector is set to 100 for all algorithms <ref type="bibr">3</ref> . We removed a set of pre- fixed stop-words and all words occurring fewer than 100 times; the resulting vocabulary comprised 126K unique words. To process our web-scale data, we im- plemented a multithread program where each thread randomly traverses over a partition of the data D to compute gradients and update the matrix W stored in a shared memory. For computational efficiency, the hog- wild lock-free approach <ref type="bibr" target="#b21">(Recht et al., 2011</ref>) is used. We set Œ∑ in Eq. 8 and Eq. 9 to 0.2 through grid search based on two-fold cross-validation. This small value indeed verifies the idea of weak negative feedback for unclicked impressions. In order to circumvent the severe influence of ad po- sition on the click prediction model, only the impres- sions that are placed at the top position were used for training the click prediction model. Note that CTR at other positions can be derived from that of the ad at the top position through scaling. <ref type="bibr" target="#b5">Dembczynski et al. (2008)</ref> showed that CTR can be decomposed as a product of the probability of an ad getting clicked given its being seen and the probability of an ad being seen at a particular position.</p><p>Results We ordered query-ad pairs by the predicted score to compute AucLoss (i.e. 1 -AUC where AUC is the area under the Receiver Operating Characteris- tic (ROC) curve). The ROC AUC is known to have a correlation with the quality of ranking by the predicted score <ref type="figure">(Fawcett, 2006)</ref>; thus is one of the most important 2 Available at https://code.google.com/p/word2vec/ 3 Higher vector dimensions such as 200 were also tried but did not give a significant improvement.   <ref type="table" target="#tab_1">(Table 2)</ref> show that the use of features derived from our proposed word embed- ding models significantly reduce AucLoss by up to 4.1%. For commercial search engines which have a very strong baseline AucLoss, a reduction of 1% can be considered large <ref type="bibr" target="#b17">(McMahan et al., 2013)</ref>. Moreover, as expected, the more issues (as identified in Section 4) an algorithm addresses, the better performance it achieves. From the comparison between the SK-WIKI model and the rest, we can recognize the importance of word em- bedding models specialized to domain text and super- vision signals. Also the difference between the CB-CI (SK-CI) model and the JIWV model indicates the nois- iness of indirect signals such as word co-occurrence compared to direct signals like click feedback. Finally the gap between the JIWV and JWV models highlights the significance of considering compositionality in the word embedding training process. Eyeballing the most similar words to several queries in the vector space is often helpful for getting some sense about how different methods influence the result- ing vector spaces. <ref type="table" target="#tab_2">Table 3</ref> lists the top 20 most similar words to the query "metal watch." The top words for the SK-WIKI model are not semantically interesting in terms of capturing the intent or desires. This clearly shows the limitation of word embedding methods that only rely on indirect signals (i.e. word co-occurrences) from a generic text corpus (e.g. Wikipedia) for spon- sored search click prediction. Noticeably the methods 487 SK-WIKI CB-CI JIWV JMV watch (0.733) metal (0.718) wwhl (0.711) tacticalwatch.com (0.701) grind (0.687) previews (0.652) cbs (0.704) watchrepairsusa.com (0.695) grease (0.682) yidio.com (0.648) station (0.668) omegas (0.689) kites (0.676) episodes (0.633) putlocker.com (0.662) watchco.com (0.682) hammer (0.675) steel (0.626) iwatch (0.659) wach (0.670) spinning (0.672) whatch (0.615) kidizoom (0.658) station (0.656) flashing (0.671) bobcometal.com (0.586) freesports360.com (0.658) shockwarehouse.com (0.628) trash (0.670) ridiculousness (0.582) tedtalks (0.655) freesports360.com (0.618) flame <ref type="formula" target="#formula_5">(0.669)</ref> www.abc.com (0.574) 11/10c (0.650) akribos (0.616) flaming (0.665) a&amp;e (0.573) movie2k (0.640) 18mm (0.616) cigar (0.664) utube (0.570) nfl.com/now (0.637) narutoget.com (0.614) home-made (0.663) instantly (0.565) criminalsgonewilddvd (0.631) watchstation.com (0.611) glow (0.662) khoobsurat (0.562) espnnfllive.com (0.631) authenticwatches.com (0.610) bouncing (0.662) outnumbered (0.561) <ref type="formula" target="#formula_5">.660)</ref> premiere (0.556) bubble (0.623) whatch (0.600) shoot (0.658) films (0.555) wewood (0.621) $109.99 (0.597) scoop (0.653) stainless (0.554) westclox (0.617) tirebuyer.com (0.596) noises (0.652) fabricators (0.550) potlocker (0.613) skagen.com (0.588) rocking (0.651) fabrication (0.550) nickelodeon (0.613) wewood (0.584)  <ref type="table">Table 4</ref>: Top 20 most similar words to "princess costumes" that incorporate click feedback find more words related to products, services or websites instead of just con- ceptuatlly related words. Given that real products or services can be regarded as the best possible surrogates to user intents and desires, this demonstrates the effec- tiveness of our methods. This tendency gets stronger as a method takes into account both positive and negative click feedback.</p><formula xml:id="formula_10">foxsports1 (0.623) interrupted (0.603) filler (0.662) itv (0.559) viooz (0.623) criminalsgonewilddvd (0.601) smoke (0</formula><p>Another very interesting observation comes from the fact that none of the methods except JMV successfully captures the composite meaning of "metal watch"; they tend to either find related words separately for each query word (e.g. "watch" is strongly associated to the sense of watching something like movie or other types of video) or find totally unrelated words (particularly SK-WIKI). This demonstrates that it is crucial to ad- dress compositionality in the very process of learning word vectors. <ref type="table">Table 4</ref> shows the top 20 most similar words to the query "princess costumes." In this example we can spot another surprising result. The JMV model pushes a lot of price related expressions to the top 4 . This may im- ply that many parents search for lower cost costumes, clearly showing a clear psychological desire in the fi- nancial dimension. This observation confirms the find- ings in ( <ref type="bibr" target="#b32">Wang et al., 2013</ref>) about the significant role of certain ad expressions in triggering users' psychologi- cal desires. We also note that the CB-CI model returns a lot of misspells for "costume(s)", which would not be possible with simple lexical features of the baseline system. A close look at this example generally con- firms the observations we made for the previous ex-  <ref type="table">Table 5</ref>: Top 20 most similar words to "game for kids"</p><p>ample. Finally <ref type="table">Table 5</ref> shows top the 20 most similar words for the query "game for kids." Once again we found the same analysis holds for this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we explored the use of word embedding techniques to overcome the shortcomings of traditional lexical features for ad click prediction in sponsored search. We identified several potential weaknesses of the plain application of conventional word embedding methodologies: the lack of the right machinery to har- ness both positive and negative click feedback, the lim- ited utility of pure word co-occurrence signals, and no consideration of vector composition in the word em- bedding training process. We proposed a set of new implicit feedback-based joint word embedding meth- ods to address those issues. We evaluated the new word embedding methods in the context of a very good base- line click prediction system, on a large scale data set collected from Yahoo! search engine logs. Our exper- imental results clearly demonstrate the effectiveness of the proposed methods. We also presented several ex- amples for qualitative analysis to advance our under- standing on how each algorithm really contributes to the improved performance. To the best of our knowl- edge this work is the first successful application of word embedding techniques for the sponsored search task. There are multiple interesting research directions for future work. One of these directions is to extend the vocabulary by identifying significant phrases (as well as words) before training word vectors. <ref type="bibr" target="#b12">Hillard et al. (2011)</ref> employed Conditional Random Fields to di- vide queries with multiple words into segments and collected historical CTR on the segment level. We also like to investigate more structured embedding methods such as RNNs (probably for ad descriptions). In case the computational cost of such methods are too high to be practical for sponsored search, we can employ them only for a small fraction of ads filtered by faster meth- ods.</p><p>It may be possible to deal with the implicit nega- tive feedback of unclicked ad impressions in a more principled way by adopting ranking-based loss func- tions. However, this is only possible with the extra cost of identifying and aggregating related ads into a single transaction.</p><p>Though not directly related to NLP, yet another promising direction is to jointly embed not only text data but also a variety of user activities (e.g., organic search results, mobile app usages, other daily activities) all together in the same vector space. Since many of the different sources contain their own unique information, we might be able to obtain a much better understand- ing about the user state and intent through this rich joint embedding space. Joint embedding with rich informa- tion can also help us to perform automatic clustering of users, eventually leading to powerful smoothing meth- ods for personalized historical CTR statistics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparative evaluation results in AucLoss re-
duction from the baseline system 

Our experimental results </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Top 20 most similar words to "metal watch" 

SK-WIKI 
CB-CI 
JIWV 
JMV 
costumes (0.762) 
princess (0.833) 
costumes (0.831) 
new-costumes.com (0.815) 
bride (0.723) 
costumed (0.814) 
wonder (0.784) 
coustume (0.808) 
princesses (0.722) 
customes (0.808) 
sweetiegames.com (0.782) 
namefully.com (0.800) 
serene (0.708) 
costume (0.806) 
cleopatra (0.753) 
$35.90 (0.789) 
highness (0.676) 
costomes (0.806) 
new-costumes.com (0.752) 
2-days (0.781) 
bess (0.674) 
costimes (0.803) 
girls.simple (0.749) 
$36.90 (0.776) 
princess]] (0.671) 
m.buycostumes.com (0.800) 
werewolf (0.749) 
costume (0.766) 
attire (0.670) 
custumes (0.796) 
yoshi (0.747) 
$28.90 (0.764) 
princess (0.662) 
officialprincesscostumes.com (0.792) 
leia (0.747) 
princess (0.758) 
dresses (0.662) 
custume (0.789) 
merida (0.742) 
cistumes (0.756) 
highness (0.658) 
coustome (0.789) 
$49.90 (0.735) 
spider-woman (0.756) 
jewels (0.652) 
cosyumes (0.787) 
low-budget (0.727) 
the-wristband-factory.com (0.750) 
wedding (0.651) 
coustume (0.786) 
babies (0.727) 
$17.90 (0.750) 
robes (0.648) 
coustums (0.786) 
fembot (0.726) 
sugarsmascotcostumesÀô com (0.748) 
prince (0.644) 
buycostumes.com (0.785) 
costums (0.722) 
cotumes (0.748) 
clothes (0.644) 
codtume (0.784) 
$3.90 (0.721) 
coneheads (0.747) 
dancing (0.644) 
leia (0.784) 
hermione (0.718) 
$23.90 (0.739) 
sophie (0.640) 
coustumes (0.784) 
supergirl (0.715) 
$7.90 (0.739) 
consorts (0.639) 
costums (0.783) 
toothless (0.713) 
costomes (0.738) 
glamorous (0.639) 
coatumes (0.782) 
starlord (0.709) 
$19.90 (0.737) 

</table></figure>

			<note place="foot" n="1"> We use only a single vector for a word unlike (Mikolov et al., 2013b) where two vectors (&quot;input&quot; and &quot;output&quot;) for a word are used. This halves the required space to store vectors without performance loss.</note>

			<note place="foot" n="3">. CBOW trained on clicked impressions (CB-CI), see Section 4.1 4. Joint individual word vector embedding (JIWV), see Section 4.2</note>

			<note place="foot" n="4"> We have not tried any normalization for numbers but it might be worth doing given the important role they play.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germ√°n</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<publisher>Nal Kalchbrenner</publisher>
			<biblScope unit="page">489</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized click prediction in sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Cant√∫-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10</title>
		<meeting>the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting ads click-through rate with decision rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kotlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW 08</title>
		<meeting>WWW 08</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<title level="m">Networks, Crowds, and Markets: Reasoning about a Highly Connected World</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thank</forename><forename type="middle">Drew</forename><surname>Fudenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Kaplow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Milgrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muriel</forename><surname>Niederle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Pakes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>American Economic Review, 97</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context-and content-aware embeddings for query rewriting in sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The sum of its parts: reducing sparsity in click estimation with query segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Hillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leggetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Cant√∫-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rukmini</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="336" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: a View from the Trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharat</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnar</forename><surname>Mar Hrafnkelsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining-KDD &apos;13</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining-KDD &apos;13<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page">1222</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09<address><addrLine>Arlington, Virginia, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting clicks: Estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International World Wide Web Conference</title>
		<meeting>the 16th International World Wide Web Conference</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning for nlp (without magic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial Abstracts of ACL 2012, ACL &apos;12</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using boosted trees for click-through rate prediction for sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Kornetova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valery</forename><surname>Topinskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy, ADKDD &apos;12</title>
		<meeting>the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy, ADKDD &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Psychological advertising: exploring user psychology for click prediction in sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="563" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haechang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
