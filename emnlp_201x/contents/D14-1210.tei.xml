<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent-Variable Synchronous CFGs for Hierarchical Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Saluja</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>scohen@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>Edinburgh</addrLine>
									<postCode>EH8 9AB</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent-Variable Synchronous CFGs for Hierarchical Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1953" to="1964"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Data-driven refinement of non-terminal categories has been demonstrated to be a reliable technique for improving mono-lingual parsing with PCFGs. In this paper , we extend these techniques to learn latent refinements of single-category synchronous grammars, so as to improve translation performance. We compare two estimators for this latent-variable model: one based on EM and the other is a spectral algorithm based on the method of moments. We evaluate their performance on a Chinese-English translation task. The results indicate that we can achieve significant gains over the baseline with both approaches , but in particular the moments-based estimator is both faster and performs better than EM.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Translation models based on synchronous context- free grammars (SCFGs) treat the translation prob- lem as a context-free parsing problem. A parser constructs trees over the input sentence by pars- ing with the source language projection of a syn- chronous CFG, and each derivation induces trans- lations in the target language <ref type="bibr" target="#b2">(Chiang, 2007)</ref>. However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the "right" tree structure for a grammatical sentence, no such intuitions are available for synchronous deriva- tions, and so learning the "right" grammars is a central challenge.</p><p>Of course, learning synchronous grammars from parallel data is a widely studied problem <ref type="bibr" target="#b33">(Wu, 1997;</ref><ref type="bibr" target="#b1">Blunsom et al., 2008;</ref><ref type="bibr" target="#b20">Levenberg et al., 2012</ref>, inter alia). However, there has been less exploration of learning rich non-terminal cat- egories, largely because previous efforts to learn such categories have been coupled with efforts to learn derivation structures-a computationally formidable challenge. One popular approach has been to derive categories from source and/or target monolingual grammars ( <ref type="bibr" target="#b11">Galley et al., 2004;</ref><ref type="bibr" target="#b36">Zollmann and Venugopal, 2006;</ref><ref type="bibr" target="#b14">Hanneman and Lavie, 2013)</ref>. While often successful, accurate parsers are not available in many languages: a more ap- pealing approach is therefore to learn the category structure from the data itself.</p><p>In this work, we take a different approach to previous work in synchronous grammar induc- tion by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing ( <ref type="bibr" target="#b29">Petrov et al., 2006</ref>; <ref type="bibr" target="#b21">Liang et al., 2007;</ref><ref type="bibr" target="#b6">Cohen et al., 2014)</ref>, where it reliably yields state- of-the-art phrase structure parsers based on gen- erative models, but we are the first to apply it to translation.</p><p>We first generalize the concept of latent PCFGs to latent-variable SCFGs ( §2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Sub- sequently, two methods for parameter estimation are presented ( §4): a spectral approach based on the method of moments, and an EM-based likeli- hood maximization. Results on a Chinese-English evaluation set ( §5) indicate significant gains over baselines and point to the promise of using latent- variable synchronous grammars in conjunction with a smaller, simpler set of rules instead of un- wieldy and bloated grammars extracted via exist- ing heuristics, where a large number of context- independent but un-generalizable rules are uti- lized. Hence, the hope is that this work pro-motes the move towards translation models that directly model the conditional likelihood of trans- lation rules via (potentially feature-rich) latent- variable models which leverage information con- tained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies ( <ref type="bibr" target="#b19">Koehn et al., 2003)</ref> from non-hierarchical phrase-based translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Latent-Variable SCFGs</head><p>Before discussing parameter learning, we in- troduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference al- gorithm for marginalizing over latent states.</p><p>We extend the definition of L-PCFGs <ref type="bibr" target="#b23">(Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b29">Petrov et al., 2006</ref>) to syn- chronous grammars as used in machine transla- tion <ref type="bibr" target="#b2">(Chiang, 2007)</ref>. A latent-variable SCFG (L- SCFG) is a 6-tuple (N , m, n s , n t , π, t) where:</p><p>• N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based transla- tion (HPBT), the set consists of only two sym- bols, X and a goal symbol S.</p><p>• [m] is the set of possible hidden states associ- ated with NTs. Aligned pairs of NTs across the source and target languages share the same hid- den state. • [n s ] is the set of source side words, i.e., the source-side vocabulary, with [n s ] ∩ N = ∅. • [n t ] is the set of target side words, i.e., the target-side vocabulary, with [n t ] ∩ N = ∅. • The synchronous production rules compose a set R = R 0 ∪ R 1 ∪ R 2 :</p><p>• Arity 2 (binary) rules (R 2 ):</p><formula xml:id="formula_0">a(h 1 ) → α 1 b(h 2 )α 2 c(h 3 )α 3 , β 1 b(h 2 )β 2 c(h 3 )β 3 or a(h 1 ) → α 1 b(h 2 )α 2 c(h 3 )α 3 , β 1 c(h 2 )β 2 b(h 3 )β 3 where a, b, c ∈ N , h 1 , h 2 , h 3 ∈ [m], α 1 , α 2 , α 3 ∈ [n s ] * and β 1 , β 2 , β 3 ∈ [n t ] * .</formula><p>• Arity 1 (unary) rules (R 1 ):</p><formula xml:id="formula_1">a(h 1 ) → α 1 b(h 2 )α 2 , β 1 b(h 2 )β 2 where a, b ∈ N , h 1 , h 2 ∈ [m], α 1 , α 2 ∈ [n s ] * and β, β 2 ∈ [n t ] * .</formula><p>• Pre-terminal rules (R 0 ):</p><formula xml:id="formula_2">a(h 1 ) → α, β where a ∈ N , α ∈ [n t ] * and β ∈ [n s ] * .</formula><p>Each of these rules is associated with a proba- bility t(a(h 1 ) → γ|a, h 1 ) where γ is the right- hand side (RHS) of the rule.</p><p>• For a ∈ N , h ∈ [m], π(a, h) is a parameter specifying the root probability of a(h).</p><p>A skeletal tree (s-tree) for a sentence is the set of rules in the synchronous derivation of that sen- tence, without any additional latent state informa- tion or decoration. A full tree consists of an s- tree r 1 , . . . , r N together with values h 1 , . . . , h N for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of syn- chronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category gram- mar introduced by <ref type="bibr" target="#b2">Chiang (2007)</ref> for HPBT in or- der to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally ap- plicable in other scenarios, e.g., the extended tree- to-string transducer (xRs) formalism <ref type="bibr" target="#b16">(Huang et al., 2006;</ref><ref type="bibr" target="#b13">Graehl et al., 2008</ref>) commonly used in syntax-directed translation, and phrase-based MT ( <ref type="bibr" target="#b19">Koehn et al., 2003</ref>).</p><p>Marginal Inference with L-SCFGs. For a pa- rameter t of rule r, the latent state h 1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are asso- ciated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interac- tions, which can be compactly represented as a 3 rd -order tensor in the case of a binary rule, a ma- trix (i.e., a 2 nd -order tensor) for unary rules, and a vector for pre-terminal (lexical) rules. Prefer- ences for certain outside-inside tree combinations are reflected in the values contained in these tensor structures. In this manner, we intend to capture in- teractions between non-local context of a phrase, which can typically be represented via features de- fined over outside trees of the node spanning the phrase, and the interior context, correspondingly defined via features over the inside trees. We re- fer to these tensor structures collectively as C r for rules r ∈ R, which encompass the parameters t.</p><p>For r ∈ R 0 : C r ∈ R m×1 ; similarly for r ∈ R 1 : C r ∈ R m×m and r ∈ R 2 : C r ∈ R m×m×m . We also maintain a vector C S ∈ R 1×m corresponding to the parameters π(S, h) for the Inputs: Sentence f1 . . . fN , L-SCFG (N , S, m, n), param- eters C r ∈ R (m×m×m) , ∈ R (m×m) , or ∈ R (m×1) for all r ∈ R, C S ∈ R (1×m) , hypergraph H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data structures:</head><p>For each node q ∈ H:</p><p>• α(q) ∈ R m×1 is a column vector of inside terms.</p><p>• β(q) ∈ R 1×m is a row vector of outside terms.</p><p>• For each incoming edge e ∈ B(q) to node q, µ(e) is a marginal probability for edge (rule) e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm:</head><p>Inside Computation For nodes q in topological order in H, α(q) = 0 For each incoming edge e ∈ B(q),</p><formula xml:id="formula_3">tail = t(e), rule = r(e) if |tail| = 0, then α(q) = α(q) + C rule else if |tail| = 1, then α(q) = α(q) + C rule ×1 α(tail0) else if |tail| = 2, then α(q) = α(q) + C rule ×2 α(tail1) ×1 α(tail0) Outside Computation For q ∈ H, β(q) = 0 β(goal) = C S For q in reverse topological order in H, For each incoming edge e ∈ B(q), tail = t(e), rule = r(e) if |tail| = 1, then β(tail0) = β(tail0) + β(q) ×0 C rule else if |tail| = 2, then β(tail0) = β(tail0) + β(q) ×0 C rule ×2 α(tail1) β(tail1) = β(tail1) + β(q) ×0 C rule ×1 α(tail0) Edge Marginals Sentence probability g = α(goal) × β(goal) For edge e ∈ H, head = h(e), tail = t(e), rule = r(e) if |tail| = 0, then µ(e) = (β(head) ×0 C rule )/g else if |tail| = 1, then µ(e) = (β(head) ×0 C rule ×1 α(tail0))/g else if |tail| = 2, then µ(e) = (β(head) ×0 C rule ×2 α(tail1) ×1 α(tail0))/g Figure 1:</formula><p>The tensor form of the hypergraph inside- outside algorithm, for calculation of rule marginals µ(e). A slight simplification in the marginal computation yields NT marginals for spans µ(X, i, j). B(q) returns the incoming hy- peredges for node q, and h(e), t(e), r(e) return the head node, tail nodes, and rule for hyperedge e.</p><p>goal node (root). These parameters participate in tensor-vector operations: a 3 rd -order tensor C r 2 can be multiplied along each of its three modes (× 0 , × 1 , × 2 ), and if multiplied by an m × 1 vec- tor, will produce an m × m matrix. 1 Note that ma- trix multiplication can be represented by × 1 when multiplying on the right and × 0 when multiplying on the left of the matrix. The decoder computes marginal probabilities for each skeletal rule in the parse forest of a source sentence by marginaliz- ing over the latent states, which in practice corre- sponds to simple tensor-vector products. This op- eration is not dependent on the manner in which the parameters were estimated. <ref type="figure">Figure 1</ref> presents the tensor version of the inside-outside algorithm for decoding L-SCFGs. The algorithm takes as input the parse forest of the source sentence represented as a hypergraph <ref type="bibr" target="#b18">(Klein and Manning, 2001</ref>), which is computed using a bottom-up parser with Earley-style rules similar to the algorithm in <ref type="bibr" target="#b2">Chiang (2007)</ref>. Hyper- graphs are a compact way to represent a forest of multiple parse trees. Each node in the hypergraph corresponds to an NT span, and can have multiple incoming and outgoing hyperedges. Hyperedges, which connect one or more tail nodes to a single head node, correspond exactly to rules, and tail or head nodes correspond to children (RHS NTs) or parent (LHS NT). The function B(q) returns all in- coming hyperedges to a node q, i.e., all rules such that the LHS NT of the rule corresponds to the NT span of the node q. The algorithm computes inside and outside probabilities over the hypergraph us- ing the tensor representations, and converts these probabilities to marginal rule probabilities. It is similar to the version presented in <ref type="bibr" target="#b6">Cohen et al. (2014)</ref>, but adapted to hypergraph parse forests.</p><p>The complexity of this decoding algorithm is O(n 3 m 3 |G|) where n is the length of the input sentence, m is the number of latent states, and |G| is the number of production rules in the grammar without latent-variable annotations (i.e., m = 1). <ref type="bibr">2</ref> The bulk of the computation is a series of tensor- vector products of relatively small size (each di- mension is of length m), which can be computed very quickly and in parallel. The tensor computa- tions can be significantly sped up using techniques described by <ref type="bibr" target="#b4">Cohen and Collins (2012)</ref>, so that they are linear in m and not cubic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Derivation Trees for Parallel Sentences</head><p>To estimate the parameters t and π of an L- SCFG (discussed in detail in the next section), we assume the existence of a dataset composed of synchronous s-trees, which can be acquired from word alignments. Normally in phrase-based translation models, we consider all possible phrase pairs consistent with the word alignments and es- timate features based on surface statistics associ- ated with the phrase pairs or rules. The weights of these features are then learned using a discrimina- tive training algorithm <ref type="bibr" target="#b26">(Och, 2003;</ref><ref type="bibr">Chiang, 2012, inter alia)</ref>. In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making pa- rameter estimation more tractable. <ref type="bibr">3</ref> To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word align- ments, as opposed to the composed set of rules ( <ref type="bibr" target="#b12">Galley et al., 2006</ref>). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applica- ble rules can be combinatorially larger than if we just consider the set of rules that cannot be formed from other rules, namely the minimal rules. The rule types across all sentence pairs are combined to form a minimal grammar. <ref type="bibr">4</ref> To extract a set of minimal rules, we use the linear-time extraction algorithm of <ref type="bibr" target="#b35">Zhang et al. (2008)</ref>. We give a rough description of their method below, and refer the reader to the original paper for additional details.</p><p>The algorithm returns a complete minimal derivation tree for each word-aligned sentence pair, and generalizes an approach for finding all common intervals (pairs of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase) between two permutations <ref type="bibr" target="#b30">(Uno and Yagiura, 2000</ref>) to se- quences with many-to-many alignment links be- tween the two sides, as in word alignment. The key idea is to encode all phrase pairs of a sen- tence alignment in a tree of size proportional to the source sentence length, which they call the normalized decomposition tree. Each node cor- responds to a phrase pair, with larger phrase spans represented by higher nodes in the tree. Construct- ing the tree is analogous to finding common in- tervals in two permutations, a property that they leverage to propose a linear-time algorithm for tree extraction. Converting the tree to a set of minimal SCFG rules for the sentence pair is straightfor- ward, by replacing nodes corresponding to spans with lexical items or NTs in a bottom-up manner. <ref type="bibr">5</ref> By using minimal rules as a starting point instead of the traditional heuristically-extracted rules <ref type="bibr" target="#b2">(Chiang, 2007)</ref> or arbitrary compositions of minimal rules ( <ref type="bibr" target="#b12">Galley et al., 2006</ref>), we are also able to explore the transition from minimal rules to composed ones in a principled manner by en- coding contextual information through the latent states. Thus, a beneficial side effect of our re- finement process is the creation of more context- specific rules without increasing the overall size of the baseline grammar, instead holding this in- formation in our parameters C r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parameter Estimation for L-SCFGs</head><p>We explore two methods for estimating the param- eters C r of the model: a likelihood-maximization approach based on EM <ref type="bibr" target="#b7">(Dempster et al., 1977)</ref>, and a spectral approach based on the method of moments ( <ref type="bibr" target="#b15">Hsu et al., 2009;</ref><ref type="bibr" target="#b6">Cohen et al., 2014</ref>), where we identify a subspace using a singular value decomposition (SVD) of the cross-product feature space between inside and outside trees and estimate parameters in this subspace. <ref type="figure" target="#fig_0">Figure 2</ref> presents a side-by-side comparison of the two algorithms, which we discuss in this sec- tion. In the spectral approach, we base our pa- rameter estimates on low-rank representations of moments of features, while EM explicitly maxi- mizes a likelihood criterion. The parameter es- timation algorithms are relatively similar, but in lieu of sparse feature functions in the spectral case, EM uses partial counts estimated with the current set of parameters. The nature of EM allows it to be susceptible to local optima, while the spectral approach comes with guarantees on obtaining the global optimum <ref type="bibr" target="#b6">(Cohen et al., 2014</ref>). Lastly, com- puting the SVD and estimating parameters in the low-rank space is a one-shot operation, as opposed to the iterative procedure of EM, and therefore is much more computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Estimation with Spectral Method</head><p>We generalize the parameter estimation algorithm presented in <ref type="bibr" target="#b5">Cohen et al. (2013)</ref> to the syn-Inputs: Training examples (r (i) , t (i,1) , t <ref type="bibr">(i,2)</ref> , t <ref type="bibr">(i,3)</ref> , o (i) , b (i) ) for i ∈ {1 . . . M }, where r (i) is a context free rule; t (i,1) , t <ref type="bibr">(i,2)</ref> , and t (i,3) are inside trees; o (i) is an out- side tree; and b (i) = 1 if the rule is at the root of tree, 0 otherwise. A function φ that maps inside trees t to feature-vectors φ(t) ∈ R d . A function ψ that maps outside trees o to feature-vectors ψ(o) ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm:</head><p>Step 0: Singular Value Decomposition</p><p>• Compute the SVD of Eq. 1 to calculate matri- cesˆUcesˆ cesˆU ∈ R (d×m) andˆVandˆ andˆV ∈ R (d ×m) .</p><p>Step 1: Projection</p><formula xml:id="formula_4">Y (t) = U φ(t) Z(o) = Σ −1 V ψ(o)</formula><p>Step 2: Calculate CorrelationsˆE</p><formula xml:id="formula_5">Correlationsˆ CorrelationsˆE r =          o∈Q r Z(o) |Q r | if r ∈ R0 (o,t)∈Q r Z(o)⊗Y (t) |Q r | if r ∈ R1 (o,t 2 ,t 3 )∈Q r Z(o)⊗Y (t 2 )⊗Y (t 3 ) |Q r | if r ∈ R2</formula><p>Q r is the set of outside-inside tree triples for binary rules, outside-inside tree pairs for unary rules, and outside trees for pre-terminals.</p><p>Step 3: Compute Final Parameters</p><p>• For all r ∈ R,</p><formula xml:id="formula_6">ˆ C r = count(r) M × ˆ E r • For all r (i) ∈ {1, . . . , M } such that b (i) is 1, ˆ C S = ˆ C S + Y (t (i,1) ) |Q S |</formula><p>Q S is the set of trees at the root.</p><p>(a) The spectral learning algorithm for estimating pa- rameters of an L-SCFG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs:</head><p>Training examples (r (i) , t (i,1) , t (i,2) , t <ref type="bibr">(i,3)</ref> , o (i) , b (i) ) for i ∈ {1 . . . M }, where r (i) is a context free rule; t (i,1) , t (i,2) , and t (i,3) are inside trees; o (i) is an outside tree; b (i) = 1 if the rule is at the root of tree, 0 otherwise; and MAX ITERATIONS. Algorithm:</p><p>Step 0: Parameter Initialization For rule r ∈ R,</p><p>• if r ∈ R0: initializê</p><formula xml:id="formula_7">C r ∈ R m×1</formula><p>• if r ∈ R1: initializê</p><formula xml:id="formula_8">C r R m×m • if r ∈ R2: initializê C r R m×m×m Initializê C S ∈ R m×1ˆC m×1ˆ m×1ˆC r 0 = ˆ C r , ˆ C S 0 = ˆ C S For iteration t = 1, . . . , MAX ITERATIONS, • Expectation Step:</formula><p>Estimate Y and Z Compute partial counts and total tree probabili- ties g for all t and o using <ref type="figure">Fig. 1</ref> and parametersˆC parametersˆ parametersˆC r t−1 , ˆ</p><formula xml:id="formula_9">C S t−1 . Calculate CorrelationsˆE Correlationsˆ CorrelationsˆE r =              o,g∈Q r Z(o) g if r ∈ R0 (o,t,g)∈Q r Z(o)⊗Y (t) g if r ∈ R1 (o,t 2 ,t 3 ,g)∈Q r Z(o)⊗Y (t 2 )⊗Y (t 3 ) g if r ∈ R2 Update Parameters For all r ∈ R, ˆ C r t = ˆ C r t−1 ˆ E r For all r (i) ∈ {1, . . . , M } such that b (i) is 1, ˆ C S t = ˆ C S t + ( ˆ C S t−1 Y (r (i) ))/g Q S is</formula><p>the set of trees at the root.</p><p>• Maximization Step if r ∈ R0: ∀h1 :</p><formula xml:id="formula_10">ˆ C r (h1) = ˆ C r (h 1 ) r =r h 1 ˆ C r (h 1 ) if r ∈ R1: ∀h1, h2 : ˆ C r (h1, h2) = ˆ C r (h 1 ,h 2 ) r =r h 2 ˆ C r (h 1 ,h 2 ) if r ∈ R2: ∀h1, h2, h3 : ˆ C r (h1, h2, h3) = ˆ C r (h 1 ,h 2 ,h 3 ) r =r h 2 ,h 3 ˆ C r (h 1 ,h 2 ,h 3 ) if LHS(r) = S: ∀h1 : ˆ C r (h1) = ˆ C r (h 1 ) r =r h 1 ˆ C r (h 1 ) (b)</formula><p>The EM-based algorithm for estimating parameters of an L- SCFG. chronous or bilingual case. The central concept of the spectral parameter estimation algorithm is to learn an m-dimensional representation of in- side and outside trees by defining these trees in terms of features, in combination with a projection step (SVD), with the hope being that the lower- dimensional space captures the syntactic and se- mantic regularities among rules from the sparse feature space. Every NT in an s-tree has an as- sociated inside and outside tree; the inside tree contains the entire sub-tree at and below the NT, and the outside tree is everything else in the syn- chronous s-tree except the inside tree. The inside feature function φ maps the domain of inside tree fragments to a d-dimensional Euclidean space, and the outside feature function ψ maps the do- main of outside tree fragments to a d -dimensional space. The specific features we used are discussed in §5.2.</p><p>Let O be the set of all tuples of inside-outside trees in our training corpus, whose size is equiva- lent to the number of rule tokens (occurrences in the corpus) M , and let φ(t) ∈ R d×1 , ψ(o) ∈ R d ×1 be the inside and outside feature functions for in- side tree t and outside tree o. By computing the outer product ⊗ between the inside and outside feature vectors for each pair and aggregating, we obtain the empirical inside-outside feature covari- ance matrix:</p><formula xml:id="formula_11">ˆ Ω = 1 |O| (o,t)∈O φ(t) (ψ(o))<label>(1)</label></formula><p>If m is the desired latent space dimension, we compute an m-rank truncated SVD of the empir- ical covariance matrixˆΩmatrixˆ matrixˆΩ ≈ U ΣV , where U ∈ R d×m and V ∈ R d ×m are the matrices containing the left and right singular vectors, and Σ ∈ R m×m is a diagonal matrix containing the m-largest sin- gular values along its diagonal. <ref type="figure" target="#fig_0">Figure 2a</ref> provides the remaining steps in the algorithm. The M training examples are obtained by considering all nodes in all of the synchronous s-trees given as input. In step 1, for each inside and outside tree, we project its high-dimensional representation to the m-dimensional latent space. Using the m-dimensional representations for in- side and outside trees, in step 2 for each rule type r we compute the covariance between the inside tree vectors and the outside tree vector using the ten- sor product, a generalized outer product to com- pute covariances between more than two random vectors. For binary rules, with two child inside vectors and one outside vector, the resultˆEresultˆ resultˆE r is a 3-mode tensor; for unary rules, a regular matrix, and for pre-terminal rules with no right-hand side non-terminals, a vector. The final parameter es- timate is then the associated tensor/matrix/vector, scaled by the maximum likelihood estimate of the rule r, as in step 3.</p><p>The corresponding theoretical guarantees from <ref type="bibr" target="#b6">Cohen et al. (2014)</ref> can also be generalized to the synchronous case. ˆ Ω is an empirical esti- mate of the true covariance matrix Ω, and if Ω has rank m, then the marginals computed using the spectrally-estimated parameters will converge to the true marginals, with the sample complexity for convergence inversely proportional to a poly- nomial function of the m th largest singular value of Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimation with EM</head><p>A likelihood maximization approach can also be used to learn the parameters of an L-SCFG. Pa- rameters are initialized by sampling each param- eter valuê C r (h 1 , h 2 , h 3 ) from the interval [0, 1] uniformly at random. <ref type="bibr">6</ref> We first decode the train- ing corpus using an existing set of parameters to compute the inside and outside probability vectors associated with NTs for every rule in each s-tree, constrained to the tree structure of the training ex- ample. These probabilities can be computed us- ing the decoding algorithm in <ref type="figure">Figure 1</ref> (where α and β correspond to the inside and outside proba- bilities respectively), except the parse forest con- sists of a single tree only. These vectors repre- sent partial counts over latent states. We then de- fine functions Y and Z (analogous to the spectral case) which map inside and outside tree instances to m-dimensional vectors containing these partial counts. In the spectral case, Y and Z are estimated just once, while in the case of EM they have to be re-estimated at each iteration.</p><p>The expectation step thus consists of comput- ing the partial counts of inside and outside trees t and o, i.e., recovering the functions Y and Z, and updating parameters C r by computing correla- tions, which involves summing over partial counts (across all occurrences of a rule in the corpus). Each partial count's contribution is divided by a normalization factor g, which is the total probabil- ity of the tree which t or o is part of. Note that unlike the spectral case, there is a specific normal- ization factor for each inside-outside tuple. Lastly, the correlations are scaled by the existing parame- ter estimates.</p><p>To obtain the next set of parameters, in the max- imization step we normalizê C r for r ∈ R such that for every h 1 , r =r,h 2 ,h 3 ˆ C r (h 1 , h 2 , h 3 ) = 1 for r ∈ R 2 , r =r,h 2 ˆ C r (h 1 , h 2 ) = 1 for r ∈ R 1 , and r =r,h 2 ˆ C r (h 2 ) = 1 for r ∈ R 0 . We also normalize the root rule parametersˆCparametersˆ parametersˆC r where LHS(r) = S. It is also possible to add sparse, overlapping features to an EM-based estimation procedure <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2010</ref>) and we leave this extension for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The goal of the experimental section is to evalu- ate the performance of the latent-variable SCFG in comparison to a baseline without any additional NT annotations (MIN-GRAMMAR), and to com- pare the performance of the two parameter esti- mation algorithms. We also compare L-SCFGs to a HIERO baseline <ref type="bibr" target="#b2">(Chiang, 2007)</ref>. The language pair of evaluation is Chinese-English (ZH-EN).</p><p>We score translations using BLEU ( <ref type="bibr" target="#b27">Papineni et al., 2002</ref>). The latent-variable model is inte- grated into the standard MT pipeline by comput- ing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in <ref type="figure">Figure 1</ref> with the parameters estimated through the algorithms in <ref type="figure" target="#fig_0">Figure 2</ref>, and is added as a fea- ture for the rule during MERT <ref type="bibr" target="#b26">(Och, 2003)</ref>. These probabilities are conditioned on the LHS (X), and are thus joint probabilities for a source-target RHS pair. We also write out as features the condi- tional relative frequenciesˆPfrequenciesˆ frequenciesˆP (e|f ) andˆPandˆ andˆP (f |e) as estimated by our latent-variable model, i.e., con- ditioned on the source and target RHS.</p><p>Overall, we find that both the spectral and the EM-based estimators improve upon a mini- mal grammar baseline with only a single cate- gory, but the spectral approach does better. In fact, it matches the performance of the standard HI- ERO baseline, despite learning on top of a minimal grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data and Baselines</head><p>The ZH-EN data is the BTEC parallel corpus <ref type="figure" target="#fig_0">(Paul, 2009)</ref>; we combine the first and second development sets in one, and evaluate on the third development set. The development and test sets are evaluated with 16 references. Statistics for the data are shown in <ref type="table" target="#tab_0">Table 1</ref>. We used the CDEC decoder ( <ref type="bibr" target="#b9">Dyer et al., 2010</ref>) to extract word align- ments and the baseline hierarchical grammars, MERT tuning, and decoding. We used a 4-gram language model built from the target-side of the parallel training data. The Python-based imple- mentation of the tensor-based decoder, as well as the parameter estimation algorithms is available at github.com/asaluja/spectral-scfg/.</p><p>The baseline HIERO system uses a grammar ex- tracted by applying the commonly used heuris- <ref type="table" target="#tab_3">ZH-EN  TRAIN (SRC)  334K  TRAIN (TGT)  366K  DEV (SRC)  7K  DEV (TGT)</ref> 7.6K TEST <ref type="bibr">(SRC)</ref> 3.8K TEST <ref type="bibr">(TGT)</ref> 3.9K tics <ref type="bibr" target="#b2">(Chiang, 2007)</ref>. Each rule is decorated with two lexical and phrasal features corresponding to the forward (e|f ) and backward (f |e) conditional log frequencies, along with the log joint frequency (e, f ), the log frequency of the source phrase (f ), and whether the phrase pair or the source phrase is a singleton. Weights for the language model (and language model OOV), glue rule, and word penalty are also tuned. The MIN-GRAMMAR baseline <ref type="bibr">7</ref> maintains the same set of weights.  Grammar sizes are presented in <ref type="table" target="#tab_1">Table 2</ref>. For the latent-variable models, we provide the effec- tive grammar size, where the number of NTs on the RHS of a rule is taken into account when com- puting the grammar size, by assuming each possi- ble latent variable configuration amongst the NTs generates a different rule. Furthermore, all single- tons are mapped to the OOV rule, while we in- clude singletons in MIN-GRAMMAR. 8 Hence, ef- fective grammar size can be computed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammar</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Rules</head><formula xml:id="formula_12">m(1 + |R &gt;1 0 |) + m 2 |R 1 | + m 3 |R 2 |,</formula><p>where R &gt;1 0 is the set of pre-terminal rules that occur more than once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Spectral Features</head><p>We use the following set of sparse, binary features in the spectral learning process:</p><p>• Rule Indicator. For the inside features, we con- sider the rule production containing the current non-terminal on the left-hand side, as well as the rules of the children (distinguishing between left and right children for binary rules). For the outside features, we consider the parent rule production along with the rule production of the sibling (if it exists).</p><p>• Lexical. for both the inside and outside fea- tures, any lexical items that appear in the rule productions are recorded. Furthermore, we con- sider the first and last words of spans (left and right child spans for inside features, distinguish- ing between the two if both exist, and sibling span for outside features). Source and target words are treated separately.</p><p>• Length. the span length of the tree and each of its children for inside features, and the span length of the parent and sibling for outside fea- tures.</p><p>In our experiments, we instantiated a total of 170,000 rule indicator features, 155,000 lexical features, and 80 length features. <ref type="table" target="#tab_3">Table 3</ref> presents a comprehensive evaluation of the ZH-EN experimental setup. The first section con- sists of the various baselines we consider. In ad- dition to the aforementioned baselines, we eval- uated a setup where the spectral parameters sim- ply consist of the joint maximum likelihood esti- mates of the rules. This baseline should perform en par with MIN-GRAMMAR, which we see is the case on the development set. The performance on the test set is better though, primarily because we also include the reverse log relative frequency (f |e) computed from the latent-variable model as an additional feature in MERT. Furthermore, in line with previous work ( <ref type="bibr" target="#b12">Galley et al., 2006</ref>) which compares minimal and composed rules, we find that minimal grammars take a hit of more than 2.5 BLEU points on the development set, compared to composed (HIERO) grammars. The m = 1 spec- tral baseline with only rule indicator features per- forms slightly better than the minimal grammar baseline, since it overtly takes into account inside- outside tree combination preferences in the param- eters, but improvement is minimal with one latent state naturally and the performance on the test set is in line with the MLE baseline. On top of the baselines, we looked at a number  of feature combinations and latent states for the spectral and EM-estimated latent-variable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Chinese-English Experiments</head><p>For the spectral models, we tuned MERT parame- ters separately for each rank on a set of parameters estimated from rule indicator features only; subse- quent variations within a given rank, e.g., the ad- dition of lexical or length features or smoothing, were evaluated with the same set of rank-specific weights from MERT. For EM, we ran parame- ter estimation with 5 randomly initialized starting points for 50 iterations; we tuned the MERT pa- rameters with EM parameters obtained after 25 th iterations. Similar to the spectral experiments, we fixed the MERT weight values and evaluated BLEU performance with parameters after every 5 iterations and chose the iteration with the highest score on the development set. The results are av- eraged over the 5 initializations, with standard de- viation in parentheses. Firstly, we can see a clear dependence on rank, with peak performance for the spectral and EM models occurring at m = 16. In this instance, the spectral model roughly matches the performance of the HIERO baseline, but it only uses rules ex- tracted from a minimal grammar, whose size is a fraction of the HIERO grammar. The gains seem to level off at this rank; additional ranks seem to add noise to the parameters. Feature-wise, addi- tional lexical and length features add little, prob-ably because much of this information is encap- sulated in the rule indicator features. For EM, m = 16 outperforms the minimal grammar base- line, but is not at the level of the spectral results. All EM, spectral, and MLE results are statistically significant (p &lt; 0.01) with respect to the MIN- GRAMMAR baseline ( <ref type="bibr" target="#b34">Zhang et al., 2004</ref>), and the improvement over the HIERO baseline achieved by the m = 16 rule indicator configuration is also sta- tistically significant.</p><p>The two estimation algorithms differ signifi- cantly in their estimation time. Given a feature covariance matrix, the spectral algorithm (SVD, which was done with Matlab, and correlation com- putation steps) for m = 16 took 7 minutes, while the EM algorithm took 5 minutes for each iteration with this rank. <ref type="figure" target="#fig_3">Figure 3</ref> presents a comparison of the non- terminal span marginals for two sentences in the development set. We visualize these differences through a heat map of the CKY parse chart, where the starting word of the span is on the rows, and the span end index is on the columns. Each cell is shaded to represent the marginal of that particular non-terminal span, with higher likelihoods in blue and lower likelihoods in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>For the most part, marginals at the leaves (i.e., pre-terminal marginals) tend to score relatively similarly across different setups. Higher up in the chart, the latent SCFG marginals look quite dif- ferent than the MLE parameters. Most noticeably, spans starting at the beginning of the sentence are much more favored. It is these rules that allow the right translation to be preferred since the MLE chooses not to place the object of the sentence in the subject's span. However, the spectral param- eters seem to discriminate between these higher- level rules better than EM, which scores spans starting with the first word uniformly highly. An- other interesting point is that the range of likeli- hoods is much larger in the EM case compared to the MLE and spectral variants. For the second sen- tence (row), the 1-best hypothesis produced by all systems are the same, but the heat map accentuates the previous observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The goal of refining single-category HPBT gram- mars or automatically learning the NT categories in a grammar, instead of relying on noisy parser outputs, has been explored from several different angles in the MT literature. <ref type="bibr" target="#b1">Blunsom et al. (2008)</ref> present a Bayesian model for synchronous gram- mar induction, and place an appropriate nonpara- metric prior on the parameters. However, their starting point is to estimate a synchronous gram- mar with multiple categories from parallel data (using the word alignments as a prior), while we aim to refine a fixed grammar with additional la- tent states. Furthermore, their estimation proce- dure is extremely expensive and is restricted to learning up to five NT categories, via a series of mean-field approximations.</p><p>Another approach is to explicitly attach a real- valued vector to each NT: <ref type="bibr" target="#b17">Huang et al. (2010)</ref> use an external source-language parser for this pur- pose and score rules based on the similarity be- tween a source sentence parse and the information contained in this vector, which explicitly requires the integration of a good-quality source-language parser. The EM-based algorithm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima'an (2011) select among linguistically moti- vated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over dif- ferent derivations therefore their inside-outside al- gorithm is O(n 6 ). In the syntax-directed trans- lation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote param- eter sharing ( <ref type="bibr" target="#b32">Venugopal et al., 2009;</ref><ref type="bibr" target="#b14">Hanneman and Lavie, 2013)</ref>, which is the complement of our aim in this paper.</p><p>The idea of automatically learned grammar re- finements comes from the monolingual parsing lit- erature, where phenomena like head lexicalization can be modeled through latent variables. <ref type="bibr" target="#b23">Matsuzaki et al. (2005)</ref> look at a likelihood-based method to split the NT categories of a gram- mar into a fixed number of sub-categories, while <ref type="bibr" target="#b29">Petrov et al. (2006)</ref> learn a variable number of sub-categories per NT. The latter's extension may be useful for finding the optimal number of latent states from the data in our case.</p><p>The question of whether we can incorporate ad- ditional contextual information in minimal rule grammars in MT via auxiliary models instead of using longer, composed rules has been investi- gated before as well. n-gram translation mod-   els <ref type="bibr" target="#b22">(Mariño et al., 2006;</ref><ref type="bibr" target="#b8">Durrani et al., 2011</ref>) seek to model long-distance dependencies and re- orderings through n-grams. Similarly, <ref type="bibr" target="#b31">Vaswani et al. (2011)</ref> use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting <ref type="bibr" target="#b25">(Ney et al., 1994)</ref>, while in our instance we capture this smoothing effect through low rank or latent states. <ref type="bibr" target="#b10">Feng and Cohn (2013)</ref> also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. <ref type="bibr" target="#b15">Hsu et al. (2009)</ref> presented one of the initial efforts at spectral-based parameter estimation (us- ing SVD) of observed moments for latent-variable models, in the case of Hidden Markov models. This idea was extended to L-PCFGs ( <ref type="bibr" target="#b6">Cohen et al., 2014)</ref>, and our approach can be seen as a bilingual or synchronous generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we presented an approach to re- fine synchronous grammars used in MT by in- ferring the latent categories for the single non- terminal in our grammar rules, and proposed two algorithms to estimate parameters for our latent- variable model. By fixing the synchronous deriva- tions of each parallel sentence in the training data, it is possible to avoid many of the computational issues associated with synchronous grammar in- duction. Improvements over a minimal grammar baseline and equivalent performance to a hierar- chical phrase-based baseline are achieved by the spectral approach. For future work, we will seek to relax this consideration and jointly reason about non-terminal categories and derivation structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The two parameter estimation algorithms proposed for L-SCFGs; (a) method of moments; (b) expectation maximization. is the element-wise multiplication operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>0</head><label>0</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Corpus statistics (in words). For the target DEV and 
TEST statistics, we take the first reference. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Grammar sizes for the different systems; for the 
latent-variable models, effective grammar sizes are provided. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for the ZH-EN corpus, comparing across 
the baselines and the two parameter estimation techniques. 
RI, Lex, and Len correspond to the rule indicator, lexical, 
and length features respectively, and Sm denotes smoothing. 
For the EM experiments, we selected the best scoring iter-
ation by tuning weights for parameters obtained after 25 it-
erations and evaluating other parameters with these weights. 
Results for EM are averaged over 5 starting points, with stan-
dard deviation given in parentheses. Spectral, EM, and MLE 
performances compared to the MIN-GRAMMAR baseline are 
statistically significant (p &lt; 0.01). 

</table></figure>

			<note place="foot" n="1"> This operation is sometimes called a contraction.</note>

			<note place="foot" n="2"> In practice, the term m 3 |G| can be replaced with a smaller term, which separates the rules in G by the number of NTs on the RHS. This idea relates to the notion of &quot;effective grammar size&quot; which we discuss in §5.</note>

			<note place="foot" n="3"> For future work, we will consider efficient algorithms for parameter estimation over derivation forests, since there may be multiple valid ways to explain the sentence pair via a synchronous tree structure. 4 Table 2 presents a comparison of grammar sizes for our experiments ( §5.1).</note>

			<note place="foot" n="5"> We filtered rules with arity 3 and above (i.e., containing more than 3 NTs on the RHS). While the L-SCFG formalism is perfectly capable of handling such cases, it would have resulted in higher order tensors for our parameter structures.</note>

			<note place="foot" n="6"> In our experiments, we also tried the initialization scheme described in Matsuzaki et al. (2005), but found that it provided little benefit.</note>

			<note place="foot" n="7"> Code to extract the minimal derivation trees is available at www.cs.rochester.edu/u/gildea/mt/. 8 This OOV mapping is done so that the latent-variable model can handle unknown tokens.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Daniel Gildea for sharing his code to extract minimal derivation trees, Stefan Riezler for useful discussions, Bren-dan O'Connor for the CKY visualization advice, and the anonymous reviewers for their feedback. This work was supported by a grant from eBay Inc. (Saluja), the U. S. Army Research Laboratory and the U. S. Army Research Office under con-tract/grant number W911NF-10-1-0533 (Dyer).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian Synchronous Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hope and Fear for Discriminative Training of Statistical Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1159" to="1187" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensor decomposition for fast parsing with latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs: Algorithms and sample complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
	<note>Rubin</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A joint sequence translation model with integrated reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Markov model of machine translation using non-parametric bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="427" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving syntax-augmented machine translation by coarsening the label set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hanneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Spectral Algorithm for Learning Hidden Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT</title>
		<meeting>COLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical syntax-directed translation with extended domain of locality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martiň</forename><surname>Cmejrek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parsing and hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Bayesian model for learning SCFGs with discontiguous rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The infinite PCFG using hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">N-grambased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>Mariño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><forename type="middle">M</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Fonollosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Costa-Jussà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="527" to="549" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning hierarchical translation structure with linguistic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markos</forename><surname>Mylonakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On Structuring Probabilistic Dependencies in Stochastic Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2009 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
		<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast algorithms to enumerate all common intervals of two permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeaki</forename><surname>Uno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutsunori</forename><surname>Yagiura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="290" to="309" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rule Markov models for fast tree-tostring translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Preference grammars: Softening syntactic constraints to improve statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>September</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interpreting BLEU/NIST scores: How much improvement do we need to have a better system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Extracting synchronous grammar rules from wordlevel alignments in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Syntax augmented machine translation via chart parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Machine Translation, StatMT &apos;06</title>
		<meeting>the Workshop on Statistical Machine Translation, StatMT &apos;06</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="138" to="141" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
