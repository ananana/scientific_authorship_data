<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
							<email>shamil@u.nus.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><forename type="middle">Tam</forename><surname>Hoang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1901" to="1911"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An important aspect for the task of grammatical error correction (GEC) that has not yet been adequately explored is adaptation based on the native language (L1) of writers, despite the marked influences of L1 on second language (L2) writing. In this paper, we adapt a neural network joint model (NNJM) using L1-specific learner text and integrate it into a statistical machine translation (SMT) based GEC system. Specifically, we train an NNJM on general learner text (not L1-specific) and subsequently train on L1-specific data using a Kullback-Leibler divergence regularized objective function in order to preserve generalization of the model. We incorporate this adapted NNJM as a feature in an SMT-based English GEC system and show that adaptation achieves significant F 0.5 score gains on English texts written by L1 Chinese, Russian, and Spanish writers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical error correction (GEC) deals with the automatic correction of errors (spelling, grammar, and collocation errors), particularly in non-native written text. The native language (L1) background of the writer has a noticeable influence on the er- rors made in second language (L2) writing, and con- sidering this factor can potentially improve the per- formance of GEC systems. For example, consider the following sentence written by a Finnish writer <ref type="bibr" target="#b17">(Jarvis and Odlin, 2000</ref>): "When they had escaped in the police car they sat under the tree." The prepo- sition in appears to be grammatically correct. How- ever, in the given context, the preposition 'from' is the correct choice in place of the preposition 'in'. Finnish learners of English tend to overgeneralize the use of the preposition 'in'. Knowledge of L1 makes the correction more probable whenever the preposition in appears in texts written by Finnish writers. Similarly, Chinese learners of English tend to make frequent verb tense and verb form errors, since Chinese lacks verb inflection <ref type="bibr" target="#b35">(Shaughnessy, 1977)</ref>. The cross-linguistic influence of L1 on L2 writing is a highly complex phenomenon, and the er- rors made by learners cannot be directly attributed to the similarities or differences between the two lan- guages. As Ortega (2009) points out, learners seem to operate on two complementary principles: "what works in L1 may work in L2 because human lan- guages are fundamentally alike; but if it sounds too L1-like, it will probably not work in L2". In this paper, we follow a data-driven approach to model these influences and adapt GEC systems using L2 texts written by writers of the same L1 background.</p><p>The two most popular approaches for grammat- ical error correction are the classification approach ( <ref type="bibr" target="#b33">Rozovskaya et al., 2014</ref>) and the statistical machine translation (SMT) ap- proach ( <ref type="bibr" target="#b19">Junczys-Dowmunt and Grundkiewicz, 2014</ref>). The SMT approach has emerged as a popular paradigm for GEC because of its ability to learn text transformations from ill- formed to well-formed text enabling it to correct a wide variety of errors including complex errors that are difficult to handle for the classification ap- proach ( <ref type="bibr" target="#b32">Rozovskaya and Roth, 2016)</ref>. The phrase- based SMT approach has been used in state-of- the-art GEC systems ( <ref type="bibr" target="#b32">Rozovskaya and Roth, 2016;</ref><ref type="bibr" target="#b16">Hoang et al., 2016</ref>). The SMT approach does not model error types specifi- cally, nor does it require linguistic analysis like pars- ing and part-of-speech (POS) tagging. We adopt a phrase-based SMT approach to GEC in this pa- per. Additionally, we implement and incorporate a neural network joint model (NNJM) <ref type="bibr" target="#b12">(Devlin et al., 2014</ref>) as a feature in our SMT-based GEC system. It is easy to integrate an NNJM into the SMT de- coding framework as it uses a fixed-window context and it has shown to improve SMT-based GEC . We adapt the NNJM to L1- specific data (i.e., English text written by writers of a particular L1) and obtain significant improvements over the baseline which uses an unadapted NNJM. Adaptation is done by using the unadapted NNJM trained on general domain data (i.e., not L1-specific) using a log likelihood objective function with self- normalization ( <ref type="bibr" target="#b12">Devlin et al., 2014</ref>) as the starting point, and training for subsequent iterations using the smaller L1-specific in-domain data with a mod- ified objective function which includes a Kullback- Leibler (KL) divergence regularization term. This modified objective function prevents overfitting on the smaller in-domain data and preserves the gener- alization capability of the NNJM. We show that this method of adaptation works on very small and high- quality L1-specific data as well (50-100 essays).</p><p>In summary, the two major contributions of this paper are as follows. (1) This is the first work that performs L1-based adaptation for GEC using the SMT approach and covering all error types. (2) We introduce a novel method of NNJM adaptation and demonstrate that this method can work with in- domain data that are much smaller than the general domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the past decade, there has been increasing atten- tion on GEC in English, mainly due to the growing number of English as second language (ESL) learn- ers around the world. The popularity of this prob- lem grew further through Helping Our Own (HOO) ( <ref type="bibr" target="#b10">Dale and Kilgarriff, 2011;</ref><ref type="bibr" target="#b11">Dale et al., 2012)</ref> and CoNLL shared tasks ( ). The majority of the published work on GEC aimed at building classifiers or rule-based systems for specific error types and combined them to build hybrid systems <ref type="bibr" target="#b33">Rozovskaya et al., 2014</ref>).</p><p>The cross-linguistic influences between L1 and L2 have been mainly used for the task of native lan- guage identification <ref type="bibr" target="#b24">(Massung and Zhai, 2016)</ref>. It has also been used in typology prediction <ref type="bibr" target="#b1">(Berzak et al., 2014</ref>) and predicting error distributions in ESL data <ref type="bibr" target="#b2">(Berzak et al., 2015</ref>). L1-based adaptation has previously shown to improve GEC for specific error types using the classification approach. <ref type="bibr" target="#b30">Rozovskaya and Roth (2010)</ref> used an approach to correct prepo- sition errors by restricting the candidate corrections to those observed in L1-specific data. They further added artificial training data that mimic the error fre- quency in L1-specific text to improve accuracy. In their later work, <ref type="bibr" target="#b31">Rozovskaya and Roth (2011)</ref> fo- cused on L1-based adaptation for preposition and article correction, by modifying the prior probabil- ities in the na¨ıvena¨ıve Bayes classifier during decision time based on L1-specific ESL learner text. Both approaches use native data for training, but rely on non-native L1-specific text to introduce artificial er- rors or to modify the prior probabilities. Dahlmeier and Ng (2011) implemented a system to correct col- location errors, by adding paraphrases derived from L1 into the confusion set. Specifically, they use a bilingual L1-L2 corpus, to obtain L2 paraphrases, which are likely to be translated to the same phrase in L1. There is no prior work on L1-based adap- tation for GEC using the machine translation ap- proach, which is a one of the most popular ap- proaches for GEC.</p><p>With the availability of large-scale error corrected data <ref type="bibr" target="#b25">(Mizumoto et al., 2011</ref>), the statistical machine translation (SMT) approach to GEC became popu- lar and was employed in state-of-the-art GEC sys- tems. Comparison of the classification approach and the machine translation approach can be found in ( <ref type="bibr" target="#b32">Rozovskaya and Roth, 2016)</ref> and . Recently, an end-to-end neural machine translation framework was proposed for GEC <ref type="bibr" target="#b39">(Yuan and Briscoe, 2016)</ref>, which was shown to achieve competitive results. Neural network joint models have shown to be improve SMT-based GEC sys- tems ( ) due to their ability to model words and phrases in a continuous space, access to larger contexts from source side, and abil-ity to learn non-linear mappings from input to out- put. In this paper, we exploit the advantages of the SMT approach and neural network joint mod- els (NNJMs) by adapting an NNJM based on the L1 background of the writers and integrating it into the SMT framework. We perform KL divergence regularized adaptation to prevent overfitting on the smaller in-domain data. KL divergence regulariza- tion was previously used by <ref type="bibr" target="#b38">Yu et al. (2013)</ref> for speaker adaptation. <ref type="bibr" target="#b18">Joty et al. (2015)</ref> proposed an- other NNJM adaptation method, which uses a regu- larized objective function that encourages a network trained on general-domain data to be closer to an in- domain NNJM. Other adaptation techniques used in SMT include mixture modeling <ref type="bibr" target="#b14">(Foster and Kuhn, 2007;</ref><ref type="bibr" target="#b26">Moore and Lewis, 2010;</ref><ref type="bibr" target="#b34">Sennrich, 2012)</ref> and alternative decoding paths ( <ref type="bibr" target="#b20">Koehn and Schroeder, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Machine Translation Framework for Grammatical Error Correction</head><p>We formulate GEC as a translation task from a pos- sibly erroneous input sentence to a corrected sen- tence. We use the popular phrase-based SMT sys- tem, Moses ( , which employs a log linear model to find the best correction hypothe- sis T * given an input sentence S:</p><formula xml:id="formula_0">T * = argmax T P (T |S) = argmax T N i=1 µ i f i (T, S)</formula><p>where µ i and f i (T, S) are the i th feature weight and feature function, respectively. We use the standard features in Moses, without any re-ordering mod- els. The two main components of an SMT system are the translation model (TM) and the language model (LM). The TM (typically, a phrase table), re- sponsible for generating hypotheses, is trained using parallel data, i.e., learner-written sentences (source data) and their corresponding corrected sentences (target data). It also scores the hypotheses us- ing features like forward and inverse phrase trans- lation probabilities and lexical weights. The LM is trained on well-formed text and ensures the flu- ency of the corrected output. The feature weights µ i are computed by minimum error rate training (MERT), optimizing the F 0.5 measure (Junczys- Dowmunt and Grundkiewicz, 2014) using a devel- opment set. The F 0.5 measure computed using the MaxMatch scorer ) is the standard evaluation metric for GEC used in the CoNLL-2014 shared task ( ), weight- ing precision twice as much as recall. Apart from the TM and the n-gram LM, we add a neural network joint model (NNJM) <ref type="bibr" target="#b12">(Devlin et al., 2014</ref>) as a feature, following , who reported that NNJM improves the per- formance of a state-of-the-art SMT-based GEC sys- tem. Unlike Recurrent Neural Networks (RNNs) and Long Short Term Memory networks (LSTMs), NNJMs have a feed-forward architecture which re- lies on a fixed context. This makes it easy to inte- grate NNJMs into a machine translation decoder as a feature. The feature value is given by log P (T |S), which is the sum of the log probabilities of individ- ual target words in the hypothesis T given the con- text:</p><formula xml:id="formula_1">log P (T |S) ≈ |T | i=1 log P (t i |h i ) (1)</formula><p>where |T | is the number of words in the target hypothesis (corrected sentence), t i is the i th target word, and h i is the context of t i . The context h i consists of n−1 previous target words and m source words surrounding the source word that is aligned to the target word t i . Each dimension in the output layer of the neural network (  gives the proba- bility of a word t in the output vocabulary given its context h:</p><formula xml:id="formula_2">P (y = t|h) = exp(U t (h)) Z(h) = exp(U t (h)) t ∈Vo exp(U t (h))</formula><p>where U t (h) is the unnormalized output score before the softmax, and V o is the output vocabulary.</p><p>The neural network parameters which include the weights, biases, and embedding matrices are trained using back propagation with stochastic gradient de- scent ( <ref type="bibr" target="#b22">LeCun et al., 1998</ref>). Instead of using the noise contrastive estimation (NCE) loss as done in , we use the log likelihood ob- jective function with a self-normalization term sim- ilar to <ref type="bibr" target="#b12">Devlin et al. (2014)</ref>:</p><formula xml:id="formula_3">L = 1 N N i=1 log p(y = t i |h i ) − α log 2 (Z(h i )) (2)</formula><p>where N is the number of training instances, and t i is the target word in the i th training instance. Each training instance consists of a target word t and its context h. α is the self-normalization coefficient which we set to 0.1, following <ref type="bibr" target="#b12">Devlin et al. (2014)</ref>. The training can be done efficiently on GPUs. We adapt this NNJM using L1-specific learner text using a Kullback-Leibler divergence regularized objective function as described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KL Divergence Regularized Adaptation</head><p>We first train an NNJM with the general-domain data (the erroneous and corrected texts, not consider- ing the L1 of the writers) as described in the previous section. Let p GD (y|h) be the probability distribu- tion estimated by the general-domain NNJM. Start- ing from this NNJM, subsequent iterations of train- ing are done using the L1-specific in-domain data alone. The in-domain data consists of the erroneous texts written by writers of a specific L1 and their cor- responding corrected texts. This adaptive training is done using a modified objective function having a regularization term K, which is used to minimize the KL divergence between p GD (y|h) and the net- work's output probability distribution p(y|h) (Yu et al., 2013):</p><formula xml:id="formula_4">K = 1 N N i=1 Vo j=1 p GD (y = t j |h i ) log p(y = t j |h i )</formula><p>The term K will prevent the estimated probability distribution from deviating too much from that of the general domain NNJM during training. Our final objective function for the adaptation step is a linear combination of the terms in L and K, with a regular- ization weight λ and a self-normalization coefficient α:</p><formula xml:id="formula_5">L =λK + (1 − λ) 1 N N i=1 log p(y = t i |h i ) − α 1 N N i=1 log 2 (Z(h i ))</formula><p>We integrate the unadapted NNJM and adapted NNJM independently into our SMT-based GEC sys- tem in order to compare the effect of adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Other Adaptation Methods</head><p>We compare our method against two other adapta- tion methods previously used in SMT. Translation Model Interpolation: Following Sennrich (2012), we linearly interpolate the fea- tures in two phrase tables, one trained on in- domain data (L1-specific learner text) and the other on out-of-domain data. The interpolation weights are set by minimization of perplexity using phrase pairs extracted from our in-domain development set. The lexical weights are re-computed from the lex- ical counts and the interpolation weights are re- normalized if a phrase pair exists only in one of the phrase tables.</p><p>Neural Domain Adaptation Model: <ref type="bibr" target="#b18">Joty et al. (2015)</ref> proposed an adaptation of NNJM for SMT. They first train an NNJM using in-domain data, and then perform regularized adaptation on the gen- eral domain data (concatenation of in-domain and out-of-domain data) which restricts the model from drifting away from the in-domain NNJM. Specifi- cally, they add a regularization term J to the objec- tive function in their adaptive training step:</p><formula xml:id="formula_6">J = 1 N N i=1 p ID (y = t i |h i ) log p(y = t i |h i )</formula><p>where p ID (y|h) id probability distribution estimated by the in-domain NNJM. NDAM has the following drawbacks compared to our method: (1) Regularization is done using proba- bilities of the target words alone and not on the entire probability distribution over all words, leading to a weak regularization. (2) If the in-domain data is too small, the probability distribution learnt by the in- domain NNJM will be overfitted. Therefore, encour- aging adaptation to be closer to this probability dis- tribution may not yield a good model. Our method, on the other hand, can utilize in-domain data of very small sizes to fine tune a general NNJM. (3) Their method requires retraining of the model on complete training data in order to adapt to each domain. On the contrary, our method can adapt a single general model to different domains using small in-domain data, leading to a considerable reduction in training time.</p><p>We re-implement their method by incorporating this regularization term into the log likelihood objec-tive function with self-normalization, L (Equation 2), during adaptive training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Data and Evaluation</head><p>The training data consist of two corpora: the NUS Corpus of Learner English (NUCLE) <ref type="bibr" target="#b9">(Dahlmeier et al., 2013)</ref> </p><note type="other">and the Lang-8 Learner Corpora v2 (Mizumoto et al., 2011). We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py 1 (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source- target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2.</note><p>A sentence pair where the source or target sentence has more than 80 words is also removed from both NUCLE and Lang-8. The statistics of the data after pre-processing are shown in  We obtain L1-specific in-domain data for adapta- tion based on the L1 information provided in Lang- 8. Adaptation is performed on English texts writ- ten by learners of three different L1 backgrounds: Chinese, Russian, and Spanish. The statistics of the in-domain data from Lang-8 for each L1 are given in <ref type="table" target="#tab_3">Table 2</ref>. For each L1, its out-of-domain data are obtained by excluding the L1-specific in-domain data (from <ref type="table" target="#tab_3">Table 2</ref>) from the combined training data (CONCAT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1</head><p>#sents #src tokens #tgt tokens  We use the publicly available CLC-FCE (Yan- nakoudakis et al., 2011) corpus to obtain the de-velopment and test data. The FCE corpus contains 1,244 scripts written by 1,244 distinct candidates sit- ting the Cambridge ESOL First Certificate in En- glish (FCE) examination in 2000 and 2001. The corpus identifies the L1 of each writer. We extract the scripts written by Chinese, Russian, and Span- ish native writers. We split the data for each L1 into two roughly equal parts based on the number of scripts, of which one part is used as the development data and other part is used as the test data. Splitting based on the number of scripts ensures that there is no overlap between the writers of the development and test data, as each script is written by a unique learner. The details of the FCE dataset correspond- ing to each L1 are given in <ref type="table" target="#tab_5">Table 3</ref>  For evaluation, we use the F 0.5 measure, com- puted by the M 2 scorer v3. <ref type="bibr">2 (Dahlmeier and Ng, 2012)</ref>, as our evaluation metric. The error annota- tions in FCE are converted to the format required by the M 2 scorer. The statistics of error annotations after converting to this format are given in <ref type="table" target="#tab_5">Table 3</ref>. To deal with the instability of parameter tuning in SMT, we perform five runs of tuning and calculate the statistical significance by stratified approximate randomization test, as recommended by <ref type="bibr" target="#b5">(Clark et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Baseline SMT-based GEC system</head><p>We use Moses (Version 3) to build all our SMT- based GEC systems. The phrase table of the base- line system (S CONCAT ) is trained using the complete    training data. We use two 5-gram language models (LMs) trained using KenLM ( <ref type="bibr" target="#b15">Heafield et al., 2013)</ref>. One LM is trained on the English Wikipedia (about 1.78 billion tokens) and another on the target side of the complete training data. The system is tuned us- ing MERT, optimizing the F 0.5 measure on the L1- specific development data in <ref type="table" target="#tab_3">Table 2</ref>. For comparison, we show two other baselines S IN and S OUT , where the phrase tables for each L1 are trained on the in-domain data only <ref type="table" target="#tab_3">(Table 2)</ref> and the out-of-domain data only, respectively. The results of the above baseline GEC systems on L1 Chinese, Russian, and Spanish FCE test data are summarized in <ref type="table" target="#tab_7">Table 4</ref>. We enhance the baseline S CONCAT with an NNJM feature, as described in following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">NNJM Adaptation</head><p>We implement NNJM in Python using the deep learning library Theano 2 ( <ref type="bibr" target="#b0">Bergstra et al., 2010</ref>) in order to use the massively parallel processing power of GPUs for training. We first train an NNJM (NNJM BASELINE ) with complete training data for 10 epochs. The source context window size is set to 5 and the target context window size is set to 4, mak- ing it a (5+5)-gram joint model. Training is done using stochastic gradient descent with a mini-batch 2 http://deeplearning.net/software/theano size of 128 and learning rate of 0.1. To speed up training and decoding, a single hidden layer neural network is used with an input embedding dimen- sion of 192 and 512 hidden units. We use a self- normalization coefficient of 0.1. We pick 16,000 and 32,000 most frequent words on the source and tar- get sides as our source context vocabulary and target context vocabulary, respectively. The output vocab- ulary is set to be the same as the target vocabulary. The vocabulary is selected from the complete train- ing data, and not based on the L1-specific in-domain data. We add the self-normalized NNJM as a fea- ture to our baseline GEC system, S CONCAT to build a stronger baseline. This is referred to as S CONCAT + NNJM BASELINE in <ref type="table" target="#tab_7">Table 4</ref>.</p><p>We perform adaptation on NNJM BASELINE by training for 10 additional epochs using the in- domain training data alone. We use the same hyper- parameters, network structure, and vocabulary, but with the KL-divergence regularized objective func- tion (regularization weight λ = 0.5). We train the adapted NNJM (NNJM ADAPTED ) specific to each L1. We integrate these to our baseline GEC system, and the adapted systems are referred to as S CONCAT + NNJM ADAPTED in <ref type="table" target="#tab_7">Table 4</ref>. The results are aver- aged over five runs of tuning and evaluation. Our evaluation shows that each adapted system S CONCAT + NNJM ADAPTED outperforms every baseline system (S IN , S OUT , S CONCAT , and S CONCAT + NNJM BASELINE ) significantly on all three L1s (p &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison to Other Adaptation Techniques</head><p>We compare our method to two different adapta- tion techniques described in Section 5: Translation Model Interpolation (TM INT ) (Sennrich, 2012) and Neural Domain Adaptation Model (NDAM) (Joty et al., 2015) 3 . The optimization of interpolation weights for TM INT is done using the L1-specific FCE development data. NDAM is trained on the complete training data (CONCAT) for 10 epochs by regularizing using an in-domain NNJM also trained for 10 epochs on L1-specific in-domain data from Lang-8. For NDAM, we use the same vocabulary and hyperparameters as our NNJMs.</p><p>The results are shown in the rows TM INT + NNJM BASELINE and S CONCAT + NDAM in <ref type="table" target="#tab_7">Table  4</ref>. Our evaluation shows that for L1 Russian and L1 Spanish, our adapted system S CONCAT + NNJM ADAPTED significantly outperforms both TM INT + NNJM BASELINE and S CONCAT + NDAM (p &lt; 0.01), but the improvement is not statistically signif- icant for L1 Chinese.</p><p>Our evaluation also shows that the combination of TM INT and adapted NNJM is similar (for L1 Chi- nese and Russian) or worse (for Spanish) in perfor- mance compared to S CONCAT + NNJM ADAPTED . This is because NNJM ADAPTED by itself is a translation model adaptation (because it considers source and target side contexts) and hence using TM INT along with it does not bring in any newer information and may even hurt the performance when the in-domain data is very small (in the case of Spanish).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Effect of Adaptation Data</head><p>We also perform adaptation on the L1-specific FCE development set in <ref type="table" target="#tab_5">Table 3</ref> (which is also our de- velopment data for the GEC systems), instead of the in-domain data from Lang-8 in <ref type="table" target="#tab_3">Table 2</ref>. Our neu- ral network overfits easily on the FCE development set due to its much smaller size. Hence, we per- form adaptive training for only 2 epochs on top of NNJM BASELINE . The results are shown in the row S CONCAT + NNJM ADAPTED (FCE) in <ref type="table" target="#tab_7">Table 4</ref>. Although the FCE development data is much smaller in size than the L1-specific in-domain data from Lang-8, we observe similar improvements on all three L1s. This is likely due to the similarity of the devel- opment and test sets, which are obtained from the same FCE corpus. These experiments show that smaller high-quality L1-specific error annotated data (1,000-2,000 sentences) similar to the target data can be used for adaptation to give competitive re- sults compared to using much larger in-domain data (20,000-250,000 sentences) from other sources.</p><p>We perform experiments with smaller general do- main data. In order to do this, we select a sub- set of CONCAT composed of the in-domain data of the three L1s along with 300,000 sentences ran- domly selected from the rest of CONCAT. This cor- pus is referred to as SMALL-CONCAT (623,717 sen- tences and 7,990,659 source tokens). We perform both KL-divergence regularized NNJM adaptation (NNJM SMALL-ADAPTED ) as well as Neural Domain Adaptation Model ( <ref type="bibr" target="#b18">Joty et al., 2015</ref>) (NDAM SMALL ) and compare them to NNJM trained with SMALL- CONCAT (NNJM SMALL-BASELINE ). We use these NNJMs with our S CONCAT baseline. The results are summarized in <ref type="table" target="#tab_7">Table 4</ref>. When the ratio be- tween in-domain data and general domain data is high, both adaptation methods do not significantly improve over an unadapted NNJM. In the case of L1 Spanish, KL-divergence regularized adaptation significantly outperforms the unadapted NNJM and NDAM as the size of in-domain data is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Effect of Regularization</head><p>To analyze the effect of regularization when smaller data are used, we vary the regularization weight λ in our objective function (Section 4). The results are shown in <ref type="figure">Figure 1</ref>. λ = 0 corresponds to no reg- ularization and training completely depends on the in-domain data apart from using the general NNJM as a starting point. On the other hand, setting λ = 1 forces the distribution learnt by the network to be similar to that of the unadapted model. We see that having no regularization (λ = 0) fails on all three L1s, due to overfitting on the smaller in-domain data. However, the effect of varying regularization is more significant on L1 Russian and Spanish, as the general NNJM has seen much smaller in-domain λ=0.00 λ=0.25 λ=0.50 λ=0.75 λ=1.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Effect of regularization for SCONCAT + NNJMADAPTED <ref type="bibr">(FCE)</ref> data compared to L1 Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Evaluation on Benchmark Dataset</head><p>We also evaluate our system on the benchmark CoNLL-2014 shared task ( ) test set for GEC in English. The CoNLL-2014 shared task consists of 1,312 sentences with two annotators. We also perform evaluation on the extension of CoNLL- 2014 test set ( <ref type="bibr" target="#b3">Bryant and Ng, 2015)</ref>, which contains eight additional sets of annotations over the two sets of annotations provided in the original test set. Fol- lowing the settings of the CoNLL-2014 shared task, we tune our unadapted baseline system and the L1- adapted systems on the CoNLL-2013 shared task test set consisting of 1,381 test sentences. The re- sults are summarized in <ref type="table" target="#tab_10">Table 5</ref>. We find that only the systems adapted based on L1 Chinese improves over the unadapted baseline sys- tem (S CONCAT + NNJM BASELINE ). When the smaller- sized, high-quality FCE data is used for adaptation the margin of improvement is higher. This could be due to large proportion of Chinese learner written text in CoNLL-2014 test set, as the essays are writ- ten by the students of National University of Sin- gapore comprising mostly of native Chinese speak- ers. Adaptation to L1 Russian and Spanish, does not help the system on CoNLL-2014 test set. We also compare our baseline SMT-based system with other state-of-the-art GEC systems. Our baseline system which is SMT-based, achieves the best F 0.5 score compared to other systems using the SMT approach alone, making it a competitive SMT-based GEC baseline. Overall, <ref type="bibr" target="#b32">(Rozovskaya and Roth, 2016</ref>  achieves the best F 0.5 score (47.40) after adding clas- sifier components, spelling checker, punctuation and capitalization correction components in a pipeline with their SMT-based system. However, their SMT- based system alone achieves an F 0.5 score of 39.48 only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Error Analysis</head><p>Our results show that L1-based adaptation of the NNJM using L1-specific in-domain data from Lang- 8 significantly improves the F 0.5 score of the GEC system on the three L1s by  which are difficult for non-native learners of En- glish.</p><p>We compare the outputs produced by our adapted system: S CONCAT + NNJM ADAPTED and the baseline: S CONCAT + NNJM BASELINE . We perform per error type quantitative analysis of our results by observ- ing the difference in the per error type F 0.5 scores averaged over five runs of tuning and evaluation of baseline and system. Computing per error type F 0.5 scores is difficult for SMT-based GEC systems, as the error types for corrections proposed by the SMT- based GEC system cannot be determined trivially. To overcome this difficulty, we attempt to determine the error type of the proposed corrections by match- ing them to the available human annotations (the source/target phrase without the surrounding con- text) in the complete FCE dataset (1,244 scripts). We select those sentences from the test data where the error type of every correction proposed by the baseline and the system can be determined. This process selects 928, 1102, and 2179 sentences for L1 Chinese, Russian, and Spanish, respectively. The differences in per error type F 0.5 scores between sys- tem and baseline are shown in <ref type="table" target="#tab_12">Table 6</ref>. For Chinese, the largest gain in F 0.5 is observed for determiner er- rors. Determiner errors are frequent in our L1 Chi- nese FCE test set (10.02%) . Moreover, we see that adaptation improves verb form errors by approxi- mately 0.4% F 0.5 . Verb form errors are the most fre- quent error type in our L1 Chinese test set (14.46%). Also, the highest gain for L1 Russian comes from determiner errors which is the most frequent error type in our FCE test data for L1 Russian (13.55%). Similarly, the highest gain for L1 Spanish comes from preposition errors which is the most frequent error type for L1 Spanish (12.51%).</p><p>From a practical standpoint, the adapted system can be used as an educational aid in English classes of local students in non-English-speaking countries, where all the students share the same L1 and their L1 is known in advance. The adapted system can give focused feedback to learners by correcting mis- takes frequently made by learners having the same L1. Also, NNJM adaptation proposed in this paper can be done using a small number of essays (50-100 essays) in a relatively short time <ref type="bibr">(20-30 minutes)</ref>, making it easy to adapt GEC systems in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we perform NNJM adaptation using L1-specific learner text with a KL divergence reg- ularized objective function. We integrate adapta- tion into an SMT-based GEC system. The systems with adapted NNJMs outperform unadapted base- lines significantly. We also demonstrate the neces- sity for regularization when adapting on smaller in- domain data. Our method of adaptation performs better compared to other adaptation methods, espe- cially when smaller in-domain data is used. Our re- sults show that adapting GEC systems for learners of similar L1 background gives significant improve- ment and can be adopted in practice to improve GEC systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Corpus 
#sents #src tokens #tgt tokens 

NUCLE 
57,063 
1,156,460 
1,151,278 
LANG-8 2,048,654 24,649,768 25,912,707 

CONCAT 2,105,717 25,806,228 27,063,985 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Statistics of training data</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of L1-specific data in Lang-8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>.</head><label></label><figDesc></figDesc><table>#scripts #sents 
#src 
tokens 
#tgt 
tokens 
#errors 

L1: Chinese 

DEV 
33 
1,041 15,424 15,601 
1,751 
TEST 
33 
1,078 15,640 15,816 
1,487 

L1: Russian 

DEV 
41 
1,125 17,021 17,267 
1,782 
TEST 
42 
1,263 18,738 18,920 
1,934 

L1: Spanish 

DEV 
100 
2,281 41,375 41,681 
4,133 
TEST 
100 
2,431 41,557 42,035 
4,237 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Statistics of the FCE dataset for each L1</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>16.11 35.09 38.11 16.99 30.52 43.40 12.74 29.28 S OUT 49.88 17.34 36.23 54.78 21.15 41.54 57.18 16.10 37.83 S CONCAT 51.72 17.56 37.23 54.17 21.70 41.62 55.45 16.93 38.09 S CONCAT + NNJM BASELINE 50.47 18.75 37.63 55.22 21.73 42.15 58.30 16.42 38.60 NNJM adaptation using KL divergence regularization S CONCAT + NNJM ADAPTED 56.02 17.59 38.90 55.71 22.53 43.03 59.05 16.77 39.24 S CONCAT + NNJM ADAPTED (FCE) 53.82 18.60 38.91 56.03 22.46 43.13 58.88 16.95 39.38 Comparison to other adaptation techniques TM INT + NNJM BASELINE 55.70 17.18 38.38 54.97 21.90 42.21 58.32 16.44 38.59 S CONCAT + NDAM 56.56 16.76 38.31 54.60 22.03 42.11 58.28 16.64 38.83 TM INT + NNJM ADAPTED 55.89 17.62 38.81 56.30 21.75 42.70 57.04 16.97 38.73 Using smaller general domain data S CONCAT + NNJM SMALL-BASELINE 53.29 17.47 37.75 55.34 20.87 41.55 58.05 16.46 38.55 S CONCAT + NDAM SMALL 53.89 17.36 37.87 55.29 21.09 41.70 56.82 16.69 38.36 S CONCAT + NNJM SMALL-ADAPTED 52.41 17.40 37.37 56.03 21.17 42.09 58.34 16.79 39.01</figDesc><table>: Russian 
L1: Spanish 
P 
R 
F 0.5 
P 
R 
F 0.5 
P 
R 
F 0.5 
S IN 
50.03 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Precision (P), recall (R), and F0.5 of L1-based adaptation of GEC systems. All results are averaged over 5 runs of tuning 

and evaluation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>)</head><label></label><figDesc></figDesc><table>System 
CoNLL-2014 
ST 
10ANN 
S CONCAT + NNJM BASELINE 
42.80 
59.14 
Adaptation based on L1 Chinese 
S CONCAT + NNJM ADAPTED 
43.06 
59.27 
S CONCAT + NNJM ADAPTED (FCE) 
44.27 
60.36 
Adaptation based on L1 Russian 
S CONCAT + NNJM ADAPTED 
42.73 
58.90 
S CONCAT + NNJM ADAPTED (FCE) 
42.12 
58.30 
Adaptation based on L1 Spanish 
S CONCAT + NNJM ADAPTED 
41.88 
58.32 
S CONCAT + NNJM ADAPTED (FCE) 
42.36 
58.54 
Best Published Results 
Rozovskaya and Roth (2016) 
(classifiers + spelling + SMT) 
47.40 
-
(SMT) 
39.48 
-
Chollampatt et al. (2016) (SMT) 41.75 
57.19 
Shared Task Teams 
CAMB (classifiers, rules, SMT) 37.33 
54.26 
CUUI (classifiers) 
36.79 
51.79 
AMU (SMT) 
35.01 
50.17 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>ST denotes F0.5 scores on the shared task test set and 

10ANN denotes the F0.5 scores on the extended test set with 10 

sets of annotations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Differences between per error type F0.5 scores of sys-

tem and baseline for the three L1s 

</table></figure>

			<note place="foot" n="1"> https://github.com/saffsd/langid.py</note>

			<note place="foot" n="3"> We use the NDAMv1 (Joty et al., 2015) trained using the log likelihood objective function with self-normalization.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kaveh Taghipour for insightful comments and discussions throughout this work. We are also grateful to the anonymous reviewers for their feed-back which helped in revising and improving the pa-per. This research is supported by Singapore Min-istry of Education Academic Research Fund Tier 2 grant MOE2013-T2-1-150.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference</title>
		<meeting>the Python for Scientific Computing Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reconstructing native language typology from foreign language usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Computational Natural Language Learning</title>
		<meeting>the 19th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contrastive analysis with predictive power: Typology driven estimation of grammatical error distributions in ESL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Computational Natural Language Learning</title>
		<meeting>the 19th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How far are we from fully automatic high quality grammatical error correction?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural network translation models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Correcting semantic collocation errors with L1-induced paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NUS at the HOO 2012 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on the Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 7th Workshop on the Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS corpus of learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Helping our own: The HOO 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HOO 2012: A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mixture-model adaptation for SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable modified KneserNey language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting n-best hypotheses to improve an SMT approach to grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Morphological type, spatial reference, and language transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Odlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Second Language Acquisition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="535" to="556" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How to avoid unwanted pregnancies: Domain adaptation using neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamla</forename><surname>Almannai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Experiments in domain adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Interactive Poster and Demonstration Sessions</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Efficient backprop. Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-domain feature selection for language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Joint Conference on Natural Language Processing</title>
		<meeting>the 5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-native text analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Massung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="163" to="186" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning SNS for automated Japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Joint Conference on Natural Language Processing</title>
		<meeting>the Fifth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The CoNLL2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding Second Language Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Ortega</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Hodder Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating confusion sets for context-sensitive error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for ESL correction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Illinois-Columbia system in the CoNLL-2014 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perplexity minimization for translation model domain adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Errors and Expectations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Shaughnessy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">System combination for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
