<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translation Rules with Right-Hand Side Lattices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Cromières</surname></persName>
							<email>fabien@pa.jst.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution" key="instit1">Japan Science and Technology Agency Kawaguchi-shi Saitama</orgName>
								<orgName type="institution" key="instit2">Kyoto University</orgName>
								<address>
									<postCode>332-0012, 606-8501</postCode>
									<settlement>Kyoto</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution" key="instit1">Japan Science and Technology Agency Kawaguchi-shi Saitama</orgName>
								<orgName type="institution" key="instit2">Kyoto University</orgName>
								<address>
									<postCode>332-0012, 606-8501</postCode>
									<settlement>Kyoto</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Translation Rules with Right-Hand Side Lattices</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="577" to="588"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In Corpus-Based Machine Translation, the search space of the translation candidates for a given input sentence is often defined by a set of (cycle-free) context-free grammar rules. This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation (where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence). But it is also possible to describe Phrase-Based Machine Translation in this framework. We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules. We also demonstrate how the representation of the search space has an impact on decoding efficiency, and how it is possible to optimize this representation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A popular approach to modern Machine Translation is to decompose the translation problem into a modeling step and a search step. The modeling step will consist in defin- ing implicitly a set of possible translations T for each input sentence. Each translation in T being associated with a real-valued model score. The search step will then consist in find- ing the translation in T with the highest model score. The search is non-trivial because it is usually impossible to enumerate all members of T (its cardinality being typically exponen- tially dependent on the size of the sentence to be translated).</p><p>Since at least <ref type="bibr" target="#b2">(Chiang, 2007)</ref>, a common way of representing T has been through a cycle-free context-free grammar.</p><p>In such a grammar, T is represented as a set of context-free rules such as can be seen on fig- ure 1. These rules themselves can be gener- ated by the modeling step through the use of phrase tables, synchronous parsing, tree-to- string rules, etc. If the model score of each translation is taken to be the sum of rule scores independently given to each rule, the search for the optimal translation is easy with some classic dynamic programming techniques.</p><p>However, if the model score is going to take into account informations such as the lan- guage model score of each sentence, it cannot be expressed in such a way. Since the lan- guage model score has proven empirically to be a very good source of information, <ref type="bibr" target="#b2">(Chiang, 2007)</ref> proposed an approximate search algo- rithm called cube pruning.</p><p>We propose here to represent T using context-free lattice-rules such as shown in fig- ure 2. This allows us to compactly encode a large number of rules. One benefit is that it adds flexibility to the modeling step, making it easier: many choices such as whether or not a function word should be included, the rela- tive position of words and non-terminal in the translation, as well as morphological variations can be delegated to the search step by encod- ing them in the lattice rules. While it is true that the same could be achieved by an explicit enumeration, lattice rules make this easier and more efficient.</p><p>In particular, we show that a decoding al- gorithm working with such lattice rules can be more efficient than one working directly on the enumeration of the rules encoded in the lattice.</p><p>A distinct but related idea of this paper is to consider how transforming the structure of the rules defining T can lead to improvements in the speed/memory performances of the de- coding. In particular, we propose a method to merge and reduce the size of the lattice rules and show that it translates into better perfor- mances at decoding time.</p><p>In this paper, we will first define more pre- cisely our concept of lattice-rules, then try to give some motivation for them in the context of a tree-to-tree MT system (section 3). In sec- tion 4, we then propose an algorithm for pre- processing a representation given in a lattice- rule form that allows for more efficient search. In section 5, we describe a decoding algorithm specially designed for handling lattice-rules. In section 6, we perform some experiments demonstrating the merit of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and Terminology</head><p>Here, we define semi-formally the terms we will use in this paper. We assume knowledge of the classic terminology of graph theory and context-free grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Expansion rules</head><p>A flat expansion rule is the association of a non-terminal and a "flat" right hand side that we note RHS. A flat RHS is a sequence of words and non-terminal. See <ref type="figure" target="#fig_0">figure 1</ref> for an example of a set of flat expansion rules.</p><p>A set of expansion rules is often produced in Hierarchical or Syntax-Based MT, by pars- ing with synchronous grammars or otherwise. In such a case, the set of rules define a rep- resentation of the (weighted) set of possible translations T of an input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lattice</head><p>In the general sense, a lattice can be described as a labeled directed acyclic graph. More pre- cisely, the type of lattice that we consider in this work is such that:</p><p>• Edges are labeled by either a word, a non-terminal or an epsilon (ie. an empty string).</p><p>• Vertices are only labeled by a unique id by which they can be designated.</p><p>Additionally, edges can also be labeled by a real-valued edge score and some real-valued edge features. Alternatively, a lattice could also be seen as an acyclic Finite State Automa- ton, with vertices and edges corresponding to states and transitions in the FSA terminology. For simplicity, we also set the constraint that each lattice has a unique "start" ver- tex labeled v S from which each vertex can be reached and a unique "end" vertex v E that can be reached from each vertex. Each path from v S to v E define thus a flat RHS, with score and features obtained by summing the score and features of each edge of the path.</p><p>A lattice expansion rule is similar to a flat expansion rule, but with the RHS being a lat- tice. Thus a set of lattice expansion rules can also define a set of possible translations T of an input sentence.</p><p>For a given lattice L, we will often note v ∈ L a vertex of L and e : v 1 → v 2 ∈ L an edge of L going from vertex v 1 to vertex v 2 . <ref type="figure" target="#fig_1">Figures 2 and 3</ref> show examples of such lat- tices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Translation set and Representations</head><p>We note T a set of weighted sentences. T is in- tended as representing the set of scored trans- lation candidates generated by a MT system for a given input sentence. As is customary in Corpus-Based MT literature, we will call de- coding the process of searching for the trans- lation with highest score in T . A representation of T , noted R T is a set of rules in a given formalism that implicitly de- fine T . As we mentioned earlier, in MT, R T is often a set of cycle-free context-free grammar rules.</p><p>In this paper, we consider representations R T consisting in a set of lattice expansion rules. With normal context-free grammar, it is usually necessary that a non-terminal is the left-hand side of several rules. Using lattice expansion rules, however, it is not necessary, as one lattice RHS can encode an arbitrary number of flat rules (see for example the RHS of X0 in <ref type="figure" target="#fig_2">figure 3</ref>). Therefore, we set the con- straint that there is only one lattice expansion rule for each left-hand non-terminal. And we will note unambiguously RHS(X) the lattice that is the right hand side of this rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setting</head><p>This work was developed mainly in the context of a syntactic-dependency-based tree-to-tree translation system described in ( <ref type="bibr" target="#b20">Richardson et al., 2014)</ref>. Although it is a tree-to-tree sys- tem, we simplify the decoding step by "flatten- ing" the target-side tree translation rules into string expansion rules (keeping track of the de- pendency structure in state features). Thus our setting is actually quite similar to that of many tree-to-string and string-to-string sys- tems. Aiming at simplicity and generality, we will set aside the question of target-side syn- tactic information and only describe our algo- rithms in a "tree-to-string" setting. We will also consider a n-gram language model score as our only stateful non-local feature.</p><p>However, this tree-to-tree original setting should be kept in mind, in particular when we describe the issue of the relative position of heads and dependents in section 3.2.2, as such issues do not appear as commonly in "X- to-string" settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rule ambiguities</head><p>Expansion rules are typically created by matching part of the input sentence with some aligned example bilingual sentence. The alignment (and the linguistic structure of the phrase in the case of Syntax-Based Ma- chine Translation) is then used to produce the target-side rule. However, it is often the case that it is difficult to fully specify a rule from an example. Such cases often come from two main reasons:</p><p>• Imperfect knowledge (eg. it is unclear whether a given unaligned word should belong to the translation)</p><p>• Context dependency (eg. the question of whether "to be" should be in plural form or not, depending on its subject in the constructed translation).</p><p>In both situation, it seems like it would be better to delay the full specification of the rule until decoding time, when the decoder can have access to the surrounding context of the rule and make a more informed choice. In particular, we can expect features such as lan- guage model or governor-dependent features (in the case of tree-to-tree Machine transla- tion) to help remove the ambiguities.</p><p>We detail some cases for which we encode variations as lattice-rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Non-aligned words</head><p>When rules are extracted from aligned exam- ples, we often find some target words which are not aligned to any source-side word and for which it is difficult to decide whether or not they should be included in the rule. Such words are often function words that do not have an equivalent in the source language. In Japanese-English translations, for example, articles such as "a" and "the" do not typically have equivalent in the Japanese side, and their necessity in the final sentence will often be a matter of context. We can make these edges optionals by doubling them with an epsilon- edge. Different weights and features can be given to the epsilon edges to balance the ten- dency of the decoder to skip edges. In figure 2, this is illustrated by the epsilon edges allowing to skip "for" and "the"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Non-terminal positions</head><p>In the context of our tree-to-tree translation system, we often find that we know which tar- get word should be the governor of a given non-terminal, but that we are unsure of the order of the words and non-terminals sharing a common governor. It can be convenient to represent such ambiguities in a lattice format as shown in <ref type="figure" target="#fig_1">figure 2</ref>. In this figure, one can see that the RHS of X0 encode two possible order- ing for the word "bus" and the non-terminal X2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Word variations</head><p>Linguistics phenomenons such as morpholog- ical variations can naturally create many mi- nor problems in the setting of Corpus-Based Translation. Especially if the variations in the target language have no equivalence in the source language. An example of this in Japanese-English translation is the fact that verbs in Japanese are "plural-independent", while the verb "to be" in English is not. There- fore, a RHS that is a candidate for translating a large part of a Japanese input sentence can easily use one of the variant of "to be" that is not consistent with the full sentence. To solve this, for each edge corresponding to the words "is" or "are", we add an alternative edge with the same start and end vertices as the other word. The decoder will then be able to choose the edge that gives the best language model score. The same can be done, for example, for the article "a/an". <ref type="figure" target="#fig_1">Figure 2</ref> provides an exam- ple of this, with two edges "is" and "are" in the RHS of X0.</p><p>Alternative edges can be labeled with differ- ent weights and features to tune the tendency of the decoder to choose a morphological vari- ation.</p><p>While such variations could be fixed in a post-processing step, we feel it is a better op- tion to let the decoder be aware of the possible options, lest it would discard rules due to lan- guage model considerations when these rules </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Representation optimisation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Goal</head><p>Given a description as a set of rule and scores R 1 T of T , it is often possible to find another de- scription R 2 T of T having the same formalism but a different set of rules. Although the T that is described remains the same, the same search algorithm applied to R 1 T or R 2 T might make approximations in a different way, be faster or use less memory.</p><p>It is an interesting question to try to trans- form an initial representation R 1 T into a rep- resentation R 2 T that will make the search step faster. This is especially interesting if one is going to search the same T several times, as is often done when one is fine-tuning the param- eters of a model, as this representation opti- misation needs only be done once.</p><p>The optimisation we propose is a natural fit to our framework of lattice rules. As lattice are a special case of Finite-State Automata (FSA), it is easy to adapt existing algorithms for FSA minimization. We describe a procedure in al- gorithm 1, which is essentially a simplification and adaptation to our case of the more gen- eral algorithm of <ref type="bibr" target="#b11">(Hopcroft, 1971)</ref> for FSA. The central parts of the algorithm are the two sub-procedures backward vertex merging and forward vertex merging. An example of the result of an optimisation is given on <ref type="figure" target="#fig_2">figure 3</ref>.</p><formula xml:id="formula_0">Data: Representation R T Result: Optimized Representation 1 for non-terminal X ∈ R T do 2</formula><p>Apply backward vertex merging to RHS(X);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Apply forward vertex merging to RHS(X); 4 end Algorithm 1: Representation optimisation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Forward and backward merging</head><p>We describe the forward vertex merging in algorithm 2. This merging will merge ver- tices and suppress redundant edges, proceed- ing from left to right. The end result is a lattice with a reduced number of vertices and edges, but encoding the same paths as the ini- tial one.</p><p>The basic idea here is to check the vertices from left to right and merge the ones that have identical incoming edges. After having been processed by the algorithm, a vertex is put in the set P (line 9). At each iteration, the can- didate set C contains the set of vertices that can potentially be merged together. It is up- dated at each iteration to contain the set of not-yet-processed vertices for which all incom- ing edges come from processed vertices (done by marking edges at line 6 and then updating C at line 10). At each iteration, the merging process consists in:</p><p>1. Eliminating duplicate edges from the pro- cessed vertices to the candidate vertices (line 5). These duplicate edges could have been introduced by the merging of previ- ously processed vertices.</p><p>2. Merging vertices whose set of incom- ing edges is identical.</p><p>Here, merg- ing two vertices v 1 and v 2 means that we create a third vertex v 3 such that incoming(v 3 ) = incoming(v 1 ) = incoming(v 2 ), and outgoing(v 3 ) = outgoing(v 3 1) ∪ outgoing(v 2 ), then re- move v 1 and v 2 .</p><p>The backward vertex merging is defined similarly to the forward merging, but with go- ing right to left and inverting the role of the incoming and outgoing edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data: Lattice RHS L Result: Optimized Lattice RHS 1 P ← ∅ //processed vertices;</head><p>2 C ← {v S } //candidate set ; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimizing the whole representation</head><p>Algorithm 1 describe the global optimisation procedure. For each lattice RHS, we just per- form first a backward merge and then a for- ward merge.</p><p>We have set the constraint in section 2.3 that each non-terminal should have only one lattice RHS. Note here that if there are sev- eral RHS for a given non-terminal, we can first merge them by merging their start vertex and end vertex, then apply this optimisation al- gorithm to obtain a representation with one optimised RHS per non-terminal.</p><p>This optimisation could be seen as doing some form of hypothesis recombination, but of- fline.</p><p>In term of rule optimisations, we only con- sider here transformations that do not mod- ify the number of non-terminals. But it is worthwhile to note that there are some se- quence appearing in the middle of some rules that cannot be merged through a lattice rep- resentation, but could be factored as sub-rules appearing in different non-terminals. Indeed, a lattice rule could actually be encoded as a set of "flat" rules by introducing a sufficient number of non-terminals, but this could pos- sibly be less efficient from the search algorithm point of view. We plan to investigate the ef- fects of this type of rule optimisations in con- junction with the described lattice-type opti-misations in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Handling of Edge Features</head><p>In the context of parameter tuning, we usually want the decoder to output not only the trans- lations, but also a list of features characteriz- ing the way the translation was constructed. Such features are, for example, the number of rules used, the language model of the transla- tion, etc. In out context, some features will be dependent on the specific edges used in a rule. For example, the epsilon edge used to option- ally skip non-aligned words (see section 3.2.1) is labeled with a feature "nb-words-skipped" set to 1, so that we can obtain the number of words skipped in a given translation and tune a score penalty for skipping such words. Similar features also exist for picking a word variation (section 3.2.3).</p><p>In the description of the merging process of section 4.2, one should thus be aware that two edges are to be considered identical only if both their associated word and their set of feature values are identical. This can some- times prevent useful merging of states to take place. A solution to this could be to follow (de <ref type="bibr" target="#b3">Gispert et al., 2010)</ref> and to discard all these features information during the decod- ing. The features values are then re-estimated afterward by aligning the translation and the input with a constrained version of the de- coder.</p><p>We prefer to actually keep track of the fea- tures values, even if it can reduce the efficiency of vertex merging. In that setting, we can also adapt the so-called Weight Pushing algorithm <ref type="bibr" target="#b19">(Mohri, 2004</ref>) to a multivalues case in order to improve the "mergeability" of vertices. The results of section 6.1 shows that it is still pos- sible to strongly reduce the size of the lattices even when keeping track of the features values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Decoding algorithm</head><p>In order to make an optimal use of these lattice-rule representations, we developed a decoding algorithm for translation candidate sets represented as a set of lattice-rules. For the most part, this algorithm re-use many of the techniques previously developed for decod- ing translation search spaces, but adapt them to our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview</head><p>The outline of the decoding algorithm is de- scribed by algorithm 3. For simplicity, the description only compute the optimal model score over the translations in the candidate set. It is however trivial to adapt the description to keep track of which sentence correspond to this optimal score and output it instead of the score. Likewise, using the technique described in <ref type="bibr" target="#b12">(Huang and Chiang, 2005</ref>), one can easily output k-best lists of translations. For sim- plicity again, we consider that a n-gram lan- guage model score is the only stateful non- local feature used for computing the model score, although in a tree-to-tree setting, other features (local in a tree representation but not in a string representation) could be used. The model score of a translation t has therefore the shape:</p><formula xml:id="formula_1">score(t) = λ · lm(t) + ∑ e score(e)</formula><p>where λ is the weight of the language model, lm(t) is the language model log-probability of t and the sum is over all edges e crossed to obtain t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scored language model states</head><p>Conceptually, in a lattice L, at each vertex v, we can consider the partial translations ob- tained by starting at v S and concatenating the words labeling each edge not labeled by a non- terminal until v. If an edge is labeled by a non- terminal X, we first traverse the correspond- ing lattice RHS(X) following the same pro- cess. Such a partial translation can be reduced compactly to a scored language model state (l, r, s), where l represent the first n words 1 of the partial translation, r its last n words and s its partial score. It is clear that if two partial translations have the same l and r parts but different score, we can discard the one with the lowest score, as it cannot be a part of the optimal translation. Further, using the state reduction tech- niques described in ( <ref type="bibr" target="#b17">Li and Khudanpur, 2008)</ref> and ), we can often reduce the size of l and r to less than n, allowing fur- ther opportunities for discarding sub-optimal partial translations. For better behavior dur- ing the cube-pruning step of the algorithm (see later), the partial score s of a partial transla- tion includes rest-costs estimates <ref type="bibr" target="#b7">(Heafield et al., 2012)</ref>.</p><p>We define the concatenation operation on scored language model states to be: (l 1 , r 1 , s 1 ) ⊕ (l 2 , r 2 , s 2 ) = (l 3 , r 3 , s 3 ), where s 3 = s 1 + s 2 + λlm(r 1 , l 2 ), with lm(r 1 , l 2 ) be- ing the language model probability of l 2 given r 1 with rest-costs adjustments. r 3 and l 3 are the resulting minimized states. Similarly, if an edge e is labeled by a word, we define the concatenation of a scored state with an edge to be (l 1 , r 1 , s 1 ) ⊕ e = (l 2 , r 2 , s 2 ) where s 2 = s 1 + score(e) + λlm(word(e)|r 1 ).</p><p>Conveniently for us, the KenLM 2 open- source library <ref type="bibr" target="#b10">(Heafield, 2011</ref>) provides func- tionalities for easily computing such concate- nation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Algorithm</head><p>Having defined these operations, we can now more easily describe algorithm 3. Each vertex v has a list best <ref type="bibr">[v]</ref> of the scored states of the best partial translations found to be ending at v. On line 1, we initialize best[v S ] with (., ., 0), where "." represent an empty language model state. We then traverse the vertices of the lattice in topological order.</p><p>For each edge e : v 1 → v 2 , we compute new scored states for best <ref type="bibr">[v 2</ref> ] as follow:</p><p>• if e is labeled by a word or an epsilon, we create a state st 2 = st 1 ⊕ e for each st 1 in best[v 1 ] (line 10).</p><p>• if e is labeled by a non-terminal X, we re- cursively call the decoding algorithm on the lattice RHS(X). The value returned by the line 15 will be a set of states corre- sponding to optimal partial translations traversing RHS(X). We can concate- nate these states with the ones in best[v 1 ] to obtain states corresponding to partial translations ending at v 2 (line 6).</p><p>Results of the calls decode(X) are memo- ized, as the same non-terminal is likely to ap- pear in several edges of a RHS and in several RHS.</p><p>2 http://kheafield.com/code/kenlm/ Lines 5 and 6 are the "cube-pruning-like" part of the algorithm. The function prune K returns the K best combinations of states in best <ref type="bibr">[v]</ref> and decode(RHS(X)), where best means "whose sum of partial score is highest". It can be implemented efficiently through the algorithms proposed in <ref type="bibr" target="#b12">(Huang and Chiang, 2005)</ref> or <ref type="bibr" target="#b2">(Chiang, 2007)</ref>.</p><p>The L ← max st operation on lines 6 and 10 has the following meaning: L is a list of scored language model state and st is a scored language model state. L ← max st means that, if L already contains a state st 2 with same left and right state as st, L is updated to contain only the scored state with the maximum score. If L do not contain a state similar to st, st in simply inserted into L. This is the "hypothe- sis recombination" part of the algorithm. The function trunc K ′ truncate the list best <ref type="bibr">[v]</ref> to its K ′ highest-scored elements.</p><p>The final result is obtained by calling decode(X 0 ), where X 0 is the "top-level" non- terminal.</p><p>The result of decode(X 0 ) will contain only one scored state of the form (BOS, EOS, s), with s being the optimal score.</p><p>The search procedure of algorithm 3 could be described as "breadth-first", since we sys- tematically visit each edge of the lattice. An alternative would be to use a "best-first" search with an A*-like procedure. We have tried this, but either because of optimisation issues or heuristics of insufficient qualities, we did not obtain better results than with the al- gorithm we describe here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We now describe a set of experiments aimed at evaluating our approach.</p><p>We use the Japanese-English data from the NTCIR-10 Patent MT task <ref type="bibr">3 (Goto et al., 2013)</ref>. The training data contains 3 millions parallel sentences for Japanese-English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effect of Lattice Representation and Optimisation</head><p>We first evaluate the impact of the lattice rep- resentation on the performances of our decod- ing algorithm. This will allow us to measure Data: Lattice RHS L Result: Sorted list of best states 1 best[v E ] = {(.,.,0.0)}; 2 for vertex v ∈ L in topological order do the benefits of our compact lattice represen- tation of rules, as well as the benefits of the representation optimisation algorithm of sec- tion 4. We use our Syntactic-dependency system to generate a lattice-rule representation of the possible translations of the 1800 sentences of the development set of the NTCIR-10 Patent MT task. We then produce two additional rep- resentations:</p><formula xml:id="formula_2">3 for edge e : v → v 2 ∈ outgoing(v) do 4 if label(e) = X then 5 for st 1 , st 2 ∈ prune K (best[v], decode(RHS(X)) do 6 best[v 2 ]← max st 1 ⊕ st 2 ; 7 end 8 else 9 for st ∈ trunc K ′ (best[v]) do 10 best[v 2 ]← max st ⊕ e;</formula><p>1. An optimized lattice-rule representation using the method described in section 4.</p><p>2. An expanded representation, that un- fold the original lattice-rule representa- tion into "flat rules" enumerating each path in the original lattice-rule represen- tation (like the list X0 ′ enumerate the lat- tice X0 in <ref type="figure" target="#fig_1">figure 2</ref>). <ref type="table" target="#tab_0">Table 1</ref> shows 3 columns. One for each of these 3 representations. We can see that, as expected, the performances in term of average search time or peak memory used are directly related to the number of vertices and edges in the representation. We can also see that our representation optimisation step is quite efficient, since it is able to divide by two the number of vertices in the representation, on average. This leads to a 2-fold speed improve- ment in the decoding step, as well as a large reduction of memory usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Decoding performances</head><p>In order to further evaluate the merit of our approach, we now compare the results ob- tained by using our decoder with lattice-rules with using a state-of-the-art decoder on the set of flat expanded rules equivalent to these lattice rules.</p><p>We use the decoder described in <ref type="bibr" target="#b8">(Heafield et al., 2013)</ref>, which is available under an open- source license 4 (henceforth called K-decoder). In this experience, we expanded the lattice rules generated by our MT system for 1800 sentences into files having the required format for the K-decoder. This basically mean we computed an equivalent of the expanded rep- resentation of section 6.1. This process gener- ated files ranging in size from 20MB to 17GB depending on the sentence. We then ran the K-decoder on these files and compared the re- sults with our own. We used a beam-width of 10000 for the K-decoder. Experiments were run in single thread mode. Partly to obtain more consistent results, and partly because the K-decoder was risking using too much memory for our system.</p><p>The <ref type="table">results on table 3</ref> show that, as the K- decoder do not have access to a more compact representation of the rules, it end up needing a much larger amount of memory for decoding the same sentences.</p><p>In term of model score obtained, the perfor- mances are quite similar, with the lattice-rule decoder providing slightly better model score.</p><p>It is interesting to note that, on "fair- ground" comparison, that is if our decoder do not have the benefit of a more compact lattice- rule representation, it actually perform quite worse as we can see by comparing with the third column of table 1 (at least in term of de- coding time and memory usage, while it would still have a very slight edge in term of model score with the selected settings). On the other hand, the K-decoder is a rather strong base- line, shown to perform several times faster than a previous state-of-the-art implementa- tion in <ref type="bibr" target="#b8">(Heafield et al., 2013)</ref>. It is well opti-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Optimized Expanded Peak memory used 39 GB 16GB 85GB Average search time 6.13s 3.31s 9.95s #vertices (avg/max) 65K (1300K) 32K (446K) 263K (5421K) #edges (avg/max) 92K (1512K) 83K (541K) 263K (5421K)  <ref type="table">Table 2</ref>: Impact on BLEU of using flexible lattice rules.</p><p>mized and makes use of advanced techniques with the language model (as the one described in ( <ref type="bibr" target="#b8">Heafield et al., 2013)</ref>) for which we do not have implemented an equivalent yet. There- fore, we are hopeful we can further improve our decoder in the future. Also, note that, for practical reason, while we only measured the decoding time for our decoder 5 , the K-decoder time include the time taken for loading the rule files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Translation quality</head><p>Finally, we evaluate the advantages of ex- tracting lattice rules such as proposed in sec- tion 3. That is, we consider rules for which null-aligned words are bypassable by epsilon- edges, for which Non-terminal are allowed to take several alternative positions around the word that is thought to be their governor, and for which we consider alternative morphologies of a few words ("is/are", "a/an"). We compare this approach with heuristically selecting only one possibility for each variation present in the lattice rule extracted from a single example.</p><p>Results shown on <ref type="figure" target="#fig_1">figure 2</ref> show that we do obtain a significant improvement in trans- lation quality. Note that the Moses score ( <ref type="bibr" target="#b15">Koehn et al., 2007)</ref>, taken from the official re- sults of NTCIR-10 is only here "for scale", as our MT system uses a quite different pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Searching for the most optimal translation in an implicitly defined set has been the focus of a lot of research in Machine Translation and it would be difficult to cover all of it. Among the most influential approaches, ( <ref type="bibr" target="#b14">Koehn et al., 2003</ref>) was using a form of stack based de- coding for Phrase-Based Machine Translation. <ref type="bibr" target="#b2">(Chiang, 2007)</ref> introduced the cube-pruning approach, which has been further improved in the previously mentioned <ref type="bibr">(Heafield et al., 2013). (Rush and</ref><ref type="bibr">Collins, 2011</ref>) recently pro- posed an algorithm promising to find the op- timal solution, but that is rather slow in prac- tice.</p><p>Weighted Finite State Machines have seen a variety of use in NLP <ref type="bibr" target="#b18">(Mohri, 1997)</ref>. More specifically, some other previous work on Ma- chine Translation have used lattices (or more generally Weighted Finite State Machines). In the context of Corpus-Based Machine Trans- lation, <ref type="bibr" target="#b13">(Knight and Al-Onaizan, 1998</ref>) was al- ready proposing to use Weighted Transducers to decode the "IBM" models of translation ( <ref type="bibr" target="#b0">Brown et al., 1993)</ref>. <ref type="bibr" target="#b1">(Casacuberta and Vidal, 2004</ref>) and ( <ref type="bibr" target="#b16">Kumar et al., 2006</ref>) also pro- pose to directly model the translation process with Finite State Transducers. ( <ref type="bibr" target="#b5">Graehl and Knight, 2004</ref>) propose to use Tree Transducers for modeling Syntactic Machine Translation. These approaches are however based on differ- ent paradigm, typically trying to directly learn a transducer rather than extracting SCFG-like rules.</p><p>Closer to our context, <ref type="bibr" target="#b3">(de Gispert et al., 2010)</ref> propose to use Finite-State Transducers in the context of Hierarchical Phrase Based Translation. Their method is to iteratively construct and minimize the full "top-level lat- tice" representing the whole set of translations bottom-up. It is an approach more focused on the Finite State Machine aspect than our, System K-decoder Lattice-rule decoder Peak memory used 52G 16G Average search time 3.47s 3.31s Average model score -107.55 -107.39 Nb wins 401 579 <ref type="table">Table 3</ref>: Evaluation of the performances of our lattice-rule decoder compared with a state-of- the-art decoder using an expanded flat representation of the lattice rules. "Nb wins" is the number of times one of the decoder found a strictly better model score than the other one, out of 1800 search.</p><p>which is more of an hybrid approach that stays closer to the paradigm of cube-pruning. The merit of their approach is that they can apply minimization globally, allowing for more possi- bilities for vertex merging. On the other hand, for large grammars, the "top-level lattice" will be huge, creating the need to prune vertices during the construction. Furthermore, the composition of the "top-level lattice" with a language model will imply redundant compu- tations (as lower-level lattices will potentially be expanded several times in the top-level lat- tice). As we do not construct the global lattice explicitly, we do not need to prune vertices (we only prune language model states). And each edge of each lattice rule is crossed only once during our decoding. Very recently, ( <ref type="bibr" target="#b9">Heafield et al., 2014</ref>) also considered using the redundancy of translation hypotheses to optimize phrase-based stack de- coding. To do so, they group the partial hy- potheses in a trie structure.</p><p>We are not aware of other work proposing "lattice rules" as a native format for express- ing translational equivalences. Work like (de <ref type="bibr" target="#b3">Gispert et al., 2010</ref>) rely on SCFG rules cre- ated along the <ref type="bibr" target="#b2">(Chiang, 2007)</ref> approach, while work like ( <ref type="bibr" target="#b1">Casacuberta and Vidal, 2004</ref>) adopt a pure Finite State Transducer paradigm (thus without explicit SCFG-like rules).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This work proposes to use a lattice-rule repre- sentation of the translation search space with two main goals:</p><p>• Easily represent the translation ambigui- ties that arise either due to lack of context or imperfect knowledge.</p><p>• Have a method for optimizing the repre- sentation of a search space to make this search more efficient.</p><p>We demonstrate that many types of am- biguities arising when extracting translation rules can easily be expressed in this frame- work, and that making these ambiguities ex- plicit and solvable at compile time through lattice-rules leads to improvement in transla- tion quality.</p><p>We also demonstrate that making a direct use of the lattice-rules representation allows a decoder to perform better than if working on the expanded set of corresponding "flat rules". And we propose an algorithm for computing more efficient representations of a translation candidate set.</p><p>We believe that the the link between the representation of a candidate set and the de- coding efficiency is an interesting issue and we intend to explore further the possibilities of optimizing representations both in the con- texts we considered in this paper and in others such as Phrase-Based Machine Translation.</p><p>The code of the decoder we implemented for this paper is to be released under a GPL li- cense 6 . <ref type="bibr">Alexander M Rush and Michael Collins. 2011</ref>.</p><p>Exact decoding of syntactic translation mod- els through lagrangian relaxation. In Proceed- ings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics: Human Language Technologies-Volume 1, pages 72-82. Association for Computational Linguistics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A simple cycle-free context grammar describing a set of possible translations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A simple example of lattice rule for non-terminal X0. The lower part list the set of "flat" rules that would be equivalent to the ones expressed by the lattice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The lattice RHS(X0) optimized with the algorithm described in section 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3 while |C| &gt; 0 do 4 for v ∈ C do 5 Eliminate duplicate edges in incoming(v); 6 Mark edges in outgoing(v); 7 end 8 Merge all vertices v 1 , v 2 ∈ C such that incoming(v 1 ) = incoming(v 2 ); 9 P ← P ∪ C; 10 C ← {v ∈ L ∖ P s.t. all edges in incoming(v) are marked}; 11 end Algorithm 2: Forward Vertex Merging</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>11 end 12 end 13 end 14 end 15 return best[v E ]; Algorithm 3: Lattice-rule decoding. See body for detailed explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Impact of the lattice representation on performances. 

System 
JA-EN 
Lattice 
29.43 
No-variations 
28.91 
Moses (for scale) 
28.86 

</table></figure>

			<note place="foot" n="1"> n being the order of the language mode</note>

			<note place="foot" n="3"> http://ntcir.nii.ac.jp/PatentMT-2/</note>

			<note place="foot" n="4"> http://kheafield.com/code/search/</note>

			<note place="foot" n="5"> in particular, we factored out the representation optimisation time, which is reasonable if we are in the setting of a parameter tuning step in which the same sentences are translated repeatedly</note>

			<note place="foot" n="6"> http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the Japanese Sci-ence and Technology Agency. We want to thank the anonymous reviewers for many very useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Machine translation with inferred stochastic finite-state transducers. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="205" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hierarchical phrasebased translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Adrià De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">R</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="505" to="533" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of the patent machine translation task at the ntcir10 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access</title>
		<meeting>the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL</title>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Left language model state for syntactic machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuo</forename><surname>Kiso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language model rest costs and spaceefficient storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1169" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grouping language model boundary words to speed k-best extraction from hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="958" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster Phrase-Based decoding by refining feature state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">KenLM: faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An n log n algorithm for minimizing states in a finite automaton. Theory of Machines and Computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better kbest parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology</title>
		<meeting>the Ninth International Workshop on Parsing Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Translation with finite-state devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine translation and the information soup</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="421" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A weighted finite state transducer translation template model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="35" to="75" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A scalable decoder for parsing-based machine translation with equivalent language model state maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation</title>
		<meeting>the Second Workshop on Syntax and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finite-state transducers in language and speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="311" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducer algorithms. an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Formal Languages and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kyotoebmt: An example-based dependency-todependency translation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Cromières</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL (System Demonstration)</title>
		<meeting>ACL (System Demonstration)<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
