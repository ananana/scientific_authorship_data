<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Greed is Good if Randomized: New Inference for Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Greed is Good if Randomized: New Inference for Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1013" to="1024"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Dependency parsing with high-order features results in a provably hard decoding problem. A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems. In contrast, we explore, analyze, and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing: a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that, as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is typically guided by param- eterized scoring functions that involve rich fea- tures exerting refined control over the choice of parse trees. As a consequence, finding the high- est scoring parse tree is a provably hard combina- torial inference problem <ref type="bibr" target="#b18">(McDonald and Pereira, 2006</ref>). Much of the recent work on parsing has focused on solving these problems using powerful optimization techniques. In this paper, we follow a different strategy, arguing that a much simpler in- ference strategy suffices. In fact, we demonstrate that a randomized greedy method of inference sur- passes the state-of-the-art performance in depen- dency parsing.</p><p>Our choice of a randomized greedy algorithm for parsing follows from a successful track record of such methods in other hard combinatorial prob- lems. These conceptually simple and intuitive algorithms have delivered competitive approxi- mations across a broad class of NP-hard prob- lems ranging from set cover <ref type="bibr" target="#b7">(Hochbaum, 1982)</ref> to MAX-SAT ( <ref type="bibr" target="#b25">Resende et al., 1997</ref>). Their success is predicated on the observation that most realiza- tions of problems are much easier to solve than the worst-cases. A simpler algorithm will therefore suffice in typical cases. Evidence is accumulating that parsing problems may exhibit similar proper- ties. For instance, methods such as dual decom- position offer certificates of optimality when the highest scoring tree is found. Across languages, dual decomposition has shown to lead to a cer- tificate of optimality for the vast majority of the sentences ( <ref type="bibr" target="#b13">Koo et al., 2010;</ref><ref type="bibr" target="#b16">Martins et al., 2011</ref>). These remarkable results suggest that, as a com- binatorial problem, parsing appears simpler than its broader complexity class would suggest. In- deed, we show that a simpler inference algorithm already suffices for superior results.</p><p>In this paper, we introduce a randomized greedy algorithm that can be easily used with any rich scoring function. Starting with an initial tree drawn uniformly at random, the algorithm makes only local myopic changes to the parse tree in an attempt to climb the objective function. While a single run of the hill-climbing algorithm may in- deed get stuck in a locally optimal solution, mul- tiple random restarts can help to overcome this problem. The same algorithm is used both for learning the parameters of the scoring function as well as for parsing test sentences.</p><p>The success of a randomized greedy algorithm is tied to the number of local maxima in the search space. When the number is small, only a few restarts will suffice for the greedy algorithm to find the highest scoring parse. We provide an al-gorithm for explicitly counting the number of lo- cal optima in the context of first-order parsing, and demonstrate that the number is typically quite small. Indeed, we find that a first-order parser trained with exact inference or using our random- ized greedy algorithm delivers basically the same performance.</p><p>We hypothesize that parsing with high-order scoring functions exhibits similar properties. The main rationale is that, even in the presence of high- order features, the resulting scoring function re- mains first-order dominant. The performance of a simple arc-factored first-order parser is only a few percentage points behind higher-order parsers. The higher-order features in the scoring function offer additional refinement but only a few changes above and beyond the first-order result. As a consequence, most of the arc choices are already determined by a much simpler, polynomial time parser.</p><p>We use dual decomposition to show that the greedy method indeed succeeds as an inference al- gorithm even with higher-order scoring functions. In fact, with second-order features, regardless of which method was used for training, the random- ized greedy method outperforms dual decomposi- tion by finding higher scoring trees. For the sen- tences that dual decomposition is optimal (obtains a certificate), the greedy method finds the same solution in over 99% of the cases. Our simple inference algorithm is therefore likely to scale to higher-order parsing and we demonstrate empiri- cally that this is indeed so. We validate our claim by evaluating the method on the CoNLL dependency benchmark that com- prises treebanks from 14 languages.</p><p>Aver- aged across all languages, our method out- performs state-of-the-art parsers, including Tur- boParser ( <ref type="bibr" target="#b17">Martins et al., 2013</ref>) and our earlier sampling-based parser ( ). On seven languages, we report the best published re- sults. The method is not sensitive to initialization. In fact, drawing the initial tree uniformly at ran- dom results in the same performance as when ini- tialized from a trained first-order distribution. In contrast, sufficient randomization of the starting point is critical. Only a small number of restarts suffices for finding (near) optimal parse trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, <ref type="bibr" target="#b18">McDonald and Pereira (2006)</ref> demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approx- imate inference has been at the center of pars- ing research. Examples of these approaches in- clude easy-first parsing <ref type="bibr" target="#b5">(Goldberg and Elhadad, 2010)</ref>, inexact search ( <ref type="bibr" target="#b11">Johansson and Nugues, 2007;</ref><ref type="bibr" target="#b29">Zhang and Clark, 2008;</ref><ref type="bibr" target="#b9">Huang et al., 2012;</ref><ref type="bibr" target="#b30">Zhang et al., 2013)</ref>, partial dynamic program- ming <ref type="bibr" target="#b8">(Huang and Sagae, 2010)</ref> and dual decom- position ( <ref type="bibr" target="#b13">Koo et al., 2010;</ref><ref type="bibr" target="#b16">Martins et al., 2011</ref>).</p><p>Our work is most closely related to the MCMC sampling-based approaches <ref type="bibr" target="#b22">(Nakagawa, 2007;</ref>). In our earlier work, we devel- oped a method that learns to take guided stochas- tic steps towards a high-scoring parse ( ). In the heart of that technique are so- phisticated samplers for traversing the space of trees. In this paper, we demonstrate that a sub- stantially simpler approach that starts from a tree drawn from the uniform distribution and uses hill- climbing for parameter updates achieves similar or higher performance.</p><p>Another related greedy inference method has been used for non-projective dependency pars- ing <ref type="bibr" target="#b18">(McDonald and Pereira, 2006</ref>). This method relies on hill-climbing to convert the highest scor- ing projective tree into its non-projective approxi- mation. Our experiments demonstrate that when hill-climbing is employed as a primary learning mechanism for high-order parsing, it exhibits dif- ferent properties: the distribution for initialization does not play a major role in the final outcome, while the use of restarts contributes significantly to the quality of the resulting tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy Approximations for NP-hard Problems</head><p>There is an expansive body of research on greedy approximations for NP-hard problems. Examples of NP-hard problems with successful greedy ap- proximations include the traveling saleman prob- lem problem <ref type="bibr" target="#b6">(Held and Karp, 1970;</ref><ref type="bibr" target="#b24">Rego et al., 2011</ref>), the MAX-SAT problem <ref type="bibr" target="#b20">(Mitchell et al., 1992;</ref><ref type="bibr" target="#b25">Resende et al., 1997</ref>) and vertex cover <ref type="bibr" target="#b7">(Hochbaum, 1982)</ref>. While some greedy methods have poor worst-case complexity, many of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, un- derstanding their properties is challenging: often their "theoretical analyses are negative and incon- clusive" <ref type="bibr">(Amenta and Ziegler, 1999;</ref><ref type="bibr" target="#b26">Spielman and Teng, 2001)</ref>. Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory <ref type="bibr" target="#b4">(Dumitrescu and Tóth, 2013;</ref><ref type="bibr" target="#b12">Jonsson et al., 2013)</ref>.</p><p>In NLP, randomized and greedy approximations have been successfully used across multiple ap- plications, including machine translation and lan- guage modeling <ref type="bibr">(Brown et al., 1993;</ref><ref type="bibr" target="#b23">Ravi and Knight, 2010;</ref><ref type="bibr" target="#b2">Daumé III et al., 2009;</ref><ref type="bibr" target="#b21">Moore and Quirk, 2008;</ref><ref type="bibr" target="#b3">Deoras et al., 2011)</ref>. In this paper, we study the properties of these approximations in the context of dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Let x be a sentence and T (x) be the set of possi- ble dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambi- guity. In addition, we define T (y, m) as the set of "neighboring trees" of y obtained by changing only the head of the modifier, i.e. y(m).</p><p>The dependency trees are scored according to S(x, y) = θ · φ(x, y), where θ is a vector of pa- rameters and φ(x, y) is a sparse feature vector rep- resentation of tree y for sentence x. In this work, φ(x, y) will include up to third-order features as well as a range of global features commonly used in re-ranking methods <ref type="bibr">(Collins, 2000;</ref><ref type="bibr">Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b10">Huang, 2008)</ref>.</p><p>The parameters θ in the scoring function are estimated on the basis of a training set</p><formula xml:id="formula_0">D = {(ˆ x i , ˆ y i )} N i=1</formula><p>of sentencesˆxsentencesˆ sentencesˆx i and the correspond- ing gold (target) treesˆytreesˆ treesˆy i . We adopt a max-margin framework for this learning problem. Specifically, we aim to find parameter values that score the gold target trees higher than others:</p><formula xml:id="formula_1">∀i ∈ {1, · · · , N }, y ∈ T (ˆ x i ), S(ˆ x i , ˆ y i ) ≥ S(ˆ x i , y) + ˆ y i − y 1 − ξ i</formula><p>where ξ i ≥ 0 is the slack variable (non-zero values are penalized against) andˆyandˆandˆy i − y 1 is the ham- ming distance between the gold treê y i and a can- didate parse y.</p><p>In an online learning setup, parameters are up- dated successively after each sentence. Each up- date still requires us to find the "strongest viola- tion", i.e., a candidate tree˜ytree˜ tree˜y that scores higher than the gold treê y i :</p><formula xml:id="formula_2">˜ y = arg max y∈T (ˆ x i ) {S(ˆ x i , y) + y − ˆ y i 1 }</formula><p>The parameters are then revised so as to select against the offending˜yoffending˜ offending˜y. Instead of a standard parameter update based oñ y as in perceptron, stochastic gradient descent, or passive-aggressive updates, our implementation follows  where the first-order parameters are broken up into a tensor. Each tensor component is updated successively in combination with the parameters corresponding to MST features ( <ref type="bibr" target="#b19">McDonald et al., 2005</ref>) and higher-order features (when included). <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm</head><p>During training and testing, the key combinatorial problem we must solve is that of decoding, i.e., finding the highest scoring tree˜ytree˜ tree˜y ∈ T (x) for each sentence x (orˆxorˆ orˆx i ). In our notation,</p><formula xml:id="formula_3">˜ y = arg max y∈T (ˆ x i ) {θ · φ(ˆ x i , y) + y − ˆ y i 1 } (train) ˜ y = arg max y∈T (x) {θ · φ(x, y)} (test)</formula><p>While the decoding problem with feature sets sim- ilar to ours has been shown to be NP-hard, many approximation algorithms work remarkably well.</p><p>We commence with a motivating example.</p><p>Locality and Parsing One possible reason for why greedy or other approximation algorithms work well for dependency parsing is that typical sentences and therefore the learned scoring func- tions S(x, y) = θ · φ(x, y) are primarily "lo- cal". By this we mean that head-modifier deci- sions could be made largely without considering the surrounding structure (the context). For exam- ple, in English an adjective and a determiner are typically attached to the following noun.</p><p>We demonstrate the degree of locality in de- pendency parsing by comparing a first-order tree- based parser to the parser that predicts each head word independently of others. Note that the in- dependent prediction of dependency arcs does not necessarily give rise to a tree. The parameters of  <ref type="table">Table 1</ref>: Head attachment accuracy of a first-order local classifier (left) and a first-order structural prediction model (right). The two types of mod- els are trained using the same set of features.</p><p>Input: parameter θ, sentence x Output: dependency tree˜ytree˜ tree˜y for each word m in list do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>y (t+1) = arg max y∈T (y (t) ,m) S(x, y);</p><formula xml:id="formula_4">7: t = t + 1;</formula><p>8:</p><p>end for 9: until no change in this iteration 10: returñ y = y (t) ; <ref type="figure">Figure 1</ref>: A randomized hill-climbing algorithm for dependency parsing.</p><p>the two parsers, the independent prediction and a tree-based parser, are trained separately with the corresponding decoding algorithm but with the same feature set. <ref type="table">Table 1</ref> shows that the accuracy of the inde- pendent prediction ranges from 79% to 93% on four CoNLL datasets. The results are on par with the first-order structured prediction model. This experiment reinforces the conclusion in <ref type="bibr" target="#b10">Liang et al. (2008)</ref>, where a local classifier was shown to achieve comparable accuracy to a sequential model (e.g. CRF) in POS tagging and named- entity recognition.</p><p>Hill-Climbing with Random Restarts We build here on the motivating example and explore greedy algorithms as generalizations of purely lo- cal decoding. Greedy algorithms break the decod- ing problem into a sequence of simple local steps, each required to improve the solution. In our case, simple local steps correspond to choosing the head for each modifier word.</p><p>We begin with a tree y (0) , which can be a sam- ple drawn uniformly from T (x) <ref type="bibr" target="#b28">(Wilson, 1996)</ref>. Our greedy algorithm then updates y (t) to a bet- ter tree y (t+1) by revising the head of one modifier word while maintaining the constraint that the re- sulting structure is a tree. The modifiers are con- sidered in the bottom-up order relative to the cur- rent tree (the word furthest from the root is consid- ered first). We provide an analysis to motivate this bottom-up update strategy in Section 4.1. The al- gorithm continues until the score can no longer be improved by changing the head of a single word. The resulting tree represents a locally optimal pre- diction relative to a single-arc greedy algorithm. <ref type="figure">Figure 1</ref> gives the algorithm in pseudo-code.</p><p>There are many possible variations of the sim- ple randomized greedy hill-climbing algorithm. First, the Wilson sampling algorithm <ref type="bibr" target="#b28">(Wilson, 1996)</ref> can be naturally extended to obtain i.i.d. samples from any first-order distributions. There- fore, we could initialize the tree y (0) with a tree from a first-order parser, or draw the initial tree from a first-order distribution other than uniform. However, perhaps surprisingly, as we demon- strate later, little is lost with uniform initializa- tion. Second, since a single run of randomized hill-climbing is relatively cheap and runs are in- dependent to each other, it is easy to execute mul- tiple runs independently in parallel. The final pre- dicted tree is then simply the highest scoring tree across the multiple runs. We demonstrate that only a small number of parallel runs are necessary for near optimal prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">First-Order Parsing</head><p>We provide here a firmer basis for why the ran- domized greedy algorithm can be expected to work. While the focus of the rest of the paper is on higher-order parsing, we limit ourselves in this subsection to first-order parsing. The reasons for this are threefold. First, a simple greedy algo- rithm is already not guaranteed a priori to work in the context of a first-order scoring function. The conclusions from this analysis are therefore likely to carry over to higher-order parsing scenarios as well. Second, a first-order arc-factored scoring provides us an easy way to ascertain when the ran- domized greedy algorithm indeed found the high- est scoring tree. Finally, we are able to count the <ref type="table" target="#tab_2">15  Turkish  12.1  1  1  2  100  100  Slovene  15.9  2  20  3647  100  98.1  English  24.0  21  121  2443  100  99.3  Arabic  36.8  2  35  &gt;10000  100  99.1   Table 2</ref>: The left part of the table shows the local optimum statistics of the first-order model. The sentences are sorted by the number of local optima. Columns 3 to 5 show the number of local optima of a sentence at different percentile of the sorted list. For example, on English 50% of the sentences have no more than 21 local optimum trees. The right part shows the fraction of finding global optima using 300 uniform restarts for each sentence.</p><note type="other">Dataset Average Len. # of local optima at percentile fraction of finding global optima (%) 50% 70% 90% 0 &lt;Len.≤ 15 Len.&gt;</note><p>number of locally optimal solutions for a greedy algorithm in the context of first-order parsing and can therefore relate this property to the success rates of the algorithm.</p><p>Reachability We begin by highlighting a basic property of trees, namely that single arc changes suffice for transforming any tree to any other tree in a small number of steps while maintaining that each intermediate structure is also a tree. In this sense, a target tree is reachable from any start- ing point using only single arc changes. More formally, let y be any starting tree and y the de- sired target. Let m 1 , m 2 , · · · , m n be the bottom- up list of words (modifiers) corresponding to tree y, where m 1 is the word furthest from the root. We can simply change each head y(m i ) to that of y (m i ) in this order i = 1, . . . , n. The bottom-up order guarantees that no cycle is introduced with respect to the remaining (yet unmodified) nodes of y. The fact that y is a valid tree implies no cycle will appear with respect to the already modified nodes. Note that, according to this property, any tree is reachable from any starting point using only k modifications, where k is the number of head dif- ferences, i.e. k = |{m : y(m) = y (m)}|. The result also suggests that it may be helpful to per- form the greedy steps in the bottom-up order, a suggestion that we follow in our implementation.</p><p>Broadly speaking, we have established that the greedy algorithm is not inherently limited by virtue of its basic steps. Of course, it is a differ- ent question whether the scoring function supports such local changes towards the correct target tree.</p><p>Locally Optimal Trees While greedy algo- rithms are notoriously prone to getting stuck in locally optimal solutions, we establish here that </p><formula xml:id="formula_5">Function CountOptima(G = V, E) V = {w 0 , w 1 , · · · , w n }</formula><formula xml:id="formula_6">V = V ∪ {w * } \ C;</formula><p>10:</p><formula xml:id="formula_7">E = E ∪ {e * j , e j * | ∀j / ∈ C} 11:</formula><p>count += CountOptima(G = V , E ); 12: end for 13: return count; <ref type="figure">Figure 2</ref>: A recursive algorithm for counting lo- cal optima for a sentence with words w 1 , · · · , w n (first-order parsing). The algorithm resembles the Chu-Liu-Edmonds algorithm for finding the max- imum directed spanning tree (Chu and <ref type="bibr">Liu, 1965)</ref>. decoding with learned scoring functions involves only a small number of local optima. In our case, a local optimum corresponds to a tree y where no single change of head y(m) results in a higher scoring tree. Clearly, the highest scoring tree is also a local optimum in this sense. If there were many such local optima, finding the one with the highest score would be challenging for a greedy algorithm, even with randomization.</p><p>We begin with a worst case analysis and estab-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Trained with Hill-Climbing (HC) Trained with Dual Decomposition (DD) %Cert (DD) sDD &gt; sHC sDD = sHC sDD &lt; sHC %Cert (DD) sDD &gt; sHC sDD = sHC sDD &lt; sHC  <ref type="table">Table 3</ref>: Decoding quality comparison between hill-climbing (HC) and dual decomposition (DD). Mod- els are trained either with HC (left) or DD (right). s HC denotes the score of the tree retrieved by HC and s DD gives the analogous score for DD. The columns show the percentage of all test sentences for which one method succeeds in finding a higher or the same score. "Cert" column gives the percentage of sentences for which DD finds a certificate.</p><p>lish a tight upper bound on the number of local optima for a first-order scoring function.</p><p>Theorem 1 For any first-order scoring function that factorizes into the sum of arc scores S(x, y) = S arc (y(m), m): (a) the number of locally op- timal trees is at most 2 n−1 for n words; (b) this upper bound is tight. 3</p><p>While the number of possible dependency trees is (n + 1) n−1 (Cayley's formula), the number of local optima is at most 2 n−1 . This is still too many for longer sentences, suggesting that, in the worst case, a randomized greedy algorithm is unlikely to find the highest scoring tree. However, the scor- ing functions we learn for dependency parsing are considerably easier.</p><p>Average Case Analysis In contrast to the worst- case analysis above, we will count here the actual number of local optima per sentence for a first- order scoring function learned from data with the randomized greedy algorithm. <ref type="figure">Figure 2</ref> provides pseudo-code for our counting algorithm. The al- gorithm is derived by tailoring the proof of Theo- rem 1 to each sentence. <ref type="table">Table 2</ref> shows the empirical number of locally optimal trees estimated by our algorithm across 4 different languages. Decoding with trained scor- ing functions in the average case is clearly sub- stantially easier than the worst case. For exam- ple, on the English test set more than 70% of the sentences have at most 121 locally optimal trees. Since the average sentence length is 24, the dis- crepancy between the typical number (e.g., 121) and the worst case (2 24−1 ) is substantial. As a re- sult, only a small number of restarts is likely to suffice for finding optimal trees in practice.</p><p>Optimal Decoding We can easily verify whether the randomized greedy algorithm indeed <ref type="bibr">3</ref> A proof sketch is given in <ref type="bibr">Appendix.</ref> succeeds in finding the highest scoring trees with a learned first-order scoring function. We have established above that there are typically only a small number of locally optimal trees. We would therefore expect the algorithm to work. We show the results in the second part of <ref type="table">Table 2</ref>. For short sentences of length up to 15, our method finds the global optimum for all the test sentences. Success rates remain high even for longer test sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Higher-Order Parsing</head><p>Exact decoding with high-order features is known to be provably hard <ref type="bibr" target="#b19">(McDonald et al., 2005</ref>). We begin our analysis here with a second-order (sib- ling/grandparent) model, and compare our ran- domized hill-climbing (HC) method to dual de- composition (DD), re-implementing <ref type="bibr" target="#b13">Koo et al. (2010)</ref>. <ref type="table">Table 3</ref> compares decoding quality for the two methods across four languages. Overall, in 97.8% of the sentences, HC obtains the same score as DD, in 1.3% of the cases HC finds a higher scoring tree, and in 0.9% of cases DD results in a better tree. The results follow the same pattern regardless of which method was used to train the scoring function. The average rate of certificates for DD was 92%. In over 99% of these sentences, HC reaches the same optimum.</p><p>We expect that these observations about the suc- cess of HC carry over to other high-order parsing models for several reasons. First, a large num- ber of arcs are pruned in the initial stage, con- siderably reducing the search space and minimiz- ing the number of possible locally optimal trees. Second, many dependencies can be determined already with independent arc prediction (see our motivating example above), predictions that are readily achieved with a greedy algorithm. Finally, high-order features represent smaller refinements, i.e., suggest only a few changes above and be- yond the dominant first-order scores. Greedy al-gorithms are therefore likely to be able to leverage at least some of this potential. We demonstrate be- low that this is indeed so.</p><p>Our methods are trained within the max-margin framework. As a result, we are expected to find the highest scoring competing tree for each train- ing sentence (the "strongest violation"). One may question therefore whether possible sub-optimal decoding for some training sentences (finding "a violation" rather than the "strongest violation") impacts the learned parser. To this end, <ref type="bibr" target="#b9">Huang et al. (2012)</ref> have established that weaker violations do suffice for separable training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Dataset and Evaluation Measures We evalu- ate our model on CoNLL dependency treebanks for 14 different languages ( <ref type="bibr">Buchholz and Marsi, 2006;</ref><ref type="bibr" target="#b27">Surdeanu et al., 2008)</ref>, using standard train- ing and testing splits. We use part-of-speech tags and the morphological information provided in the corpus. Following standard practice, we use Unla- beled Attachment Score (UAS) excluding punctu- ation ( <ref type="bibr" target="#b13">Koo et al., 2010;</ref><ref type="bibr" target="#b17">Martins et al., 2013)</ref> as the evaluation metric in all our experiments.</p><p>Baselines We compare our model with the Tur- boParser ( <ref type="bibr" target="#b17">Martins et al., 2013</ref>) and our earlier sampling-based parser ( ). For both parsers, we directly compare with the re- cent published results on the CoNLL datasets. We also compare our parser against the best pub- lished results for the individual languages in our datasets. This comparison set includes four ad- ditional parsers: Martins et al. (2011), <ref type="bibr" target="#b13">Koo et al. (2010)</ref>, <ref type="bibr" target="#b30">Zhang et al. (2013)</ref> and our tensor-based parser ( ).</p><p>Features We use the same feature templates as in our prior work (   <ref type="bibr">4</ref> . <ref type="figure">Figure 3</ref> shows the first-to third-order feature templates that we use in our model. For the global features we use right-branching, coor- dination, PP attachment, span length, neighbors, valency and non-projective arcs features.</p><p>Implementation Details Following standard practices, we train our model using the passive- aggressive online learning algorithm (MIRA) and parameter averaging <ref type="bibr" target="#b1">(Crammer et al., 2006</ref>; <ref type="bibr">4</ref> We refer the readers to  and  for the detailed definition of each feature template.</p><formula xml:id="formula_8">arc! head bigram! ! h h m m +1 h m consecutive sibling! h m s grandparent! g h m grand-sibling! g h m s tri-siblings! h m s t grand-grandparent! g h m gg outer-sibling-grandchild! h m s gc h s gc m</formula><p>inner-sibling-grandchild! <ref type="figure">Figure 3</ref>: First-to third-order features.  <ref type="bibr">Collins, 2002</ref>). By default we use an adaptive strategy for running the hill-climbing algorithm -for a given sentence we repeatedly run the al- gorithm in parallel 5 until the best tree does not change for K = 300 consecutive restarts. For each restart, by default we initialize the tree y (0) by sampling from the first-order distribution us- ing the current learned parameter values (and first- order scores). We train our first-order and third- order model for 10 epochs and our full model for 20 epochs for all languages, and report the average performance across three independent runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Comparison with the Baselines   <ref type="table">Table 5</ref>: Comparison between different initializa- tion strategies: (a) MAP-1st: only the MAP tree of the first-order score; (b) Uniform: random trees are sampled from the uniform distribution; and (c) Rnd-1st: random trees are sampled from the first-order distribution. For each method, the table shows the average accuracy of the initial tree and the final parsing accuracy. with TurboParser is achieved by restricting our model to third order features which still outper- forms TurboParser (89.10% vs 88.72%). To com- pare against the sampling-based parser, we em- ploy our model without the tensor component. The two models achieve a similar average performance (89.24% and 89.23% respectively). Since relative parsing performance depends on a target language, we also include comparison with the best pub- lished results. The model achieves the best pub- lished results for seven languages.</p><p>Another noteworthy comparison concerns first- order parsers. As <ref type="table" target="#tab_2">Table 4</ref> shows, the exact and ap- proximate versions of the first-order parser deliver almost identical performance. <ref type="table" target="#tab_2">Table 4</ref> shows that the model can effectively utilize high-order features. Comparing the average performance of the model variants, we see that the accuracy on the benchmark languages consistently improves when higher-order features are added. This char- acteristic of the randomized greedy parser is in line with findings about other state-of-the-art high- order parsers <ref type="bibr" target="#b17">(Martins et al., 2013;</ref>). <ref type="figure" target="#fig_2">Figure 4</ref> breaks down these gains based on the sentence length. As expected, on most lan- guages high-order features are particularly helpful when parsing longer sentences. <ref type="table">Table 5</ref> shows the impact of initialization on the model performance for several languages. We consider three strategies: the MAP estimate of the first- order score from the model, uniform sampling and sampling from the first-order distribution. The ac- curacy of initial trees varies greatly, ranging from 78.4% for the MAP estimate to 25.9% and 44.5% for the latter randomized strategies. However, the resulting parsing accuracy is not determined by the initial accuracy. In fact, the two sampling strategies result in almost identical parsing perfor- mance. While the first-order MAP estimate gives the best initial guess, the overall parsing accuracy of this method lags behind. This result demon- strates the importance of restarts -in contrast to the randomized strategies, the MAP initialization performs only a single run of hill-climbing. <ref type="bibr">Length ≤ 15 Length &gt;</ref>  <ref type="table">15  Slovene  100  98.11  English  100</ref> 99.12 <ref type="table">Table 6</ref>: Fractions (%) of the sentences that find the best solution among 3,000 restarts within the first 300 restarts. Figure 5: Convergence analysis on Slovene and English datasets. The graph shows the normalized score of the output tree as a function of the number of restarts. The score of each sentence is normal- ized by the highest score obtained for this sentence after 3,000 restarts. We only show the curves up to 1,000 restarts because they all reach convergence after around 500 restarts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of High-Order Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Initialization and Restarts</head><p>Convergence Properties <ref type="figure">Figure 5</ref> shows the score of the trees retrieved by our full model with respect to the number of restarts, for short and long sentences in English and Slovene. To facilitate the comparison, we normalize the score of each sen- tence by the maximal score obtained for this sen- tence after 3,000 restarts. Overall, most sentences converge quickly. This view is also supported by <ref type="table">Table 6</ref> which shows the fraction of the sentences that converge within the first 300 restarts. We can see that all the short sentences (length up to 15) reach convergence within the allocated restarts. Perhaps surprisingly, more than 98% of the long sentences also converge within 300 restarts.</p><p>Decoding Speed As the number of restarts im- pacts the parsing accuracy, we can trade perfor- mance for speed. <ref type="figure" target="#fig_4">Figure 6</ref> shows that the model achieves high performance with acceptable pars- ing speed. While various system implementation issues such as programming language and com- putational platform complicate a direct compari- son with other parsing systems, our model deliv- ers parsing time roughly comparable to other state- of-the-art graph-based systems (for example, Tur- boParser and MST parser) and the sampling-based parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have shown that a simple, generally appli- cable randomized greedy algorithm for inference suffices to deliver state-of-the-art parsing perfor- mance. We argued that the effectiveness of such greedy algorithms is contingent on having a small number of local optima in the scoring function. By algorithmically counting the number of locally op- timal solutions in the context of first-order parsing, we show that this number is indeed quite small. Moreover, we show that, as a decoding algorithm, the greedy method surpasses dual decomposition in second-order parsing. Finally, we empirically demonstrate that our approach with up to third- order and global features outperforms the state-of- the-art parsers when evaluated on 14 languages of • By contracting cycle C we obtain a new graph G of size |G| − |C| + 1 (Lines 5-11 of the algorithm). Easy to verify that (not shown): any local optimum in G is a local optimum in G and vice versa.</p><p>The theorem follows as a corollary of these steps. To see this, let F (G m ) be the number of local optima in the graph of size m:</p><formula xml:id="formula_9">F (G m ) ≤ max C⊆V (G) i F (G (i) m−c+1 )</formula><p>where G By solving forˆFforˆ forˆF (m) we getˆFgetˆ getˆF (m) ≤ 2 m−2 . Since m = n + 1 for a sentence with n words, the upper- bound of local optima is 2 n−1 .</p><p>To show the tightness, for any n &gt; 0, create the graph G n+1 with arc scores e ij = e ji = i for any 0 ≤ i &lt; j ≤ n. Note that w n → w n−1 → w n forms the circle C of size 2, it can be shown by induction on n and F (G n+1 ) that F (G n+1 ) = F (G n ) × 2 = 2 n−1 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where w 0 is the root E = {e ij ∈ R} are the arc scores Return: the number of local optima 1: Let y(0) = ∅ and y(i) = arg max j e ji ; 2: if y is a tree (no cycle) then return 1; 3: Find a cycle C ⊂ V in y; 4: count = 0; // contract the cycle 5: create a vertex w * ; 6: ∀j / ∈ C : e * j = max k∈C e kj ; 7: for each vertex w i ∈ C do 8: ∀j / ∈ C : e j * = e ji ; 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Absolute UAS improvement of our full model over the first-order model. Sentences in the test set are divided into 2 groups based on their lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Trade-off between performance and speed on Slovene and English datasets. The graph shows the accuracy as a function of decoding speed measured in second per token. Variations in decoding speed is achieved by changing the number of restarts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>m−c+1 is the graph (of size m − c + 1) created by selecting the i th arc in cycle C and con- tracting G m accordingly, and c = |C| is the size of the cycle. Definê F (m) as the upper bound of F (G m ) for any graph of size m. By the above formula, we know thatˆF thatˆ thatˆF (m) ≤ max 2≤c&lt;mˆF 2≤c&lt;mˆ 2≤c&lt;mˆF (m − c + 1) × c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 sum</head><label>4</label><figDesc></figDesc><table>-
marizes the results of our model, along with the 
state-of-the-art baselines. On average across 14 
languages, our full model with the tensor com-
ponent outperforms both TurboParser and the 
sampling-based parser. The direct comparison </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of our model and several state-of-the-art systems. "Best Published UAS" includes the 
most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et 
al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set 
of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser 
(Zhang et al., 2014) and tensor features (Lei et al., 2014). 

Dataset 
MAP-1st 
Uniform 
Rnd-1st 
UAS Init. UAS Init. UAS Init. 
Slovene 
85.2 80.1 86.7 13.7 86.7 34.2 
Arabic 
78.8 75.1 79.7 12.4 80.2 32.8 
English 
91.1 82.0 93.3 39.6 93.3 55.6 
Chinese 
87.2 75.3 93.2 36.8 93.0 54.5 
Dutch 
84.8 79.5 87.0 26.9 87.4 45.6 
Average 85.4 78.4 88.0 25.9 88.1 44.5 

</table></figure>

			<note place="foot">* Both authors contributed equally. 1 Our code is available at https://github.com/ taolei87/RBGParser.</note>

			<note place="foot" n="2"> We refer the readers to Lei et al. (2014) for more details about the tensor scoring function and the online update.</note>

			<note place="foot" n="5"> We use 8 threads in all the experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is developed in collaboration with the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the IYAS project. The authors acknowl-edge the support of the U.S. Army Research Of-fice under grant number W911NF-10-1-0533, and of the DARPA BOLT program. We thank the MIT NLP group and the ACL reviewers for their com-ments. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide here a more detailed justification for the counting algorithm in <ref type="figure">Figure 2</ref> and, by exten- sion, a proof sketch of Theorem 1. The bullets below follow the operation of the algorithm.</p><p>• Whenever independent selection of the heads results in a valid tree, there is only 1 opti- mum (Lines 1&amp;2 of the algorithm). Other- wise there must be a cycle C in y (Line 3 of the algorithm)</p><p>• We claim that any locally optimal tree y of the graph G = (V, E) must contain |C| − 1 arcs of the cycle C ⊆ V . This can be shown by contradiction. If y contains less than |C| − 1 arcs of C, then (a) we can construct a tree y that contains |C| − 1 arcs; (b) the heads in y are strictly better than those in y over the unused part of the cycle; (c) by reachability, there is a path y → y so y cannot be a local optimum.</p><p>• Any locally optimal tree in G must select an arc in C and reassign it. The rest of the |C|−1 arcs will then result in a chain.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">EMNLP &apos;02. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast re-scoring strategy to capture long distance dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1116" to="1127" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The traveling salesman problem for lines, balls and planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Csaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tóth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="828" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Elhadad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="742" to="750" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The traveling-salesman problem and minimum spanning trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1138" to="1162" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximation algorithms for the set covering and vertex cover problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><forename type="middle">S</forename><surname>Hochbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="556" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured perceptron with inexact search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental dependency parsing using online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1134" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complexity of sat problems, clone theory and the exponential time hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lagerkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Nordh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Zanuttini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1264" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual decomposition for parsing with non-projective head automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure compilation: trading structure for features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="592" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual decomposition with many overlapping components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In EACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hard and easy distributions of sat problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="459" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random restarts in minimum error rate training for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual dependency parsing using global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="952" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Does giza++ make search errors? Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Traveling salesman problem heuristics: leading methods, implementations and latest advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Rego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorabela</forename><surname>Gamboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Osterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="441" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Approximate solution of weighted max-sat problems using grasp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Mauricio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Resende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Pitsoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pardalos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="393" to="405" />
		</imprint>
	</monogr>
	<note>Satisfiability problems</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-third annual ACM symposium on Theory of computing</title>
		<meeting>the thirty-third annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL &apos;08</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning, CoNLL &apos;08</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating random spanning trees more quickly than the cover time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-eighth annual ACM symposium on Theory of computing</title>
		<meeting>the twenty-eighth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online learning for inexact hypergraph search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Steps to excellence: Simple inference with refined scoring of dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
