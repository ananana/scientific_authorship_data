<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting and Explaining Causes From Text For a Time Series Event</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technology Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gangal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technology Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technology Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technology Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technology Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting and Explaining Causes From Text For a Time Series Event</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2758" to="2767"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with tex-tual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a com-monsense causative knowledge base with efficient reasoning. To ensure good in-terpretability and appropriate lexical usage we combine symbolic and neural representations , using a neural reasoning algorithm trained on commonsense causal tu-ples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Producing true causal explanations requires deep understanding of the domain. This is beyond the capabilities of modern AI. However, it is possible to collect large amounts of causally related events, and, given powerful enough representational vari- ability, to construct cause-effect chains by select- ing individual pairs appropriately and linking them together. Our hypothesis is that chains composed of locally coherent pairs can suggest overall cau- sation.</p><p>In this paper, we view causality as (common- sense) cause-effect expressions that occur fre- quently in online text such as news articles or tweets. For example, "greenhouse gases causes global warming" is a sentence that provides an 'atomic' link that can be used in a larger chain. By connecting such causal facts in a sequence, the result can be regarded as a causal explanation be- tween the two ends of the sequence (see <ref type="table">Table 1</ref> for examples). This paper makes the following contributions:</p><p>• we define the problem of causal explanation generation, • we detect causal features of a time series event (CSPIKES) using Granger <ref type="bibr" target="#b16">(Granger, 1988)</ref> method with features extracted from text such as N-grams, topics, sentiments, and their com- position, • we produce a large graph called CGRAPH of lo- cal cause-effect units derived from text and de- velop a method to produce causal explanations by selecting and linking appropriate units, using neural representations to enable unit matching and chaining. <ref type="table">Table 1</ref>: Examples of generated causal expla- nation between some temporal causes and target companies' stock prices. The problem of causal explanation generation arises for systems that seek to determine causal factors for events of interest automatically. For given time series events such as companies' stock market prices, our system called CSPIKES detects events that are deemed causally related by time series analysis using Granger Causality regres- sion <ref type="bibr" target="#b16">(Granger, 1988)</ref>. We consider a large amount of text and tweets related to each company, and produces for each company time series of values for hundreds of thousands of word n-grams, topic labels, sentiment values, etc. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of causal features that temporally causes Facebook's stock rise in August.</p><p>However, it is difficult to understand how the statistically verified factors actually cause the changes, and whether there is a latent causal struc- ture relating the two. This paper addresses the challenge of finding such latent causal structures, in the form of causal explanations that connect the given cause-effect pair. <ref type="table">Table 1</ref> shows example causal explanation that our system found between party and Facebook's stock fall (↓).</p><p>To construct a general causal graph, we extract all potential causal expressions from a large cor- pus of text. We refer to this graph as CGRAPH. We use FrameNet ( <ref type="bibr" target="#b4">Baker et al., 1998</ref>) semantics to provide various causative expressions (verbs, relations, and patterns), which we apply to a resource of 183, 253, 995 sentences of text and tweets. These expressions are considerably richer than previous rule-based patterns ( <ref type="bibr" target="#b33">Riaz and Girju, 2013;</ref><ref type="bibr" target="#b25">Kozareva, 2012)</ref>. CGRAPH contains 5,025,636 causal edges.</p><p>Our experiment demonstrates that our causal- ity detection algorithm outperforms other baseline methods for forecasting future time series values. Also, we tested the neural reasoner on the infer- ence generation task using the BLEU score. Addi- tionally, our human evaluation shows the relative effectiveness of neural reasoners in generating ap- propriate lexicons in explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CSPIKES: Temporal Causality Detection from Textual Features</head><p>The objective of our model is, given a target time series y, to find the best set of textual features F = {f 1 , ..., f k } ⊆ X, that maximizes sum of causality over the features on y, where X is the set of all features. Note that each feature is itself a time series:</p><p>arg max</p><formula xml:id="formula_0">F C(y, Φ(X, y))<label>(1)</label></formula><p>where C(y, x) is a causality value function be- tween y and x, and Φ is a linear composition func- tion of features f . Φ needs target time series y as well because of our graph based feature selection algorithm described in the next sections. We first introduce the basic principles of Granger causality in Section 2.1. Section 2.2 de- scribes how to extract good source features F = {f 1 , ..., f k } from text. Section 2.3 describes the causality function C and the feature composition function Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Granger Causality</head><p>The essential assumption behind Granger causal- ity is that a cause must occur before its effect, and can be used to predict the effect. Granger showed that given a target time series y (effect) and a source time series x (cause), forecasting future tar- get value y t with both past target and past source time series E(y t |y &lt;t , x &lt;t ) is significantly power- ful than with only past target time series E(y t |y &lt;t ) (plain auto-regression), if x and y are indeed a cause-effect pair. First, we learn the parameters α and β to maximize the prediction expectation:</p><formula xml:id="formula_1">E(y t |y &lt;t , x t−l ) = m j=1 α j y t−j + n i=1 β i x t−i (2)</formula><p>where i and j are size of lags in the past obser- vation. Given a pair of causes x and a target y, if β has magnitude significantly higher than zero (according to a confidence threshold), we can say that x causes y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction from Text</head><p>Extracting meaningful features is a key compo- nent to detect causality. For example, to predict future trend of presidential election poll of Don- ald Trump, we need to consider his past poll data as well as people's reaction about his pledges such as Immigration, Syria etc. To extract such "good" features crawled from on-line media data, we pro- pose three different types of features: F words , F topic , and F senti .</p><p>F words is time series of N-gram words that re- flect popularity of the word over time in on-line media. For each word, the number of items (e.g., tweets, blogs and news) that contains the N-gram word is counted to get the day-by-day time series. For example, x M ichael Jordan = [12, 51, ..] is a time series for a bi-gram word Michael Jordan. We filter out stationary words by using simple measures to estimate how dynamically the time se- ries of each word changes over time. Some of the simple measures include Shannon entropy, mean, standard deviation, maximum slope, and number of rise and fall peaks.</p><p>F topic is time series of latent topics with re- spect to the target time series. The latent topic is a group of semantically similar words as identi- fied by a standard topic clustering method such as LDA ( <ref type="bibr" target="#b7">Blei et al., 2003)</ref>. To obtain temporal trend of the latent topics, we choose the top ten frequent words in each topic and count their occurrence in the text to get the day-by-day time series. For ex- ample, x healthcare means how popular the topic healthcare that consists of insurance, obamacare etc, is through time.</p><p>F senti is time series of sentiments (positive or negative) for each topic. The top ten frequent words in each topic are used as the keywords, and tweets, blogs and news that contain at least one of these keywords are chosen to calculate the senti- ment score. The day-by-day sentiment series are then obtained by counting positive and negative words using OpinionFinder ( <ref type="bibr" target="#b36">Wilson et al., 2005)</ref>, and normalized by the total number of the items that day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Temporal Causality Detection</head><p>We define a causality function C for calculating causality score between target time series y and source time series x. The causality function C uses Granger causality <ref type="bibr" target="#b16">(Granger, 1988)</ref> by fitting the two time series with a Vector AutoRegressive model with exogenous variables (VARX) <ref type="bibr" target="#b18">(Hamilton, 1994</ref>): y t = αy t−l + βx t−l + t where t is a white Gaussian random vector at time t and l is a lag term. In our problem, the number of source time series x is not single so the predic- tion happens in the k multi-variate features X = (f 1 , ...f k ) so:</p><formula xml:id="formula_2">y t = αy t−l + β(f 1,t−l + ... + f k,t−l ) + t (3)</formula><p>where α and β is the coefficient matrix of the tar- get y and source X time series respectively, and is a residual (prediction error) for each time se- ries. β means contributions of each lagged feature f k,t−l to the predicted value y t . If the variance of β k is reduced by the inclusion of the feature terms f k,t−l ∈ X, then it is said that f k,t−l Granger- causes y.</p><p>Our causality function C is then C(y, f, l) = ∆(β y,f,l ) where ∆ is change of variance by the feature f with lag l. The total Granger causality of target y is computed by summing the change of variance over all lags and all features:</p><formula xml:id="formula_3">C(y, X) = k,l C(y, f k , l)<label>(4)</label></formula><p>We compose best set of features Φ by choos- ing top k features with highest causality scores for each target y. In practice, due to large amount of computation for pairwise Granger calculation, we make a bipartite graph between features and tar- gets, and address two practical problems: noisi- ness and hidden edges. We filter out noisy edges based on TFIDF and fill out missing values using non-negative matrix factorization (NMF) <ref type="bibr" target="#b23">(Hoyer, 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CGRAPH Construction</head><p>Formally, given source x and target y events that are causally related in time series, if we could find a sequence of cause-effect pairs (x → e 1 ), (e 1 → e 2 ), ... (e t → y), then e 1 → e 2 , ... → e t might be a good causal explanation between x and y. Sec- tion 3 and 4 describe how to bridge the causal gap between given events (x, y) by (1) constructing a large general cause-effect graph (CGRAPH) from text, (2) linking the given events to their equivalent entities in the causal graph by finding the internal paths (x → e 1 , ...e t → y) as causal explanations, using neural algorithms.</p><p>CGRAPH is a knowledge base graph where edges are directed and causally related between entities. To address less representational variabil- ity of rule based methods <ref type="bibr" target="#b13">(Girju, 2003;</ref><ref type="bibr" target="#b6">Blanco et al., 2008;</ref><ref type="bibr" target="#b35">Sharp et al., 2016</ref>) in the causal graph construction, we used FrameNet ( <ref type="bibr" target="#b4">Baker et al., 1998</ref>) semantics. Using a semantic parser such <ref type="table">Table 2</ref>: Example (relation, cause, effect) tuples in different categories (manually labeled): general, company, country, and people. FrameNet labels related to causation are listed inside parentheses. The number of distinct relation types are 892.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Cause as SEMAFOR ( <ref type="bibr" target="#b9">Chen et al., 2010</ref>) that produces a FrameNet style analysis of semantic predicate- argument structures, we could obtain lexical tu- ples of causation in the sentence. Since our goal is to collect only causal relations, we extract total 36 causation related frames 1 from the parsed sen- tences. To generate meaningful explanations, high cov- erage of the knowledge is necessary. We collect six years of tweets and NYT news articles from 1989 to 2007 (See Experiment section for details). In total, our corpus has 1.5 billion tweets and 11 million sentences from news articles. The <ref type="table" target="#tab_1">Table 3</ref> has the number of sentences processed and num- ber of entities, relations, and tuples in the final CGRAPH.</p><p>Since the tuples extracted from text are very noisy 2 , we constructed a large causal graph by linking the tuples with string match and filter out the noisy nodes and edges based on some graph statistics. We filter out nodes with very high de- gree that are mostly stop-words or auto-generated sentences. Too long or short sentences are also fil- tered out. <ref type="table">Table 2</ref> shows the (case, relation, effect) tuples with manually annotated categories such as General, Company, Country, and People.</p><p>1 Causation, Cause change, Causation scenario, Cause benefit or detriment, Cause bodily experience, etc.</p><p>2 SEMAFOR has around 62% of accuracy on held-out set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Causal Reasoning</head><p>To generate a causal explanation using CGRAPH, we need traversing the graph for finding the path between given source and target events. This section describes how to efficiently traverse the graph by expanding entities with external knowl- edge base and how to find (or generate) appropri- ate causal paths to suggest an explanation using symbolic and neural reasoning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity Expansion with Knowledge Base</head><p>A simple choice for traversing a graph are the traditional graph searching algorithms such as Breadth-First Search (BFS). However, the graph searching procedure is likely to be incomplete (low recall), because simple string match is in- sufficient to match an effect to all its related en- tities, as it misses out in the case where an entity is semantically related but has a lexically different name.</p><p>To address the low recall problem and generate better explanations, we propose the use of knowl- edge base to augment our text-based causal graph with real-world semantic knowledge. We use Freebase (Google, 2016) as the external knowl- edge base for this purpose. Among 1.9 billion edges in original Freebase dump, we collect its first and second hop neighbours for each target events.</p><p>While our CGRAPH is lexical in nature, Free- base entities appear as identifiers (MIDs). For en- tity linking between two knowledge graphs, we need to annotate Freebase entities with their lex- ical names by looking at the wiki URLs. We re- fer to the edges with freebase expansion as KB-KB edges, and link the KB-KB with our CGRAPH us-ing lexical matching, referring as KBcross edges (See <ref type="table" target="#tab_1">Table 3</ref> for the number of the edges).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Symbolic Reasoning</head><p>Simple traversal algorithms such as BFS are infea- sible for traversing the CGRAPH due to the large number of nodes and edges. To reduce the search space k in e t → {e 1 t+1 , ...e k t+1 }, we restricted our search by depth of paths, length of words in en- tity's name, and edge weight.</p><p>Algorithm 1 Backward Causal Inference. y is tar- get event, d is depth of BFS, l is lag size, BF S back is Breadth-First search for one depth in backward direction, and l C is sum of Granger causality over the lags.</p><formula xml:id="formula_4">1: S ← y, d = 0 2: while (S = ∅) or (d &gt; D max ) do 3: {e 1 −d , ...e k −d } ← BF S back (S) 4: d = d + 1, S ← ∅ 5:</formula><p>for j in {1, ..., k} do 6:</p><formula xml:id="formula_5">if l C(y, e j −d , l) &lt; then S ← e j −d</formula><p>For more efficient inference, we propose a back- ward algorithm that searches potential causes (in- stead of effects) {e 1 t , ...e k t } ← e t+1 starting from the target node y = e t+1 using Breadth-first search (BFS). It keeps searching backward until the node e j i has less Granger confident causality with the target node y (See Algorithm 4 for causality calcu- lation). This is only possible because our system has temporal causality measure between two time series events. See Algorithm 1 for detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neural Reasoning</head><p>While symbolic inference is fast and straightfor- ward, the sparsity of edges may make our infer- ence semantically poor. To address the lexical sparseness, we propose a lexically relaxed reason- ing using a neural network.</p><p>Inspired by recent success on alignment task such as machine translation ( <ref type="bibr" target="#b3">Bahdanau et al., 2014</ref>), our model learns the causal alignment be- tween cause phrase and effect phrase for each type of relation between them. Rather than traversing the CGRAPH, our neural reasoner uses CGRAPH as a training resource. The encoder, a recurrent neural network such as LSTM (Hochre- iter and Schmidhuber, 1997), takes the causal phrase while the decoder, another LSTM, takes the effectual phrase with their relation specific atten- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A submarine driver Soviet nuclear secrets</head><p>Figure 2: Our neural reasoner. The encoder takes causal phrases and decoder takes effect phrases by learning the causal alignment between them. The MLP layer in the middle takes different types of FrameNet relation and locally attend the cause to the effect w.r.t the relation (e.g., "because of", "led to", etc).</p><p>In original attention model ( <ref type="bibr" target="#b3">Bahdanau et al., 2014</ref>), the contextual vector c is computed by c i = a ij * h j where h j is hidden state of causal sequence at time j and a ij is soft attention weight, trained by feed forward network a ij = F F (h j , s i−1 ) be- tween input hidden state h j and output hidden state s i−1 . The global attention matrix a, how- ever, is easy to mix up all local alignment patterns of each relation. google (Agent)) in terms of local type of causality. To deal with the local attention, we decomposed the attention weight a ij by relation specific transformation in feed forward network:</p><formula xml:id="formula_6">a ij = F F (h j , s i−1 , r)</formula><p>where F F has relation specific hidden layer and r ∈ R is a type of relation in the distinct set of relations R in training corpus (See <ref type="figure">Figure 2)</ref>.</p><p>Since training only with our causal graph may not be rich enough for dealing various lexical variation in text, we use pre-trained word em- bedding such as word2vec <ref type="bibr" target="#b27">(Mikolov and Dean, 2013</ref>) trained on GoogleNews corpus 3 for initial- ization. For example, given a cause phrase weapon equipped, our model could generate multiple ef- fect phrases with their likelihood: (  We trained our neural reasoner in either forward or backward direction. In prediction, decoder in- ferences by predicting effect (or cause) phrase in forward (or backward) direction. As described in the Algorithm 1, the backward inference continue predicting the previous causal phrases until it has high enough Granger confidence with the target event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>Data. We collect on-line social media from tweets, news articles, and blogs. Our Twitter data has one million tweets per day from 2008 to 2013 that are crawled using Twitter's Garden Hose API. News and Blog dataset have been crawled from 2010 to 2013 using Google's news API. For target time series, we collect companies' stock prices in NASDAQ and NYSE from 2001 until present for 6,200 companies. For presidential election polls, we collect polling data of the 2012 presidential election from 6 different websites, including USA Today , Huffington Post, Reuters, etc.</p><p>Features. For N-gram word features F word ,we choose the spiking words based on their temporal dynamics (See <ref type="table" target="#tab_2">Table 4</ref>). For example, if a word is too frequent or the time series is too burst, the word should be filtered out because the trend is too general to be an event. We choose five types of temporal dynamics: Shannon entropy, mean, stan- dard deviation, maximum slope of peak, and num- ber of peaks; and delete words that have too low or high entropy, too low mean and deviation, or the number of peaks and its slope is less than a certain threshold. Also, we filter out words whose frequency is less than five. From the 1, 677, 583 original words, we retain 21, 120 words as final candidates for F words including uni-gram and bi- gram words.</p><p>For sentiment F senti and topic F topic features, we choose 50 topics generated for both politicians and companies separately using LDA, and then use top 10 words for each topic to calculate sen- Tasks. To show validity of causality detector, first we conduct random analysis between target time series and randomly generated time series. Then, we tested forecasting stock prices and elec- tion poll values with or without the detected tex- tual features to check effectiveness of our causal features. We evaluate our reasoning algorithm for generation ability compared to held-out cause- effect tuples using BLEU metric. Then, for some companies' time series, we describe some qual- itative result of some interesting causal text fea- tures found with Granger causation and explana- tions generated by our reasoners between the tar- get and the causal features. We also conducted hu- man evaluation on the explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Random Causality Analysis</head><p>To check whether our causality scoring function C detects the temporal causality well, we con- duct a random analysis between target time se- ries and randomly generated time series (See <ref type="figure" target="#fig_4">Fig- ure 3)</ref>. For Google's stock time series, we regu-larly move window size of 30 over the time and generate five days of time series with a random peak strength using a SpikeM model ( <ref type="bibr" target="#b26">Matsubara et al., 2012)</ref>  <ref type="bibr">4</ref> . The color of random time series rf changes from blue to yellow according to causal- ity degree with the target C(y, rf ). For example, blue is the strongest causality with target time se- ries, while yellow is the weakest.</p><p>We observe that the strong causal (blue) features are detected just before (or after) the rapid rise of Google' stock price on middle October in (a) (or in (b)). With the lag size of three days, we observe that the strength of the random time series gradu- ally decreases as it grows apart from the peak of target event. The random analysis shows that our causality function C appropriately finds cause or effect relation between two time series in regard of their strength and distance. We use time series forecasting task as an eval- uation metric of whether our textual features are appropriately causing the target time series or not. Our feature composition function Φ is used to ex- tract good causal features for forecasting. We test forecasting on stock price of companies (Stock) and predicting poll value for presidential election (Poll). For stock data, We collect daily closing stock prices during 2013 for ten IT companies <ref type="bibr">5</ref> . For poll data, we choose ten candidate politicians 6 in the period of presidential election in 2012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Forecasting with Textual Features</head><p>For each of stock and poll data, the future trend of target is predicted only with target's past time <ref type="table">Table 6</ref>: Beam search results in neural reason- ing. These examples could be filtered out by graph heuristics before generating final explana- tion though.  <ref type="bibr" target="#b18">(Hamilton, 1994)</ref> from different compo- sition function such as C random , C words , C topics , C senti , and C composition . Each composition func- tion except C random uses top ten textual features that causes each target time series. We also tested LSTM with past time series and textual features but VARX outperforms LSTM. <ref type="table" target="#tab_3">Table 5</ref> shows root mean square error (RMSE) for forecasting with different step size (time steps to predict), different set of features, and different regression algorithms on stock and poll data. The forecasting error is summation of errors over mov- ing a window (30 days) by 10 days over the period. Our C composition method outperforms other time series only models and time series plus text mod- els in both stock and poll data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generating Causality with Neural Reasoner</head><p>The reasoner needs to predict the next effect phrase (or previous cause phrase) so the model should be evaluated in terms of generation task. We used the BLEU ( <ref type="bibr" target="#b30">Papineni et al., 2002</ref>) met- ric to evaluate the predicted phrases on held out phrases in our CGRAPH . Since our CGRAPH has many edges, there may be many good paths (ex- planations), possibly making our prediction di- verse. To evaluate such diversity in prediction, we used ranking-based BLEU method on the k set of predicted phrases by beam search. For example, B@k means BLEU scores for generating k num- ber of sentences and B@kA means the average of them. <ref type="table">Table 6</ref> shows some examples of our beam search results when k = 3. Given a cause phrase, the neural reasoner sometime predicts semanti- cally similar phrases (e.g., against the yen, against the dollar), while it sometimes predicts very di- verse phrases (e.g., a different, the risk). <ref type="table" target="#tab_4">Table 7</ref> shows BLEU ranking results with dif- ferent reasoning algorithms: S2S is a sequence to sequence learning trained on CGRAPH by de- fault, S2S+WE adds word embedding initializa- tion, and S2S+REL+WE adds relation specific at- tention. Initializing with pre-trained word embed- dings (+WE) helps us improve on prediction. Our relation specific attention model outperforms the others, indicating that different type of relations have different alignment patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Generating Explanation by Connecting</head><p>Evaluating whether a sequence of phrases is rea- sonable as an explanation is very challenging task. Unfortunately, due to lack of quantitative evalua- tion measures for the task, we conduct a human annotation experiment. <ref type="table">Table 8</ref> shows example causal chains for the rise (↑) and fall (↓) of companies' stock price, contin- uously produced by two reasoners: SYBM is sym- bolic reasoner and NEUR is neural reasoner.</p><p>We also conduct a human assessment on the ex- planation chains produced by the two reasoners, asking people to choose more convincing expla- nation chains for each feature-target pair. <ref type="table" target="#tab_5">Table 9</ref> shows their relative preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Prior works on causality detection <ref type="bibr" target="#b1">(Acharya, 2014;</ref><ref type="bibr" target="#b2">Anand, 2014;</ref><ref type="bibr" target="#b32">Qiu et al., 2012</ref>) in time series data (e.g., gene sequence, stock prices, tempera- ture) mainly use Granger <ref type="bibr" target="#b16">(Granger, 1988)</ref> ability for predicting future values of a time series us- ing past values of its own and another time series. <ref type="bibr" target="#b21">(Hlaváčková-Schindler et al., 2007</ref>) studies more theoretical investigation for measuring causal in- fluence in multivariate time series based on the entropy and mutual information estimation. How- ever, none of them attempts generating explana- tion on the temporal causality.</p><p>Previous works on text causality detection use syntactic patterns such as X verb −−→ Y , where the verb is causative <ref type="bibr" target="#b13">(Girju, 2003;</ref><ref type="bibr" target="#b33">Riaz and Girju, 2013;</ref><ref type="bibr" target="#b25">Kozareva, 2012;</ref><ref type="bibr" target="#b12">Do et al., 2011</ref>) with ad- ditional features ( <ref type="bibr" target="#b6">Blanco et al., 2008)</ref>. <ref type="bibr" target="#b25">(Kozareva, 2012</ref>) extracted cause-effect relations, where the pattern for bootstrapping has a form of</p><formula xml:id="formula_7">X * verb −−→ Z *</formula><p>Y from which terms X * and Z * was learned. The syntax based approaches, however, are not robust to semantic variation. As a part of SemEval ( <ref type="bibr" target="#b14">Girju et al., 2007)</ref>, (Mirza and Tonelli, 2016) also uses syntactic causative patterns <ref type="bibr" target="#b28">(Mirza and Tonelli, 2014</ref>) and supervised classifier to achieve the state-of-the-art perfor- mance. Extracting the cause-effect tuples with such syntactic features or temporality ( <ref type="bibr" target="#b5">Bethard et al., 2008</ref>) would be our next step for better causal graph construction.</p><p>(Grivaz, 2010) conducts very insightful anno- tation study of what features are used in human reasoning on causation. Beyond the linguistic tests and causal chains for explaining causality in our work, other features such as counterfactuality, temporal order, and ontological asymmetry remain as our future direction to study.</p><p>Textual entailment also seeks a directional re- lation between two given text fragments <ref type="bibr" target="#b11">(Dagan et al., 2006</ref>). Recently, ( <ref type="bibr" target="#b34">Rocktäschel et al., 2015</ref>) developed an attention-based neural net- work method, trained on large annotated pairs of textual entailment, for classifying the types of relations with decomposable attention ( <ref type="bibr" target="#b31">Parikh et al., 2016)</ref> or sequential tree structure <ref type="bibr" target="#b10">(Chen et al., 2016)</ref>. However, the dataset ( <ref type="bibr" target="#b8">Bowman et al., 2015</ref>) used for training entailment deals with just three categories, contradiction, neutral, and entailment, and focuses on relatively simple lexical and syntactic transformations <ref type="bibr" target="#b24">(Kolesnyk et al., 2016)</ref>. Our causal explanation generation task is also similar to future scenario genera- tion ( <ref type="bibr" target="#b20">Hashimoto et al., 2014</ref><ref type="bibr" target="#b19">Hashimoto et al., , 2015</ref>. Their scoring <ref type="table">Table 8</ref>: Example causal chains for explaining the rise (↑) and fall (↓) of companies' stock price. The temporally causal f eature and target are linked through a sequence of predicted cause-effect tuples by different reasoning algorithms: a symbolic graph traverse algorithm SYMB and a neural causality reasoning model NEUR.  function uses heuristic filters and is not robust to lexical variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper defines the novel task of detecting and explaining causes from text for a time series. First, we detect causal features from online text. Then, we construct a large cause-effect graph us- ing FrameNet semantics. By training our relation specific neural network on paths from this graph, our model generates causality with richer lexical variation. We could produce a chain of cause and effect pairs as an explanation which shows some appropriateness. Incorporating aspects such as time, location and other event properties remains a point for future work. In our following work, we collect a sequence of causal chains verified by domain experts for more solid evaluation of gen- erating explanations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of causal features for Facebook's stock change in 2013. The causal features (e.g., martino, k-rod) rise before the Facebook's rapid stock rise in August.</figDesc><graphic url="image-1.png" coords="1,319.28,222.54,194.25,96.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>For example, a tuple,</head><label></label><figDesc>(north korea (Agent) developing − −−−−−−−−−−−−−−−− → (Cause to make progress) nuclear weapons (Project)), is different with another tuple, (chrome (Item) promotes − −−−−−−−−−−−−−−−−− → (Cause change of position)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>army reorganized),</head><label></label><figDesc>etc, even though there are no tuples exactly matched in CGRAPH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Random causality analysis on Googles's stock price change (y) and randomly generated features (rf ) during 2013-01-01 to 2013-12-31. (a) shows how the random features rf cause the target y, while (b) shows how the target y causes the random features rf with lag size of 3 days. The color changes according to causality confidence to the target (blue is the strongest, and yellow is the weakest). The target time series has y scale of prices, while random features have y scale of causality degree C(y, rf ) ⊂ [0, 1].</figDesc><graphic url="image-3.png" coords="6,316.65,159.75,193.12,62.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>their weight series or with target's past time series and past time series of textual features found by our system. Forecasting only with target's past time series uses SpikeM (Matsubara et al., 2012) that models a time series with small number of parameters and simple LSTM (Hochreiter and Schmidhuber, 1997; nne, 2015) based time series model. Forecasting with target and textual features' time series use Vector AutoRegressive model with exogenous variables (VARX)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>SYMB medals match − −−− → gold and silver medals swept −−−→ korea improving −−−−−−→ relations widened − −−−− → gap widens − −−− → facebook ↑ excess match − −−− →excess materialism cause −−−→people make films make − −− →money changed − −−−− → twitter turned − −−− →facebook ↓ clinton match − −−− →president clinton raised − −−− →antitrust case match − −−− →government's antitrust case against microsoft match − −−− →microsoft beats − −− →apple ↓ NEUR google f orc − −− → microsoft to buy computer company dell announces recall of batteries cause −−−→ microsoft ↑ the deal make − −− → money rais −−→ at warner music and google with protest videos things caus − −− → google ↓ party cut − − → budget cuts lower −−−→ budget bill decreas −−−−→ republicans caus − −− → obama leadto − −−− → facebook polls caus − −− → facebook ↓ company f orc − −− → to stock price leadto − −−− → investors increas −−−−→ oracle s stock increas −−−−→ oracle ↑</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of sentences parsed, number of 
entities and tuples, and number of edges (KB-KB, 
KBcross) expanded by Freebase in CGRAPH. 

# Sentences 
# Entities 
# Tuples 
# KB-KB # KBcross 

183,253,995 5,623,924 5,025,636 
470,250 
151,752 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 : Examples of F words with their temporal dynamics: Shannon entropy, mean, standard devi- ation, slope of peak, and number of peaks.</head><label>4</label><figDesc></figDesc><table>entropy mean STD max slope #-peaks 

#lukewilliamss 0.72 22.01 18.12 
6.12 
31 
happy thanksgiving 0.40 61.24 945.95 3423.75 
414 
michael jackson 0.46 141.93 701.97 389.19 
585 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Forecasting errors (RMSE) on Stock 
and Poll data with time series only (SpikeM and 
LSTM) and with time series plus text feature (ran-
dom, words, topics, sentiment, and composition). 

Time Series 
Time Series + Text 
Step SpikeM LSTM C rand C words Ctopics Csenti Ccomp 

Stock 
1 102.13 6.80 3.63 
2.97 
3.01 
3.34 
1.96 
3 99.8 
7.51 4.47 
4.22 
4.65 
4.87 
3.78 
5 97.99 7.79 5.32 
5.25 
5.44 
5.95 
5.28 

Poll 
1 10.13 1.46 1.52 
1.27 
1.59 
2.09 
1.11 
3 10.63 1.89 1.84 
1.56 
1.88 
1.94 
1.49 
5 11.13 2.04 2.15 
1.84 
1.88 
1.96 
1.82 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 7 : BLEU ranking. Additional word rep- resentation +WE and relation specific alignment +REL help the model learn the cause and effect generation task especially for diverse patterns.</head><label>7</label><figDesc></figDesc><table>B@1 B@3A B@5A 

S2S 
10.15 
8.80 
8.69 
S2S + WE 
11.86 10.78 
10.04 
S2S + WE + REL 12.42 12.28 
11.53 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Human evaluation on explanation chains 
generated by symbolic and neural reasoners. 

Reasoners SYMB NEUR 

Accuracy (%) 
42.5 
57.5 

</table></figure>

			<note place="foot" n="3"> https://code.google.com/archive/p/word2vec/</note>

			<note place="foot" n="4"> SpikeM has specific parameters for modeling a time series such as peak strength, length, etc. 5 Company symbols used: TSLA, MSFT, GOOGL, YHOO, FB, IBM, ORCL, AMZN, AAPL and HPO 6 Name of politicians used: Santorum, Romney, Pual, Perry, Obama, Huntsman, Gingrich, Cain, Bachmann</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural network architecture for time series forecasting</title>
		<ptr target="https://github.com/hawk31/nnet-ts" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Causal modeling and prediction over event streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Acharya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Web Metric Summarization using Causal Relationship Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surya Pratap Singh Tanwar Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehndiratta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a corpus of temporal-causal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Corvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James H</forename><surname>Klingenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Causal relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Castell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Latent dirichlet allocation. JMLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semafor: Frame argument resolution with log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation</title>
		<meeting>the 5th international workshop on semantic evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="264" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Enhancing and combining sequential and tree lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Minimally supervised event causality identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Quang Xuan Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic detection of causal relations for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering</title>
		<meeting>the ACL 2003 workshop on Multilingual summarization and question answering</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 04: Classification of semantic relations between nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Freebase Data Dumps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://developers.google.com/freebase/data" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Some recent development in a concept of causality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Clive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of econometrics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human judgements on causation in french texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cécile</forename><surname>Grivaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Douglas</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Princeton university press Princeton</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating event causality hypotheses through semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2396" to="2403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward future scenario generation: Extracting event causality exploiting semantic relation, context, and association features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Kidawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="987" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Causality detection based on information-theoretic approaches in time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Hlaváčková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Paluš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladyslav</forename><surname>Kolesnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01404</idno>
		<title level="m">Generating natural language inference chains</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cause-effect relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Proceedings of TextGraphs-7 on Graph-based Methods for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rise and fall patterns of information diffusion: model and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuko</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="6" to="14" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An analysis of causality between events and its relation to temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Catena: Causal and temporal relation extraction from natural language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 26th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<title level="m">A decomposable attention model for natural language inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Granger causality for timeseries anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huida</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichang</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1074" to="1079" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward a better understanding of causality between verbal events: Extraction and analysis of the causal power of verb-verb associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehwish</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Creating causal embeddings for question answering with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hammond</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08097</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
