<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Recurrent Units for Highly Parallelizable Recurrence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
							<email>sidaw@cs.princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asapp</forename><surname>Inc</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Recurrent Units for Highly Parallelizable Recurrence</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4470" to="4481"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Common recurrent neural architectures scale poorly due to the intrinsic difficulty in par-allelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5-9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convo-lutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation by incorporating SRU into the architecture. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNN) are at the core of state-of-the-art approaches for a large num- ber of natural language tasks, including machine translation ( <ref type="bibr">Cho et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr">Jean et al., 2015;</ref><ref type="bibr" target="#b13">Luong et al., 2015)</ref>, lan- guage modeling ( <ref type="bibr" target="#b36">Zaremba et al., 2014;</ref><ref type="bibr">Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b41">Zoph and Le, 2016)</ref>, opin- ion mining <ref type="bibr">(Irsoy and Cardie, 2014)</ref>, and situated language understanding ( <ref type="bibr" target="#b14">Mei et al., 2016;</ref><ref type="bibr" target="#b18">Misra et al., 2017;</ref>. Key to many of these advancements are architectures of increased capacity and computa- tion. For instance, the top-performing models for semantic role labeling and translation use eight re- current layers, requiring days to train <ref type="bibr">(He et al., 2017;</ref><ref type="bibr">Wu et al., 2016b</ref>). The scalability of these models has become an important problem that im- pedes NLP research.</p><p>The difficulty of scaling recurrent networks arises from the time dependence of state com- putation. In common architectures, such as Long Short-term Memory (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU; <ref type="bibr">Cho et al., 2014)</ref>, the computation of each step is suspended until the complete ex- ecution of the previous step. This sequential de- pendency makes recurrent networks significantly slower than other operations, and limits their ap- plicability. For example, recent translation mod- els consist of non-recurrent components only, such as attention and convolution, to scale model train- ing ( <ref type="bibr">Gehring et al., 2017;</ref><ref type="bibr" target="#b29">Vaswani et al., 2017)</ref>.</p><p>In this work, we introduce the Simple Recurrent Unit (SRU), a unit with light recurrence that offers both high parallelization and sequence modeling capacity. The design of SRU is inspired by pre- vious efforts, such as Quasi-RNN (QRNN; <ref type="bibr">Bradbury et al., 2017)</ref> and Kernel NN (KNN; <ref type="bibr" target="#b10">Lei et al., 2017)</ref>, but enjoys additional benefits:</p><p>• SRU exhibits the same level of parallelism as convolution and feed-forward nets. This is achieved by balancing sequential dependence and independence: while the state compu- tation of SRU is time-dependent, each state dimension is independent. This simplifica- tion enables CUDA-level optimizations that parallelize the computation across hidden di- mensions and time steps, effectively using the full capacity of modern GPUs. <ref type="figure" target="#fig_0">Figure 1</ref> com- pares our architecture's runtimes to common architectures.</p><p>• SRU replaces the use of convolutions (i.e., n- gram filters), as in QRNN and KNN, with more recurrent connections. This retains modeling capacity, while using less compu- tation (and hyper-parameters). • SRU improves the training of deep recur- rent models by employing highway connec- tions ( <ref type="bibr" target="#b26">Srivastava et al., 2015</ref>) and a parame- ter initialization scheme tailored for gradient propagation in deep architectures.</p><p>We evaluate SRU on a broad set of problems, including text classification, question answering, translation and character-level language model- ing. Our experiments demonstrate that light re- currence is sufficient for various natural language tasks, offering a good trade-off between scala- bility and representational power. On classifica- tion and question answering datasets, SRU out- performs common recurrent and non-recurrent ar- chitectures, while achieving 5-9x speed-up com- pared to cuDNN LSTM. Stacking additional lay- ers further improves performance, while incurring relatively small costs owing to the cheap compu- tation of a single layer. We also obtain an average improvement of 0.7 BLEU score on the English to German translation task by incorporating SRU into Transformer ( <ref type="bibr" target="#b29">Vaswani et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Improving on common architectures for sequence processing has recently received significant atten- tion ( <ref type="bibr">Greff et al., 2017;</ref><ref type="bibr">Balduzzi and Ghifary, 2016;</ref><ref type="bibr" target="#b17">Miao et al., 2016;</ref><ref type="bibr" target="#b41">Zoph and Le, 2016;</ref><ref type="bibr" target="#b8">Lee et al., 2017)</ref>. One area of research involves incor- porating word-level convolutions (i.e. n-gram fil- ters) into recurrent computation ( <ref type="bibr" target="#b9">Lei et al., 2015;</ref><ref type="bibr">Bradbury et al., 2017;</ref><ref type="bibr" target="#b10">Lei et al., 2017</ref>). For ex- ample, Quasi-RNN ( <ref type="bibr">Bradbury et al., 2017)</ref> pro- poses to alternate convolutions and a minimal- ist recurrent pooling function and achieves sig- nificant speed-up over LSTM. While <ref type="bibr">Bradbury et al. (2017)</ref> focus on the speed advantages of the network, <ref type="bibr" target="#b10">Lei et al. (2017)</ref> study the theoret- ical characteristics of such computation and pos- sible extensions. Their results suggest that sim- plified recurrence retains strong modeling capac- ity through layer stacking. This finding motivates the design of SRU for both high parallelization and representational power. SRU also relates to <ref type="bibr">IRNN (Le et al., 2015)</ref>, which uses an identity di- agonal matrix to initialize hidden-to-hidden con- nections. SRU uses point-wise multiplication for hidden connections, which is equivalent to using a diagonal weight matrix. This can be seen as a constrained version of diagonal initialization.</p><p>Various strategies have been proposed to scale network training ( <ref type="bibr">Goyal et al., 2017)</ref> and to speed up recurrent networks ( <ref type="bibr">Diamos et al., 2016;</ref><ref type="bibr" target="#b29">Shazeer et al., 2017;</ref><ref type="bibr" target="#b6">Kuchaiev and Ginsburg, 2017)</ref>. For instance, <ref type="bibr">Diamos et al. (2016)</ref> utilize hardware infrastructures by stashing RNN param- eters on cache (or fast memory). <ref type="bibr" target="#b29">Shazeer et al. (2017)</ref> and <ref type="bibr" target="#b6">Kuchaiev and Ginsburg (2017)</ref> im- prove the computation via conditional computing and matrix factorization respectively. Our imple- mentation for SRU is inspired by the cuDNN- optimized LSTM ( <ref type="bibr">Appleyard et al., 2016</ref>), but en- ables more parallelism -while cuDNN LSTM re- quires six optimization steps, SRU achieves more significant speed-up via two optimizations.</p><p>The design of recurrent networks, such as SRU and related architectures, raises questions about representational power and interpretability <ref type="bibr">(Chen et al., 2018;</ref><ref type="bibr" target="#b23">Peng et al., 2018</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simple Recurrent Unit</head><p>We present and explain the design of Simple Re- current Unit (SRU) in this section. A single layer of SRU involves the following computation:</p><formula xml:id="formula_0">f t = (W f x t + v f c t1 + b f )<label>(1)</label></formula><formula xml:id="formula_1">c t = f t c t1 + (1 f t ) (Wx t ) (2) r t = (W r x t + v r c t1 + b r )<label>(3)</label></formula><formula xml:id="formula_2">h t = r t c t + (1 r t ) x t<label>(4)</label></formula><p>where W, W f and W r are parameter matrices and v f , v r , b f and b v are parameter vectors to be learnt during training. The complete architec- ture decomposes to two sub-components: a light recurrence (Equation 1 and 2) and a highway net- work (Equation 3 and 4). The light recurrence component successively reads the input vectors x t and computes the se- quence of states c t capturing sequential informa- tion. The computation resembles other recurrent networks such as LSTM, GRU and RAN ( <ref type="bibr" target="#b8">Lee et al., 2017)</ref>. Specifically, a forget gate f t controls the information flow (Equation 1) and the state vector c t is determined by adaptively averaging the previous state c t1 and the current observation Wx t according to f t (Equation 2).</p><p>One key design decision that differs from previ- ous gated recurrent architectures is the way c t1 is used in the sigmoid gate. Typically, c t1 is multiplied with a parameter matrix to compute f t , e.g., f t = (W f x t + V f c t1 + b f ). However, the inclusion of V f c t1 makes it difficult to par- allelize the state computation: each dimension of c t and f t depends on all entries of c t1 , and the computation has to wait until c t1 is fully com- puted. To facilitate parallelization, our light recur- rence component uses a point-wise multiplication v f c t1 instead. With this simplification, each dimension of the state vectors becomes indepen- dent and hence parallelizable.</p><p>The highway network component ( <ref type="bibr" target="#b26">Srivastava et al., 2015</ref>) facilitates gradient-based training of deep networks. It uses the reset gate r t (Equation 3) to adaptively combine the input x t and the state c t produced from the light recurrence (Equation 4), where (1 r t ) x t is a skip connection that allows the gradient to directly propagate to the pre- vious layer. Such connections have been shown to improve scalability ( <ref type="bibr" target="#b33">Wu et al., 2016a;</ref><ref type="bibr" target="#b3">Kim et al., 2016;</ref><ref type="bibr">He et al., 2016;</ref><ref type="bibr" target="#b40">Zilly et al., 2017</ref>).</p><p>The combination of the two components makes the overall architecture simple yet expressive, and easy to scale due to enhanced parallelization and gradient propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parallelized Implementation</head><p>Despite the parallelization friendly design of SRU, a naive implementation which computes equations (1)-(4) for each step t sequentially would not achieve SRU's full potential. We employ two op- timizations to enhance parallelism. The optimiza- tions are performed in the context of GPU / CUDA programming, but the general idea can be applied to other parallel programming models.</p><p>We re-organize the computation of equations <ref type="formula" target="#formula_0">(1)</ref>- <ref type="formula" target="#formula_2">(4)</ref> into two major steps. First, given the input sequence {x 1 · · · x L }, we batch the matrix multi- plications across all time steps. This significantly improves the computation intensity (e.g. GPU uti- lization). The batched multiplication is:</p><formula xml:id="formula_3">U &gt; = 0 @ W W f W r 1 A [x 1 , x 2 , · · · , x L ] ,</formula><p>where L is the sequence length, U 2 R L⇥3d is the computed matrix and d is the hidden state size. When the input is a mini-batch of B sequences, U would be a tensor of size (L, B, 3d). The second step computes the remaining point- wise operations. Specifically, we compile all point-wise operations into a single fused CUDA kernel and parallelize the computation across each dimension of the hidden state. Algorithm 1 shows the pseudo code of the forward function. The com- plexity of this step is O(L · B · d) per layer, where L is the sequence length and B is the batch size. In contrast, the complexity of LSTM is O(L · B · d 2 ) because of the hidden-to-hidden multiplications (e.g. Vh t1 ), and each dimension can not be in- dependently parallelized. The fused kernel also reduces overhead. Without it, operations such as sigmoid activation would each invoke a separate function call, adding kernel launching latency and more data moving costs.</p><p>The implementation of a bidirectional SRU is similar: the matrix multiplications of both direc- tions are batched, and the fused kernel handles and parallelizes both directions at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initialization</head><p>Proper parameter initialization can reduce gradient propagation difficulties and hence have a positive Algorithm 1 Mini-batch version of the forward pass defined in Equations (1)-(4).</p><p>Indices: Sequence length L, mini-batch size B, hidden state dimension d.</p><formula xml:id="formula_4">Input: Input sequences batch x[l, i, j]; grouped matrix multiplication U[l, i, j 0 ]; initial state c 0 [i, j]; parameters v f [j], v r [j], b f [j] and b r [j]. Output: Output h[·, ·, ·] and internal c[·, ·, ·] states. Initialize h[·, ·, ·] and c[·, ·, ·] as two L ⇥ B ⇥ d tensors. for i = 1, · · · , B; j = 1, · · · , d do // Parallelize each example i and dimension j c = c 0 [i, j] for l = 1, · · · , L do f = ( U[l, i, j + d] + v f [j] ⇥ c + b f [j] ) c = f ⇥ c + (1 f ) ⇥ U[l, i, j] r = ( U[l, i, j + d ⇥ 2] + v r [j] ⇥ c + b r [j] ) h = r ⇥ c + (1 r) ⇥ x[l, i, j] c[l, i, j] = c h[l, i, j] = h return h[·, ·, ·] and c[·, ·, ·]</formula><p>impact on the final performance. We now describe an initialization strategy tailored for SRU.</p><p>We start by adopting common initializations de- rived for feed-forward networks <ref type="bibr">(Glorot and Bengio, 2010;</ref><ref type="bibr">He et al., 2015)</ref>. The weights of param- eter matrices are drawn with zero mean and 1/d variance, for instance, via the uniform distribution</p><formula xml:id="formula_5">[ p 3/d, + p 3/d].</formula><p>This ensures the output vari- ance remains approximately the same as the input variance after the matrix multiplication.</p><p>However, the light recurrence and highway computation would still reduce the variance of hidden representations by a factor of 1/3 to 1/2:</p><formula xml:id="formula_6">1 3  Var[h t ] Var[x t ]  1 2 ,</formula><p>and the factor converges to 1/2 in deeper layers (see Appendix A). This implies the output h t and the gradient would vanish in deep models. To off- set the problem, we introduce a scaling correction constant ↵ in the highway connection</p><formula xml:id="formula_7">h t = r t c t + (1 r t ) x t · ↵ ,</formula><p>where ↵ is set to p 3 such that Var[h t ] ⇡ Var[x t ] at initialization. When the highway network is ini- tialized with a non-zero bias b r = b, the scaling constant ↵ can be accordingly set as:  </p><formula xml:id="formula_8">↵ = p 1 + exp(b) ⇥ 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate SRU on several natural language pro- cessing tasks and perform additional analyses of the model. other architectures. We stack multiple layers of SRU to directly substitute other recurrent, convo- lutional or feed-forward modules. We minimize hyper-parameter tuning and architecture engineer- ing for a fair comparison. Such efforts have a non- trivial impact on the results, which are beyond the scope of our experiments. Unless noted otherwise, the hyperparameters are set identical to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Classification</head><p>Dataset We use six sentence classification benchmarks: movie review sentiment (MR; Pang and <ref type="bibr" target="#b21">Lee, 2005</ref>), sentence subjectivity (SUBJ; <ref type="bibr" target="#b20">Pang and Lee, 2004</ref>), customer reviews polar- ity (CR; <ref type="bibr">Hu and Liu, 2004</ref>), question type (TREC; <ref type="bibr" target="#b12">Li and Roth, 2002</ref>), opinion polarity (MPQA; <ref type="bibr" target="#b32">Wiebe et al., 2005</ref>), and the Stanford sentiment treebank (SST; <ref type="bibr" target="#b25">Socher et al., 2013</ref>). <ref type="bibr">2</ref> Following Kim (2014), we use word2vec em- beddings trained on 100 billion Google News to- kens. For simplicity, all word vectors are normal- ized to unit vectors and are fixed during training.</p><p>Setup We stack multiple SRU layers and use the last output state to predict the class label for a given sentence. We train for 100 epochs and use the validation (i.e., development) set to se- lect the best training epoch. We perform 10-fold <ref type="bibr">2</ref> We use the binary version of SST dataset.</p><p>cross validation for datasets that do not have a standard train-evaluation split. The result on SST is averaged over five independent trials. We use Adam ( <ref type="bibr" target="#b4">Kingma and Ba, 2014</ref>) with the default learning rate 0.001, a weight decay 0 and a hid- den dimension of 128.</p><p>We compare SRU with a wide range of meth- ods on these datasets, including various convo- lutional models <ref type="bibr" target="#b1">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b2">Kim, 2014;</ref><ref type="bibr" target="#b37">Zhang and Wallace, 2017)</ref> and a hierarchical sentence model ( <ref type="bibr" target="#b39">Zhao et al., 2015</ref>) reported as the state of the art on these datasets ( <ref type="bibr">Conneau et al., 2017)</ref>. Their setups are not exactly the same as ours, and may involve more tuning on word em- beddings and other regularizations. We use the setup of Kim (2014) but do not fine-tune word embeddings and the learning method for simplic- ity. In addition, we directly compare against three baselines trained using our code base: a re- implementation of the CNN model of <ref type="bibr" target="#b2">Kim (2014)</ref>, a two-layer LSTM model and Quasi-RNN <ref type="bibr">(Bradbury et al., 2017)</ref>. We use the official implemen- tation of Quasi-RNN and also implement a ver- sion with highway connection for a fair compar- ison. These baselines are trained using the same hyper-parameter configuration as SRU.</p><p>Results <ref type="table">Table 1</ref> compares the test results on the six benchmarks. We select the best number re- ported in previous methods when multiple model variants were explored in their experiments. De- spite our simple setup, SRU outperforms most pre- vious methods and achieves comparable results compared to the state-of-the-art but more sophisti- cated model of <ref type="bibr" target="#b39">Zhao et al. (2015)</ref>. <ref type="figure" target="#fig_3">Figure 3</ref> shows validation performance relative to training time for SRU, cuDNN LSTM and the CNN model. Our SRU implementation runs 5-9 times faster than cuDNN LSTM, and 6-40% faster than the CNN model of <ref type="bibr" target="#b2">Kim (2014)</ref>. On the movie review (MR) dataset for instance, SRU completes 100 training epochs within 40 seconds, while LSTM takes over 320 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Answering</head><p>Dataset We use the Stanford Question Answer- ing Dataset (SQuAD; <ref type="bibr">Rajpurkar et al., 2016)</ref>. SQuAD is a large machine comprehension dataset that includes over 100K question-answer pairs ex- tracted from Wikipedia articles. We use the stan- dard train and development sets.</p><p>Setup We use the Document Reader model of <ref type="bibr">Chen et al. (2017)</ref> as our base architecture for this task. The model is a combination of word- level bidirectional RNNs and attentions, providing a good testbed to compare our bidirectional SRU implementation with other RNN components. <ref type="bibr">3</ref> We use the open source implementation of Doc- ument Reader in our experiments. <ref type="bibr">4</ref> We train mod- els for up to 100 epochs, with a batch size of 32 and a hidden dimension of 128. Following the author suggestions, we use the Adamax op- timizer ( <ref type="bibr" target="#b4">Kingma and Ba, 2014</ref>) and variational dropout ( <ref type="bibr">Gal and Ghahramani, 2016</ref>) during train- ing. We compare with two alternative recurrent components: the bidirectional LSTM adopted in the original implementation of <ref type="bibr">Chen et al. (2017)</ref> and Quasi-RNN with highway connections for im- proved performance.</p><p>Results <ref type="table" target="#tab_3">Table 2</ref> summarizes the results on SQuAD. SRU achieves 71.4% exact match and 80.2% F1 score, outperforming the bidirectional LSTM model by 1.9% (EM) and 1.4% (F1) re- spectively. SRU also exhibits over 5x speed-up over LSTM and 53-63% reduction in total train- ing time. In comparison with QRNN, SRU ob- tains 0.8% improvement on exact match and 0.6% on F1 score, and runs 60% faster. This speed im- provement highlights the impact of the fused ker-   nel (Algorithm 1). While the QRNN baseline in- volves a similar amount of computation, assem- bling all element-wise operations of both direc- tions in SRU achieves better GPU utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Machine Translation</head><p>Dataset We train translation models on the WMT English!German dataset, a standard benchmark for translation systems ( <ref type="bibr" target="#b22">Peitz et al., 2014;</ref><ref type="bibr" target="#b11">Li et al., 2014;</ref><ref type="bibr">Jean et al., 2015</ref>). The dataset consists of 4.5 million sentence pairs. We obtain the pre-tokenized dataset from the Open- NMT project ( <ref type="bibr" target="#b5">Klein et al., 2017</ref>). The sentences were tokenized using the word-piece model ( <ref type="bibr">Wu et al., 2016b</ref>), which generates a shared vocabu- lary of about 32,000 tokens. Newstest-2014 and newstest-2017 are provided and used as the vali- dation and test sets. 5</p><p>Setup We use the state-of-the-art Transformer model of <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref> as our base archi- tecture. In the base model, a single Transformer consists of a multi-head attention layer and a bot- tleneck feed-forward layer. We substitute the feed- forward network using our SRU implementation:</p><formula xml:id="formula_9">base: W · ReLU_layer(x) + b ours: W · SRU_layer(x) + b .</formula><p>The intuition is that SRU can better capture se- quential information as a recurrent network, and potentially achieve better performance while re- quiring fewer layers. We keep the model configuration the same as <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref>: the model dimension is d model = 512, the feed-forward and SRU layer has inner dimensionality d ff = d sru = 2048, and posi- tional encoding <ref type="bibr">(Gehring et al., 2017</ref>) is applied on the input word embeddings. The base model with- out SRU has 6 layers, while we set the number of layers to 4 and 5 when SRU is added. Following the original setup, we use a dropout probability 0.1 for all components, except the SRU in the 5-layer model, for which we use a dropout of 0.2 as we observe stronger over-fitting in training.</p><p>We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtained using 8 GPUs in parallel, which provide a large ef- fective batch size during training. To approximate the setup, we update the model parameters ev- ery 5⇥5120 tokens and use 16,000 warm-up steps following OpenNMT suggestions. We train each model for 40 epochs (250,000 steps), and perform 3 independent trials for each model configuration. A single run takes about 3.5 days with a Tesla V100 GPU.</p><p>Results <ref type="table" target="#tab_5">Table 3</ref> shows the translation results. When SRU is incorporated into the architecture, both the 4-layer and 5-layer model outperform the Transformer base model. For instance, our 5- layer model obtains an average improvement of 0.7 test BLEU score and an improvement of 0.5 BLEU score by comparing the best results of each model achieved across three runs. SRU also ex- hibits more stable performance, with smaller vari- ance over 3 runs. <ref type="figure">Figure 4</ref> further compares the validation accuracy of different models. These re- sults confirm that SRU is better at sequence mod- eling compared to the original feed-forward net- work (FFN), requiring fewer layers to achieve sim-    <ref type="bibr" target="#b6">Kuchaiev and Ginsburg, 2017)</ref> to reduce the total number of parameters without decreasing the performance. See details in Appendix B.</p><p>Results <ref type="table" target="#tab_7">Table 4</ref> presents the results of SRU and other recurrent models. The 8-layer SRU model achieves validation and test bits per char- acter (BPC) of 1.21, outperforming previous best reported results of LSTM, QRNN and recurrent highway networks (RHN). Increasing the layer of SRU to 12 and using a longer context of 256 char- acters in training further improves the BPC to 1.19</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Analysis</head><p>We perform ablation analyses on SRU by succes- sively disabling different components:</p><p>(1) Remove the point-wise multiplication term v c t1 in the forget and reset gates. The resulting variant involves less recurrence and has less representational capacity.</p><p>(2) Disable the scaling correction by setting the constant ↵ = 1.</p><p>(3) Remove the skip connections.</p><p>We train model variants on the classification and question answering datasets. <ref type="table" target="#tab_8">Table 5</ref> and <ref type="figure" target="#fig_6">Figure 5</ref> confirm the impact of our design decisions -re- moving these components result in worse classifi- cation accuracies and exact match scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Size # layers Unroll size Valid Test Time MI-LSTM ( <ref type="bibr" target="#b35">Wu et al., 2016c</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>This work presents Simple Recurrent Unit (SRU), a scalable recurrent architecture that operates as fast as feed-forward and convolutional units. We confirm the effectiveness of SRU on multiple nat- ural language tasks ranging from classification to translation. We open source our implementation to facilitate future NLP and deep learning research.</p><p>Trading capacity with layers SRU achieves high parallelization by simplifying the hidden-to- hidden dependency. This simplification is likely to reduce the representational power of a single layer and hence should be balanced to avoid perfor- mance loss. However, unlike previous work that suggests additional computation (e.g., n-gram fil- ters) within the layer ( <ref type="bibr">Balduzzi and Ghifary, 2016;</ref><ref type="bibr">Bradbury et al., 2017)</ref>, we argue that increasing the depth of the model suffices to retain modeling capacity. Our empirical results on various tasks confirm this hypothesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average processing time in milliseconds of a batch of 32 samples using cuDNN LSTM, wordlevel convolution conv2d (with filter width k = 2 and k = 3), and the proposed SRU. We vary the number of tokens per sequence (l) and feature dimension (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 compares</head><label>2</label><figDesc>Figure 2 compares the training progress with and without the scaling correction. See Appendix A for the derivation and more discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training curves of SRU on classification. The x-axis is the number of training steps and the y-axis is the training loss. Scaling correction improves the training progress, especially for deeper models with many stacked layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean validation accuracies (y-axis) and standard deviations of the CNN, 2-layer LSTM and 2-layer SRU models. We plot the curves of the first 100 epochs. X-axis is the training time used (in seconds). Timings are performed on NVIDIA GeForce GTX 1070 GPU, Intel Core i7-7700K Processor and cuDNN 7003.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation analysis on the classification datasets. Average validation results are presented. We compare the full SRU implementation (left blue), the variant without v c t1 multiplication (middle green) and the variant without highway connection (right yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). The first block presents best reported results of various methods. The second block compares SRU and other baselines given the same setup. For the SST dataset, we report average results of 5 runs. For other datasets, we perform 3 independent trials of 10-fold cross validation (3⇥10 runs). The last column compares the wall clock time (in seconds) to finish 100 epochs on the SST dataset.</figDesc><table>The set of tasks includes text classifica-
tion, question answering, machine translation, and 
character-level language modeling. Training time 
on these benchmarks ranges from minutes (classi-
fication) to days (translation), providing a variety 
of computation challenges. 
The main question we study is the performance-
speed trade-off SRU provides in comparison to Model 
Size 
CR 
SUBJ 
MR 
TREC 
MPQA 
SST 
Time 

Best reported results: 

Wang and Manning (2013) 
82.1 
93.6 
79.1 
-
86.3 
-
-
Kalchbrenner et al. (2014) 
-
-
-
93.0 
-
86.8 
-
Kim (2014) 
85.0 
93.4 
81.5 
93.6 
89.6 
88.1 
-
Zhang and Wallace (2017) 
84.7 
93.7 
81.7 
91.6 
89.6 
85.5 
-
Zhao et al. (2015) 
86.3 
95.5 
83.1 
92.4 
93.3 
-
-

Our setup (default Adam, fixed word embeddings): 

CNN 
360k 83.1±1.6 92.7±0.9 78.9±1.3 93.2±0.8 89.2±0.8 85.1±0.6 
417 
LSTM 
352k 82.7±1.9 92.6±0.8 79.8±1.3 93.4±0.9 89.4±0.7 88.1±0.8 2409 
QRNN (k=1) 
165k 83.5±1.9 93.4±0.6 82.0±1.0 92.5±0.5 90.2±0.7 88.2±0.4 
345 
QRNN (k=1) + highway 204k 84.0±1.9 93.4±0.8 82.1±1.2 93.2±0.6 89.6±1.2 88.9±0.2 
371 

SRU (2 layers) 
204k 84.9±1.6 93.5±0.6 82.3±1.2 94.0±0.5 90.1±0.7 89.2±0.3 
320 
SRU (4 layers) 
303k 85.9±1.5 93.8±0.6 82.9±1.0 94.8±0.5 90.1±0.6 89.6±0.5 
510 
SRU (8 layers) 
502k 86.4±1.7 93.7±0.6 83.1±1.0 94.7±0.5 90.2±0.8 88.9±0.6 
879 

Table 1: Test accuracies on classification benchmarks (Section 4.1</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Exact match (EM) and F1 scores of various models on SQuAD (Section 4.2). We also report 
the total processing time per epoch and the time spent in RNN computations. SRU outperforms other 
models, and is more than five times faster than cuDNN LSTM. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>English!German translation results (Section 4.3). We perform 3 independent runs for each 
configuration. We select the best epoch based on the valid BLEU score for each run, and report the 
average results and the standard deviation over 3 runs. In addition, we experiment with averaging model 
checkpoints and use the averaged version for evaluation, following (Vaswani et al., 2017). We show the 
best BLEU results achieved in brackets. 

3.8 

3.9 

4.1 

4.2 

4.3 

4.0 
4.3 
4.5 
4.8 
5.0 

67% 

68% 

70% 

71% 

72% 

1 
10 
20 
30 
40 

Base model (6) 
w/ SRU (4, 0.1) 
w/ SRU (5, 0.2) 

Train PPL 

Valid 

Train-valid perplexity 
Valid accuracy 

Epoch 

67% 

68% 

70% 

71% 

72% 

1 
10 
20 
30 
40 

Base model 
w/ SRU (4 layer) 
w/ SRU (5 layer) 

Valid accuracy 

Epoch 

3.8 

3.9 

4.1 

4.2 

4.3 

4.0 
4.3 
4.5 
4.8 
5.0 

Base model 
w/ SRU (5 layer) 
w/ SRU (4 layer) 

Train-valid perplexity 

Valid 

Train PPL 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Validation and test BPCs of different recurrent models on Enwik8 dataset. The last column 
presents the training time per epoch. For SRU with projection, we set the projection dimension to 512. 

Model 
4 layers 6 layers 

SRU (full) 
70.7 
71.4 
remove v c t1 
70.6 
71.1 
remove ↵-scaling 
70.3 
71.0 
remove highway 
69.4 
69.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ablation analysis on SQuAD. Compo-
nents are successively removed and the EM scores 
are averaged over 4 runs. 

CR 
CR 
SUBJ SUBJ 
MR 
MR 
Trec 
Trec 

CR 
CR 
SUBJ 
SUBJ 
MR 
MR 
Trec 
Trec 

CR 
CR 
SUBJ 
SUBJ 
MR 
MR 
Trec 
Trec 

CR 
SUBJ 
MR 
Trec 

91.2 

82.8 

94.8 

85.9 

92.2 

83.1 

95.3 

84.9 

92.8 

83.3 

95.4 

85.3 

</table></figure>

			<note place="foot" n="1"> Our code is available at https://github.com/ taolei87/sru.</note>

			<note place="foot" n="3"> The current state-of-the-art models (Seo et al., 2016; Wang et al., 2017) make use of additional components such as character-level embeddings, which are not directly comparable to the setup of Chen et al. (2017). However, these models can potentially benefit from SRU since RNNs are incorporated in the model architecture. 4 https://github.com/hitvoice/DrQA</note>

			<note place="foot" n="5"> https://github.com/OpenNMT/ OpenNMT-tf/tree/master/scripts/wmt</note>

			<note place="foot">Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep convolutional networks are hierarchical kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Anselmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<idno>abs/1508.01084</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Factorization tricks for lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno>abs/1703.10722</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Recurrent additive networks. CoRR, abs/1705.07393</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The DCUICTCAS MT system at WMT 2014 on germanenglish translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><forename type="middle">Cortes</forename><surname>Vaillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Computational linguistics</title>
		<meeting>the international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Matthew</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An analysis of neural language modeling at multiple scales. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1803.08240</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simplifying long short-term memory acoustic models for fast training and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mapping instructions and visual observations to actions with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast-slow recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asier</forename><surname>Mujika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelika</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting on Association for Computational Linguistics</title>
		<meeting>the annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting on association for computational linguistics</title>
		<meeting>the annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The RWTH aachen germanenglish machine translation system for wmt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rational recurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The sparsely-gated mixture-of-experts layer</title>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Situated mapping of sequential instructions to actions with single-step reward observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to map context-dependent sentences to executable formal queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An empirical exploration of skip connections for sequential tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguisticss</title>
		<meeting>the International Conference on Computational Linguisticss</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Å ˛ Aukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016b. Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On multiplicative integration with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing</title>
		<meeting>the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">1-regularized neural networks are improperly learnable in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
