<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The strange geometry of skip-gram with negative sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
							<email>mimno@cornell.edu,</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Thompson</surname></persName>
							<email>laurejt@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The strange geometry of skip-gram with negative sampling</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2873" to="2878"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Despite their ubiquity, word embeddings trained with skip-gram negative sampling (SGNS) remain poorly understood. We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is generally assumed that the geometry of word embeddings is determined by semantic relatedness. Vectors are assumed to be distributed throughout a K-dimensional space, with specific regions de- voted to specific concepts. We find that vectors trained with the skip-gram with negative sampling (SGNS) algorithm <ref type="bibr" target="#b7">(Mikolov et al., 2013</ref>) are not only influenced by semantics but are also strongly influenced by the negative sampling objective. In fact, far from spanning the possible space, they exist only in a narrow cone in R K . Nevertheless, SGNS vectors have become a foundational tool in NLP and perform as well or better than numerous methods with similar objectives ( <ref type="bibr" target="#b10">Turian et al., 2010;</ref><ref type="bibr" target="#b0">Dhillon et al., 2012;</ref><ref type="bibr" target="#b8">Pennington et al., 2014;</ref><ref type="bibr" target="#b5">Luo et al., 2015</ref>) with respect to evaluations of intrinsic and extrinsic quality ( <ref type="bibr" target="#b9">Schnabel et al., 2015)</ref>.</p><p>SGNS works by training two sets of embeddings: the "official" word embeddings and a second set of context embeddings, with one K-dimensional vec- tor in each set for each word in the vocabulary. The objective tries to make the word vector and con- text vector closer for a pair of words that actually occur together than for randomly sampled "nega- tive" words. Following training, the word vectors are typically saved; the context vectors are deleted. Any difference between these two sets of vectors is puzzling, since the sliding window used in training is symmetrical: a word and its context word reverse roles almost immediately. Indeed, the superficially similar GloVe algorithm ( <ref type="bibr" target="#b8">Pennington et al., 2014</ref>) also defines word and context vectors and by de- fault returns the mean of these two vectors.</p><p>Previous work has analyzed what the algorithm might be doing in theory, as an approximation to a matrix factorization ( <ref type="bibr" target="#b3">Levy and Goldberg, 2014</ref>). Other work has considered the empirical effects of some of the more arbitrary-seeming algorithmic choices ( <ref type="bibr" target="#b4">Levy et al., 2015)</ref>. But we still have rel- atively little understanding of how the algorithm actually determines parameter values. In this work we measure geometric properties of SGNS-trained word vectors and their context vectors. Although the word vectors appear to span K-dimensional space, we find that the SGNS ob- jective results in vectors that are narrowly clustered in a single orthant, and can be made non-negative without significant loss. <ref type="figure" target="#fig_0">Figure 1</ref> shows two vi- sualizations of SGNS vectors and context vectors. The context vectors mirror the "official" word vec- tors, with the angle between vectors effectively controlled by the number of negative samples. We show that this effect is due to negative sampling and not the general embedding objective. We note that this relationship between vectors is effectively hidden by the commonly-used t-SNE projection <ref type="bibr" target="#b6">(van der Maaten and Hinton, 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word embeddings with SGNS</head><p>The SGNS algorithm defines two sets of param- eters, K-dimensional word vectors w i and con- text vectors c i for each word i. We define a weight between a word i and a context word j as</p><formula xml:id="formula_0">σ ij = exp(w T i c j ) 1+exp(w T i c j )</formula><p>. For each observed pair i, j we sample S "negative" context words from a mod- ified unigram distribution p(w) 0.75 . The stochastic gradient update for one parameter w ik is then</p><formula xml:id="formula_1">dd dw ik = (1 − σ ij )c jk + S s=1 −σ is c sk ,<label>(1)</label></formula><p>suppressing for clarity a learning rate parameter λ. A symmetrical update is performed for the context word parameters c j and c s , substituting w i for c. This update has been shown to be equivalent to the gradient of a factorization of a pointwise mutual information matrix ( <ref type="bibr" target="#b3">Levy and Goldberg, 2014</ref>). The impact of the update is to push the vector w i closer to the context vector of the observed context word c j and away from the context vectors of the negatively sampled words. The amount of change at any given update is dependent on the degree to which the current model predicts the "correct" source of the context word, whether from the real data distribution or the negative sampling distribu- tion. If the model is infinitely certain that the real word is real (σ ij = 1.0) and the fake words are fake (σ is = 0.0 ∀s), it will make no change to the current parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We first present a series of empirical observa- tions based on vectors trained from a corpus of Wikipedia articles that is commonly distributed with word embedding implementations. <ref type="bibr">1</ref> We then evaluate the sensitivity of these properties to differ- ent algorithmic parameters. We make no assertion that these are optimal (or even particularly good) vectors, only that they are representative of the properties of the algorithm. To determine whether observed properties are due to SGNS specifically or to embeddings in gen- eral, we compare SGNS-trained vectors to vectors trained by the GloVe algorithm ( <ref type="bibr" target="#b8">Pennington et al., 2014</ref>). The choice of GloVe as a comparison is due to its popularity and superficial similarity to SGNS. <ref type="bibr">2</ref> We begin by examining one set of em- beddings from each algorithm, both with K = 50 dimensions, a vocabulary of ≈ 70k words, and context window 5. We then evaluate sensitivity to negative samples, window size, and dimension.</p><p>Embeddings are sensitive to word frequency ( <ref type="bibr" target="#b2">Hellrich and Hahn, 2016)</ref>. Following Zipf's law, words in natural language tend to sort into ranges of frequent words (the majority of tokens) and rare words (the majority of types), with a large class of intermediate-frequency terms in the middle. As a result, the large majority of interactions are be- tween frequent terms or between frequent and in- frequent terms. Interactions between infrequent terms are rare, no matter how large the corpus. We define four categories of words by ranked fre- quency: the top 100 words (ultra-high frequency), the 100-500th ranked words (high frequency), the 500-5000th ranked words (moderate frequency) and the remaining (low frequency) words.</p><p>SGNS vectors are arranged along a primary axis. Our first observation is that SGNS-trained vectors all point in roughly the same direction. We can define a mean vector ¯ w by averaging the vec- tors of the complete vocabulary w. We sample a balanced set of 400 total words with 100 each from the four frequency categories. <ref type="figure" target="#fig_1">Figure 2</ref> shows the  <ref type="table">Top5000  Remaining   0  1000  2000  3000  4000   0  1000  2000  3000  4000   0  1000  2000  3000  4000   0  1000  2000  3000  4000</ref> Top100 distribution of inner products between these 400 sampled words and their mean vectorˆwvectorˆ vectorˆw. All vec- tors have a large, positive inner product with the mean, indicating that they are not evenly dispersed through the space. Furthermore, the frequency cat- egory of words has relatively little effect on the inner product, with the exception of the rare words, which have slightly less positive inner products. As a comparison, the vectors trained by GloVe show a clear relationship with word frequency, with low- frequency words opposing the frequency-balanced mean vector. This result does not depend on a specific mean vector. Using the global mean vector rather than the frequency-balanced mean vector reverses the order of frequency categories within each plot, but does not change their overall shape. SGNS vector inner products are all positive, with low-frequency words the most positive. GloVe inner products become positive for low-frequency words and negative for high-frequency words.</p><p>The inner product between vectors is used by the algorithm during training, but in practice vectors are often normalized to have unit length before use. It is possible that the apparent pattern shown in <ref type="figure" target="#fig_1">Fig- ure 2</ref> may be an artifact of differing average lengths between words of different frequencies. After nor- malizing SGNS vectors to length 1.0, the lowest and highest frequency words are most similar to the mean vector, with the moderate-frequency words showing the greatest deviation. Normalization does not change the relative order for GloVe vectors.</p><p>SGNS vectors point away from context vectors. It is possible that vectors could have a positive inner product with the mean vector but be mutu- ally orthogonal. <ref type="figure" target="#fig_3">Figure 3</ref> shows the distribution of inner products w T i c j for pairs of words divided by frequency for SGNS and GloVe. Almost all interactions have similar, negative inner products for SGNS, while GloVe interactions are sensitive to frequency and vary more widely. We note that the high-frequency words in GloVe appear to form a cohesive cluster between themselves (positive inner products) that points away from the lower frequency words (negative inner products), while infrequent words are more dispersed and have no clear pattern relative to each other.</p><p>SGNS vectors are mostly non-negative. Not only do SGNS vectors occupy a narrow region of embedding space, it appears that the vectors can be rotated to fall mostly within the positive orthant. For each column of the matrix of vectors w we can compute the dimension-wise mean ¯ w k . Mul- tiplying w by a diagonal matrix of the signs of the means diag(sign( ¯ w k )) flips each dimension so that its mean is positive. <ref type="figure">Figure 4</ref> shows the resulting positive-mean histogram for 12 of the 50 dimensions trained by SGNS (the remaining dimen- sions are similar). Some dimensions have medians close to 0.0, but most skew positive.</p><p>Indeed, it is possible to simply drop all remain- ing negative values without radically changing the properties of the vectors. Embeddings are of- ten evaluated based on word similarity prediction ( <ref type="bibr" target="#b9">Schnabel et al., 2015)</ref>. Using only positive en- tries, Spearman rank correlation drops from 0.283 to .276 on the SIMLEX word similarity task and from 0.556 to 0.542 on the MEN task. Subtracting the global mean vector has similarly little impact, reducing SIMLEX correlation to 0.271 and increas- ing MEN correlation to 0.575. This property may help explain why sparse <ref type="bibr" target="#b1">(Faruqui et al., 2015</ref>) and non-negative ( <ref type="bibr" target="#b5">Luo et al., 2015</ref>) embeddings do not lose significant performance.</p><p>SGNS context vectors point away from the word vectors. What then is the geometry of the context vectors c? The two sets of vectors appear to present a noisy mirror image of each other. <ref type="figure">Figure  5</ref> shows the distribution of inner products between  the context vectors and the same mean vectorˆwvectorˆ vectorˆw used in <ref type="figure" target="#fig_1">Figure 2</ref>. These inner products are negative, indicating that the context vectors point in the op- posite direction from the word vectors. In contrast, the GloVe context vectors have essentially the same relationship to the mean of the word vectors as the word vectors themselves. This property explains why it is common to output the mean of w i and c i for each word for GloVe but not for SGNS: in Glove these two vectors are essentially noisy copies of one another, while in SGNS the two vectors are pointing almost in the opposite direction.</p><p>Positive and negative weights come to equilib- rium. Eq. 1 balances two terms, a positive inter- action term 1.0 − σ ij between a word and a con- text word and negative interaction terms 0.0 − σ is between a word and one of S randomly sampled words. These terms can be viewed as a "label" minus an expectation, as in the gradient for lo- gistic regression. Since there is no 1/S term to balance the number of random samples, one might expect that the "power" of the sampled context terms might overwhelm the true interaction term. In practice, these samples appear to find an equi- librium that effectively balances out the number of random samples after a short burn-in phase. We recorded a moving average of positive and negative weights for an ultra-frequent word (the) and a mod- erately frequent word (tuesday). In both cases, the mean of the values for positive samples starts at 0. The negative objective is optimized when each model vector points away from the context vectors. The positive objective, in contrast, is maximized when word and context vectors for related words are pointing in the same direction. The negative force acts to repel the vectors, the positive force acts to pull them together.</p><p>During the crucial early phases of the algorithm, negative samples have more weight than positive samples: when inner products are near zero, both types of samples will have values of σ ij and σ is close to 0.5, so negative samples will "count" S times more than positive. The early phases of the algorithm will focus on pushing the two sets of vectors apart into separate regions of the latent space. Once vectors and context vectors separate, inner products will become negative, so σ ij and σ is will move closer to 0.0.</p><p>The balance between positive and negative sam- ples consistently affects the geometry of the vec- tors, and is not sensitive to random initialization. We varied the number of negative samples from S = 1 to S = 15, and ran 10 trials for each value with different random initializations. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, as we increase S, the average inner prod- uct between vectors and the mean vector within each model increases.</p><p>SGNS vectors are concentrated and point away from their context vectors, and changing the num- ber of negative samples appears to affect this prop- erty. We now consider whether other factors could also cause this behavior.</p><p>Effect of window size Both SGNS and GloVe operate over word co-occurrences within a sliding window centered around each token in the corpus. This window size parameter has an effect on the semantics of vectors, so it is important to consider whether it has an effect on the geometry of vectors. Simply setting an equal window size for SGNS and GloVe does not, however, guarantee that the two algorithms are seeing equivalent data, because each pair is weighted linearly by token distance in SGNS and by 1/distance in GloVe. <ref type="figure">Figure 7</ref> shows av- erage inner products for each frequency with the global mean vector for 10 trials each at window size 5, 10, 15, 20 with K = 50. Increasing window size leads to greater divergence between high-and low-frequency words for word and context vectors, but does not change their pattern. GloVe results are similarly unchanged.</p><p>Effect of vector size As with window size, the di- mensionality K of the word vectors can affect their ability to represent semantic relationships. <ref type="figure">Figure 8</ref> shows an increase in inner product with the global mean as we increase K (10 trials each, window size 15), but the effect is small relative to that of the number of negative samples S. GloVe inner products change by less than 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>SGNS vectors encode semantic relatedness, but their arrangement is much more strongly influenced by the negative sampling objective than is usually   assumed. We find that vectors lie on a narrow primary axis that is effectively non-negative. Users should not interpret relationships between vectors without recognizing this geometric context. In this work we have deliberately restricted our- selves to describing the geometric properties of vectors. We see several areas for further work. First, there are likely to be theoretical reasons why the observed concentration of SGNS vectors in a narrow cone does not appear to affect performance relative to algorithms that do not have this property. Second, measuring the interplay between positive and negative objectives may provide insight into al- gorithmic choices that are now poorly understood, such as the effect of reducing the occurrence of frequent words in the corpus and the sampling dis- tribution of negative examples. Finally, we suggest that in addition to theoretical analysis, more work should be done to understand the actual working of algorithms on real data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SGNS word vectors and their context vectors projected using PCA (left) and t-SNE (right). t-SNE provides a more readable layout, but masks the divergence between word and context vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SGNS-trained vectors mostly point in the same direction, towards a mean vectorˆwvectorˆ vectorˆw.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Almost all combinations of words have negative inner products for SGNS, unlike GloVe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Most latent dimensions show significant skew. Each panel shows a histogram of values for one of the first 12 latent dimensions, after multiplication by the sign of the mean for that dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The number of negative samples affects the inner product between vectors and the mean vector. Results are indistinguishable across 10 initializations for each value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: SGNS word and context vectors face in opposite directions regardless of window size.</figDesc></figure>

			<note place="foot" n="2"> We make no attempt in this work to compare the quality of SGNS and GloVe vectors, nor should the omission of other algorithms be attributed to anything but space constraints.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Anonymous reviewers helped make this paper much better. This work was supported by NSF #1526155, #1652536, and #DGE-1144153; and the Alfred P. Sloan Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two step CCA: A new spectral method for estimating vector models of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1551" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse overcomplete word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bad company-neighborhoods in neural embedding spaces considered harmful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hellrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online learning of interpretable word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<title level="m">Linguistic Regularities in Continuous Space Word Representations. HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
