<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multilingual Information Extraction Pipeline for Investigative Journalism</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Wiedemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Language Technology Group</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seid</forename><forename type="middle">Muhie</forename><surname>Yimam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Language Technology Group</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Language Technology Group</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multilingual Information Extraction Pipeline for Investigative Journalism</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations)</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations) <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="78" to="83"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce an advanced information extraction pipeline to automatically process very large collections of unstructured textual data for the purpose of investigative journalism. The pipeline serves as a new input processor for the upcoming major release of our New/s/leak 2.0 software, which we develop in cooperation with a large German news organization. The use case is that journalists receive a large collection of files up to several Giga-bytes containing unknown contents. Collections may originate either from official disclosures of documents, e.g. Freedom of Information Act requests, or unofficial data leaks. Our software prepares a visually-aided exploration of the collection to quickly learn about potential stories contained in the data. It is based on the automatic extraction of entities and their co-occurrence in documents. In contrast to comparable projects, we focus on the following three major requirements particularly serving the use case of investigative journalism in cross-border collaborations: 1) composition of multiple state-of-the-art NLP tools for entity extraction, 2) support of multilingual document sets up to 40 languages, 3) fast and easy-to-use extraction of full-text, metadata and entities from various file formats.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Support Investigative Journalism</head><p>Journalists usually build up their stories around entities of interest such as persons, organizations, companies, events, and locations in combination with the complex relations they have. This is es- pecially true for investigative journalism which, in the digital age, more and more is confronted to find such relations between entities in large, un- structured and heterogeneous data sources.</p><p>Usually, this data is buried in unstructured texts, for instance from scanned and OCR-ed docu- ments, letter correspondences, emails or protocols. Sources typically range from 1) official disclo- sures of administrative and business documents, 2) court-ordered revelation of internal communi- cation, 3) answers to requests based on Freedom of Information (FoI) acts, and 4) unofficial leaks of confidential information. Well-known exam- ples of such disclosed or leaked datasets are the Enron email dataset ( <ref type="bibr" target="#b8">Keila and Skillicorn, 2005)</ref> or the Panama Papers (O' <ref type="bibr">Donovan et al., 2016)</ref>.</p><p>To support investigative journalism in their work, we have developed New/s/leak ( <ref type="bibr">Yimam et al., 2016</ref>), a software implemented by experts from natural language processing and visualiza- tion in computer science in cooperation with jour- nalists from Der Spiegel, a large German news or- ganization. Due to its successful application in the investigative research as well as continued feed- back from academia, we further extend the func- tionality of New/s/leak, which now incorporates better pre-processing, information extraction and deployment features. The new version New/s/leak 2.0 serves four central requirements that have not been addressed by the first version or other ex- isting solutions for investigative and forensic text analysis: Improved NLP processing: We use stable and robust state-of-the-art natural language processing (NLP) to automatically extract valuable informa- tion for journalistic research. Our pipeline com- bines extraction of temporal entities, named en- tities, key-terms, regular expression patterns (e.g. URLs, emails, phone numbers) and user-defined dictionaries. Multilingualism: Many tools only work for En- glish documents or a few other 'big languages'. In the new version, our tool allows for automatic language detection and information extraction in 40 different languages. Support of multilingual collections and documents is specifically useful to foster cross-country collaboration in journalism. Multiple file formats: Extracting text and meta- data from various file formats can be a daunting task, especially in journalism where time is a very scarce resource. In our architecture, we include a powerful data wrangling software to automatize this process as much as possible. We further put emphasis on scalability in our pipeline to be able to process very large datasets. For easy deploy- ment, New/s/leak 2.0 is distributed as a Docker setup. Keyword graphs: We have implemented key- word network graphs, which is build based on the set of keywords representing the current document selection. The keyword network enables to fur- ther improve the investigation process by display- ing entity networks related to the keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are already a handful of commercial and open-source software products to support inves- tigative journalism. Many of the existing tools such as OpenRefine 1 , Datawrapper 2 , Tabula 3 , or Sisense 4 focus solemnly on structured data and most of them are not freely available. For un- structured text data, there are costly products for forensic text analysis such as Intella <ref type="bibr">5</ref> . Targeted user groups are national intelligence agencies. For smaller publishing houses, acquiring a license for those products is simply not possible. Since we also follow the main idea of openness and freedom of information, we concentrate on other open- source products to compare our software to.</p><p>DocumentCloud <ref type="bibr">6</ref> is an open-source tool specif- ically designed for journalists to analyze, annotate and publish findings from textual data. In addition to full-text search, it offers named entity recogni- tion (NER) based on OpenCalais 7 for person and location names. In addition to automatic NER for multiple languages, our pipeline supports the iden- tification of keyterms as well as temporal and user- defined entities.</p><p>Overview <ref type="bibr" target="#b3">(Brehmer et al., 2014</ref>) is another open-source application developed by computer scientists in collaboration with journalists to sup- port investigative journalism. The application sup- ports import of PDF, MS Office, and HTML doc- uments, document clustering based on topic sim- ilarity, a simple location entity detection, full-text search, and document tagging. Since this tool is already mature and has successfully been used in a number of published news stories, we adapted some of its most useful features such as document tagging, full-text search and a keyword-in-context (KWIC) view for search hits.</p><p>The Jigsaw visual analytics ( <ref type="bibr">Görg et al., 2014</ref>) system is a third tool that supports analyzing and understanding of textual documents. The Jigsaw system focuses on the extraction of entities us- ing Gate tool suite for NLP <ref type="bibr" target="#b4">(Cunningham et al., 2013)</ref>. Hence, support for multiple languages is somewhat limited. It also lacks sophisticated data import mechanisms.</p><p>The new version of New/s/leak was built tar- geting these drawbacks and challenges. With New/s/leak 2.0 we aim to support the journalist throughout the entire process of collaboratively analyzing large, complex and heterogeneous doc- ument collections: data cleaning and formatting, metadata extraction, information extraction, inter- active filtering, visualization, close reading and tagging, and providing provenance information. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall architecture of New/s/leak. In order to allow users to analyze a wide range of document types, our system in- cludes a document processing pipeline, which ex- tracts text and metadata from a variety of doc- ument types into a unified representation. On this unified text representation, a number of NLP pre-processing tasks are performed as a UIMA pipeline <ref type="bibr" target="#b5">(Ferrucci and Lally, 2004</ref>), e.g. automatic identification of the document language, segmen- tation into paragraph, sentence and token units, and extraction of named entities, keywords and metadata. ElasticSearch is used to store the pro- cessed data and create aggregation queries for dif- ferent entity types to generate network graphs. The user interface is implemented with a RESTful web service based on the Scala Play framework in combination with an AngularJS browser app to present information to the journalists. Visualiza- tions are realized with D3 <ref type="bibr" target="#b2">(Bostock et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>In order to enable a seamless deployment of the tool by journalists with limited technical skills, we have integrated all of the required components of the architecture into a Docker 8 setup. Via docker- compose, a software to orchestrate Docker con-</p><formula xml:id="formula_0">IE pipeline • Language detection • Segmentation • Temporal Entity extraction • Named Entity Extraction • Key-term extraction • Dictionary extraction</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Interface</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hoover</head><p>• processing of documents, archives, email inbox formats tainers for complex architectures, end-users can download and run locally a preconfigured version of New/s/leak with one single command. Being able to process data locally and even without any connection to the internet is a vital prerequisite for journalists when they work with sensitive data. All necessary source code and installation instructions can be found on our Github project page. <ref type="bibr">9</ref> </p><formula xml:id="formula_1">• fulltext extraction • metadata extraction • duplicate detection</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Wrangling</head><p>Extracting text and metadata from various formats into a format readable by a specific analysis tool can be a tedious task. In an investigative journal- ism scenario, it can even be a deal breaker since time is an especially scarce resource and file for- mat conversion might not be a task journalists are well trained in. To offer access to as many file formats as possible in New/s/leak, we opted for a close integration with Hoover, <ref type="bibr">10</ref>  New/s/leak connects directly to Hoover's index to read full-texts and metadata for its own infor- mation extraction pipeline. Through this close in- tegration with Hoover, New/s/leak can offer infor- mation extraction to a wide variety of data formats. In many cases, this drastically limits or even com- pletely eliminates the amount of work needed to clean and preprocess large datasets beforehand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multilingual Information Extraction</head><p>The core functionality of New/s/leak is the auto- matic extraction of various kinds of entities from text to facilitate the exploration and sense-making process from large collections. Since a lot of steps in this process involve language-dependent resources, we put an emphasis on the work to sup- port as many languages as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Preprocessing</head><p>Information extraction in New/s/leak is imple- mented as a configurable UIMA pipeline <ref type="bibr" target="#b5">(Ferrucci and Lally, 2004</ref>). Text documents and metadata from a Hoover collection (see Section 4) are read in parallelized manner and put through a chain of annotators. In a final step of the chain, results from annotation processes are indexed in an Elas- ticSearch index for later retrieval and visualiza- tion.</p><p>First, we identify the language of each doc- ument. Alternatively, language can also be de- termined on a paragraph level to support multi- language documents, which can occur quite of- ten, for instance in email leaks or bilingual con-tracts. Second, we separate sentences and tokens in each text. To guarantee compatibility with var- ious Unicode scripts in different languages, we rely on the ICU4J library 11 for this task. ICU4J provides locale-specific sentence and word bound- ary detection relying on a simple rule-based ap- proach. While the quality of the segmentation and tokenization results might be better when using specifically trained segmentation models, the ad- vantage of the rule-based approach in ICU4J is that it works robustly not only for many languages but also for noisy data, which we expect to be abundant in real-life datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dictionaries and RE-patterns</head><p>In many cases, journalists follow some hypothesis to test for their investigative work. Such a pro- ceeding can involve looking for mentions of al- ready known terms or specific entities in the data. This can be realized by lists of dictionaries pro- vided to the initial information extraction process. New/s/leak annotates every mention of a dictio- nary term with the respective list type. Dictionar- ies can be defined in a language-specific fashion, but also applied across documents of all languages in the corpus. Extracted dictionary entities are dis- played along with extracted named entities in the visualization.</p><p>In addition to self-defined dictionaries, we an- notate email addresses, telephone numbers, and URLs with regular expression patterns. This is useful, especially for email leaks to reveal commu- nication networks of persons and filter for specific email account related content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Temporal Expressions</head><p>Tracking documents across the time of their cre- ation or by temporal events they mention can pro- vide valuable information during investigative re- search. Unfortunately, many document sets (e.g. collections of scanned pages) do not come with a specific document creation date as structured metadata. To offer a temporal selection of contents to the user, we extract mentions of temporal ex- pressions. This is done by integrating the Heidel- Time temporal tagger <ref type="bibr" target="#b11">(Strötgen and Gertz, 2015</ref>) in our UIMA workflow. HeidelTime provides au- tomatically learned rules for temporal tagging in more than 200 languages. Extracted timestamps can be used to select and filter documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Named Entity Recognition</head><p>We automatically extract person, organization and location names from all documents to allow for an entity-centric exploration of the data collec- tion. Named entity recognition is done using the polyglot-NER library (Al- <ref type="bibr" target="#b0">Rfou et al., 2015)</ref>. Polyglot-NER contains sequence classification for named entities based on weakly annotated training data automatically composed from Wikipedia 12 and Freebase <ref type="bibr">13</ref> . Relying on the automatic com- position of training data allows polyglot-NER to provide pre-trained models for 40 languages. <ref type="bibr">14</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Keyterm Extraction</head><p>To further summarize document contents in addi- tion to named entities, we automatically extract keyterms and phrases from documents. For this, we implemented a keyterm extraction library for the 40 languages also supported in the previous step. <ref type="bibr">15</ref> Our approach is based on a statistical com- parison of document contents with generic ref- erence data. Reference data for each language is retrieved from the Leipzig Corpora Collection ( <ref type="bibr" target="#b6">Goldhahn et al., 2012)</ref>, which provides large rep- resentative corpora for language statistics. We em- ploy log-likelihood significance as described in <ref type="bibr" target="#b10">(Rayson et al., 2004</ref>) to measure the overuse of terms (i.e. keyterms) in our target documents com- pared to the generic reference data. Ongoing se- quences of keyterms in target documents are con- catenated to key phrases if they occur regularly in that exact same order. Regularity is determined with the Dice coefficient. This simple method al- lows to reliably extract multiword units such as "stock market" or "machine learning" in the docu- ments. Since this method also extracts named en- tities if they occur significantly often in a docu- ment, there can be a substantial overlap between both types. To allow for a separate display of named entities and keywords, we filter keyterms if they already have been annotated as a named en- tity. The remaining top keyterms are used to cre- ate a brief summary of each document for the user and to generate keyterm networks for document browsing. The entity and keyword graphs of New/s/leak based on the WW2 collection (see Section 7). Net- works are visualized based on the current document selection, which can be filtered by full-text search, en- tities or metadata. Visualization parameters such as the number of nodes per type or minimum edge strength can be set by the user. Hovering over nodes and edges in one graph highlights information present in the re- spective another graph to show which entities and key- words frequently co-occur with each other in docu- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">User Interface</head><p>Browsing entity networks: Access to unstruc- tured text collections via named entities is essen- tial for journalistic investigations. To support this, we included two types of graph visualization, as it is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The first graph, called entity network, displays entities in a current document selection as nodes and their joint occurrence as edges between nodes. Different node colors rep- resent different types such as person, organization or location names. Furthermore, mentions of enti- ties that are annotated based on dictionary lists are included in the entity network graph. The second graph, called keyword network, is build based on the set of keywords representing the current doc- ument selection. The keyword network also in- cludes tags that can be attached to documents by journalists during work with the collection.</p><p>Journalist in the loop: In addition to the auto- matic annotation of entities and keyterms, we fur- ther enable journalists to: 1) annotate new entity types that are not in the system at all, 2) correct au- tomatic annotations provided by the pipeline, e.g. to remove false positives or false entity type labels annotated by the NER process, 3) merge identi- cal entities which have different forms (e.g. last names to full names, or spelling variants in differ- ent languages), and 4) label documents with user- defined terms called tags. The tags are mainly used to annotate the document either for later read- ing or to share with collaborators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Case Study</head><p>To illustrate analysis capabilities of the new ver- sion of New/s/leak, we present an exemplary case study at https://ltdemos.informatik. uni-hamburg.de/newsleak/ (login with "user" and "password"). Since we refrain from publishing any confidential leak data, we created an artificial dataset from publicly available doc- uments that share certain characteristics with the data from intended use cases in investigative jour- nalism. It contains documents written in multiple languages, roughly centered on one topic and is full of references to entities.</p><p>Ca. 27.000 documents in our sample set are Wikipedia articles related to the topic of World War II. Articles were crawled from the encyclo- pedia in four languages (English:en, Spanish:es, Hungarian:hu, German:de) as a link network start- ing from the article "Second World War" in each respective language. Preprocessing and data ex- traction took around 75 minutes on a moderately fast server with 12 parallel CPU threads.</p><p>Analysis: From a perspective of national his- tory discourses and education, a certain common knowledge about WW2 can be expected. But, the topic becomes quickly a novel unexplored terrain for most people when it comes to aspects outside of the own region, e.g. the involvement of Asian powers. In our test case, we strive to fill gaps in our knowledge by identifying interesting de- tails regarding this question. First, we start with a visualization of entities from the entire collec- tion which highlights central actors of WW2 in general. In the list of extracted location entities, we can filter for ca. 2,000 articles referencing to Asia (en, es), ´ Azsia (hu) or Asien (de). In this subselection, we find most references to China as a political power of the region followed by India and Japan. Further refinement of the collection by references to China highlights a central per- son name in the network, Chiang Kai-shek, who raises our interest. To find out more, we start the filter process all over again, subselecting all articles referencing this name. The resulting en- tity network reveals a close connection to the or- ganization Kuomintang (KMT). Filtering for this organization, too, we can quickly identify arti- cles centrally referencing to both entities by look- ing at their titles and extracted keywords. From the corresponding keyterm network and a KWIC view into the article full-texts, we learn that KMT is the national party of China and Kai-Shek as their leader ruled the country during the period of WW2. A second central actor, Mao Zedong, is strongly connected with both, KMT and Chiang Kai-shek in our entity network. From articles also prominently referencing Zedong, we learn from sections highlighting both person names that Kai- shek and Zedong, also a member of KMT and later leader of the Chinese Communists, shared a com- plicated relationship. By filtering for both names, we can now explore the nature of this relationship in more detail and compare its display across the four languages in our dataset. <ref type="bibr">16</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Future Work</head><p>In this paper, we introduced the completely renewed information extraction pipeline of New/s/leak 2.0, an open-source software to support investigative journalism.</p><p>As major requirements based on prior experiences, we identified the automatic annotation of various entity types in very large, multi-lingual document sets contained in heterogeneous file formats. Our solution involves a combination of powerful NLP libraries for temporal and named entities, own developments for keyterm and pattern extraction, and a powerful data wrangling suite for text and metadata extraction. The pipeline is capable to process information extraction in 40 languages.</p><p>New/s/leak has been in use successfully at the German news organization Der Spiegel. It recently has also been introduced as an open-source tool to the community of investigative journalists at re- spective conferences. We expect to collect more user feedback and experiences from case studies in the near future to further improve the software.</p><p>As a new main feature, we plan to extend the in- formation extraction pipeline for user-defined cat- egories into the direction of adaptive and active machine learning approaches. Currently, while reading the full-texts, users can manually annotate new entity types in the text or tag the entire docu- ments. In combination with an adaptive and active learning approach, users will be able to train auto- matic tagging of documents and extraction of in- formation while working with the data in the user interface.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of New/s/leak 2.0</figDesc><graphic url="image-15.png" coords="3,381.02,181.29,75.31,55.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The entity and keyword graphs of New/s/leak based on the WW2 collection (see Section 7). Networks are visualized based on the current document selection, which can be filtered by full-text search, entities or metadata. Visualization parameters such as the number of nodes per type or minimum edge strength can be set by the user. Hovering over nodes and edges in one graph highlights information present in the respective another graph to show which entities and keywords frequently co-occur with each other in documents.</figDesc><graphic url="image-16.png" coords="5,72.00,62.81,218.27,106.77" type="bitmap" /></figure>

			<note place="foot" n="1"> http://openrefine.org 2 https://www.datawrapper.de 3 http://tabula.technology 4 https://www.sisense.com 5 https://www.vound-software.com 6 https://www.documentcloud.org 7 http://www.opencalais.com</note>

			<note place="foot" n="8"> https://www.docker.com</note>

			<note place="foot" n="11"> http://icu-project.org/apiref/icu4j</note>

			<note place="foot" n="12"> https://wikipedia.org 13 https://developers.google.com/ freebase 14 A list of the 40 languages covered by Polyglot-NER can be found at https://tinyurl.com/yaju7bf7 15 https://github.com/uhh-lt/lt-keyterms</note>

			<note place="foot" n="16"> A video of the proceeding can be found at: http:// youtu.be/96f_4Wm5BoU</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot-NER: Massive Multilingual Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">SIAM/ICDM-2015</title>
		<imprint>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D3 Data-Driven Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization &amp; Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview: The Design, Adoption, and Analysis of a Visual Document Mining Tool for Investigative Journalists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization &amp; Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2271" to="2280" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Getting more out of biomedical documents with GATE&apos;s full lifecycle open source text analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1002854</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UIMA: An architectural approach to unstructured information processing in the corporate research environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="327" to="348" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building large monolingual dictionaries at the Leipzig corpora collection: From 100 to 200 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Goldhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Quasthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC 2012</title>
		<meeting>LREC 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="759" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reflections on the evolution of the Jigsaw visual analytics system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Görg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="336" to="345" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting unusual email communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Keila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Skillicorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CASCON 2005</title>
		<meeting>CASCON 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The value of offshore secrets evidence from the panama papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><forename type="middle">F</forename><surname>James O&amp;apos;donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extending the Cochran rule for the comparison of word frequencies between corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Berridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Francis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. JADT &apos;04</title>
		<meeting>JADT &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="926" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Baseline Temporal Tagger for all Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="541" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Chris Biemann, and Kathrin Ballweg. 2016. new/s/leak-Information Extraction and Visualization for Investigative Data Journalists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Seid Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Tatiana Von Landesberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Rosenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uli</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fahrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL 2016 System Demonstrations</title>
		<meeting>ACL 2016 System Demonstrations</meeting>
		<imprint>
			<biblScope unit="page" from="163" to="168" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
