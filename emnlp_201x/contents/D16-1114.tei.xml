<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimpleScience: Lexical Simplification of Scientific Terminology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yea-Seul</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">School of Information</orgName>
								<orgName type="laboratory">University Washington Information School</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hullman</surname></persName>
							<email>yeaseul1, jhullman@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">School of Information</orgName>
								<orgName type="laboratory">University Washington Information School</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Burgess</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">School of Information</orgName>
								<orgName type="laboratory">University Washington Information School</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Adar</surname></persName>
							<email>eadar@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">School of Information</orgName>
								<orgName type="laboratory">University Washington Information School</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimpleScience: Lexical Simplification of Scientific Terminology</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1066" to="1071"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Lexical simplification of scientific terms represents a unique challenge due to the lack of a standard parallel corpora and fast rate at which vocabulary shift along with research. We introduce SimpleScience, a lexical simplification approach for scientific terminology. We use word embeddings to extract simplification rules from a parallel corpora containing scientific publications and Wikipedia. To evaluate our system we construct SimpleSciGold, a novel gold standard set for science-related simplifications. We find that our approach out-performs prior context-aware approaches at generating simplifications for scientific terms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lexical simplification, the process of reducing the complexity of words by replacing them with sim- pler substitutes (e.g., sodium in place of Na; insects in place of lepidopterans) can make scientific texts more accessible to general audiences. Human-in- the-loop interfaces present multiple possible simpli- fications to a reader (on demand) in place of jargon and give the reader familiar access points to under- standing jargon <ref type="bibr" target="#b2">(Kim et al., 2015)</ref>. Unfortunately, simplification techniques are not yet of high enough quality for fully automated scenarios.</p><p>Currently lexical simplification pipelines for sci- entific texts are rare. The vast majority of prior methods assume a domain independent context, and rely on Wikipedia and Simple English Wikipedia, a subset of Wikipedia using simplified grammar and terminology, to learn simplifications <ref type="bibr" target="#b0">(Biran et al., 2011;</ref><ref type="bibr" target="#b4">Paetzold and Specia, 2015)</ref>, with translation- based approaches using an aligned version <ref type="bibr" target="#b1">(Coster and Kauchak, 2011;</ref><ref type="bibr">Horn et al., 2014;</ref><ref type="bibr" target="#b9">Yatskar et al., 2010)</ref>. However, learning simplifications from Wikipedia is not well suited to lexical sim- plification of scientific terms. Though generic or established terms may appear in Wikipedia, novel terms associated with new advances may not be re- flected. Wikipedia's editing rules also favor gener- ality over specificity and eliminate redundancy, both of which are problematic in providing a rich train- ing set that matches simple and complex terms. Fur- ther, some approaches work by detecting all pairs of words in a corpus and filtering to isolate synonym or hypernym-relationship pairs using WordNet <ref type="bibr" target="#b0">(Biran et al., 2011</ref>). Like Wikipedia, WordNet is a general purpose semantic database <ref type="bibr" target="#b4">(Miller, 1995)</ref>, and does not cover all branches of science nor integrate new terminology quickly.</p><p>Word embeddings do not require the use of pre- built ontologies to identify associated terms like simplifications. Recent work indicates that they may improve results for simplification selection: deter- mining which simplifications for a given complex word can be used without altering the meaning of the text <ref type="bibr" target="#b4">(Paetzold and Specia, 2015)</ref>. Embeddings have also been explored to extract hypernym rela- tions from general corpora <ref type="bibr">(Rei and Briscoe, 2014</ref>). However, word embeddings have not been used for generating lexical simplifications. We provide a novel demonstration of how using embeddings on a scientific corpus is better suited to learning scien- tific term simplifications than prior approaches that use WordNet as a filter and Wikipedia as a corpus.</p><p>INPUT: Finally we show that the transient immune activation that renders mosquitoes resistant to the human malaria parasite has little to no effect on mosquito fitness as a measure of sur- vival or fecundity under laboratory conditions. CANDIDATE RULES: {fecundity→fertility} {fecundity→productivity} OUTPUT: Finally we show that the transient immune activation that ren- ders mosquitoes resistant to the human malaria parasite has lit- tle to no effect on mosquito fitness as a measure of survival or (fertility; productivity) under laboratory conditions. We introduce SimpleScience, a novel lexical sim- plification pipeline for scientific terms, which we apply to a scientific corpus of nearly 500k publi- cations in Public Library of Science (PLOS) and PubMed Central (PMC) paired with a general cor- pus from Wikipedia. We validate our approach us- ing SimpleSciGold, a gold standard set that we cre- ate using crowdsourcing that contains 293 sentences containing scientific terms with an average of 21 simplifications per term. We show how the Sim- pleScience pipeline achieves better performance (F- measure: 0.285) than the prior approach to simplifi- cation generation applied to our corpus (F-measure: 0.136). We further find that the simplification se- lection techniques used in prior work to determine which simplifications are a good fit for a sentence do not improve performance when our generation pipeline is used. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parallel corpora: Scientific and General</head><p>We assembled a scientific corpus of papers from the entire catalog of PLOS articles and the National Li- brary of Medicine's Pubmed Central (PMC) archive (359,324 fulltext articles). The PLOS corpus of 125,378 articles includes articles from PLOS One and each speciality PLOS journal (e.g., Pathogens, Computational Biology). Our general corpus in- cludes all 4,776,093 articles from the Feb. 2015 En- glish Wikipedia snapshot. We chose Wikipedia as it covers many scientific concepts and usually con- tains descriptions of those concepts using simpler language than the research literature. We obtained all datasets from DeepDive ( <ref type="bibr" target="#b6">Ré and Zhang, 2015</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SimpleScience Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating Simplifications</head><p>Our goal is to learn simplification rules in the form complex word→simple word. One approach identi- fies all pairwise permutations of 'content' terms and then applies semantic (i.e., WordNet) and simplic- ity filters to eliminate pairs that are not simplifica- tions <ref type="bibr" target="#b0">(Biran et al., 2011</ref>). We adopt a similar pipeline but leverage distance metrics on word embeddings and a simpler frequency filter in place of WordNet. Embeddings identify words that share context in an unsupervised, scalable way and are more efficient than constructing co-occurrence matrices <ref type="bibr" target="#b0">(Biran et al., 2011)</ref>. As our experiments demonstrate, our ap- proach improves performance on a scientific test set over prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Step 1: Generating Word Embeddings</head><p>We used the Word2Vec system ( <ref type="bibr">Mikolov et al., 2013</ref>) to learn word vectors for each content word in the union of vocabulary of the scientific and general corpus. While other approaches exist ( <ref type="bibr" target="#b5">Pennington et al., 2014;</ref><ref type="bibr" target="#b3">Levy and Goldberg, 2014)</ref>, Word2Vec has been shown to produce both fast and accurate results ( <ref type="bibr">Mikolov et al., 2013</ref>). We set the embed- ding dimension to 300, the context-window to 10, and use the skip-gram architecture with negative- sampling,which is known to produce quality results for rare entities <ref type="bibr">(Mikolov et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Step 2: Filtering Pairs</head><p>Given the set of all pairwise permutations of words, we retain a simplification rule of two words w 1 , w 2 if the cosine similarity cos(w 1 , w 2 ) between the word vectors is greater than a threshold a. We use grid search, described below, to parameterize a.</p><p>We then apply additional filtering rules. To avoid rules comprised of words with the same stem (e.g., permutable, permutation) we stem all words (us- ing the Porter stemmer in the Python NLTK li- brary ( <ref type="bibr">Bird et al., 2009)</ref>). The POS of each word is determined by Morphadorner ( <ref type="bibr" target="#b1">Burns, 2013)</ref> and pairs that differ in POS are omitted (e.g., permu- tation (noun), change(d) (verb)); Finally, we omit rules where one word is a prefix of the other and the suffix is one of s, es, ed, ly, er, or ing.</p><p>To retain only rules of the form complex word → simple word we calculate the corpus complexity, C (Biran et al., 2011) of each word w as the ratio be- tween the frequency (f ) in the scientific versus gen- eral corpus: C w = f w,scientif ic /f w,general . The lex- ical complexity, L, of a word is calculated as the word's character length, and the final complexity of the word as C w × L w . We require that the final com- plexity score of the first word in the rule be greater than the second. While this simplicity filter has been shown to work well in general corpora <ref type="figure" target="#fig_0">(Biran et al., 2011</ref>), it is sensitive to very small differences in the frequen- cies with which both words appear in the corpora. This is problematic given the distribution of terms in our corpora, where many rarer scientific terms may appear in small numbers in both corpora.</p><p>We introduce an additional constraint that re- quires that the second (simple) word in the rule oc- cur in the general corpus at least k times. This helps ensure that we do not label words that are at a simi- lar complexity level as simplifications. We note that this filter aligns with prior work that suggests that features of the hypernym in hypernym-hyponym re- lations influence performance more than features of the hyponym <ref type="bibr">(Rei and Briscoe, 2014)</ref>.</p><p>Parameterization: We use a grid search anal- ysis to identify which measures of the set in- cluding cos(w 1 , w 2 ), f w1,scientif ic , f w2,scientif ic , f w1,general , and f w2,general most impact the F- measure when we evaluate our generation approach against our scientific gold standard set (Sec. 4), and to set the specific parameter values. Using this method we identify a=0.4 for cosine similarity and k=3,000 for the frequency of the simple term in the general corpus. Full results are available in supple- mentary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Applying Simplifications</head><p>In prior context-aware simplification systems, the decision of whether to apply a simplification rule in an input sentence is complex, involving several similarity operations on word co-occurrence matri- ces (Biran et al., 2011) or using embeddings to incorporate co-occurrence context for pairs gener- ated using other means ( <ref type="bibr" target="#b4">Paetzold and Specia, 2015)</ref>. However, the SimpleScience pipline already consid- ers the context of appearance for each word in de- riving simplifications via word embeddings learned from a large corpus. We see no additional improve- ments in F-measure when we apply two variants of context similarity thresholds to decide whether to apply a rule to an input sentence. The first is the cosine similarity between the distributed represen- tation of the simple word and the sum of the dis- tributed representations of all words within a win- dow l surrounding the complex word in the input sentence <ref type="bibr" target="#b4">(Paetzold and Specia, 2015)</ref>. The second is the cosine similarity of a minimum shared frequency co-occurrence matrix for the words in the pair and the co-occurrence matrix for the input sentence <ref type="bibr" target="#b0">(Biran et al., 2011</ref>).</p><p>In fully automated applications, the top rule from the ranked candidate rules is used. We find that rank- ing by the cosine similarity between the word em- beddings for the complex and simple word in the rule leads to the best performance at the top slot (full results in supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SimpleSciGold Test Set</head><p>To evaluate our pipeline, we develop Sim- pleSciGold, a scientific gold standard set of sen- tences containing complex scientific terms which is modeled after the general purpose gold standard set created by <ref type="bibr">Horn et al. (2014)</ref>.</p><p>To create SimpleSciGold, we start with scientific terms from two sources: we utilized all 304 com- plex terms from unigram rules by <ref type="bibr">(Vydiswaran et al., 2014)</ref>, and added another 34,987 child terms from rules found by mining direct parent-child rela- tions for unigrams in the Medical Subject Headings (MeSH) ontology (United States National Library of <ref type="bibr">Medicine, 2015)</ref>. We chose complex terms with pre- existing simplifications as it provided a means by which we could informally check the crowd gener- ated simplifications for consistency.</p><p>To obtain words in context, we extracted 293 sentences containing unique words in this set from PLOS abstracts from PLOS Biology, Pathology, Ge- netics, and several other journals. We present 10 MTurk crowdworkers with a task ("HIT") show- ing one of these sentences with the complex word bolded. Workers are told to read the sentence, con- sult online materials (we provide direct links to a Wikipedia search, a standard Google search, and  a Google "define" search on the target term), and add their simplification suggestions. Crowdworkers first passed a multiple choice practice qualification in which they were presented with sentences con- taining three complex words in need of simplifica- tion along with screenshots of Wikipedia and dictio- nary pages for the terms. The workers were asked to identify which of 5 possible simplifications listed for each complex word would preserve the mean- ing while simplifying. 108 workers took part in the gold standard set creation task, completing an aver- age of 27 HITs each. The resulting SimpleSciGold standard set consists of an average of 20.7 simplifi- cations for each of the 293 complex words in corre- sponding sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simplification Generation</head><p>We compare our word embedding generation pro- cess (applied to our corpora) to <ref type="bibr">Biran et al.'s (2011)</ref> approach (applied to the Wikipedia and Simple En- glish Wikipedia corpus as well as our scientific cor- pora). Following the evaluation method used in <ref type="bibr" target="#b4">Paetzold and Specia (2015)</ref>, we calculate potential as the proportion of instances for which at least one of the substitutions generated is present in the gold standard set, precision as the proportion of generated instances which are present in the gold standard set, and F-measure as their harmonic mean. Our SimpleScience approach outperforms the original approach by <ref type="bibr" target="#b0">Biran et al. (2011)</ref> applied to the Wikipedia and SEW corpus as well as to the sci- entific corpus <ref type="table" target="#tab_0">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Applying Simplifications</head><p>We find that neither prior selection approaches yield performance improvements over our generation pro- cess. We evaluate the performance of ranking can- didate rules by cosine similarity (to find the top rule for a fully automated application), and achieve pre- cision of 0.389 at the top slot. In our supplementary materials, we provide additional results for poten- tial, precision and F-measure at varying numbers of slots (up to 5), where we test ranking by cosine sim- ilarity of embeddings as well as by the second filter used in our pair generation step: the frequency of the simple word in the simple corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Antonym Prevalence Analysis</head><p>A risk of using Word2Vec in place of WordNet is that the simpler terms generated by our ap- proach may represent terms with opposite mean- ings (antonyms). While a detailed analysis is be- yond the scope of this paper, we compared the like- lihood of seeing antonyms in our results using a gold standard set of antonyms for biology, chem- istry, and physics terms from WordNik <ref type="bibr" target="#b8">(Wordnik, 2009)</ref>. Specifically, we created an antonym set con- sisting of the 304 terms from the biology, chemistry, and physics categories in Wictionary for which at least one antonym is listed in WordNik. We com- pared antonym pairs with rules that produced by the SimpleScience pipeline <ref type="figure" target="#fig_0">(Fig. 1)</ref>. We observed that 14.5% of the time (44 out of 304 instances), an antonym appeared at the top slot among results. 51.3% of the time (156 out of 304 instances), no antonyms in the list appeared within the top 100 ranked results. These results suggest that further filters are necessary to ensure high enough quality results for fully automated applications of scientific term simplification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Future Work</head><p>A risk of using Word2Vec to find related terms, rather than querying a lexical database like Word- Net, is that generated rules may include antonyms. Adding techniques to filter antonym rules, such as using co-reference chains <ref type="bibr" target="#b0">(Adel and Schütze, 2014)</ref>, is important in future work.</p><p>We achieve a precision of 0.389 at the top slot on our SimpleSciGold standard set when we ap- ply our generation method and rank candidates by cosine similarity. This level of precision is higher than that achieved by various prior ranking meth- ods used in <ref type="bibr">Lexenstein (Paetzold and Specia, 2015)</ref>, with the exception of using machine learning tech- niques like SVM ( <ref type="bibr" target="#b4">Paetzold and Specia, 2015)</ref>. Fu- ture work should explore how much the precision of our SimpleScience pipeline can be improved by adopting more sophisticated ranking methods. How- ever, we suspect that even the highest precision ob- tained on general corpora and gold standard sets in prior work is not sufficient for fully automated sim- plification. An exciting area for future work is in applying the SimpleScience pipeline in interactive simplification suggestion interfaces for those read- ing or writing about science ( <ref type="bibr" target="#b2">Kim et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce SimpleScience, a lexical simplification approach to address the unique chal- lenges of simplifying scientific terminology, includ- ing a lack of parallel corpora, shifting vocabulary, and mismatch with using general purpose databases for filtering. We use word embeddings to extract simplification rules from a novel parallel corpora that contains scientific publications and Wikipedia.</p><p>Using SimpleSciGold, a gold standard set that we created using crowdsourcing, we show that using embeddings and simple frequency filters on a sci- entific corpus outperforms prior approaches to sim- plification generation, and renders the best prior ap- proach to simplification selection unnecessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Probability of an antonym in our test set occurring as a suggested simpler term in the top 100 slots in the SimpleScience pipeline.</figDesc><graphic url="image-1.png" coords="5,77.40,57.82,215.99,89.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Input sentence, candidate rules and output sentence. 

(Further examples provided as supplementary material.) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Simplification Generation Results. SimpleScience achieves the highest F-measure with a cosine threshold of 0.4 and a 

frequency of the simple word in the general corpus of 3000. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Putting it simply: A context-aware approach to lexical simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">] Heike</forename><surname>Schütze2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze ; Biran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;11</title>
		<editor>Biran, Samuel Brody, and Noémie Elhadad</editor>
		<meeting><address><addrLine>Steven Bird, Ewan Klein, and Edward Loper</addrLine></address></meeting>
		<imprint>
			<publisher>Reilly Media, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1447" to="1452" />
		</imprint>
	</monogr>
	<note>Natural language processing with Python</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Morphadorner v2: a java library for the morphological adornment of english language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Philip R Burns ;</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on monolingual text-to-text generation</title>
		<editor>Horn et al.2014] Colby Horn, Cathryn Manduca, and David Kauchak</editor>
		<meeting>the workshop on monolingual text-to-text generation<address><addrLine>Evanston, IL</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="458" to="463" />
		</imprint>
		<respStmt>
			<orgName>Northwestern University</orgName>
		</respStmt>
	</monogr>
	<note>ACL (2)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Descipher: A text simplification tool for science journalism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation+Journalism Symposium</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldberg2014] Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lexenstein: A framework for lexical simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Gustavo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="1995-11" />
		</imprint>
	</monogr>
	<note>ACL-IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Looking for hyponyms in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://deepdive.stanford.edu/opendata" />
	</analytic>
	<monogr>
		<title level="m">Deepdive open datasets</title>
		<editor>Rei and Briscoe2014] Marek Rei and Ted Briscoe</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining consumer health vocabulary from community-generated text</title>
	</analytic>
	<monogr>
		<title level="m">AMIA &apos;14</title>
		<editor>Vinod Vydiswaran, Qiaozhu Mei, David A. Hanauer, and Kai Zheng</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>States National Library of Medicine2015] United States National Library of Medicine</orgName>
		</respStmt>
	</monogr>
	<note>Medical subject headings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wordnik online english dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wordnik</surname></persName>
		</author>
		<ptr target="https://www.wordnik.com/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yatskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL &apos;10</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
