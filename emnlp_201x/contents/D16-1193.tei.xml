<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Approach to Automated Essay Scoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>13 Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>13 Computing Drive</addrLine>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Approach to Automated Essay Scoring</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1882" to="1891"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a recent surge of interest in neural networks, which are based on continuous-space representation of the input and non-linear functions. Hence, neural networks are capable of modeling complex patterns in data. Moreover, since these methods do not de- pend on manual engineering of features, they can be applied to solve problems in an end-to-end fashion. SENNA <ref type="bibr" target="#b5">(Collobert et al., 2011</ref>) and neural machine translation ( <ref type="bibr" target="#b2">Bahdanau et al., 2015</ref>) are two notable examples in natural language processing that oper- ate without any external task-specific knowledge. In this paper, we report a system based on neural net- works to take advantage of their modeling capacity and generalization power for the automated essay scoring (AES) task.</p><p>Essay writing is usually a part of the student as- sessment process. Several organizations, such as Educational Testing Service (ETS) 1 , evaluate the writing skills of students in their examinations. Be- cause of the large number of students participat- ing in these exams, grading all essays is very time- consuming. Thus, some organizations have been us- ing AES systems to reduce the time and cost of scor- ing essays.</p><p>Automated essay scoring refers to the process of grading student essays without human interference. An AES system takes as input an essay written for a given prompt, and then assigns a numeric score to the essay reflecting its quality, based on its content, grammar, and organization. Such AES systems are usually based on regression methods applied to a set of carefully designed features. The process of fea- ture engineering is the most difficult part of building AES systems. Moreover, it is challenging for hu- mans to consider all the factors that are involved in assigning a score to an essay.</p><p>Our AES system, on the other hand, learns the features and relation between an essay and its score automatically. Since the system is based on recur- rent neural networks, it can effectively encode the information required for essay evaluation and learn the complex patterns in the data through non-linear neural layers. Our system is among the first AES systems based on neural networks designed with- out any hand-crafted features. Our results show that our system outperforms a strong baseline and achieves state-of-the-art performance in automated essay scoring. In order to make it easier for other re- searchers to replicate our results, we have made the source code of our system publicly available 2 .</p><p>The rest of this paper is organized as follows. Sec- tion 2 gives an overview of related work in the liter- ature. Section 3 describes the automated essay scor- ing task and the evaluation metric used in this paper. We provide the details of our approach in Section 4, and present and discuss the results of our experi- mental evaluation in Section 5. Finally, we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There exist many automated essay scoring systems <ref type="bibr">(Shermis and Burstein, 2013</ref>) and some of them are being used in high-stakes assessments. e-rater <ref type="bibr" target="#b1">(Attali and Burstein, 2004</ref>) and Intelligent Essay As- sessor ( <ref type="bibr" target="#b8">Foltz et al., 1999</ref>) are two notable examples of AES systems. In 2012, a competition on auto- mated essay scoring called 'Automated Student As- sessment Prize' (ASAP) 3 was organized by Kaggle and sponsored by the Hewlett Foundation. A com- prehensive comparison of AES systems was made in the ASAP competition. Although many AES sys- tems have been developed to date, they have been built with hand-crafted features and supervised ma- chine learning algorithms.</p><p>Researchers have devoted a substantial amount of effort to design effective features for automated es- say scoring. These features can be as simple as es- say length <ref type="bibr" target="#b3">(Chen and He, 2013</ref>) or more compli- cated such as lexical complexity, grammaticality of a text ( <ref type="bibr" target="#b1">Attali and Burstein, 2004</ref>), or syntactic features <ref type="bibr" target="#b3">(Chen and He, 2013)</ref>. Readability features ( <ref type="bibr" target="#b25">Zesch et al., 2015</ref>) have also been proposed in the liter- ature as another source of information. Moreover, text coherence has also been exploited to assess the flow of information and argumentation of an essay <ref type="bibr" target="#b3">(Chen and He, 2013)</ref>. A detailed overview of the features used in AES systems can be found in ( <ref type="bibr" target="#b25">Zesch et al., 2015)</ref>. Moreover, some attempts have been made to address different aspects of essay writing independently. For example, argument strength and organization of essays have been tackled by some researchers through designing task-specific features for each aspect <ref type="bibr" target="#b18">(Persing et al., 2010;</ref><ref type="bibr" target="#b16">Persing and Ng, 2015)</ref>.</p><p>Our system, however, accepts an essay text as input directly and learns the features automatically from the data. To do so, we have developed a method based on recurrent neural networks to score the essays in an end-to-end manner. We have ex- plored a variety of neural network models in this pa- per to identify the most suitable model. Our best model is a long short-term memory neural network <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref> and is trained as a regression method. Similar recurrent neural net- work approaches have recently been used success- fully in a number of other NLP tasks. For exam- ple, <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref> have proposed an atten- tive neural approach to machine translation based on gated recurrent units ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>). Neural ap- proaches have also been used for syntactic parsing. In ( <ref type="bibr" target="#b23">Vinyals et al., 2015)</ref>, long short-term memory networks have been used to obtain parse trees by using a sequence-to-sequence model and formulat- ing the parsing task as a sequence generation prob- lem. Apart from these examples, recurrent neural networks have also been used for opinion mining <ref type="bibr" target="#b10">(Irsoy and Cardie, 2014</ref>), sequence labeling ( <ref type="bibr" target="#b14">Ma and Hovy, 2016)</ref>, language modeling ( <ref type="bibr" target="#b13">Kim et al., 2016;</ref><ref type="bibr" target="#b22">Sundermeyer et al., 2015)</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Automated Essay Scoring</head><p>In this section, we define the automated essay scor- ing task and the evaluation metric used for assessing the quality of AES systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description</head><p>Automated essay scoring systems are used in evalu- ating and scoring student essays written based on a given prompt. The performance of these systems is assessed by comparing their scores assigned to a set of essays to human-assigned gold-standard scores. Since the output of AES systems is usually a real- valued number, the task is often addressed as a su- pervised machine learning task (mostly by regres- sion or preference ranking). Machine learning algo- rithms are used to learn the relationship between the essays and reference scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metric</head><p>The output of an AES system can be compared to the ratings assigned by human annotators using various measures of correlation or agreement <ref type="bibr" target="#b24">(Yannakoudakis and Cummins, 2015)</ref>. These measures include Pearson's correlation, Spearman's correla- tion, Kendall's Tau, and quadratic weighted Kappa (QWK). The ASAP competition adopted QWK as the official evaluation metric. Since we use the ASAP data set for evaluation in this paper, we also use QWK as the evaluation metric in our experi- ments.</p><p>Quadratic weighted Kappa is calculated as fol- lows. First, a weight matrix W is constructed ac- cording to Equation 1:</p><formula xml:id="formula_0">W i,j = (i − j) 2 (N − 1) 2<label>(1)</label></formula><p>where i and j are the reference rating (assigned by a human annotator) and the hypothesis rating (as- signed by an AES system), respectively, and N is the number of possible ratings. A matrix O is cal- culated such that O i,j denotes the number of essays that receive a rating i by the human annotator and a rating j by the AES system. An expected count matrix E is calculated as the outer product of his- togram vectors of the two (reference and hypothe- sis) ratings. The matrix E is then normalized such that the sum of elements in E and the sum of ele- ments in O are the same. Finally, given the matrices O and E, the QWK score is calculated according to Equation 2:</p><formula xml:id="formula_1">κ = 1 − i,j W i,j O i,j i,j W i,j E i,j<label>(2)</label></formula><p>In our experiments, we compare the QWK score of our system to well-established baselines. We also perform a one-tailed paired t-test to determine whether the obtained improvement is statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Recurrent Neural Network Approach</head><p>Recurrent neural networks are one of the most suc- cessful machine learning models and have attracted the attention of researchers from various fields. Compared to feed-forward neural networks, recur- rent neural networks are theoretically more powerful and are capable of learning more complex patterns from data. Therefore, we have mainly focused on recurrent networks in this paper. This section gives a description of the recurrent neural network archi- tecture that we have used for the essay scoring task and the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Architecture</head><p>The neural network architecture that we have used in this paper is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We now describe each layer in our neural network in detail.</p><p>Lookup <ref type="table">Table Layer</ref>: The first layer of our neural network projects each word into a d LT di- mensional space. Given a sequence of words W represented by their one-hot representations (w 1 , w 2 , · · · , w M ), the output of the lookup table layer is calculated by Equation 3:</p><formula xml:id="formula_2">LT (W) = (E.w 1 , E.w 2 , · · · , E.w M ) (3)</formula><p>where E is the word embeddings matrix and will be learnt during training.</p><p>Convolution Layer: Once the dense represen- tation of the input sequence W is calculated, it is fed into the recurrent layer of the network. How- ever, it might be beneficial for the network to ex- tract local features from the sequence before apply- ing the recurrent operation. This optional charac- teristic can be achieved by applying a convolution layer on the output of the lookup table layer. In order to extract local features from the sequence, the convolution layer applies a linear transformation to all M windows in the given sequence of vec- tors <ref type="bibr">4</ref> . Given a window of dense word representa- tions x 1 , x 2 , · · · , x l , the convolution layer first con- catenates these vectors to form a vector ¯ x of length l.d LT and then uses Equation 4 to calculate the out- put vector of length d c :</p><formula xml:id="formula_3">Conv(¯ x) = W.¯ x + b<label>(4)</label></formula><p>In Equation 4, W and b are the parameters of the network and are shared across all windows in the sequence.</p><formula xml:id="formula_4">w 1 w 2 w M-1 w M ... ... w 3 ...</formula><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lookup table layer</head><p>Convolution layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent layer</head><p>Mean over time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score</head><p>Linear layer with Sigmoid activation The convolution layer can be seen as a function that extracts feature vectors from n-grams. Since this layer provides n-gram level information to the subsequent layers of the neural network, it can po- tentially capture local contextual dependencies in the essay and consequently improve the perfor- mance of the system.</p><p>Recurrent Layer: After generating embeddings (whether from the convolution layer or directly from the lookup table layer), the recurrent layer starts pro- cessing the input to generate a representation for the given essay. This representation should ideally en- code all the information required for grading the es- say. However, since the essays are usually long, consisting of hundreds of words, the learnt vector representation might not be sufficient for accurate scoring. For this reason, we preserve all the inter- mediate states of the recurrent layer to keep track of the important bits of information from process- ing the essay. We experimented with basic recur- rent units (RNN) <ref type="bibr" target="#b7">(Elman, 1990)</ref>, gated recurrent units (GRU) ( <ref type="bibr" target="#b4">Cho et al., 2014)</ref>, and long short-term memory units (LSTM) (Hochreiter and Schmidhu- ber, 1997) to identify the best choice for our task. Since LSTM outperforms the other two units, we only describe LSTM in this section.</p><p>Long short-term memory units are modified re- current units that can cope with the problem of van- ishing gradients more effectively ( <ref type="bibr" target="#b15">Pascanu et al., 2013)</ref>. LSTMs can learn to preserve or forget the information required for the final representation. In order to control the flow of information during pro- cessing of the input sequence, LSTM units make use of three gates to discard (forget) or pass the informa- tion through time. The following equations formally describe the LSTM function:</p><formula xml:id="formula_5">i t = σ(W i .x t + U i .h t−1 + b i ) f t = σ(W f .x t + U f .h t−1 + b f ) ˜ c t = tanh(W c .x t + U c .h t−1 + b c ) c t = i t • ˜ c t + f t • c t−1 o t = σ(W o .x t + U o .h t−1 + b o ) h t = o t • tanh(c t )<label>(5)</label></formula><p>x t and h t are the input and output vectors at time t, </p><formula xml:id="formula_6">respectively. W i , W f , W c , W o , U i , U f , U c ,</formula><formula xml:id="formula_7">M oT (H) = 1 M M t=1 h t<label>(6)</label></formula><p>The mean-over-time layer is responsible for aggre- gating the variable number of inputs into a fixed length vector. Once this vector is calculated, it is fed into the linear layer to be mapped into a score. Instead of taking the mean of the intermediate re-</p><note type="other">current layer states h t , we could use the last state vector h M to compute the score and remove the mean-over-time layer. However, as we will show in Section 5.2, it is much more effective to use the mean-over-time layer and take all recurrent states into account. Linear Layer with Sigmoid Activation: The linear layer maps its input vector generated by the mean-over-time layer to a scalar value. This map- ping is simply a linear transformation of the in- put vector and therefore, the computed value is not bounded. Since we need a bounded value in the range of valid scores for each prompt, we apply a sigmoid function to limit the possible scores to the range of (0, 1). The mapping of the linear layer after applying the sigmoid activation function is given by Equation 7:</note><formula xml:id="formula_8">s(x) = sigmoid(w.x + b)<label>(7)</label></formula><p>where x is the input vector (M oT (H)), w is the weight vector, and b is the bias value.</p><p>We normalize all gold-standard scores to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> and use them to train the network. However, dur- ing testing, we rescale the output of the network to the original score range and use the rescaled scores to evaluate the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We use the RMSProp optimization algorithm ( <ref type="bibr" target="#b6">Dauphin et al., 2015)</ref> to minimize the mean squared error (MSE) loss function over the training data. Given N training essays and their corresponding normalized gold-standard scores s * i , the model com- putes the predicted scores s i for all training essays and then updates the network parameters such that the mean squared error is minimized. The loss func- tion is shown in Equation 8:</p><formula xml:id="formula_9">M SE(s, s * ) = 1 N N i=1 (s i − s * i ) 2<label>(8)</label></formula><p>Additionally, we make use of dropout regularization to avoid overfitting. We also clip the gradient if the norm of the gradient is larger than a threshold. We do not use any early stopping methods. In- stead, we train the neural network model for a fixed number of epochs and monitor the performance of the model on the development set after each epoch. Once training is finished, we select the model with the best QWK score on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we describe our experimental setup and present the results. Moreover, an analysis of the results and some discussion are provided in this sec- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>The dataset that we have used in our experiments is the same dataset used in the ASAP competition run by Kaggle (see <ref type="table">Table 1</ref> for some statistics). We use quadratic weighted Kappa as the evaluation metric, following the ASAP competition. Since the test set used in the competition is not publicly available, we use 5-fold cross validation to evaluate our systems. In each fold, 60% of the data is used as our train- ing set, 20% as the development set, and 20% as the test set. We train the model for a fixed number of epochs and then choose the best model based on the development set. We tokenize the essays using the NLTK 5 tokenizer, lowercase the text, and normalize the gold-standard scores to the range of <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Dur- ing testing, we rescale the system-generated normal- ized scores to the original range of scores and mea- sure the performance. In order to evaluate the performance of our sys- tem, we compare it to a publicly available open- source <ref type="bibr">6</ref> AES system called 'Enhanced AI Scor-ing Engine' (EASE). This system is the best open- source system that participated in the ASAP com- petition, and was ranked third among all 154 par- ticipating teams. EASE is based on hand-crafted features and regression methods. The features that are extracted by EASE can be categorized into four classes:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt #Essays Avg length Scores</head><p>• Length-based features</p><p>• Parts-of-Speech (POS)</p><p>• Word overlap with the prompt</p><p>• Bag of n-grams After extracting the features, a regression algorithm is used to build a model based on the training data. The details of the features and the results of using support vector regression (SVR) and Bayesian linear ridge regression (BLRR) are reported in ( <ref type="bibr" target="#b19">Phandi et al., 2015</ref>). We use these two regression methods as our baseline systems.</p><p>Our system has several hyper-parameters that need to be set. We use the RMSProp optimizer with decay rate (ρ) set to 0.9 to train the network and we set the base learning rate to 0.001. The mini-batch size is 32 in our experiments <ref type="bibr">7</ref> and we train the net- work for 50 epochs. The vocabulary is the 4,000 most frequent words in the training data and all other words are mapped to a special token that represents unknown words. We regularize the network by us- ing dropout ( <ref type="bibr" target="#b21">Srivastava et al., 2014</ref>) and we set the dropout probability to 0.5. During training, the norm of the gradient is clipped to a maximum value of 10. We set the word embedding dimension (d LT ) to 50 and the output dimension of the recurrent layer (d r ) to 300. If a convolution layer is used, the win- dow size (l) is set to 3 and the output dimension of this layer (d c ) is set to 50. Finally, we initialize the lookup table layer using pre-trained word embed- dings 8 released by <ref type="bibr" target="#b26">Zou et al. (2013)</ref>. Moreover, the bias value of the linear layer is initialized such that the network's output before training is almost equal to the average score in the training data.</p><p>We have performed several experiments to iden- tify the best model architecture for our task. These architectural choices are summarized below:</p><p>• Convolutional vs. recurrent neural network</p><p>• RNN unit type (basic RNN, GRU, or LSTM)</p><p>• Using mean-over-time over all recurrent states vs. using only the last recurrent state</p><p>• Using mean-over-time vs. an attention mecha- nism</p><p>• Using a recurrent layer vs. a convolutional re- current layer</p><p>• Unidirectional vs. bidirectional LSTM We have used 8 Tesla K80 GPUs to perform our ex- periments in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>In this section, we present the results of our eval- uation by comparing our system to the above- mentioned baselines (SVR and BLRR). <ref type="table" target="#tab_2">Table 2</ref> (rows 1 to 4) shows the QWK scores of our sys- tems on the eight prompts from the ASAP dataset <ref type="bibr">9</ref> . This table also contains the results of our statistical significance tests. The baseline score that we have used for hypothesis testing is underlined and the sta- tistically significant improvements (p &lt; 0.05) over the baseline are marked with '*'. It should be noted that all neural network models in <ref type="table" target="#tab_2">Table 2</ref> are unidi- rectional and include the mean-over-time layer. Ex- cept for the CNN model, convolution layer is not included in the networks. According to Table 2, all model variations are able to learn the task properly and perform competitively compared to the baselines. However, LSTM per- forms significantly better than all other systems and outperforms the baseline by a large margin (4.1%). However, basic RNN falls behind other models and does not perform as accurately as GRU or LSTM.   This behaviour is probably because of the relatively long sequences of words in essays. GRU and LSTM have been shown to 'remember' sequences and long- term dependencies much more effectively and there- fore, we believe this is the reason behind RNN's rel- atively poor performance.</p><p>Additionally, we perform some experiments to evaluate ensembles of our systems. We create vari- ants of our network by training with different ran- dom initializations of the parameters. To combine these models, we simply take the average of the scores predicted by these networks. This approach is shown to improve performance by reducing the variance of the model and therefore make the predic- tions more accurate. <ref type="table" target="#tab_2">Table 2</ref> (rows 5 and 6) shows the results of CNN and LSTM ensembles over 10 runs. Moreover, we combine CNN ensembles and LSTM ensembles together to make the predictions (row 7).</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, ensembles of models always lead to improvements. We obtain 0.4% and 1.0% improvement from CNN and LSTM ensembles, re- spectively. However, our best model (row 7 in <ref type="table" target="#tab_2">Table  2</ref>) is the ensemble of 10 instances of CNN models and 10 instances of LSTM models and outperforms the baseline BLRR system by 5.6%.</p><p>It is possible to use the last state of the recurrent layer to predict the score instead of taking the mean over all intermediate states. In order to observe the effects of this architectural choice, we test the net- work with and without the mean-over-time layer. The results of this experiment are presented in Ta- ble 3, clearly showing that the neural network fails to learn the task properly in the absence of the mean- over-time layer. When the mean-over-time layer is not used in the model, the network needs to effi- ciently encode the whole essay into a single state vector and then use it to predict the score. How- ever, when the mean-over-time layer is included, the model has direct access to all intermediate states and can recall the required intermediate information much more effectively and therefore is able to pre- dict the score more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systems</head><p>Avg  Additionally, we experiment with three other neu- ral network architectures. Instead of using mean- over-time to average intermediate states, we use an attention mechanism ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref> to compute a weighted sum of the states. In this case, we calculate the dot product of the intermediate states and a vector trained by the neural network, and then apply a softmax operation to obtain the normalized weights. Another alternative is to add a convolution layer before feeding the embeddings to the recurrent LSTM layer (CNN+LSTM) and eval- uate the model. We also use a bidirectional LSTM model (BLSTM), in which the sequence of words is processed in both directions and the intermediate states generated by both LSTM layers are merged and then fed into the mean-over-time layer. The re- sults of testing these architectures are summarized in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>The attention mechanism significantly improves the results compared to LSTM without mean-over- time, but it does not perform as well as LSTM with mean-over-time. The other two architectural choices do not lead to further improvements over the LSTM neural network. This observation is in line with the findings of some other researchers ( <ref type="bibr" target="#b11">Kadlec et al., 2015)</ref> and is probably because of the relatively small number of training examples compared to the capac- ity of the models.</p><p>We have also compared the accuracy of our best system (shown as 'AES') with human performance, presented in <ref type="table">Table 4</ref>. To do so, we calculate the agreement (QWK scores) between our system and each of the two human annotators separately ('AES -H1' and 'AES -H2'), as well as the agreement be- tween the two human annotators ('H1 -H2'). Ac- cording to <ref type="table">Table 4</ref>, the performance of our system on average is very close to human annotators. In fact, for some of the prompts, the agreement between our system and the human annotators is even higher than the agreement between human annotators. In gen- eral, we can conclude that our method is just below the upper limit and approaching human-level perfor- mance.</p><p>We also compare our system to a recently pub- lished automated essay scoring method based on neural networks <ref type="bibr" target="#b0">(Alikaniotis et al., 2016)</ref>. Instead of performing cross validation, <ref type="bibr" target="#b0">Alikaniotis et al. (2016)</ref> partition the ASAP dataset into two parts by using 80% of the data for training and the remaining 20% for testing. For comparison, we also carry out an experiment on the same training and test data used in ( <ref type="bibr" target="#b0">Alikaniotis et al., 2016)</ref>. Following how QWK scores are computed in <ref type="bibr" target="#b0">Alikaniotis et al. (2016)</ref>, in- stead of calculating QWK for each prompt sepa- rately and averaging them, we calculate the QWK score for the whole test set, by setting the minimum score to 0 and the maximum score to 60. Using this evaluation setup, our LSTM system achieves a QWK score of 0.987, higher than the QWK score of 0.96 of the best system in ( <ref type="bibr" target="#b0">Alikaniotis et al., 2016)</ref>. In this way of calculating QWK scores, since the majority of the test essays have a much smaller score range (see <ref type="table">Table 1</ref>) compared to <ref type="bibr">[0,</ref><ref type="bibr">60]</ref>, the differences between the system-predicted scores and the gold-standard scores will be small most of the time. For example, more than 55% of the essays in the test set have a score range of <ref type="bibr">[0,</ref><ref type="bibr">3]</ref> or <ref type="bibr">[0,</ref><ref type="bibr">4]</ref> and therefore, for these prompts, the differences be- tween human-assigned gold-standard scores and the scores predicted by an AES system will be small in the range of <ref type="bibr">[0,</ref><ref type="bibr">60]</ref>. For this reason, in contrast to prompt-specific QWK calculation, the QWK scores are much higher in this evaluation setting and far exceed the QWK score for human agreement when computed in a prompt-specific way (see <ref type="table">Table 4</ref>).</p><p>Interpreting neural network models and the inter- actions between nodes is not an easy task. However, it is possible to gain an insight of a network by an- alyzing the behavior of particular nodes. In order to understand how our neural network assigns the scores, we monitor the score variations while test- ing the model. <ref type="figure" target="#fig_3">Figure 2</ref> displays the score variations for three essays after processing each word (at each timestamp) by the neural network. We have selected a poorly written essay, a well written essay, and an average essay with normalized gold-standard scores of 0.2, 0.8, and 0.6, respectively.</p><p>According to <ref type="figure" target="#fig_3">Figure 2</ref>, the network learns to take essay length into account and assigns a very low score to all short essays with fewer than 50 words, regardless of the content. This pattern recurs for all essays and is not specific to the three selected essays in <ref type="figure" target="#fig_3">Figure 2</ref>. However, if an essay is long enough, the content becomes more important and the AES system starts discriminating well written <ref type="table" target="#tab_2">Description   Prompts  1  2  3  4  5  6  7  8</ref> Avg QWK AES -H1 0.750 0.684 0.662 0.759 0.751 0.791 0.731 0.607 0.717 AES -H2 0.767 0.690 0.632 0.762 0.769 0.775 0.752 0.530 0.710 H1 -H2 0.721 0.812 0.769 0.851 0.753 0.776 0.720 0.627 0.754 <ref type="table">Table 4</ref>: Comparison with human performance. H1 and H2 denote human rater 1 and human rater 2, respectively, and AES refers to our best system (ensemble of CNN and LSTM models).</p><p>essays from poorly written ones. As shown in <ref type="figure" target="#fig_3">Fig- ure 2</ref>, the model properly assigns a higher score to the well written essay 2, while giving lower scores to the other essays. This observation confirms that the model successfully learns the required features for automated essay scoring. While it is difficult to associate different parts of the neural network model with specific features, it is clear that appropriate in- dicators of essay quality are being learnt, including essay length and essay content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed an approach based on recurrent neural networks to tackle the task of au- tomated essay scoring. Our method does not rely on any feature engineering and automatically learns the representations required for the task. We have ex- plored a variety of neural network model architec- tures for automated essay scoring and have achieved significant improvements over a strong open-source baseline. Our best system outperforms the baseline by 5.6% in terms of quadratic weighted Kappa. Fur- thermore, an analysis of the network has been per- formed to get an insight of the recurrent neural net- work model and we show that the method effectively utilizes essay content to extract the required infor- mation for scoring essays.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The convolutional recurrent neural network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and U o are weight matrices and b i , b f , b c , and b o are bias vectors. The symbol • denotes element-wise multiplication and σ represents the sigmoid func- tion. Mean over Time: The outputs of the recurrent layer, H = (h 1 , h 2 , · · · , h M ), are fed into the mean-over-time layer. This layer receives M vec- tors of length d r as input and calculates an average vector of the same length. This layer's function is defined in Equation 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ID</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Score variations per timestamp. All scores are normalized to the range of [0, 1].</figDesc><graphic url="image-1.png" coords="8,313.20,57.82,226.80,128.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The QWK scores of the various neural network models and the baselines. The baseline for the statistical significance tests 

is underlined and statistically significant improvements (p &lt; 0.05) are marked with '*'. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The QWK scores of LSTM neural network vari-

ants. The baseline for the statistical significance tests is un-

derlined and statistically significant improvements (p &lt; 0.05) 

are marked with '*'. 

</table></figure>

			<note place="foot" n="1"> https://www.ets.org</note>

			<note place="foot" n="2"> https://github.com/nusnlp/nea 3 https://www.kaggle.com/c/asap-aes</note>

			<note place="foot" n="4"> The number of input vectors and the number of output vectors of the convolution layer are the same because we pad the sequence to avoid losing border windows.</note>

			<note place="foot" n="5"> http://www.nltk.org 6 https://github.com/edx/ease</note>

			<note place="foot" n="7"> To create mini-batches for training, we pad all essays in a mini-batch using a dummy token to make them have the same length. To eliminate the effect of padding tokens during training, we mask them to prevent the network from miscalculating the gradients. 8 http://ai.stanford.edu/∼wzou/mt</note>

			<note place="foot" n="9"> To aggregate the QWK scores of all prompts, Fisher transformation was used in the ASAP competition before averaging QWK scores. However, we found that applying Fisher transformation only slightly changes the scores. (If we apply this method to aggregate QWK scores, our best ensemble system (row 7, Table 2) would obtain a QWK score of 0.768.) Therefore we simply take the average of QWK scores across prompts.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by Singapore Ministry of Education Academic Research Fund Tier 2 grant MOE2013-T2-1-150. We are also grateful to the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic text scoring using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automated essay scoring with e-rater R v. 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>tional Testing Service</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated essay scoring by maximizing human-machine agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Equilibrated adaptive learning rates for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Harm de Vries, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Intelligent Essay Assessor: Applications to educational technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Peter W Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas K</forename><surname>Laham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactive Multimedia Electronic Journal of Computer-Enhanced Learning</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved deep learning baselines for Ubuntu corpus dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceesings of the NIPS</title>
		<meeting>eesings of the NIPS</meeting>
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Workshop on Machine Learning for Spoken Language Understanding and Interaction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling argument strength in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling organization in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">A</forename><surname>Kian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Handbook of Automated Essay Evaluation: Current Applications and New Directions</title>
		<editor>Mark D. Shermis and Jill Burstein</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From feedforward to recurrent LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating the performance of automated text scoring systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Task-independent features for automated essay grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Scholtenakoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
