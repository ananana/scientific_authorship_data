<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Autoregressive Decoder for Neural Response Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Du</surname></persName>
							<email>dujiachen@stmail.hitsz.edu.cn, cswjli@comp.polyu.edu.hk y.he@cantab.net, lyndonbing@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
							<email>xuruifeng@hit.edu.cn, wangxuan@cs.hitsz.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Autoregressive Decoder for Neural Response Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3154" to="3163"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3154</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Combining the virtues of probability graphic models and neural networks, Conditional Variational Auto-encoder (CVAE) has shown promising performance in many applications such as response generation. However, existing CVAE-based models often generate responses from a single latent variable which may not be sufficient to model high variability in responses. To solve this problem, we propose a novel model that sequentially introduces a series of latent variables to condition the generation of each word in the response sequence. In addition, the approximate posteriors of these latent variables are augmented with a backward Recurrent Neural Network (RNN), which allows the latent variables to capture long-term dependencies of future tokens in generation. To facilitate training , we supplement our model with an auxiliary objective that predicts the subsequent bag of words. Empirical experiments conducted on the OpenSubtitle and Reddit datasets show that the proposed model leads to significant improvements on both relevance and diversity over state-of-the-art baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, variational Bayesian models have shown attractive merits from both theoretical and practi- cal perspectives <ref type="bibr" target="#b12">(Kingma and Welling, 2013)</ref>. As one of the most successful variational Bayesian models, Conditional Variational Auto-Encoder (CVAE) ( ) was proposed to im- prove upon the traditional Sequence-to-Sequence (Seq2Seq) dialogue models. The CVAE based models incorporate stochastic latent variables into decoders in order to generate more relevant and diverse responses <ref type="bibr" target="#b17">(Serban et al., 2017;</ref><ref type="bibr" target="#b18">Shen et al., 2017)</ref>. However, existing CVAE * Corresponding author based models normally rely on the unimodal dis- tribution with a single latent variable to provide the global guidance to response generation, which is not sufficient to capture the complex semantics and high variability of responses. As a result, the autoregressive decoders used in response genera- tion always tend to ignore these oversimple latent variables and degrade the CVAE based model to the simple Seq2Seq model (aka. the model col- lapse problem). As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the unimodal latent variable z used in the conventional VAE usually captures simple unimodal pattern of responses.</p><p>However, in open-domain conversations, an ut- terance may have various responses which form complex multimodal distributions. To overcome this problem and improve the quality of gener- ated responses, we propose a novel model, named Variational Autoregressive Decoder (VAD) to iter- atively incorporate a series of latent variables into the autoregressive decoder. In particular, a dis- tinct latent variable sampled from CVAE is asso- ciated with each time step of the generation, and it is used to condition the next state of the autore- gressive decoder (e.g., the hidden state of a RNN). These latent variables at different time steps are integrated by autoregressive decoder to model mu- tilmodal distribution of text sequences and capture variability of responses as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Partially inspired by the sequential VAE-based models adopted in speech generation ( <ref type="bibr" target="#b8">Goyal et al., 2017;</ref><ref type="bibr" target="#b0">Bayer and Osendorfer, 2014)</ref>, in our VAD the approximate posterior of the latent variable at each time step is augmented by the corresponding hidden state of a backward RNN running through the remaining response sequence. Since the hid- den states of the backward RNN contain the infor- mation of the succeeding words in the response, they can be used as the guidance for the latent vari- ables to capture the long-term dependency on the future content.</p><p>It has been found that auxiliary losses that pre- dict another task-related objective could help la- tent variables capture more information from dif- ferent perspectives when training the VAE based models ( . To enhance VAD, we propose a purposely designed auxiliary loss to use the latent variable at each time step to predict the Bag-Of-Words (BOW) of the succeeding sub- sequence. The proposed auxiliary loss could es- sentially help VAD to generate more coherent re- sponses.</p><p>Experimental results show that the proposed VAD model outperforms the conventional re- sponse generation models when evaluated auto- matically and manually on the OpenSubtitle and Reddit datasets. The contributions in this work are two-fold:</p><p>• We propose a novel VAD model for response generation that can better capture the high variability of responses by sequentially asso- ciating latent variables to different time steps of autoregressive decoder and approximating the posterior of latent variables by augment- ing the hidden states of a backward RNN.</p><p>• A BOW based auxiliary objective is proposed to help preserving the diversity of generated responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conversational Systems</head><p>As neural network based models dominate the re- search in natural language processing, Seq2Seq models have been widely used for response gen- eration ( <ref type="bibr" target="#b19">Sordoni et al., 2015)</ref>. However, Seq2seq models suffer from the problem of generating generic responses, such as I don't know ( <ref type="bibr" target="#b13">Li et al., 2016a</ref>). Various approaches have been proposed to address this problem, including adding additional information ( <ref type="bibr" target="#b14">Li et al., 2016b;</ref><ref type="bibr" target="#b22">Xing et al., 2017;</ref><ref type="bibr" target="#b26">Zhou et al., 2017b</ref>) and modifying the architec- ture of existing models ( <ref type="bibr" target="#b13">Li et al., 2016a;</ref><ref type="bibr" target="#b23">Xu et al., 2017;</ref><ref type="bibr" target="#b25">Zhou et al., 2017a)</ref>. Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE <ref type="bibr" target="#b12">(Kingma and Welling, 2013</ref>) is one of the most successful models ( <ref type="bibr" target="#b17">Serban et al., 2017;</ref><ref type="bibr" target="#b18">Shen et al., 2017;</ref><ref type="bibr" target="#b2">Cao and Clark, 2017)</ref>. However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem ( <ref type="bibr" target="#b1">Bowman et al., 2016)</ref>. To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better represent highly structural latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variational Autoregressive Models</head><p>Recently, some works attempted to combine VAE with autoregressive models to better process in- put sequences. Broadly speaking, they can be categorized into two groups. Methods in the first group leverage autoregressive models to im- prove the inference of traditional VAEs. The most well-known model is Inverse Autoregressive Flow (IAF), which used a series of invertible transfor- mations based on the autoregressive model to con- struct the latent variables ( <ref type="bibr" target="#b11">Kingma et al., 2016;</ref><ref type="bibr" target="#b22">Chen et al., 2017)</ref>. Methods in the second group focus on improving autoregressive models like RNNs by adding variational inference ( <ref type="bibr" target="#b0">Bayer and Osendorfer, 2014;</ref><ref type="bibr" target="#b5">Chung et al., 2015;</ref><ref type="bibr" target="#b6">Fraccaro et al., 2016;</ref><ref type="bibr" target="#b8">Goyal et al., 2017</ref>). These models usually modeled continuous data such as images and audio signals. For dealing with discrete data such as text, ( ) applied variational recurrent neural networks (VRNN) for text sum- marization.</p><p>Our proposed framework is based on the second line of research, but is different from the previous research as it develops a new strategy of combin- ing VAE with RNN for response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed VAD Model</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we use the Seq2Seq model as the basic architecture. The Seq2Seq model is an encoder-decoder neural framework for mapping a source sequence to a target sequence <ref type="bibr" target="#b20">(Sutskever et al., 2014</ref>). The input of Seq2seq response gen- eration model is variable-length query sequence x = {x 1 , . . . , x m }, and the output is a response sequence y = {y 1 , . . . , y n }. Both the encoder and decoder are the Recurrent Neural Networks (RNN) with Gated Recurrent Units (GRU) ( <ref type="bibr" target="#b4">Chung et al., 2014</ref>).</p><p>The encoder is a bidirectional GRU that encodes the query sequence as the concatenation of the hid- den states of a forward and a backward GRUs. The semantic of word t in the query sequence is repre- sented by h e t = [</p><formula xml:id="formula_0">− → h e t , ← − h e t ], where − → h e t = −−→ GRU(x t , − − → h e t−1 ) ← − h e t = ←−− GRU(x t , ← − − h e t+1 ) (1)</formula><p>The decoder is a GRU with hidden state h d t at each step. The input at step t is the concatenation of previous word in response sequence y t−1 and the context vector c t computed by a neural atten- tion model. The context vector c t is the weighted sum of the whole encoder's hidden states com- puted by:</p><formula xml:id="formula_1">α s,t = f attention ([h e s , h d t−1 ]) c t = m s=1 α s,t h e s (2)</formula><p>where f attention is a one-layer neural network that produces attention weights, α s,t is the atten- tion weight evaluating the correlation between en- coder's hidden state h e s and hidden state of decoder h d t−1 . The decoder predicts the next wordˆywordˆ wordˆy t by jointly considering previous word y t−1 , attentional context c t and previous hidden state h d t−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional Variational Auto-Encoder</head><p>The decoder of VAD is based on the Condi- tional VAE (CVAE) framework ( , which approximates the distribution of random variable y (response) conditioned on x (i.e., query) by incorporating an latent variable z. CVAE introduces a parameterized conditional posterior distribution q θ (z|y, x) to approximate true posterior distribution p(z|y, x). By injecting q θ (z|y, x), the conditional marginal distribution of p(y|x) can be maximized by approximating the Evidence Lower Bound (ELBO):</p><formula xml:id="formula_2">log p φ (y|x) ≥ log p(y|x) − KL(q θ (z|y, x)||p(z|y, x))</formula><p>where KL denotes the Kullback-Leibler diver- gence. ELBO can be rewritten as a regularized auto-encoder function:</p><formula xml:id="formula_3">L = E q θ (z|y,x) [p φ (y|z, x)] − KL(q θ (z|y, x)||p φ (z|x))</formula><p>where p φ (y|z, x) is the decoder that decodes y from the latent variable z and conditional variable x, q θ (z|y, x) is the inference model that approxi- mates the true posterior, p φ (z|x) is the prior model that samples the latent variable from the prior dis- tribution, θ, φ are the parameters of the inference and decoder models, respectively. All parameter- ized distributions are modeled by neural networks.</p><p>In the training phase, the latent variable z is sampled from both the inference model and the prior model. z from the inference model is then used to condition the generated distribution p(y|z, x). Meanwhile, CVAE minimizes the KL divergence between the latent variables from these two models. This process makes it possible for CVAE to samples z from the prior model only when decoding in the testing phase.</p><p>Different from the previous work on CVAE- based response generation that only relys on a sin- gle latent variable ( <ref type="bibr" target="#b17">Serban et al., 2017;</ref><ref type="bibr" target="#b18">Shen et al., 2017</ref>), our proposed model in- corporates a series of latent variables into the au- toregressive decoder. Inspired by the work on variational recurrent neural networks ( <ref type="bibr" target="#b8">Goyal et al., 2017;</ref><ref type="bibr" target="#b0">Bayer and Osendorfer, 2014</ref>), our model se- quentially decodes the response sequence condi- tioned on the latent variable z t at each time step by p φ (y|z, x) = t p(y t |y &lt;t , z t , x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational Autoregressive Decoder</head><p>Traditional CVAE-based models only use a single standard normal distribution to model the latent variable z. They are usually difficult to model the multi-modal distribution of responses p(y|z, x).</p><p>To overcome this limitation, we propose a Varia- tional Autoregressive Decoder (VAD) that decom- poses z into sequential variables z t at each time step t during response generation. Owing to the autoregressive structure of VAD, the hidden state of backward RNN ← − h d t is used to condition the la- tent variable z t , which can be seen as a long-term guidance to the generation. Moreover, we propose a novel auxiliary objective, which is specially de- signed for VAD, to avoid model collapse.</p><p>At each time step, the decoder uses a forward GRU to process the sequence and predicts the next token by a feed-forward network f output with the softmax activation function. The input to GRU is the combination of the previous word's embedding y t−1 , the context vector produced by an attention model c t and the latent variable z t . The process is described by,</p><formula xml:id="formula_4">− → h d t = −−→ GRU([y t−1 , c t , z t ], − − → h d t−1 )<label>(3)</label></formula><formula xml:id="formula_5">p φ (y t |y &lt;t , z t , x) = f output ([ − → h d t , c t ])<label>(4)</label></formula><p>where, − → h d t is the hidden state produced by the forward GRU at time step t. c t is the attentional weighted sum of the encoder's output.</p><p>Inference Model We use the hidden states of the backward RNN running through the response sequence as an additional input to the inference model. The backward RNN processes the se- quence by,</p><formula xml:id="formula_6">← − h d t = ←−− GRU(y t+1 , ← − − h d t+1 )<label>(5)</label></formula><p>The backward hidden state ← − h d t contains the in- formation of succeeding tokens, and it serves as a future plan for generation. By combining the information produced by the backward RNN, the inference model has a better capability of approx- imating the real posterior distribution.</p><p>Considering context variable c t at each time step as a substitute of the condition variable x in (3.1), c t is also fed to the inference model. The inference model is a feed-forward neural network f infer . The approximated distribution q(z t |y, x) is a normal distribution N (µ i , σ i ), which is parame- terized by the output of f infer :</p><formula xml:id="formula_7">[µ i , σ i ] = f infer ([ − − → h d t−1 , c t , ← − h d t ])<label>(6)</label></formula><formula xml:id="formula_8">q θ (z t |y, x) = N (µ i , σ i )<label>(7)</label></formula><p>where the sampling process of z t is done by re- parameterization ( <ref type="bibr" target="#b12">Kingma and Welling, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Model</head><p>The prior network can only use the observable variables in the testing phase to sample z t . The observable variables include the previous hidden state − − → h d t−1 and the context variable c t . The prior model is also modeled by a feed-forward net- work f prior as follows.</p><formula xml:id="formula_9">[µ p , σ p ] = f prior ([ − − → h d t−1 , c t ]) (8) p φ (z t |y &lt;t , x) = N (µ p , σ p )<label>(9)</label></formula><p>where µ p , σ p are the parameters of prior normal distribution.</p><p>Auxiliary Objective As discussed in Section 1, the decoder based on the autoregressive model often ignores the latent variables and causes the model to collapse. One way to alleviate this prob- lem is to add an auxiliary loss to the training ob- jective ( <ref type="bibr" target="#b8">Goyal et al., 2017)</ref>. To allow the latent variables to capture the informa- tion from a different perspective, we use Sequen- tial Bag of Word (SBOW) as the auxiliary objec- tive for the proposed VAD model. The idea of the SBOW auxiliary objective is to sequentially pre- dict the bag of succeeding words y bow(t+1,T ) in the response using the latent variable z t at each time step. This auxiliary objective can be seen as the prediction of candidate words for future gener- ation.</p><p>Our SBOW is specially designed for VAD. It is different from the Bag-of-Words (BOW) auxil- iary loss used in the CVAE-based models ( , which only uses the latent variable to predict the Bag-Of-Words of the whole sequence. VAD with SBOW sequentially produces the aux- iliary loss for each time step of generation. The auxiliary loss at each time step is computed by</p><formula xml:id="formula_10">p ξ (y bow(t+1,T ) |z t:T ) = f auxiliary (z t )<label>(10)</label></formula><p>where y bow(t+1,T ) is the bag-of-word vector of the words from t + 1 to T in the response, and f auxiliary is a feed-forward neural network with the softmax output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>The loss function of our model is the sum of the losses at each time step, including the weighed sum of the ELBO loss L ELBO (t) and the auxil- iary loss L AU X (t) where L ELBO (t) can be further decomposed into a log-likelihood loss and the KL divergence:</p><formula xml:id="formula_11">L = t [L ELBO (t) + αL AU X (t)] = t [(L LL (t) − L KL (t)) + αL AU X (t)]<label>(11)</label></formula><p>Here, L LL (t) denotes the log-likelihood loss when predicting y t . L KL (t) is the KL-divergence of the approximate posteriori q θ and priori p φ at time step t. L AU X (t) is the auxiliary loss when predicting SBOW as described in Section 3.2. α is the weight controlling the auxiliary loss. The losses are computed by</p><formula xml:id="formula_12">L LL (t) = E q θ (zt|y,z) [log p θ (y t |y &lt;t , z t , x t )] L KL (t) = KL(q θ (z t |y, x)||p φ (z t |y &lt;t , x)) L AU X (t) = E q θ (zt|y,z)</formula><p>log p ξ (y bow(t+1,T ) |z t )</p><p>All the parameters are learned by optimizing Equation (11) and updated with back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the proposed model on two datasets: OpenSubtitles and Reddit. The OpenSubtitles dataset contains subtitles for movies in various languages. Here, we only choose the English version of OpenSubtiles. The Reddit dataset is crawled from comments of Reddit 1 which is an American social news discussion website. We col- lected more than 10 million single-turn dialogues from 100 topics posted in 2017. For each dataset, we randomly select 6 million conversations for training, 10k for validation and 5k for testing. For every conversation, we remove the sentences whose length is shorter than 6 words and only keep the first 40 words for sentences longer than 40. We keep top 15k frequent words as the vocabulary for OpenSubtitles and 20k frequent words for Reddit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyper-parameters and Training Setup</head><p>We use the pre-trained GloVe 300-dimensional word embeddings for both the encoder and the de- coder. The encoder is a bidirectional RNN with GRU with the size of the hidden state set to 512. The size of the hidden states of GRU in the de- coder is also set to 512. We apply Layer Normal- ization when training the decoder. The size of the latent variables is set to 400. The inference net- work and the prior network are all one-layer feed- forward network. All weights are initialized by the xavier method ( <ref type="bibr" target="#b7">Glorot and Bengio, 2010</ref>). The model is trained end-to-end by Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2014</ref>) with the learning rate set to 10 −4 and gradient clipped at 1. When gener- ating text, we adopt the greedy strategy and the KL-annealing strategy, with the temperature vary- ing from 0 to 1 and increased by 10 −5 after each iteration of batch update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare our proposed model with the follow- ing three baselines:</p><p>• Seq2Seq: Sequence-to-Sequence model with attention ( <ref type="bibr" target="#b19">Sordoni et al., 2015</ref>).</p><p>• CVAE: Conditional Variational Auto- Encoder for generating responses <ref type="bibr" target="#b17">(Serban et al., 2017)</ref>. Different from our model, CVAE uses a unimodal Gaussian distribution to model the whole response and append the output of VAE as an additional input to decoder. We also use the KL annealing strategy when training CVAE with the same parameter setting as in our model.</p><p>• CVAE+BOW loss: CVAE model with the auxiliary bag-of-words loss ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Metrics</head><p>We employ three types of commonly used auto- matic evaluation metrics and human evaluation in our experiments:</p><p>Embedding Similarity: Embedding-based met- rics compute the cosine similarity between the sentence embedding of a ground-truth response and that of the generated one. There are vari- ous ways to derive the sentence-level embedding from the constituent word embeddings. In our ex- periments, we apply three most commonly used strategies to obtain the sentence-level embeddings. EMB A calculates the average of word embeddings in a sentence. EMB E takes the most extreme value among all words for each dimension of word em- beddings in a sentence. EMB G greedily calculates the maximum of cosine similarity of each token in two sentences and take the average of them to get the final matching score ( <ref type="bibr" target="#b16">Liu et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUBER Score: RUBER (Referenced metric and Unreferenced metric Blended Evaluation</head><p>Routine) is a newly proposed metric for evaluating the quality of response in conversations that show high correlation with human annotation ( <ref type="bibr" target="#b21">Tao et al., 2017)</ref>. RUBER evaluates the generated responses by taking into account both the ground-truth re- sponses and the given queries. For the referenced metric, RUBER calculates the embedding-based cosine similarity between a generated response and its corresponding ground-truth. For the un- referenced metric, RUBER firstly trains a neural network by a response retrieval task and evaluates the relatedness between a generated response and its query. Evaluating RUBER score can be treated as a rough simulation to the well-known Turing Test. For blending the two metrics, there are two strategies: taking the geometric mean (RUB G ) or the arithmetic mean (RUB A ). The RUBER score ranges between 0 and 1 and higher scores imply better relatedness.</p><p>Diversity: Diversity metrics evaluate the infor- mativeness and diversity of generated responses.</p><p>In our experiments, we use Dist 1 and Dist 2 (Li et al., 2016a) to evaluate the diversity and En- tropy to measure the informativeness. Dist 1 (or Dist 2 ) calculates the ratio of the number of unique unigrams (or bigrams) against the total number of unigrams (or bigrams). Higher Dist 1 (or Dist 2 ) implies more diverse vocabularies used in re- sponses. Entropy as a metric proposed by <ref type="bibr" target="#b17">(Serban et al., 2017</ref>) calculates the average entropy in a generated response. According to informa- tion theory, it is known that low-frequent words have higher entropy and carries more information. Therefore, we use this Entropy to measure the in- formativeness and diversity of the generated re- sponses. The unit of Entropy is bit and Higher Entropy correlates to more informative response.</p><p>Human Evaluation: In human evaluation, 10 research students are arranged to rate the gen- erated responses generated by CVAE with BOW auxiliary loss and our model. We randomly se- lected 100 queries from the Reddit dataset 2 and used each model to generate the best responses. Each query with its ground-truth response and the two generated responses are simultaneously shown to the human evaluators. The evaluators are asked to rate the responses based on grammatical correctness, coherence and relevance to queries (tie is permitted).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Analysis</head><p>The experimental results evaluated by automatic metrics on the OpenSubtitles and the Reddit datasets are shown in <ref type="table">Table 1</ref> and 2, respectively. It is observed that both CVAE-based models and our proposed models outperform Seq2Seq by a large margin, showing the effectiveness of adding variational latent variable for response genera- tion. However, using different structure of varia- tional models leads to differences in performance on both plausibility and diversity. Our model with or without the SBOW auxiliary loss outper- forms CVAE as observed by the significant boost in semantic relevance-oriented metrics (embed- ding similarities and RUBER score) and diversity- oriented metrics. This is mainly due to the differ- ent strategy employed for representing latent vari- ables. CVAE only uses a unimodal latent vari- able as the semantic signal of the whole response sequence which limits its capability of capturing   variability of response sequences. By incorporat- ing a series of time-varying latent variables into each step of autoregressive decoder, our model is able to model more complicated multimodal dis- tributions of response sequences and capture more detailed semantic information. Since adding the auxiliary loss could alleviate the model collapse problem, we found that CVAE model with the BOW auxiliary loss outperforms our basic model without auxiliary loss, especially on the diversity metrics. When adding the pro- posed SBOW auxiliary loss into our model, we found that our generated responses have shown better diversity compared to those generated by CVAE+BOW loss. The encouraging improve- ment is attributed to the autoregressive structure of our variational inferences, which makes it pos- sible to gradually introduce additional informa- tion of SBOW. To better demonstrate the impact of SBOW, we calculate the average length of the generated responses of our model and CVAE with BOW loss and show the results in   <ref type="table" target="#tab_4">Table 4</ref>. It is observed that the responses generated by our proposed VAD is more plausi- ble than CVAE+BOW from human perspectives. We also conduct t-test to compare our model with CVAE+BOW. The results show that the improve- ment of VAD over CVAE+BOW is statistically sig- nificant (p &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Analysis</head><p>Case Study To empirically analyze the quality of the generated responses, we show some ex- ample responses generated by our model and two baselines (Seq2Seq and CVAE+BOW) in <ref type="table" target="#tab_6">Table 5</ref>. It is observed that Seq2seq often generates generic responses that starting with 'I don't know' or 'I am not sure', since the deterministic structure of Seq2seq limits the diversity of generation. In- jecting variational latent variables avoids dull re- sponses as can be seen from the responses gen- cuz linux can be a great os . i hope the grand tour will make an episode . i ca n't commit post though .</p><p>i 'm wondering getting it .</p><p>i would hope that it will be on netflix as well . erated by CVAE+BOW and our model. How- ever, we found that CVAE+BOW tends to copy the given queries (the first and fourth example in Ta- ble 4) and repeatedly generate redundant tokens (the second example). The generated responses of our model are more fluent and relevant to queries. Also, our model generates longer responses com- pared to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KL Divergence Visualization</head><p>In order to demonstrate that our model is able to alleviate the model collapse problem of VAE, we visualize the KL divergence between the approximate posterior distribution q θ (z|y, x) and priori p φ (z|x) during the training process of our models and CVAE with BOW loss in <ref type="figure" target="#fig_2">Figure 3</ref>. As we know, when varia- tional models ignore the latent variable, the gener- ated value y will be independent of the latent vari- able z which causes the KL divergence in Equa- tion (3.1) to approach 0. The higher KL value dur- ing training means more dependence between y and z. In this experiment, we use the same KL an- nealing strategies for our model and CVAE+BOW as described in Section 4.2. The KL divergence of the two models on the OpenSubtitles and the Red- dit datasets during training is plotted in <ref type="figure" target="#fig_2">Figure 3</ref>. It is observed that the KL divergence of our model converges to a higher value compared to that of CVAE+BOW. It shows that our model could better alleviate the model collapse problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, a novel variational autoregressive decoder is proposed to improve the performance of VAE-based models for open-domain response generation. By injecting the variational inference into the RNN-based decoder and applying care- fully designed conditional variables and auxiliary objective for latent variables, the proposed model is expected to better modeling semantic informa- tion of text in conversations. Quantitative and qualitative experimental results show clear perfor- mance improvement of the proposed model over competitive baselines. In future works, we will explore the use of other attributes of responses such as Part-of-Speech (POS) tags and chunking sequences as additional conditions for better re- sponse generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributions of latent variable</figDesc><graphic url="image-1.png" coords="1,314.36,370.57,204.08,98.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sequence-to-sequence model using sequential variational decoder.</figDesc><graphic url="image-2.png" coords="3,94.68,62.81,408.19,187.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: KL divergence during training.</figDesc><graphic url="image-3.png" coords="8,325.70,255.92,181.41,154.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Experimental results on the Reddit dataset.</head><label>2</label><figDesc></figDesc><table>Models 
OpenSubtitles Reddit 
Ground Truth 
15.31 
17.48 
CVAE+BOW 
9.66 
10.83 
Ours 
11.81 
14.09 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : The average length of responses.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>It is 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of human judgment on the generated 
responses. 

longer responses than CVAE+BOW. The results 
validate the effectiveness of adding the SBOW 
auxiliary objective into our model. 
The evaluation results of human judgment is 
shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example responses generated by our model and two baselines (Seq2Seq and CVAE+BOW) from the 
OpenSubtitles and the Reddit Datasets. 

</table></figure>

			<note place="foot" n="1"> http://www.reddit.com</note>

			<note place="foot" n="2"> The reason of not conducting the human evaluation on the OpenSubtitles dataset is that query-response pairs in the OpenSubtitles dataset are extracted from movie scripts and hence are more difficult to evaluate without the context information.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Advances in Variational Inference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent variable dialogue models and their diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Pieter Abbeel. 2017. Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 30th Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 31st Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6697" to="6707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Advances in Neural Information Processing Systems</title>
		<meeting>the 30th Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep recurrent generative decoder for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A conditional variational framework for dialog generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (EMNLP)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="504" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03079</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural response generation via gan with an approximate embedding layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Sun Chengjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="617" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mechanism-aware neural machine for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganbin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>eddings of 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
