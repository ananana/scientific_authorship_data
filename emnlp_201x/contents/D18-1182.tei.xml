<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Reed College</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1533" to="1542"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1533</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose Odd-Man-Out, a novel task which aims to test different properties of word representations. An Odd-Man-Out puzzle is composed of 5 (or more) words, and requires the system to choose the one which does not belong with the others. We show that this simple setup is capable of teasing out various properties of different popular lexical resources (like WordNet and pre-trained word embeddings), while being intuitive enough to annotate on a large scale. In addition, we propose a novel technique for training multi-prototype word representations, based on unsupervised clustering of ELMo embeddings, and show that it surpasses all other representations on all Odd-Man-Out collections.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Correctly disambiguating the sense of a polyse- mous word (e.g., "spring is a beautiful season" versus "John was ready to spring into action") is a crucial part of various NLP tasks, such as trans- lation, question answering, or textual entailment. The state-of-the-art, and the de-facto common practice for essentially all of these tasks, involves neural networks (see <ref type="bibr" target="#b12">(Goldberg, 2015</ref>) for a recent survey), which are commonly initialized with pre- trained word vectors, such as Word2Vec ( <ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>) or GloVe ( <ref type="bibr" target="#b29">Pennington et al., 2014</ref>).</p><p>These widely-used representations often signif- icantly improve performance in downstream tasks, as they are able to leverage large amounts of un- structured data. However, most of the popular collections of word embeddings assign only one vector to each word, thus shifting the burden of word disambiguation to deeper, task-specific lay- ers, which commonly rely on data of much smaller scales.</p><p>While there has been a significant body of work around sense embeddings (i.e., embedding senses, instead of lexical units), evaluating such repre- sentations remains the subject of debate <ref type="bibr" target="#b6">(Faruqui et al., 2016;</ref><ref type="bibr" target="#b11">Gladkova and Drozd, 2016)</ref>.</p><p>In this work, we propose a new evaluation task called Odd-Man-Out. The goal of an Odd-Man- Out puzzle is simple. Given a set of words 1 like cherry, orange, apple, grass, grape, the objec- tive is to identify the word that does not belong (here, the answer is grass, because it is not a fruit). While there are often multiple relationships among the words, we will show that non-experts typically agree on the odd-man-out, and are able to generate hard puzzles on a large scale, using a novel crowdsourcing protocol.</p><p>Following the creation of this large test set, we conduct a thorough analysis of the ability of various lexical resources to correctly solve the task. In doing so, we embrace the suggestion of <ref type="bibr" target="#b11">Gladkova and Drozd (2016)</ref>, which argue for "a shift from absolute ratings of word embeddings towards more exploratory evaluations that would aim not for generic scores, but for identification of strengths and weaknesses of embeddings", and conduct rigorous analysis of each representation. Overall, we find that all lexical resources are prone to miss associations which are intuitive for hu- mans, leaving ample room for future improve- ment. Moreover, we show empirical evidence that lexical resources that do not account for polysemy are handicapped by this weakness.</p><p>Finally, we propose a new sense embedding technique, which leverages the recent introduction of ELMo embeddings ( <ref type="bibr" target="#b10">Peters et al., 2018</ref>) by per- forming unsupervised clustering over a large un- structured corpus. We show that this new resource surpasses all other baselines on various Odd-Man- Out datasets.</p><p>We make all our code and models publicly available. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Existing Evaluation Methods</head><p>In this section, we briefly survey some existing evaluation methods for lexical resources, and dis- cuss their pros and cons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Similarity</head><p>The word similarity task <ref type="bibr" target="#b32">(Rubenstein and Goodenough, 1965</ref>) has been a dominant approach to as- sess word vector quality. In this task, systems are required to score the similarity of two words on a numeric scale, sometimes without context and sometimes in a sentential context <ref type="bibr" target="#b15">(Huang et al., 2012</ref>). For instance, the pair (tiger, mammal) has a similarity score of 6.85 (out of 10) on the Word- Sim dataset ( <ref type="bibr" target="#b8">Finkelstein et al., 2001)</ref>.</p><p>A common criticism <ref type="bibr" target="#b6">(Faruqui et al., 2016;</ref><ref type="bibr" target="#b11">Gladkova and Drozd, 2016</ref>) of the word similarity task is that "similarity" is subjective, and conflates several different potential relationships between words. For instance, <ref type="bibr" target="#b6">(Faruqui et al., 2016</ref>) ques- tions why the pair (cup, coffee) should be consid- ered more similar than (car, train), as it is accord- ing to the WordSim dataset.</p><p>Odd-man-out puzzles naturally disambiguate the nebulous concept of "similarity", because each puzzle implicitly defines a specific relationship. The words "car" and "train" may be similar, but this similarity is irrelevant is the context of a puz- zle like car, train, checkered flag, racetrack, pit stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Analogies</head><p>Analogies like "king is to queen as man is to..." <ref type="bibr" target="#b25">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b17">Jurgens et al., 2012</ref>) are related to the odd-man-out task. Analogies, how- ever, are best suited to particular relationships, such as hypernym → hyponym (which are the sub- ject of extensive research, e.g., <ref type="bibr" target="#b34">(Shwartz et al., 2016)</ref>), while odd-man-out puzzles can capture a broader range of associations (see for instance, the auto-racing puzzle from the previous section). Analogies are also more subject to ambiguity, since the premise can involve only two words. For instance, the puzzle "cherry is to strawberry as grass is to..." could refer to the fact that cherry and strawberry are both fruits, both red, or both red fruits. Odd-man-out puzzles provide a simple way of reducing ambiguity: adding more choices to the puzzle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word Sense Disambiguation and Induction</head><p>Word sense disambiguation <ref type="bibr" target="#b27">(Navigli, 2009</ref>) is a popular way to evaluate polysemous word rep- resentations. The common criticism is that sys- tems are rewarded based on their ability to classify words according to a fixed inventory of senses, whose granularity is regarded by some as too coarse and others as too fine. An alternative is word sense induction ( <ref type="bibr" target="#b22">Manandhar et al., 2010)</ref>, which allows systems to cluster word senses with- out an agreed-upon sense inventory. However, there is not an obvious evaluation metric. The two metrics used in <ref type="bibr">SemEval 2010</ref><ref type="bibr">Task 14 (Manandhar et al., 2010</ref>) yielded highly divergent system rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Word Context Relevance</head><p>A recent evaluation method is Word Context Rel- evance ( <ref type="bibr" target="#b0">Arora et al., 2016;</ref><ref type="bibr" target="#b35">Sun et al., 2017a</ref>). The task is to identify whether a particular bag of words is "relevant" to a target word. For instance, "tie" is considered relevant to the bag "winner, score, tied, completion, identical, results, sports," but irrelevant to the bag "domestic, hog, pig, culi- nary, eaten, cooked, fat". This task has the attrac- tiveness of being a simple binary evaluation, but demands only that a model can identify a broad sense of relatedness, not the ability to pinpoint specific relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Lexical Substitution</head><p>Lexical substitution tasks <ref type="bibr" target="#b23">(McCarthy and Navigli, 2007;</ref><ref type="bibr" target="#b2">Biemann, 2013;</ref><ref type="bibr" target="#b18">Kremer et al., 2014</ref>), in which the task is to determine whether one word can replace another word in a particular context, are effective, but are restricted in the kind of rela- tionships they can test (mainly synonymy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Odd Man Out Datasets</head><p>In this section, we describe the creation of several Odd-Man-Out datasets. We begin by describing a small-scale, curated annotation, then show how to scale the annotation using crowdsourcing tech- niques. In the subsequent sections, we use these datasets to explore the properties of a wide array of lexical resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expert annotation</head><p>To create our first odd-man-out datasets, we used categories from the card game Anomia. 3 Anomia is a slapjack-style game in which players attempt to name instances of a particular category as quickly as possible. Categories include: "percus- sion instrument", "Mexican food", and "Michael Jackson song". A team of 4 people, trained in- house, created one odd-man-out puzzle for each category, yielding a total of 404 puzzles. For in- stance, the puzzle corresponding to "percussion instrument" is clarinet, drum, xylophone, tam- bourine, cymbals, where clarinet is the odd-man- out. The puzzle-writing team attempted to make the odd-man-out similar to the category, e.g. clar- inet is a musical instrument but not a percussion instrument.</p><p>We divided the puzzles into 2 sets, one for puzzles comprised of common words (like the "percussion instruments" example above), and one for puzzles comprised of proper nouns (like the puzzle derived from the category "Michael Jackson songs"). Each set contained exactly 202 puzzles. We further divided each of these sets into a development set (used for error analysis) of size 100 and a test set of size 102.</p><p>Henceforth, we will refer to these four datasets as ANOMIACOMMONDEV, ANOMIACOMMONTEST,ANOMIAPROPERDEV, andANOMIAPROPERTEST.</p><p>There are two potential pitfalls in creating these puzzles. First, the underlying category may not be detectable by humans. Second, we may inad- vertently create an ambiguous puzzle with multi- ple odd-men-out. For instance, given the category "vegetables", we might create the puzzle grass, celery, cucumber, carrot, lettuce, for which the intended odd-man-out is grass, but the answer carrot is also a reasonable odd-man-out for the category "things that are green."</p><p>To ensure the answerability of our puzzles, we administered the ANOMIACOMMONDEV to five college-educated individuals, three of whom were native English speakers. Both groups did well. The native speakers averaged 95.3% accuracy, while the non-native speakers averaged 91.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Category expansion (crowdsourced):</head><p>Given a seed word and a corresponding category (w, c) turkers are asked to provide five more examples of the category c , which are similar to, but different than, w. For example, given ("bat", "nocturnal mammal") turkers are expected to provide examples such as "beaver", "badger", or "hedgehog", while for ("bat", "baseball instrument"), proper answers include "ball" or "cap". We used the Amazon Mechanical Turk (AMT) 5 platform, paying 15¢ per elicitation.</p><p>3. Puzzle creation (automatic): For each poly- semous seed word w, belonging to categories c 1 , c 2 , we automatically create Odd-Man-Out puzzles by concatenating to w each possi- ble combination of three words from c 1 (c 2 ), while the intended odd-man-out is any word from c 2 (c 1 ). For example, using the previous seed word and categories, we get the follow- ing puzzle: ("cap", "bat", "beaver", "bad- ger", "hedgehog"). Creating puzzles by com- bining words from c 1 and c 2 in this manner ensures that task is challenging, as it requires to disambiguate the polysemous word (e.g., "bat").</p><p>Performing the process described above with the 249 polysemous seed words yielded a large cor- pus of 497, 365 puzzles, thanks to the combina- tion process, with a vocabulary of 1, 312 differ- ent words. To create a gold high quality test corpus, and to assess the validity of this semi- automatic process, we sampled 1000 puzzles and administered each puzzle to three annotators on AMT, paying 6¢ per response. We found that for 84.3% of the instances there was a major- ity vote agreement on the intended odd-man-out word. We refer to this set of 843 puzzles as CROWDSOURCED843. <ref type="table">Table 1</ref> shows examples of the crowdsourced puzzles.</p><p>From an examination of turkers disagreement, we can attribute the vast majority to noise in one of the annotation stages, e.g., turkers which provide examples of the wrong sense of the seed word, re- sulting in invalid puzzles (where all of the words belong to the same category), or turkers marking the wrong odd-man-out, thus invalidating an oth- erwise correct puzzle.</p><p>Overall, the cost of the annotation and valida- tion was below $500, for a large high-quality an- notated resource and a smaller gold standard test corpus. Both corpora are made available. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Taxonomy-Based Solvers</head><p>In this section, we show how to create odd-man- out solvers for taxonomies like WordNet <ref type="bibr" target="#b26">(Miller, 1992)</ref>.</p><p>Define a taxonomy as a triple (V, E, L), where (V, E) is a directed acyclic graph, and L maps each vertex V to a string. A simple example is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, where the each vertex v is la- beled with L(v).</p><p>We create an odd-man-out solver from a taxon- omy as follows:</p><p>• The specificity of a vertex v is defined as the reciprocal of the number of its descendants.</p><p>6 https://github.com/gabrielStanovsky/ odd-man-out</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Puzzle construction crane, pelican, excavator, hoist, upraise guitar part fret, crying, inlays, truss rod, neck alcoholic drinks gin, poker, vodka, tequila, wine card games gin, vodka, bridge canasta, uno <ref type="table">Table 1</ref>: Selection of examples from the crowd- sourced Odd-Man-Out dataset. The polysemous seed word appears in bold, while the correct an- swer is in italics. The last two examples demon- strate how a polysemous word (gin) can partici- pate in two different puzzles. • Given an odd-man-out puzzle w 1 , ..., w n , the explanation of word w k is the vertex v of highest specificity such that: (i) for each word w j such that j = k, there exists some</p><formula xml:id="formula_0">descendent v ′ of v where L(v ′ ) = w j , (ii) there does not exist a descendent v ′ of v such that L(v ′ ) = w k .</formula><p>For instance, the explana- tion of helium with respect to the puzzle he- lium, mercury, lead, silver, gold is the node labeled "metallic element."</p><p>• If some word does not have an explanation, or if there is no word whose explanation is uniquely most specific, then the solver ab- stains from answering. Otherwise, the solver returns the word with the most specific expla- nation.</p><p>Using WordNet 3.0 <ref type="bibr" target="#b26">(Miller, 1992;</ref><ref type="bibr" target="#b7">Fellbaum, 1998)</ref> as the taxonomy, the solver correctly solves 40.6% of the ANOMIACOMMON puzzles, answer- ing incorrectly for 13.4% and abstaining for 46.0% of the puzzles. Unsurprisingly (since WordNet focuses on common words), the WordNet solver gets only 1 out of 202 ANOMIAPROPER puz- zles, abstaining from all the rest. On CROWD- SOURCED843, the solver correctly solves 22.0% of the puzzles, answering 15.1% incorrectly and abstaining from the rest.</p><note type="other">Puzzle Explanation chicken, screwdriver, margarita, mixed drink mimosa, daiquiri silver, steel, brass, alloy bronze, pewter canoe, school, flock, animal group herd, pack nightgown, afternoon, morning, abstraction evening, midnight king, president, queen, leader prince, princess dinghy, crab, boat, travel (verb) canoe, raft</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Error Analysis</head><p>A nice property of the taxonomy-based solver is that it provides an explanation for its answer (i.e. the "explanation" vertex defined above). In <ref type="table" target="#tab_0">Ta- ble 2</ref>, we show some of the WordNet solver's an- swers and explanations (specifically we show the name of the WordNet synset corresponding to the explanation vertex). For 85.4% of its correct an- swers, the solver also returns the correct expla- nation (like the "mixed drink," "alloy," and "ani- mal group" explanations). In the handful of cases when it does not, it finds either an incorrect or an overly vague explanation (like the "abstraction" explanation) that still results in the correct answer.</p><p>Typically, the incorrect answers result from in- completeness in the WordNet taxonomy. For instance, the word "king" is not a hyponym of "leader," even though "president," "queen," "prince," and "princess" all are. In the bottom- most puzzle of <ref type="table" target="#tab_0">Table 2</ref>, the error results from "raft" not being considered a watercraft (or a con- veyance of any kind) by WordNet. Because of this, the solver finds a more tenuous connection between "crab" and three of the other words, lever- aging a rare sense of "crab" as a verb meaning "to move like a crab."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Embedding-Based Solvers</head><p>In this section, we create odd-man-out solvers from collections of word embeddings. Specifi- cally, we experiment with two types of embed- dings: (1) traditional word embeddings, which map words to a single vector representation (Sub- section 5.2), and (2) sense embeddings, which map words to a set of vectors, each pertaining to a different sense of the word (Subsection 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Embedding Evaluation Framework</head><p>For the sake of comparison, we use a common framework to create odd-man-out solvers based on traditional word embeddings and sense embed- dings. In this framework, given n puzzle options, we find the subset of n − 1 words of maximal simi- larity (given some similarity score), and return the excluded word as the odd-man-out.</p><p>Specifically, define an embedding as a function that maps every word to a set (possibly empty) of real vectors. Note that this definition is gen- eral enough to allow for sense embeddings as well as traditional word embeddings, for which the re- turned sets are either singletons or the empty set (in case the word is not in the embedding's vocab- ulary).</p><p>Next, given a similarity score σ that maps any vector pair to a real number and an embedding E, define the cohesion κ σ,E of a set of words W as:</p><formula xml:id="formula_1">κ σ,E (W ) = max v 1 ∈E(w 1 ),...,vn∈E(wn) 1≤i&lt;j≤n σ(v i , v j )</formula><p>(1) Cohesion is undefined if any word w ∈ W maps to the empty set. Throughout this paper we employ the widely used cosine similarity as our similiarity score.</p><p>Finally, given an embedding E and puzzle W = w 1 , ..., w n , we create an odd-man-out solver as follows:</p><p>1. If E(w i ) is the empty set for any puzzle choice w i ∈ W , then the solver abstains from answering.</p><p>2. Otherwise, the solver returns the wordˆwwordˆ wordˆw ∈ W whose omission from the puzzle word set   <ref type="table" target="#tab_2">Table 3</ref> shows the results of embedding-based solvers on the Anomia and crowdsourced datasets, using several different pre-trained embedding maps.</p><formula xml:id="formula_2">ˆ w = argmax w i ∈W κ σ (W {w i })<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word Embeddings Solvers</head><p>We find the best performance on ANOMIACOMMON and ANOMIAPROPER using the word2vec vectors trained on 100 billion tokens from the Google News corpus. <ref type="bibr">7</ref> An analysis of the best embedding-based solver (w2v.googlenews) on ANOMIACOMMON- DEV shows that a high proportion of incorrect an- swers were polysemous (20 out of the 27 incor- rect answers). A selection of these are shown in <ref type="table" target="#tab_3">Table 4</ref>. We observe that for the "cocktails" puzzle, the word2vec solver zeroes in on "screw- driver" (whose dominant sense is the tool, not the cocktail), and for the "types of lettuce" puzzle, the solver selects "iceberg" (whose dominant sense is "large floating block of ice," not the lettuce).</p><p>To provide additional evidence of this bias, we came up with 5 cocktails whose dominant sense is the cocktail itself (mint julep, mai tai, mojito, mar- tini, and bloody mary) and 5 polysemous cocktails (old fashioned, hurricane, cosmopolitan, zombie, and Manhattan). We then replaced "screwdriver" in the "cocktails" puzzle with each of these op- tions and solved the resulting puzzle with the <ref type="bibr">w2v</ref>.googlenews solver. In all 5 monosemous in- stances, the correct answer of "chicken" was se- lected. In 4 of the 5 polysemous instances (the 7 https://code.google.com/archive/p/word2vec exception being "Manhattan"), the polysemous re- placement was selected. We repeated this exper- iment with the "groups of animals" puzzle, and again all 5 monosemous replacements yielded the correct answer, while 3 of 5 polysemous replace- ments were incorrectly chosen by the solver.</p><p>In a more rigorous experiment, we randomly generated 500 puzzles in the following way. Given a category (hyponym), we came up with 10 in- stances (hypernyms) of that category. For ex- ample, the category "type of transport" yielded: train, car, bus, airplane, helicopter, boat, ferry, taxi, tram, and monorail. We did this for 10 cate- gories, yielding a total of 100 words. From these 10 lists, we randomly generated 500 puzzles by sampling 4 words from one list and 1 word from another. We call this puzzle set HYPERNYMS500.</p><p>The <ref type="bibr">w2v</ref>.googlenews solver performs impres- sively on HYPERNYMS500, getting 90.8% cor- rect, with only 46 incorrect answers (and no ab- stentions). Tellingly however, only 3 words are responsible for 67% of the incorrect answers: saw (as a kind of tool), lead (as a kind of metal), and rose (as a kind of flower). All three of these words have at least one dominant alternate sense. Given that there are 100 possible incorrect answers (which all appear with roughly equal frequency in the puzzles), the fact that only 3 of them com- prise 67% of the incorrect answers suggests that the inability of vector directories to model multi- ple senses is an Achilles heel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ELMo Sense Vectors</head><p>The previous analysis suggests that embeddings that explicitly model multiple senses may be im- portant for the odd-man-out task. In this section, Selection of errors made by the w2v.googlenews solver on ANOMIACOMMON- DEV, indicating a tendency to choose polysemous words as the odd-man-out. The incorrect selection is in bold, while the correct answer is in italics.</p><note type="other">Category Puzzle cocktails screwdriver, chicken, margarita mimosa, daiquiri mammals bear, cobra, cat dog, leopard nocturnal bat, robin, owl animals raccoon, coyote military private, judo, general ranks corporal, colonel whales blue, grizzly, humpback orca, beluga groups of school, canoe, flock animals herd, pack shades of navy, amber, cobalt blue azure, sky aquatic seal, horse, whale animals manatee, dolphin bird tweet, snore, chirp sounds cluck, quack types of iceberg, serrano, romaine lettuce butterhead, bibb political green, garden, democratic parties republican, libertarian</note><p>we investigate this further.</p><p>Background: sense vectors There is a signif- icant literature on how to learn a one-to-many mapping from words to vector representations. An early paradigm <ref type="bibr" target="#b31">(Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b15">Huang et al., 2012;</ref><ref type="bibr">Liu et al., 2015;</ref><ref type="bibr" target="#b40">Wu and Giles, 2015</ref>) took a 2-pass approach. First, they clustered contexts of a target word like "bank." For instance, "the bank had an ATM" and "I got money from the bank" might fall into one cluster, while "I fished at the river bank" might fall into a second cluster. Second, they annotated each in- stance of the target word with its sense cluster and then learned a standard one-to-one vector map- ping from the annotated corpus. For instance, they would learn vector representations using the sen- tences "the bank-1 had an ATM," "I got money from the bank-1," and "I fished at the river bank- 2." Other researchers ( <ref type="bibr" target="#b28">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b38">Tian et al., 2014;</ref><ref type="bibr" target="#b19">Li and Jurafsky, 2015;</ref><ref type="bibr" target="#b1">Bartunov et al., 2016</ref>) focused on mod- ifying the vector learning model itself, typically the skip-gram model ( <ref type="bibr" target="#b25">Mikolov et al., 2013b</ref>), to directly learn multiple embeddings for each word. Additional work focused on using the technique of retrofitting <ref type="bibr" target="#b5">(Faruqui et al., 2014</ref>) to adapt pre- trained word vectors into sense vectors using aux- iliary resources like WordNet ( <ref type="bibr" target="#b16">Jauhar et al., 2015)</ref> or parallel corpora <ref type="bibr" target="#b4">(Ettinger et al., 2016</ref>). Other work ( <ref type="bibr" target="#b13">Guo et al., 2014;</ref><ref type="bibr" target="#b37">Suster et al., 2016;</ref><ref type="bibr" target="#b39">Upadhyay et al., 2017</ref>) used parallel corpora as the main signal for learning sense vectors.</p><p>Background: ELMo Recently, <ref type="bibr" target="#b10">Peters et al. (2018)</ref> introduced the concept of Embeddings from Language Models (ELMo). ELMo dynam- ically represents each word based on the con- text with which it appears, achieved by repre- senting a word in a sentence using its represen- tation from a pretrained bidirectional Language Model (biLM), encoded using a bi-directional RNN <ref type="bibr" target="#b33">(Schuster and Paliwal, 1997</ref>). Subsequently, the same word may get different representations in different contexts. For example, the represen- tation of "bank" may differ between "She fished by the river bank" and "She deposited her check at the nearest bank". While ELMo embeddings were recently proven extremely beneficial in vari- ous sentence-level tasks (e.g., semantic role label- ing, question answering, and textual entailment), many lexical tasks require the interpretation of words out of context, to which ELMo cannot be readily applied. We suggest to port ELMo's effec- tiveness back to the context-free setting, and pro- pose a first approach for doing so.</p><p>Unsupervised ELMo clustering We compute pre-trained ELMo embedding (using the Al- lenNLP framework ( <ref type="bibr" target="#b30">Gardner et al., 2018)</ref>) in con- text of sentences in a large corpus C, while record- ing the observed representation r w,s ∈ R d of each word w, along with the corresponding context in which it appeared, i.e., the sentence s ∈ C. The result of this process is an embedding space per word, r w = {r w,s s ∈ C, w ∈ s}. Ideally, simi- lar senses of w would appear in similar contexts, and would therefore be closer in r w , while differ- ent senses would be further apart. Following this intuition, we cluster each word to K clusters, us- ing the k-means algorithm <ref type="bibr" target="#b21">(Lloyd, 1982)</ref>, whereby every vector in a cluster is interpreted as pertain- ing to the same sense of w. We collect the centroid vector w i (where 1 ≤ i ≤ K) of each cluster as a "sense vector" of w. Using K = 1, we get a sin- gle representation per word, averaging the differ- ent senses of w, comparable to "traditional" word embeddings. Conversely, we can get senses of ar- bitrarily fine granularity by increasing the number of clusters. These sense representations can then be readily plugged in to the embedding solver, as described in Subsection 5.1. We computed the sense vectors using a 2 billion token (97 Million sentences) Wikipedia dump from January 2018, which was extracted using WikiExtractor, <ref type="bibr">8</ref> and tokenized with the SpaCy 2.0 toolkit <ref type="bibr">(Honnibal and Montani, 2017)</ref>. 9 These pre-computed vec- tors, which are readily applicable for other tasks, are made publicly available. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the resulting embedding space and clustering for the word "bat".</p><p>Evaluation We start by estimating an ideal value for K (the number of clusters). In <ref type="figure" target="#fig_2">Fig- ure 3</ref> we evaluate the performance of the ELMo clustering method on the expert development set, using different values for K, compared to the second best performing vectors on that dataset (Word2Vec). Several observations can be made based on this analysis. First, using K = 1, Word2Vec outperforms ELMo. This may be ex- plained due to Word2Vec's larger training set (100B tokens versus only 1B). However, as K in- creases, ELMo clusters outperform the baseline, <ref type="bibr">8</ref> https://github.com/attardi/ wikiextractor 9 https://spacy.io reaching its maximum performance at K = 5, in- dicating that a finer level of sense granularity is beneficial for focusing on the intended word sense. Inversely, having too many clusters hurts perfor- mance. This may be due to over-specification, harming the sense generalization obtained by a smaller number of clusters. Based on this tun- ing, we fixed the value of K to 5, and repeated the experiments from Section 5 (see the first row in <ref type="table" target="#tab_2">Table 3</ref>). ELMo sense vectors clearly outper- form all previous baselines on all Odd-Man-Out datasets. This improved performance can be at- tributed both to ELMo's better ability to capture context, as well as to the finer sense representa- tion, as opposed to the single representation per word in most of the other baselines. We also evalu- ated another publicly available collection of sense embeddings <ref type="bibr" target="#b28">(Neelakantan et al., 2014</ref>), but it did not perform on par with solvers based on conven- tional single-sense embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion: Hypernyms vs. Other Associations</head><p>The attentive reader may have wondered why the w2v.googlenews solver performed so much bet- ter on the HYPERNYMS dataset (over 90% cor- rect) than on the ANOMIACOMMON dataset (ap- proximately 62% correct). Hypothesizing that embedding-based solvers can identify hypernym- hyponym relationships more easily than other associations, we created another odd-man-out dataset using the same methodology as HYPER-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYMS.</head><p>This time, given a color, we came up with 8 to 13 objects that are typically associated with that color. For example, the category "yellow" yielded (among others): taxi, canary, corn kernel, daffodil, lemon, sun, and school bus. We did this for 10 col- ors. From these 10 lists, we again randomly gen- erated 500 puzzles by sampling 4 words from one list and 1 word from another. We call this puzzle set COLORS500.</p><p>The <ref type="bibr">w2v</ref>.googlenews solver performed consid- erably worse on COLORS500 than on HYPER- NYMS500. Out of the puzzles it attempted, it guessed only 30% correctly (compared to the ran- dom chance baseline of 20%). There are several possible reasons why this dataset may be more difficult for an embedding-based solver. Possibly colors are not easily identifiable using embeddings built from language cues (maybe people do not of- ten explicitly talk about how lemons are yellows). But perhaps it is also true that using the cosine similarity of the embeddings is not a good way to spot non-fundamental properties (like color) that link a group of words. We leave open how best to identify more oblique relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a new task for the evaluation of lexi- cal resources, the Odd-Man-Out task. We showed that the task can be annotated reliably on a large scale. Following the creation of several Odd-Man- Out datasets, we conducted analyses showing that current word representations are suboptimal, espe- cially in the presence of polysemous words. We concluded with a novel ELMo clustering sense- embedding technique which surpasses all base- lines on the Odd-Man-Out task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example taxonomy.</figDesc><graphic url="image-1.png" coords="4,307.28,298.50,218.26,105.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of a 2d projection of an ELMo embedding space for the word "bat" with 4 senses (denoted by different colors). Two example sentence excerpts appear next to their respective cluster.</figDesc><graphic url="image-2.png" coords="8,99.28,62.80,163.70,152.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of the ELMo cluster model (blue line) by the varying number of cluster (K), compared to the best performing baseline (word2vec), in the red horizontal line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Some of the WordNet solver's answers and explanations on ANOMIACOMMONDEV. The solver's answer is in bold, while the correct answer is in italics.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance (%Correct, %Wrong, %Abstained) of the different odd-man-out solvers on the 
ANOMIATEST and crowdsourcing datasets, using different vector directories.  *  We used ELMo clusters 
pretrained on 1B tokens and clustered on a 2B token Wikipedia dump. 

yields maximal cohesion: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> In this paper, we use the term &quot;word&quot; loosely to also include the multi-word expressions like &quot;fire engine&quot; and &quot;magnifying glass.&quot;</note>

			<note place="foot" n="2"> https://github.com/gabrielStanovsky/ odd-man-out</note>

			<note place="foot" n="3"> https://boardgamegeek.com/boardgame/142271/anomiaparty-edition 3.2 Crowdsourcing Following the success of the curated annotation, as described above, we devised a crowdsourcing protocol to achieve annotation at a much larger scale. In this section, we describe this protocol and show that the Odd-Man-Out task is intuitive enough to be collected and annotated with high agreement by non-trained annotators, using a small expert seed annotation. This enables us to efficiently obtain a large set of 500K hard &quot;training&quot; samples on a small budget of $400. Finally, we also validate a set of 1000 puzzles, which we use in following sections for testing purposes. Crowdsourcing protocol Our semi-automatic crowdsourcing protocol starts from a seed set of about 250 polysemous words, 4 and is composed of the following three consecutive stages: 1. Seed categorization (expert): given a polysemous word w, we ask expert annotators to come up with at least two categories c 1 , c 2 , which describe w. For example, given w = &quot;bat&quot;, the corresponding categories can be c 1 = &quot;nocturnal mammal&quot; and c 2 = &quot;baseball instrument&quot;. This categorization was performed by the authors of this paper, and was done in about 3 person hours.</note>

			<note place="foot" n="4"> https://en.wikipedia.org/wiki/List_ of_true_homonyms, following (Sun et al., 2017b). 5 https://www.mturk.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Santosh Divvala, Fereshteh Sadeghi, Marco Valenzuela, and Isaac Cowhey, who initiated this project during an AI2 hackathon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Linear algebraic structure of word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>with applications to polysemy. CoRR, abs/1601.03764</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Breaking sticks and ambiguities with adaptive skip-gram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kondrashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Creating a system for lexical substitutions from scratch using crowdsourcing. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Biemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="97" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Retrofitting sense-specific word vectors using parallel text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4166</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Problems with evaluation of word embeddings using word similarity tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In RepEval@ACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Book reviews: Wordnet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1803.07640</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Intrinsic evaluations of word embeddings: What can we do better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In RepEval@ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00726</idno>
		<title level="m">A primer on neural network models for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning sense-specific word embeddings by exploiting bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<editor>COLING</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">2017. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ontologically grounded multi-sense representation learning for semantic vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semeval-2012 task 2: Measuring degrees of relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
		<editor>SemEval@NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What substitutes tell us-analysis of an &quot;all-words&quot; lexical substitution corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Tat-Seng Chua, and Maosong Sun. 2015. Topical word embeddings. In AAAI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 14: Word sense induction and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semeval2007 task 10: English lexical substitution task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>In SemEval@ACL</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew D</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
		<title level="m">Deep contextualized word representations. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2389" to="2398" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A simple approach to learn polysemous word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicong</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
		<idno>abs/1707.01793</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A simple approach to learn polysemous word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicong</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
		<idno>abs/1707.01793</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bilingual learning of multi-sense embeddings with discrete autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond bilingual: Multi-sense word embeddings using multilingual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">Tauman</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rep4NLP@ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Lee</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
