<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translation Modeling with Bidirectional Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">Univ. Paris-Sud</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">LIMSI/CNRS</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Translation Modeling with Bidirectional Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="14" to="25"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work presents two different translation models using recurrent neural networks. The first one is a word-based approach using word alignments. Second, we present phrase-based translation models that are more consistent with phrase-based decoding. Moreover, we introduce bidirectional recurrent neural models to the problem of machine translation, allowing us to use the full source sentence in our models, which is also of theoretical interest. We demonstrate that our translation models are capable of improving strong baselines already including recurrent neu-ral language models on three tasks: IWSLT 2013 German→English, BOLT Arabic→English and Chinese→English. We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network models have recently experienced unprecedented attention in research on statistical machine translation (SMT). Several groups have reported strong improvements over state-of-the-art baselines using feedforward neural network-based language models ( <ref type="bibr" target="#b31">Schwenk et al., 2006;</ref><ref type="bibr" target="#b37">Vaswani et al., 2013)</ref>, as well as translation models ( <ref type="bibr" target="#b21">Le et al., 2012;</ref><ref type="bibr" target="#b32">Schwenk, 2012;</ref><ref type="bibr" target="#b7">Devlin et al., 2014</ref>). Different from the feedforward design, recurrent neural networks (RNNs) have the advantage of be- ing able to take into account an unbounded his- tory of previous observations. In theory, this en- ables them to model long-distance dependencies of arbitrary length. However, while previous work on translation modeling with recurrent neural net- works shows its effectiveness on standard base- lines, so far no notable gains have been presented on top of recurrent language models ( <ref type="bibr" target="#b1">Auli et al., 2013;</ref><ref type="bibr" target="#b18">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b16">Hu et al., 2014)</ref>.</p><p>In this work, we present two novel approaches to recurrent neural translation modeling: word- based and phrase-based. The word-based ap- proach assumes one-to-one aligned source and target sentences. We evaluate different ways of resolving alignment ambiguities to obtain such alignments. The phrase-based RNN approach is more closely tied to the underlying translation paradigm. It models actual phrasal translation probabilities while avoiding sparsity issues by us- ing single words as input and output units. Fur- thermore, in addition to the unidirectional formu- lation, we are the first to propose a bidirectional architecture which can take the full source sen- tence into account for all predictions. Our ex- periments show that these models can improve state-of-the-art baselines containing a recurrent language model on three tasks. For our compet- itive IWSLT 2013 German→English system, we observe gains of up to 1.6% BLEU and 1.7% TER. Improvements are also demonstrated on top of our evaluation systems for BOLT Arabic→English and Chinese→English, which also include recur- rent neural language models.</p><p>The rest of this paper is structured as follows. In Section 2 we review related work and in Section 3 an overview of long short-term memory (LSTM) neural networks, a special type of recurrent neural networks we make use of in this work, is given. Section 4 describes our novel translation models. Finally, experiments are presented in Section 5 and we conclude with Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14</head><p>In this Section we contrast previous work to ours, where we design RNNs to model bilingual depen- dencies, which are applied to rerank n-best lists after decoding.</p><p>To the best of our knowledge, the earliest at- tempts to apply neural networks in machine trans- lation (MT) are presented in ( <ref type="bibr" target="#b4">Castaño and Casacuberta, 1999</ref>), where they were used for example-based MT.</p><p>Recently, <ref type="bibr" target="#b21">Le et al. (2012)</ref> presented translation models using an output layer with classes and a shortlist for rescoring using feedforward net- works. They compare between word-factored and tuple-factored n-gram models, obtaining their best results using the word-factored approach, which is less amenable to data sparsity issues. Both of our word-based and phrase-based models eventually work on the word level. <ref type="bibr" target="#b18">Kalchbrenner and Blunsom (2013)</ref> use recurrent neural networks with full source sentence representations. The continu- ous representations are obtained by applying a se- quence of convolutions, and the result is fed into the hidden layer of a recurrent language model. Rescoring results indicate no improvements over the state of the art. <ref type="bibr" target="#b1">Auli et al. (2013)</ref> also in- clude source sentence representations built either using Latent Semantic Analysis or by concatenat- ing word embeddings. This approach produced no notable gain over systems using a recurrent language model. On the other hand, our pro- posed bidirectional models include the full source sentence relying on recurrency, yielding improve- ments over competitive baselines already includ- ing a recurrent language model. RNNs were also used with minimum translation units ( <ref type="bibr" target="#b16">Hu et al., 2014)</ref>, which are phrase pairs un- dergoing certain constraints. At the input layer, each of the source and target phrases are mod- eled as a bag of words, while the output phrase is predicted word-by-word assuming conditional independence. The approach seeks to alleviate data sparsity problems that would arise if phrases were to be uniquely distinguished. Our proposed phrase-based models maintain word order within phrases, but the phrases are processed in a word- pair manner, while the phrase boundaries remain implicitly encoded in the way the words are pre- sented to the network. <ref type="bibr" target="#b32">Schwenk (2012)</ref> proposed a feedforward network that predicts phrases of a fixed maximum length, such that all phrase words are predicted at once. The prediction is condi- tioned on the source phrase. Since our phrase- based model predicts one word at a time, it does not assume any phrase length. Moreover, our model's predictions go beyond phrase boundaries and cover unbounded history and future contexts.</p><p>Using neural networks during decoding re- quires tackling the costly output normalization step. <ref type="bibr" target="#b37">Vaswani et al. (2013)</ref> avoid this step by training feedforward neural language models us- ing noise contrastive estimation, while <ref type="bibr" target="#b7">Devlin et al. (2014)</ref> augment the training objective function to produce approximately normalized scores di- rectly. The latter work makes use of translation and joint models, and pre-computes the first hid- den layer beforehand, resulting in large speedups. They report major improvements over strong base- lines. The speedups achieved by both works al- lowed to integrate feedforward neural networks into the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LSTM Recurrent Neural Networks</head><p>Our work is based on recurrent neural networks. In related fields like e. g. language modeling, this type of neural network has been shown to perform considerably better than standard feedforward ar- chitectures <ref type="bibr" target="#b24">(Mikolov et al., 2011;</ref><ref type="bibr" target="#b0">Arisoy et al., 2012;</ref><ref type="bibr" target="#b36">Sundermeyer et al., 2013;</ref><ref type="bibr" target="#b22">Liu et al., 2014</ref>).</p><p>Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is com- puted with the backpropagation through time al- gorithm <ref type="bibr" target="#b29">(Rumelhart et al., 1986;</ref><ref type="bibr" target="#b39">Werbos, 1990;</ref><ref type="bibr" target="#b40">Williams and Zipser, 1995)</ref>. However, the combi- nation of RNN networks with conventional back- propagation training leads to conceptual difficul- ties which are known as the vanishing (or explod- ing) gradient problem, described e. g. in <ref type="bibr" target="#b2">(Bengio et al., 1994)</ref>. To remedy this problem, in (Hochre- iter and Schmidhuber, 1997) it was suggested to modify the architecture of a standard RNN in such a way that vanishing and exploding gradients are avoided during backpropagation. In particular, no modification of the training algorithm is necessary. The resulting architecture is referred to as long short-term memory (LSTM) neural network.</p><p>Bidirectional recurrent neural networks (BRNNs) were first proposed in <ref type="bibr" target="#b30">(Schuster and Paliwal, 1997</ref>) and applied to speech recognition tasks. They have been since applied to different  tasks like parsing <ref type="bibr" target="#b14">(Henderson, 2004</ref>) and spoken language understanding <ref type="bibr" target="#b23">(Mesnil et al., 2013</ref>).</p><p>Bidirectional long short-term memory (BLSTM) networks are BRNNs using LSTM hidden layers (Graves and <ref type="bibr" target="#b13">Schmidhuber, 2005</ref>). This work introduces BLSTMs to the problem of machine translation, allowing powerful models that employ unlimited history and future information to make predictions.</p><p>While the proposed models do not make any as- sumptions about the type of RNN used, all of our experiments make use of recurrent LSTM neural networks, where we include later LSTM exten- sions proposed in ( <ref type="bibr" target="#b9">Gers et al., 2000;</ref><ref type="bibr" target="#b10">Gers et al., 2003)</ref>. The cross-entropy error criterion is used for training. Further details on LSTM neural net- works can be found in ( <ref type="bibr" target="#b13">Graves and Schmidhuber, 2005;</ref><ref type="bibr" target="#b35">Sundermeyer et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Translation Modeling with RNNs</head><p>In the following we describe our word-and phrase-based translation models in detail. We also show how bidirectional RNNs can enable such models to include full source information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Resolving Alignment Ambiguities</head><p>Our word-based recurrent models are only de- fined for one-to-one-aligned source-target sen- tence pairs. In this work, we always evaluate the model in the order of the target sentence. How- ever, we experiment with several different ways to resolve ambiguities due to unaligned or mul- tiply aligned words. To that end, we introduce two additional tokens, aligned and unaligned . Un- If an unaligned is introduced on the target side, its posi- tion is determined by the aligned source word that is closest to the unaligned source word in question, preferring left to right. To resolve one-to-many alignments, we use an IBM-1 translation table to decide for one of the alignment connections to be kept. The remaining words are also either deleted or aligned to additionally introduced aligned to- kens on the opposite side. <ref type="figure" target="#fig_1">Fig. 1</ref> shows an ex- ample sentence from the IWSLT data, where all tokens are introduced.</p><p>In a short experiment, we evaluated 5 differ- ent setups with our unidirectional RNN translation model (cf. next Section): without any tokens, without unaligned , source identity, target identity and using all tokens. Source identity means we introduce no tokens on source side, but all on target side. Target identity is defined analogously. The results can be found in Tab. 1. We use the setup with all tokens in all following experi- ments, which showed the best BLEU performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word-based RNN Models</head><p>Given a pair of source sequence f I 1 = f 1 . . . f I and target sequence e I 1 = e 1 . . . e I , where we as- sume a direct correspondence between f i and e i , we define the posterior translation probability by factorizing on the target words:</p><formula xml:id="formula_0">p(e I 1 |f I 1 ) = I i=1 p(e i |e i−1 1 , f I 1 ) (1) ≈ I i=1 p(e i |e i−1 1 , f i+d 1 ) (2) ≈ I i=1 p(e i |f i+d 1 ).<label>(3)</label></formula><p>We denote the formulation <ref type="formula">(1)</ref> as the bidirectional joint model (BJM). This model can be simplified by several independence assumptions. First, we drop the dependency on the future source infor- mation, receiving what we denote as the unidirec- tional joint model (JM) in (2). Here, d ∈ N 0 is a delay parameter, which is set to d = 0 for all experiments, except for the comparative results re- ported in <ref type="figure" target="#fig_10">Fig. 7</ref>. Finally, assuming conditional in- dependence from the previous target sequence, we receive the unidirectional translation model (TM) in (3). Analogously, we can define a bidirectional translation model (BTM) by keeping the depen- dency on the full source sentence f I 1 , but dropping the previous target sequence e i−1 1 :  is illustrated in <ref type="figure" target="#fig_4">Fig. 3</ref>, which corresponds to the following set of equations:</p><formula xml:id="formula_1">p(e I 1 |f I 1 ) ≈ I i=1 p(e i |f I 1 ).<label>(4)</label></formula><formula xml:id="formula_2">y i = A 1 ˆ f i + A 2 ˆ e i−1 z i = ξ(y i ; A 3 , y i−1 1 ) p c(e i )|e i−1 1 , f i 1 = ϕ c(e i ) (A 4 z i ) p e i |c(e i ), e i−1 1 , f i 1 = ϕ e i (A c(e i ) z i ) p(e i |e i−1 1 , f i 1 ) = p e i |c(e i ), e i−1 1 , f i 1 · p c(e i )|e i−1 1 , f i 1</formula><p>Here, byˆfbyˆ byˆf i andêandˆandê i−1 we denote the one-hot en- coded vector representations of the source and target words f i and e i−1 . The outgoing activa- tion values of the projection layer and the LSTM layer are y i and z i , respectively. The matrices A j contain the weights of the neural network layers. By ξ(· ; A 3 , y i−1 1 ) we denote the LSTM formalism that we plug in at the third layer. As the LSTM layer is recurrent, we explicitly include the de- pendence on the previous layer activations y i−1</p><p>1 . Finally, ϕ is the widely-used softmax function to obtain normalized probabilities, and c denotes a word class mapping from any target word to its unique word class. For the bidirectional model, the equations can be defined analogously.</p><p>Due to the use of word classes, the output layer consists of two parts. The class probabil- ity p c(e i )|e i−1 1 , f i 1 is computed first, and then is ob- tained given the word class. This trick helps avoid- ing the otherwise computationally expensive nor- malization sum, which would be carried out over all words in the target vocabulary. In a class- factorized output layer where each word belongs to a single class, the normalization is carried out over all classes, whose number is typically much less than the vocabulary size. The other normal- ization sum needed to produce the word probabil- ity is limited to the words belonging to the same class <ref type="bibr" target="#b11">(Goodman, 2001;</ref><ref type="bibr" target="#b26">Morin and Bengio, 2005</ref>).</p><formula xml:id="formula_3">p c(ei)|e i−1 1 , f i+d 1 p ei|c(ei), e i−1 1 , f i+d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Phrase-based RNN Models</head><p>One of the conceptual disadvantages of word- based modeling as introduced in the previous sec- tion is that there is a mismatch between train- ing and testing conditions: During neural network training, the vocabulary has to be extended by ad- ditional tokens, and a one-to-one alignment is used which does not reflect the situation in decod- ing. In phrase-based machine translation, more complex alignments in terms of multiple words on both the source and the target sides are used, which allow the decoder to make use of richer short-distance dependencies and are crucial for the performance of the resulting system. From this perspective, it seems interesting to standardize the alignments used in decoding, and in training the neural network. However, it is dif- ficult to use the phrases themselves as the vocab- ulary of the RNN. Usually, the huge number of potential phrases in comparison to the relatively small amount of training data makes the learn- ing of continuous phrase representations difficult  due to data sparsity. This is confirmed by results presented in ( <ref type="bibr" target="#b21">Le et al., 2012)</ref>, which show that a word-factored translation model outperforms the phrase-factored version. Therefore, in this work we continue relying on source and target word vo- cabularies for building our phrase representations. However, we no longer use a direct correspon- dence between a source and a target word, as en- forced in our word-based models. <ref type="figure" target="#fig_6">Fig. 4</ref> shows an example phrase alignment, where a sequence of source words˜fwords˜ words˜f i is directly mapped to a sequence of target words˜ewords˜ words˜e i for 1 ≤ i ≤ ˜ I. By˜IBy˜ By˜I, we denote the number of phrases in the alignment. We decompose the target sentence posterior probability in the following way:</p><formula xml:id="formula_4">p(e I 1 |f J 1 ) = ˜ I i=1 p(˜ e i |˜e|˜e i−1 1 , ˜ f ˜ I 1 )<label>(5)</label></formula><formula xml:id="formula_5">≈ ˜ I i=1 p(˜ e i |˜e|˜e i−1 1 , ˜ f i 1 )<label>(6)</label></formula><p>where the joint model in Eq. 5 would correspond to a bidirectional RNN, and Eq. 6 only requires a unidirectional RNN. By leaving out the condition- ing on the target side, we obtain a phrase-based translation model. As there is no one-to-one correspondence be- tween the words within a phrase, the basic idea of our phrase-based approach is to let the neural net- work learn the dependencies itself, and present the full source side of the phrase to the network be- fore letting it predict target side words. Then the probability for the target side of a phrase can be computed, in case of Eq. 6, by:</p><formula xml:id="formula_6">p(˜ e i |˜e|˜e i−1 1 , ˜ f ˜ I 1 ) = |˜e|˜e i | j=1 p (˜ e i ) j |(˜ e i ) j−1 1 , ˜ e i−1 1 , ˜ f i 1 ,</formula><p>and analogously for the case of Eq. 5. Here, (˜ e i ) j denotes the j-th word of the i-th aligned target phrase.</p><p>We feed the source side of a phrase into the neu- ral network one word at a time. Only when the presentation of the source side is finished we start estimating probabilities for the target side. There- fore, we do not let the neural network learn a target distribution until the very last source word is con- sidered. In this way, we break up the conventional RNN training scheme where an input sample is di- rectly followed by its corresponding teacher sig- nal. Similarly, the presentation of the source side of the next phrase only starts after the prediction of the current target side is completed.</p><p>To this end, we introduce a no-operation token, denoted by ε, which is not part of the vocabulary (which means it cannot be input to or predicted by the RNN). When the ε token occurs as input, it in- dicates that no input needs to be processed by the RNN. When the ε token occurs as a teacher signal for the RNN, the output layer distribution is ig- nored, and does not even have to be computed. In both cases, all the other layers are still processed during forward and backward passes such that the RNN state can be advanced even without addi- tional input or output.  <ref type="figure" target="#fig_6">Fig. 4</ref>. For a source phrase˜fphrase˜ phrase˜f i , we include (|˜e|˜e i |−1) many ε symbols at the end of the phrase. Conversely, for a target phrase˜ephrase˜ phrase˜e i , we include (| ˜ f i | − 1) many ε symbols at the beginning of the phrase. E. g., in the figure, the second dashed rectan- gle from the left depicts the training of the English phrase ", for example ," and its German transla- tion "zum Beispiel". At the input layer, we feed in the source words one at a time, while we present ε tokens at the target side input layer and the out- put layer (with the exception of the very first time step, where we still have the last target word from the previous phrase as input instead of ε). With the last word of the source phrase "Beispiel" being presented to the network, the full source phrase is stored in the hidden layer, and the neural network is then trained to predict the target phrase words at the output layer. Subsequently, the source input is ε, and the target input is the most recent target side history word.</p><p>To obtain a phrase-aligned training sequence for the phrase-based RNN models, we force-align the training data with the application of leave-one-out as described in <ref type="bibr" target="#b41">(Wuebker et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Bidirectional RNN Architecture</head><p>While the unidirectional RNNs include an un- bounded sentence history, they are still limited in the number of future source words they include. Bidirectional models provide a flexible means to also include an unbounded future context, which, unlike the delayed unidirectional models, require no tuning to determine the amount of delay. <ref type="figure" target="#fig_9">Fig. 6</ref> illustrates the bidirectional model archi- tecture, which is an extension of the unidirectional model of <ref type="figure" target="#fig_4">Fig. 3</ref>. First, an additional recurrent hidden layer is added in parallel to the existing one. This layer will be referred to as the back- ward layer, since it processes information in back- ward time direction. This hidden layer receives source word input only, while target words in the case of a joint model are fed to the forward layer as in the unidirectional case. Due to the backward recurrency, the backward layer will make the in- formation f I i available when predicting the target word e i , while the forward layer takes care of the source history f i 1 . Jointly, the forward and back- ward branches include the full source sentence f I 1 , as indicated in <ref type="figure" target="#fig_2">Fig. 2. Fig. 6</ref> shows the "deep" variant of the bidirectional model, where the for- ward and backward layers converge into a hidden layer. A shallow variant can be obtained if the parallel layers converge into the output layer di- rectly <ref type="bibr">1</ref> .</p><formula xml:id="formula_7">p c(ei)|e i−1 1 , f I 1 p ei|c(ei), e i−1 1 , f I</formula><p>Due to the full dependence on the source se- quence, evaluating bidirectional networks requires computing the forward pass of the forward and backward layers for the full sequence, before be- ing able to evaluate the next layers. In the back- ward pass of backpropagation, the forward and backward recurrent layers are processed in de- creasing and increasing time order, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>All translation experiments are performed with the Jane toolkit ( <ref type="bibr" target="#b38">Vilar et al., 2010;</ref><ref type="bibr" target="#b42">Wuebker et al., 2012</ref>). The largest part of our experiments is car- ried out on the IWSLT 2013 German→English shared translation task. <ref type="bibr">2</ref> The baseline system is trained on all available bilingual data, 4.3M sen- tence pairs in total, and uses a 4-gram LM with modified Kneser-Ney smoothing <ref type="bibr" target="#b19">(Kneser and Ney, 1995;</ref><ref type="bibr" target="#b6">Chen and Goodman, 1998)</ref>, trained with the SRILM toolkit <ref type="bibr" target="#b34">(Stolcke, 2002</ref>). As additional data sources for the LM we selected parts of the Shuffled News and LDC English Gigaword cor- pora based on cross-entropy difference <ref type="bibr" target="#b25">(Moore and Lewis, 2010)</ref>, resulting in a total of 1.7 bil- lion running words for LM training. The state-of- the-art baseline is a standard phrase-based SMT system ( <ref type="bibr" target="#b20">Koehn et al., 2003</ref>) tuned with MERT <ref type="bibr" target="#b27">(Och, 2003)</ref>. It contains a hierarchical reorder- ing model ( <ref type="bibr" target="#b8">Galley and Manning, 2008</ref>) and a 7- gram word cluster language model ). Here, we also compare against a feed- forward joint model as described by <ref type="bibr" target="#b7">Devlin et al. (2014)</ref>, with a source window of 11 words and a target history of three words, which we denote as BBN-JM. Instead of POS tags, we predict word classes trained with mkcls. We use a shortlist of size 16K and 1000 classes for the remaining words. All neural networks are trained on the TED portion of the data (138K segments) and are ap- plied in a rescoring step on 1000-best lists.</p><p>To confirm our results, we run additional experiments on the Arabic→English and Chinese→English tasks of the DARPA BOLT project. In both cases, the neural network models are added on top of our most competitive eval- uation system. On Chinese→English, we use a hierarchical phrase-based system trained on 3.7M segments with 22 dense features, including an ad- vanced orientation model <ref type="bibr" target="#b17">(Huck et al., 2013</ref>). For the neural network training, we selected a subset of 9M running words. The Arabic→English system is a standard phrase-based decoder trained on 6.6M segments, using 17 dense features. The neural network training was performed using a selection amounting to 15.5M running words. For both tasks we apply the neural networks by rescoring 1000-best lists and evaluate results on two data sets from the 'discussion forum' domain, test1 and test2. The sizes of the data sets for the Arabic→English system are: 1219 (dev), 1510 (test1), and 1137 (test2) segments, and for the Chinese→English system are: 5074 (dev), 1844 (test1), and 1124 (test2) segments. All results are measured in case-insensitive BLEU <ref type="bibr">[%]</ref> ( <ref type="bibr" target="#b28">Papineni et al., 2002</ref>) and TER [%] ( <ref type="bibr" target="#b33">Snover et al., 2006</ref>) on a single reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Our results on the IWSLT German→English task are summarized in <ref type="table">Tab</ref>  guage model yet. Here, the delay parameter d from Equations 2 and 3 is set to zero. We ob- serve that for all recurrent translation models, we achieve substantial improvements over the base- line on the test data, ranging from 0.9 BLEU up to 1.6 BLEU. These results are also consistent with the improvements in terms of TER, where we achieve reductions by 0.8 TER up to 1.8 TER.</p><p>These numbers can be directly compared to the case of feedforward neural network-based transla- tion modeling as proposed in <ref type="bibr" target="#b7">(Devlin et al., 2014</ref>) which we include in the very last row of the table. Nearly all of our recurrent models outperform the feedforward approach, where the RNN model per- forming best on the dev data is better on test by 0.3 BLEU and 1.0 TER.</p><p>Interestingly, for the recurrent word-based mod- els, on the test data it can be seen that TMs per- form better than JMs, even though TMs do not take advantage of the target side history words. However, exploiting this extra information does not always need to result in a better model, as the target side words are only derived from the given source side, which is available to both TMs and JMs. On the other hand, including future source words in a bidirectional model clearly improves the performance further. By adding another LSTM  layer that combines forward and backward time directions (indicated as 'deep' in the table), we ob- tain our overall best model.</p><p>In <ref type="figure" target="#fig_10">Fig. 7</ref> we compare the word-based bidirec- tional TM with a unidirectional TM that uses dif- ferent time delays d = 0, . . . , 4. For a delay d = 2, the same performance is obtained as with the bidirectional model, but this comes at the price of tuning the delay parameter.</p><p>In comparison to the unidirectional word-based models, phrase-based models perform similarly. In the tables, we include those phrase-based vari- ants which perform best on the dev data, where phrase-based JMs always are at least as good or better than the corresponding TMs in terms of BLEU. Therefore, we mainly report JM results for the phrase-based networks. A phrase-based model can also be trained on multiple variants for the phrase alignment. For our experiments, we tested 10-best alignments against the single best alignment, which resulted in a small improvement of 0.2 TER on both dev and test. We did not ob- serve consistent gains by using an additional hid- den layer or bidirectional models. To some ex- tent, future information is already considered in unidirectional phrase-based models by feeding the complete source side before predicting the target side.</p><p>Tab. 3 shows different model combination re- sults for the IWSLT task, where a recurrent lan- guage model is included in the baseline. Adding a deep bidirectional TM or JM to the recur- rent language model improves the RNN-LM base- line by 1.2 BLEU or 1.1 BLEU, respectively. A phrase-based model substantially improves over dev eval11</p><p>test BLEU <ref type="bibr">[%]</ref> TER <ref type="bibr">[%]</ref> BLEU <ref type="bibr">[%]</ref> TER <ref type="bibr">[%]</ref> BLEU <ref type="bibr">[%]</ref> TER <ref type="bibr">[%]</ref> baseline <ref type="formula">(</ref>  the RNN-LM baseline, but performs not as good as its word-based counterparts. By adding four different translation models, including models in reverse word order and reverse translation direc- tion, we are able to improve these numbers even further. However, especially on the test data, the gains from model combination saturate quickly. Apart from the IWSLT track, we also ana- lyze the performance of our translation models on the BOLT Chinese→English and Arabic→English translation tasks. Due to the large amount of train- ing data, we concentrate on models of high perfor- mance in the IWSLT experiments. The results can be found in Tab. 4 and 5. In both cases, we see consistent improvements over the recurrent neural network language model baseline, improving the Arabic→English system by 0.6 BLEU and 0.5 TER on test1. This can be compared to the rescoring results for the same task reported by <ref type="bibr" target="#b7">(Devlin et al., 2014</ref>), where they achieved 0.3 BLEU, despite the fact that they used multiple references for scoring, whereas in our experiments we rely on a single reference only. The models are also able to im- prove the Chinese→English system by 0.5 BLEU and 0.5 TER on test2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>To investigate whether bidirectional models ben- efit from future source information, we compare the single-best output of a system reranked with a unidirectional model to the output reranked with a bidirectional model. We choose the models to be translation models in both cases, as they predict target words independent of previous predictions, given the source information (cf. Eqs. <ref type="figure" target="#fig_4">(3, 4)</ref>). This makes it easier to detect the effect of including future source information or the lack thereof. The examples are taken from the IWSLT  <ref type="table">Table 4</ref>: Results for the BOLT Arabic→English task with different RNN models. The "+" sign in the last two rows indicates that either of the corre- sponding deep models (BTM and BJM) are added to the baseline including the recurrent language model (i.e. they are not applied at the same time). T: translation, J: joint, B: bidirectional.</p><p>task, where we include the one-to-one source information, reordered according to the target side.</p><p>source: nicht so wie ich reference: not like me</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis 1:</head><p>1-to-1 source: so ich nicht wie 1-to-1 target: so I do n't like Hypothesis 2:</p><p>1-to-1 source: nicht so wie ich 1-to-1 target: not like me</p><p>In this example, the German phrase "so wie" translates to "like" in English. The bidirectional model prefers hypothesis 2, making use of the future word "wie" when translating the German word "so" to , because it has future insight that this move will pay off later when translating 18.8 63.3 17.5 62.5 <ref type="table">Table 5</ref>: Results for the BOLT Chinese→English task with different RNN models. The "+" sign in the last two rows indicates that either of the corre- sponding deep models (BTM and BJM) are added to the baseline including the recurrent language model (i.e. they are not applied at the same time). T: translation, B: bidirectional.</p><p>the rest of the sentence. This information is not available to the unidirectional model, which prefers hypothesis 1 instead.</p><p>source: das taten wir dann auch und verschafften uns so eine Zeit lang einen Wettbewerbs Vorteil .</p><p>reference: and we actually did that and it gave us a competitive advantage for a while .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis 1:</head><p>1-to-1 source: das wir dann auch taten und verschafften uns so eine Zeit lang einen Wettbewerbs</p><p>Vorteil .</p><p>1-to-1 target: that 's just what we did and gave us a time , a competitive advantage .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis 2:</head><p>1-to-1 source: das wir dann auch taten und verschafften uns so einen Wettbewerbs Vorteil eine Zeit lang .</p><p>1-to-1 target: that 's just what we did and gave us a competitive advantage for a while .</p><p>Here, the German phrase "eine Zeit lang" trans- lates to "for a while" in English. Bidirectional scoring favors hypothesis 2, while unidirectional scoring favors hypothesis 1. It seems that the uni- directional model translates "Zeit" to "time" as the object of the verb "give" in hypothesis 1, being blind to the remaining part "lang" of the phrase which changes the meaning. The bidirectional model, to its advantage, has the full source infor- mation, allowing it to make the correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We developed word-and phrase-based RNN trans- lation models. The former is simple and performs well in practice, while the latter is more consistent with the phrase-based paradigm. The approach in- herently evades data sparsity problems as it works on words in its lowest level of processing. Our experiments show the models are able to achieve notable improvements over baselines containing a recurrent LM. In addition, and for the first time in statistical machine translation, we proposed a bidirectional neural architecture that allows modeling past and future dependencies of any length. Besides its good performance in practice, the bidirectional ar- chitecture is of theoretical interest as it allows the exact modeling of posterior probabilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example sentence from the German→English IWSLT data. The one-to-one alignment is created by introducing aligned and unaligned tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 shows the dependencies of the wordbased neural translation and joint models. The alignment points are traversed in target order and at each time step one target word is predicted. The pure translation model (TM) takes only source words as input, while the joint model (JM) takes the preceding target words as an additional input. A delay of d &gt; 0 is implemented by shifting the target sequence by d time steps and filling the first d target positions and the last d source positions with a dedicated padding symbol. The RNN architecture for the unidirectional word-based models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dependencies modeled within the wordbased RNN models when predicting the target word 'know'. Directly processed information is depicted with solid rectangles, and information available through recurrent connections is marked with dashed rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of a recurrent unidirectional translation model. By including the dashed parts, a joint model is obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example phrase alignment for a sentence from the IWSLT training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A recurrent phrase-based joint translation model, unfolded over time. Source words are printed in normal face, while target words are printed in bold face. Dashed lines indicate phrases from the example sentence. For brevity, we omit the precise handling of sentence begin and end tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 depicts the evaluation of a phrase-based joint model for the example alignment from Fig. 4. For a source phrase˜fphrase˜ phrase˜f i , we include (|˜e|˜e i |−1) many ε symbols at the end of the phrase. Conversely, for a target phrase˜ephrase˜ phrase˜e i , we include (| ˜ f i | − 1) many ε symbols at the beginning of the phrase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architecture of a recurrent bidirectional translation model. By (+) and (−), we indicate a processing in forward and backward time directions, respectively. The inclusion of the dashed parts leads to a bidirectional joint model. One source projection matrix is used for the forward and backward branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: BLEU scores on the IWSLT test set with different delays for the unidirectional RNNTM and the bidirectional RNN-BTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for the IWSLT 2013 
German→English task with different RNN 
models. T: translation, J: joint, B: bidirectional, 
P: phrase-based. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for the IWSLT 2013 German→English task with different RNN models. All results 
include a recurrent language model. T: translation, J: joint, B: bidirectional, P: phrase-based. 

</table></figure>

			<note place="foot" n="1"> In our implementation, the forward and backward layers converge into an intermediate identity layer, and the aggregate is weighted and fed to the next layer. 2 http://www.iwslt2013.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is partially based upon work sup-ported by the DARPA BOLT project under Con-tract No. HR0011-12-C-0015. Any opinions, findings and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of DARPA. The research leading to these results has also re-ceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreements n o 287658 and n o 287755. Experiments were performed with computing re-sources granted by JARA-HPC from RWTH Aachen University under project 'jara0085'. We would like to thank Jan-Thorsten Peter for provid-ing the BBN-JM system.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint Language and Translation Modeling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Seattle, USA, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A connectionist approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Asunción</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Speech Communication and Technology (EUROSPEECH97)</title>
		<meeting><address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text-to-text machine translation using the RECONTRA connectionist model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Asunción</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (IWANN 99)</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1607</biblScope>
			<biblScope unit="page" from="683" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine translation using neural networks and finite-state models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Maria Asunción Castaño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Theoretical and Methodological Issues in Machine Translation. TMI&apos;97</title>
		<meeting><address><addrLine>Santa Fe, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Empirical Study of Smoothing Techniques for Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998-08" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Group, Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
	<note>page to appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple and effective hierarchical phrase reordering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Jürgen Schmidhuber, and Fred Cummins</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;01)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A phrase orientation model for hierarchical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Rietig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2013 Eighth Workshop on Statistical Machine Translation</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="452" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved backing-off for M-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech, and Signal Processingw</title>
		<meeting>the International Conference on Acoustics, Speech, and Signal Processingw</meeting>
		<imprint>
			<date type="published" when="1995-05" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03)</title>
		<meeting>the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03)<address><addrLine>Alberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
	<note>Edmonton</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continuous Space Translation Models with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Hai Son Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient lattice rescoring using recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><forename type="middle">C</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4941" to="4945" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3771" to="3775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Intelligent Selection of Language Model Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Short Papers)</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on artificial intelligence and statistics</title>
		<meeting>the international workshop on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 41th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The Pdp Research</forename><surname>Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Distributed Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1986" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Continuous Space Language Models for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Déchelotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous Space Translation Models for Phrase-Based Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Computational Linguistics (COLING)</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Study of Translation Edit Rate with Targeted Human Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the 7th Conference of the Association for Machine Translation in the Americas<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SRILM-An Extensible Language Modeling Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Speech and Language Processing (ICSLP)</title>
		<meeting>of the Int. Conf. on Speech and Language essing (ICSLP)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparison of feedforward and recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Freiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="8430" to="8434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jane: Open source hierarchical translation, extended with reordering and lexicon models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">GradientBased Learning Algorithms for Recurrent Networks and Their Computational Complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
		<editor>Yves Chauvain and David E. Rumelhart</editor>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Lawrence Erlbaum Publishers</publisher>
		</imprint>
	</monogr>
	<note>BackPropagation: Theory, Architectures and Applications</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training phrase translation models with leaving-one-out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Assoc. for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Jane 2: Open source phrase-based and hierarchical statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation with word class models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Rietig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1377" to="1381" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
