<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Skip-Grams for Event Detection with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><forename type="middle">Huu</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">New York University New York</orgName>
								<address>
									<postCode>10003</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
							<email>grishman@cs.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">New York University New York</orgName>
								<address>
									<postCode>10003</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Skip-Grams for Event Detection with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="886" to="891"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Convolutional neural networks (CNN) have achieved the top performance for event detection due to their capacity to induce the underlying structures of the k-grams in the sentences. However, the current CNN-based event detectors only model the consecutive k-grams and ignore the non-consecutive k-grams that might involve important structures for event detection. In this work, we propose to improve the current CNN models for ED by introducing the non-consecutive convolu-tion. Our systematic evaluation on both the general setting and the domain adaptation setting demonstrates the effectiveness of the non-consecutive CNN model, leading to the significant performance improvement over the current state-of-the-art systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of event detection (ED) is to locate event triggers of some specified types in text. Triggers are generally single verbs or nominalizations that evoke the events of interest. This is an important and challenging task of information extraction in natu- ral language processing (NLP), as the same event might appear in various expressions, and an expres- sion might express different events depending on contexts.</p><p>The current state-of-the-art systems for ED have involved the application of convolutional neural net- works (CNN) <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b;</ref><ref type="bibr" target="#b4">Chen et al., 2015</ref>) that automatically learn effective fea- ture representations for ED from sentences. This has overcome the two fundamental limitations of the tra- ditional feature-based methods for ED: (i) the com- plicated feature engineering for rich feature sets and (ii) the error propagation from the NLP toolkits and resources (i.e, parsers, part of speech taggers etc) that generate such features.</p><p>The prior CNN models for ED are characterized by the temporal convolution operators that linearly map the vectors for the k-grams in the sentences into the feature space. Such k-gram vectors are ob- tained by concatenating the vectors of the k con- secutive words in the sentences <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b;</ref><ref type="bibr" target="#b4">Chen et al., 2015)</ref>. In other words, the previous CNN models for ED only focus on modeling the consecutive k-grams. Unfortunately, such consecutive mechanism is unable to capture the long-range and non-consecutive dependencies that are necessary to the prediction of trigger words. For instance, consider the following sentence with the trigger word "leave" from the ACE 2005 corpus:</p><p>The mystery is that she took the job in the first place or didn't leave earlier.</p><p>The correct event type for the trigger word "leave" in this case is "End-Org". However, the previous CNN models might not be able to detect "leave" as an event trigger or incorrectly predict its type as "Movement". This is caused by their reliance on the consecutive local k-grams such as "leave ear- lier". Consequently, we need to resort to the non- consecutive pattern "job leave" to correctly deter- mine the event type of "leave" in this case.</p><p>Guided by this intuition, we propose to improve the previous CNN models for ED by operating the convolution on all possible non-consecutive k-grams in the sentences. We aggregate the resulting con- volution scores via the max-pooling function to un- veil the most important non-consecutive k-grams for ED. The aggregation over all the possible non- consecutive k-grams is made efficient with dynamic programming.</p><p>Note that our work is related to ( <ref type="bibr" target="#b11">Lei et al., 2015</ref>) who employ the non-consecutive convolution for the sentence and news classification problems. Our work is different from ( <ref type="bibr" target="#b11">Lei et al., 2015</ref>) in that we model the relative distances of words to the trigger candidates in the sentences via position embeddings, while ( <ref type="bibr" target="#b11">Lei et al., 2015)</ref> use the absolute distances between words in the k-grams to compute the decay weights for aggregation. To the best of our knowl- edge, this is the first work on non-consecutive CNN for ED.</p><p>We systematically evaluate the proposed model in the general setting as well as the domain adaptation setting. The experiment results demonstrate that our model significantly outperforms the current state-of- the-art models in such settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We formalize ED as a multi-class classification problem. Given a sentence, for every token in that sentence, we want to predict if the current token is an event trigger of some event in the pre-defined event set or not? The current token along with its context in the sentence constitute an event trigger candidate.</p><p>In order to make it compatible with the pre- vious work, we follow the procedure in <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b</ref>) to process the trigger candi- dates for CNN. In particular, we limit the context of the trigger candidates to a fixed window size by trimming longer sentences and padding shorter sen- tences with a special token when necessary. Let 2n + 1 be the fixed window size, and W = [w 0 , w 1 , . . . , w n , . . . , w 2n1 , w 2n ] be some trigger candidate where the current token is positioned in the middle of the window (token w n ). Before enter- ing CNN, each token w i is first transformed into a real-valued vector x i using the concatenation of the following vectors:</p><p>1. The word embedding vector of w i : This is ob- tained by looking up a pre-trained word embedding table D ( <ref type="bibr" target="#b28">Turian et al., 2010;</ref><ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>).</p><p>2. The position embedding vector of w i : We ob- tain this vector by looking up the position embed- ding table for the relative distance i n from the token w i to the current token w n . The position em- bedding table is initialized randomly.</p><p>3. The real-valued embedding vector for the en- tity type of w i : This vector is generated by look- ing up the entity type embedding table (initialized randomly) for the entity type of w i . Note that we employ the BIO annotation schema to assign entity type labels to each token in the sentences using the entity mention heads as in <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b</ref>).</p><p>The transformation from the token w i to the vec- tor x i (x i 2 R d ) essentially converts the input can- didate W into a sequence of real-valued vectors X = (x 0 , x 1 , . . . , x 2n ). This sequence is used as input in the following CNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Traditional CNN</head><p>Given the window size k, the traditional CNN mod- els for ED consider the following set of 2n + 1 con- secutive k-gram vectors:</p><formula xml:id="formula_0">C = {u i : 0  i  2n} (1)</formula><p>Vector u i is the concatenation of the k consecutive vectors preceding position i in the sequence X:</p><formula xml:id="formula_1">u i = [x ik+1 , x ik+2 , . . . , x i ] 2 R dk</formula><p>where the out-of- index vectors are simply set to all zeros. The core of the CNN models is the convolution operation, specified by the filter vector f 2 R dk . In CNN, f can be seen as a feature extractor for the k-grams that operates via the dot product with each element in C. This produces the following convolu- tion score set:</p><formula xml:id="formula_2">S(C) = {f T u i : 0  i  2n}.</formula><p>In the next step, we aggregate the features in S with the max function, resulting in the aggregation score:</p><formula xml:id="formula_3">p f k = max S(C) = max{s i : 0  i  2n} (2)</formula><p>Afterward, p f k is often transformed by a non- linear function G 1 to generate the transformed score G(p f k ), functioning as the extracted feature for the initial trigger candidate W .</p><p>We can then repeat this process for different win- dow sizes k and filters f , generating multiple fea- tures G(p f k ) to capture various aspects of the trig- ger candidate W . Finally, such features are concate- nated into a single representation vector for W , to be fed into a feed-forward neural network with a soft- max layer in the end to perform classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Non-consecutive CNN</head><p>As mentioned in the introduction, the limitation of the previous CNN models for ED is the inability to encode the non-consecutive k-grams that might be crucial to the trigger prediction. This limitation orig- inates from Equation 1 in which only the consecu- tive k-gram vectors are considered. In order to over- come such limitation, we propose to model all pos- sible non-consecutive k-grams in the trigger candi- date, leading to the following set of non-consecutive k-gram vectors:</p><formula xml:id="formula_4">N = {v i 1 i 2 ...i k : 0  i 1 &lt; i 2 &lt; . . . &lt; i k  2n}</formula><p>where:</p><formula xml:id="formula_5">v i 1 i 2 ...i k = [x i 1 , x i 2 , . . . , x i k ] 2 R dk and the number of elements in N is |N | = 2n+1 k .</formula><p>The non-consecutive CNN model then follows the procedure of the traditional CNN model in Section 2.1 to compute the representation vector for classifi- cation. The only difference is that the computation is done on the input set N instead of C. In partic- ular, the convolution score set in this case would be S(N ) = {f T v : v 2 N }, while the aggregating score would be:</p><formula xml:id="formula_6">p f k = max S(N ) = max{s : s 2 S(N )}<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation</head><p>Note that the maximum operation in Equation 2 only requires O(n) operations while the naive implemen- tation of Equation 3 would need O(|N |) = O(n k ) operations. In this work, we employ the dynamic programming (DP) procedure below to reduce the computation time for Equation 3. Assuming the filter vector f is the concatenation of the k vectors f 1 , . . . , f k 2 R d : f = [f 1 , . . . , f k ], Equation 3 can be re-written by:</p><formula xml:id="formula_7">p f k = max{f T 1 x i 1 + . . . + f T k x i k : 0  i 1 &lt; i 2 &lt; . . . &lt; i k  2n}</formula><p>Let D j t be the dynamic programming table repre- senting the maximum convolution score for the sub- filter [f 1 , . . . , f j ] over all possible non-consecutive j- gram vectors in the subsequence (x 0 , x 1 , . . . , x t ) of X:</p><formula xml:id="formula_8">D j t = max{f T 1 x i 1 + . . . + f T j x i j : 0  i 1 &lt; i 2 &lt; . . . &lt; i j  t}</formula><note type="other">where 1  j  k, j 1  t  2n. Note that p f k = D k 2n . We can solve this DP problem by the following recursive formulas 2 :</note><formula xml:id="formula_9">D j t = max{D j t1 , D j1 t1 + f T j x t }</formula><note type="other">The computation time for this procedure is O(kn) and remains linear in the sequence length.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>We train the networks using stochastic gradient de- scent with shuffled mini-batches, the AdaDelta up- date rule, back-propagation and dropout. During the training, we also optimize the embedding tables (i.e, word, position and entity type embeddings) to achieve the optimal states. Finally, we rescale the weights whose l 2 -norms exceed a predefined thresh- old (Nguyen and Grishman (2015a)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset, Parameters and Resources</head><p>We apply the same parameters and resources as <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b</ref>) to ensure the com- patible comparison. Specifically, we employ the window sizes in the set {2, 3, 4, 5} for the convo- lution operation with 150 filters for each window size. The window size of the trigger candidate is 31 while the dimensionality of the position embed- dings and entity type embeddings is 50. We use word2vec from (Mikolov et al., 2013b) as the pre- trained word embeddings. The other parameters in- clude the dropout rate ⇢ = 0.5, the mini-batch size = 50, the predefined threshold for the l 2 norms = 3.</p><p>Following the previous studies ( <ref type="bibr" target="#b12">Li et al., 2013;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2015b)</ref>, we evaluate the models on the ACE 2005 corpus with 33 event subtypes. In order to make it compat- ible, we use the same test set with 40 newswire ar- ticles, the same development set with 30 other doc- uments and the same training set with the remain- ing 529 documents. All the data preprocessing and evaluation criteria follow those in <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The General Setting</head><p>We compares the non-consecutive CNN model (NC- CNN) with the state-of-the-art systems on the ACE 2005 dataset in <ref type="table" target="#tab_0">Table 1</ref>. These systems include:</p><p>1 2) The neural network models, i.e, the CNN model in <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b</ref> 3) The probabilistic soft logic based model to cap- ture the event-event correlation in ( <ref type="bibr" target="#b15">Liu et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>F Sentence-level in Hong et al <ref type="bibr">(2011)</ref> 59.7 MaxEnt ( <ref type="bibr" target="#b12">Li et al., 2013)</ref> 65.9 Joint+Local ( <ref type="bibr" target="#b12">Li et al., 2013)</ref> 65.7 Joint+Local+Global ( <ref type="bibr" target="#b12">Li et al., 2013)</ref> 67.5 Cross-entity in  The most important observation from the table is that the non-consecutive CNN model significantly outperforms all the compared models with large margins. In particular, NC-CNN is 2% better than B-RNN ( <ref type="bibr" target="#b22">Nguyen et al., 2016a</ref>), the state-of-the- art system that only relies on the context informa- tion within the sentences of the trigger candidates. In addition, although NC-CNN only employs the sentence-level information, it is still better than the other models that further exploit the document-level information for prediction (an improvement of 1.9% over the probabilistic soft logic based model in ( <ref type="bibr" target="#b15">Liu et al., 2016)</ref>). Finally, comparing NC-CNN and the CNN model in <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b)</ref>, we see that the non-consecutive mechanism signifi- cantly improves the performance of the traditional CNN model for ED (up to 2.3% in absolute F- measures with p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Domain Adaptation Experiments</head><p>Previous studies have shown that the NLP models would suffer from a significant performance loss when domains shift <ref type="bibr" target="#b1">(Blitzer et al., 2006;</ref><ref type="bibr" target="#b5">Daume III, 2007;</ref><ref type="bibr" target="#b25">Plank and Moschitti, 2013;</ref><ref type="bibr" target="#b21">Nguyen et al., 2015c</ref>). In particular, if a model is trained on some source domain and applied to a different domain (the target domain), its performance would degrade sig- nificantly. The domain adaptation (DA) studies aim to overcome this issue by developing robust tech- niques across domains.</p><p>The best reported system in the DA setting for ED is <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b)</ref>, which demonstrated that the CNN model outperformed the feature-based models in the cross-domain setting. In this section, we compare NC-CNN with the CNN model in <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b</ref>) (as well as the other models above) in the DA setting to further investigate their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Dataset</head><p>This section also uses the ACE 2005 dataset but focuses more on the difference between domains. The ACE 2005 corpus includes 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and webblogs (wl). Following <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b</ref>), we use news (the union of bn and nw) as the source domain and bc, cts, wl and un as four different target domains <ref type="bibr">3</ref> . We take half of bc as the development set and use the remaining data for testing. Our data split is the same as that in <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b)</ref>.    <ref type="table" target="#tab_2">Table 2</ref> reports the performance of the systems with 5-fold cross validation. Note that we focus on the systems exploiting only the sentence level infor- mation in this section. For each system, we train a model on the training data of the source domain and evaluate this model on the test set of the source do- main (in-domain performance) as well as on the four target domains bc, cts, wl and un.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Performance</head><p>We emphasize that the performance of the sys- tems MaxEnt, Joint+Local, Joint+Local+Global, B-RNN, and CNN is obtained from the actual sys- tems in the original work ( <ref type="bibr" target="#b12">Li et al., 2013;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2015b;</ref><ref type="bibr" target="#b22">Nguyen et al., 2016a</ref>). The performance of DM-CNN, on the other hand, is from our re-implementation of the system in ( <ref type="bibr" target="#b4">Chen et al., 2015</ref>) using the same hyper-parameters and re- sources as CNN and NC-CNN for a fair comparison.</p><p>From the table, we see that NC-CNN is signifi- cantly better than the other models on the source domain. This is consistent with the conclusions in Section 3.2 and further confirms the effectiveness of NC-CNN. More importantly, NC-CNN outperforms CNN and the other models on the target domains bc, cts and un, and performs comparably with CNN on wl. The performance improvement is significant on bc and un (p &lt; 0.05), thereby verifying the robust- ness of NC-CNN for ED across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There have been three major approaches to event de- tection in the literature. First, the pattern-based ap- proach explores the application of patterns to iden- tify the instances of events, in which the patterns are formed by predicates, event triggers and constraints on the syntactic context ( <ref type="bibr" target="#b6">Grishman et al., 2005;</ref><ref type="bibr" target="#b2">Cao et al., 2015a;</ref><ref type="bibr" target="#b3">Cao et al., 2015b</ref>).</p><p>Second, the feature-based approach relies on lin- guistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b12">Li et al., 2013)</ref>, to the higher level structures such as the cross-sentence or cross-event information <ref type="bibr" target="#b10">(Ji and Grishman, 2008;</ref><ref type="bibr" target="#b7">Gupta and Ji, 2009;</ref><ref type="bibr" target="#b24">Patwardhan and Riloff, 2009;</ref><ref type="bibr" target="#b14">Liao and Grishman, 2011;</ref><ref type="bibr" target="#b8">Hong et al., 2011;</ref><ref type="bibr" target="#b16">McClosky et al., 2011;</ref><ref type="bibr" target="#b13">Li et al., 2015)</ref>. Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction ( <ref type="bibr" target="#b27">Riedel et al., 2009;</ref><ref type="bibr" target="#b26">Poon and Vanderwende, 2010;</ref><ref type="bibr" target="#b12">Li et al., 2013;</ref><ref type="bibr" target="#b29">Venugopal et al., 2014</ref>) to benefit from their inter-dependencies.</p><p>Finally, neural networks have been introduced into ED very recently with the early work on con- volutional neural networks <ref type="bibr" target="#b20">(Nguyen and Grishman, 2015b;</ref><ref type="bibr" target="#b4">Chen et al., 2015</ref>). The other work includes: <ref type="bibr" target="#b22">(Nguyen et al., 2016a</ref>) who employ bidirectional recurrent neural networks to perform event trig- ger and argument labeling jointly, (Jagannatha and Yu, 2016) who extract event instances from health records with recurrent neural networks and ( <ref type="bibr" target="#b23">Nguyen et al., 2016b</ref>) who propose a two-stage training al- gorithm for event extension with neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a new CNN architecture for ED that exploits the non-consecutive convolution for sen- tences. Our evaluation of the proposed model on the general setting and the DA setting demonstrates the effectiveness of the non-consecutive mechanism. We achieve the state-of-the-art performance for ED in both settings. In the future, we plan to investigate the non-consecutive architecture on other problems such as relation extraction or slot filling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>)</head><label></label><figDesc>The feature-based systems with rich hand- designed feature sets, including: the MaxEnt model with local features in (Li et al., 2013) (MaxEnt); the structured perceptron model for joint beam search with local features (Joint+Local), and with both lo- cal and global features (Joint+Local+Global) in (Li et al., 2013); and the sentence-level and cross-entity models in (Hong et al., 2011).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) (CNN), the dynamic multi-pooling CNN model (DM-CNN) in (Chen et al., 2015) and the bidirectional recurrent neural networks (B-RNN) in (Nguyen et al., 2016a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Hong et al. (2011) † 68.3 Probabilistic soft logic (Liu et al., 2016) † 69.4 CNN (Nguyen and Grishman, 2015b) 69.0 DM-CNN (Chen et al., 2015) 69.1 B-RNN (Nguyen et al., 2016a) 69.3 NC-CNN 71.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>System</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Performance with Gold-Standard Entity Men-

tions and Types.  † beyond sentence level. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on the source domain and on the target domains. Cells marked with  †designates that NC-CNN 

significantly outperforms (p &lt; 0.05) all the compared methods on the specified domain. 

</table></figure>

			<note place="foot" n="1"> The tanh function in this work.</note>

			<note place="foot" n="2"> We ignore the base cases as they are trivial.</note>

			<note place="foot" n="3"> Note that (Nguyen and Grishman, 2015b) does not report the performance on un but we include it here for completeness.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning about Time and Events</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving event detection with dependency regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In RANLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving event detection with active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In RANLP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACLIJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nyus english ace 2005 system description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACE 2005 Evaluation Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting unknown time arguments based on cross-event propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bidirectional rnn for medical event detection in electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abhyuday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jagannatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving event detection with abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP Workshop on Computing News Storylines (CNewS)</title>
		<meeting>ACL-IJCNLP Workshop on Computing News Storylines (CNewS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acquiring topic features to improve event extraction: in pre-selected and balanced collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A probabilistic soft logic based approach to exploiting latent and global information in event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event extraction as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioNLP Shared Task Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st NAACL Workshop on Vector Space Modeling for NLP (VSM)</title>
		<meeting>the 1st NAACL Workshop on Vector Space Modeling for NLP (VSM)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic representations for domain adaptation: A case study on the tree kernel-based method for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A two-stage approach for extending event detection to new types via neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisheng</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACL Workshop on Representation Learning for NLP</title>
		<meeting>the 1st ACL Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A unified model of phrasal and sentential evidence for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint inference for knowledge extraction from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A markov logic approach to bio-molecular event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Woo</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihisa</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relieving the computational bottleneck: Joint inference for event extraction with highdimensional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Vibhav Gogate, and Vincent Ng</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
