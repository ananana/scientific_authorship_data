<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
							<email>zitnick@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="932" to="937"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We conduct large-scale studies on &apos;human at-tention&apos; in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualiza-tions) and quantitatively (via rank-order correlation). Overall, our experiments show that current VQA attention models do not seem to be looking at the same regions as humans.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It helps to pay attention. Humans have the ability to quickly perceive a scene by selectively attending to parts of the image instead of processing the whole scene in its entirety <ref type="bibr" target="#b11">(Rensink, 2000</ref>). Inspired by hu- man attention, a recent trend in computer vision and deep learning is to build computational models of at- tention. Given an input signal, these models learn to attend to parts of it for further processing and have been successfully applied in machine transla- tion ( <ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr">Firat et al., 2016)</ref>, ob- ject recognition ( <ref type="bibr" target="#b2">Ba et al., 2015;</ref><ref type="bibr">Mnih et al., 2014;</ref><ref type="bibr" target="#b12">Sermanet et al., 2014</ref>), image captioning ( <ref type="bibr" target="#b14">Xu et al., 2015;</ref><ref type="bibr" target="#b3">Cho et al., 2015)</ref> and visual question answer- ing ( <ref type="bibr" target="#b10">Lu et al., 2016;</ref><ref type="bibr" target="#b14">Xu and Saenko, 2015;</ref><ref type="bibr" target="#b14">Xiong et al., 2016)</ref>. In this work, we study attention for the task of Vi- sual Question Answering (VQA). Unlike image cap- tioning, where a coarse understanding of an image * Denotes equal contribution. is often sufficient for producing generic descriptions <ref type="bibr" target="#b4">(Devlin et al., 2015)</ref>, visual questions selectively tar- get different areas of an image including background details and underlying context. This suggests that a VQA model may benefit from an explicit or implicit attention mechanism to answer a question correctly. In this work, we are interested in the following ques- tions: 1) Which image regions do humans choose to look at in order to answer questions about images? 2) Do deep VQA models with attention mechanisms attend to the same regions as humans? We design and conduct studies to collect "human attention maps". <ref type="figure" target="#fig_0">Figure 1</ref> shows human attention maps on the same image for two different ques- tions. When asked 'What type is the surface?', hu- mans choose to look at the floor, while attention for 'Which game is being played?' is concentrated around the player and racket. These human attention maps can be used both for evaluating machine-generated attention maps and for explicitly training attention-based models. Contributions. First, we design game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer ques- tions from the large-scale VQA dataset ( <ref type="bibr" target="#b1">Antol et al., 2015)</ref>; this VQA-HAT (Human ATtention) dataset is publicly available at our project webpage 1 Sec- ond, we perform qualitative and quantitative com- parison of the maps generated by state-of-the-art attention-based VQA models ( <ref type="bibr" target="#b10">Lu et al., 2016</ref>) and a task-independent saliency base- line ( <ref type="bibr" target="#b8">Judd et al., 2009</ref>) against our human atten- tion maps through visualizations and rank-order cor- relation. We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human atten- tion maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49. It is well understood that task-independent saliency maps have a 'center bias' <ref type="bibr" target="#b13">(Tatler, 2007;</ref><ref type="bibr" target="#b8">Judd et al., 2009</ref>). After we control for this center bias, we find that the correlation of task-independent saliency is poor (as expected), while trends for machine-generated VQA-attention maps remain the same, which confirms our key finding that current VQA attention models do not seem to be looking at the same regions as humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work draws on recent work in attention-based VQA and human studies in saliency prediction.</p><p>We work with the free-form and open-ended VQA dataset released by <ref type="bibr" target="#b1">(Antol et al., 2015)</ref>. Note that all these works are unsupervised attention models, where "attention" is simply an intermedi- ate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy). The fact that some (it's unclear how many) of these spatial distributions end up being interpretable is simply fortuitous. In contrast, we study where humans choose to look to answer vi- sual questions. These human attention maps can be used to evaluate unsupervised maps.</p><p>Human Studies. There's a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and vi- sual perception ( <ref type="bibr" target="#b6">Jiang et al., 2014;</ref><ref type="bibr" target="#b8">Judd et al., 2009;</ref><ref type="bibr" target="#b4">Fei-Fei et al., 2007;</ref><ref type="bibr" target="#b16">Yarbus, 1967)</ref>. Eye tracking data to study natural visual exploration ( <ref type="bibr" target="#b6">Jiang et al., 2014;</ref><ref type="bibr" target="#b8">Judd et al., 2009</ref>) is useful but difficult and expensive to collect on a large scale. (Jiang et al., 2015) established mouse tracking as an accu- rate alternative to eye tracking for collecting atten- tion maps. They collected large-scale attention an- notations for MS COCO ( <ref type="bibr" target="#b9">Lin et al., 2014</ref>) on Ama-zon Mechanical Turk (AMT). While ( <ref type="bibr" target="#b7">Jiang et al., 2015</ref>) studies natural exploration and collects task- independent human annotations by asking subjects to freely move the mouse cursor to anywhere they wanted to look on a blurred image, our approach is task-driven. (Jia Deng and Jonathan <ref type="bibr">Krause and Li Fei-Fei, 2013;</ref><ref type="bibr" target="#b4">Deng et al., 2015</ref>) leverage crowd- sourcing to help computers select discriminative fea- tures for fine-grained recognition. They introduce a novel gamified setting where the humans can reveal regions with certain penalty which ensures discrim- inative regions with assured quality. Related to this is the work of (von <ref type="bibr">Ahn and Dabbish, 2004</ref>) who ex- plore gamification to locate objects in an image. To the best of our knowledge, this is the first work to collect human attention maps for VQA. Specifically, as described in Section 3, we collect ground truth attention annotations by instructing subjects to sharpen parts of a blurred image that are important for answering the questions accurately. Section 4 covers evaluation of unsupervised atten- tion maps generated by VQA models against our hu- man attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VQA-HAT (Human ATtention) Dataset</head><p>We design and test multiple game-inspired novel in- terfaces for conducting large-scale human studies on AMT. Our basic interface design consists of a "de- blurring" exercise for answering visual questions. Specifically, we present subjects with a blurred im- age and a question about the image, and ask subjects to sharpen regions of the image that will help them answer the question correctly, in a smooth, click- and-drag, 'coloring' motion with the mouse. The sharpening is gradual: successively scrubbing the same region progressively sharpens it.</p><p>We experiment with multiple variants of the data collection interface. Analysis of the interfaces as well as details of the human evaluation studies con- ducted to converge on the final interface used for re- sults in this main document have been included in the supplement. The human evaluation studies con- sisted of showing these attention-sharpened images to humans and asking them to answer the question. Based on these human studies, we pick the "Blurred Image with Answer" interface, where subjects were shown the correct answer in addition to the ques- tion and blurred image, and asked to deblur as few regions as possible such that someone can answer the question just by looking at the sharpened re- gions. Since the payment structure on AMT encour- age completing tasks as quickly as possible, this im- plicitly incentivizes subjects to deblur as few regions as possible. Our followup human studies on these collected maps show that other subjects are able to answer questions based on these collected maps (de- tails in supplement). Thus, overall we achieve a bal- ance between highlighting too little or too much. Note that the "Blurred Image with Answer" inter- face used to collect attention maps is a verification task as opposed to actual question answering. We show subjects an answer and ask them to sharpen regions that will help them answer the question cor- rectly, as opposed to showing them just the ques- tion and asking them for the answer as well as rel- evant sharpened regions in the image ("Blurred Im- age without Answer" interface). Attention maps col- lected via this verification task "Blurred Image with Answer" are more informative (in terms of human VQA accuracy) than those collected for "Blurred Image without Answer" -78.7% vs. 75.2%. We collected human attention maps for 58475 train (out of 248349 total) and 1374 val (out of 121512 total) question-image pairs from the VQA dataset. This dataset is publicly available 1 . Overall, we con- ducted approximately 20000 Human Intelligence Tasks (HITs) on AMT, among 800 unique workers. <ref type="figure" target="#fig_1">Figure 2</ref> shows examples of collected human atten- tion maps. To visualize the collected dataset, we cluster the hu- man attention maps and visualize the average atten- tion map and example questions falling in each of them for 6 selected clusters in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>934</head><p>Now that we have collected these human attention maps, we can ask the following question -do unsu- pervised attention models learn to predict attention maps that are similar to human attention maps? To rephrase, do neural networks look at the same re- gions as humans to answer a visual question? VQA Attention Models. We evaluate maps gener- ated by the following unsupervised models:</p><p>• Stacked Attention Network (SAN) ( ) with two attention layers <ref type="figure" target="#fig_1">(SAN-2)</ref> 3 .</p><p>• Hierarchical</p><p>Co-Attention Network (HieCoAtt) ( <ref type="bibr" target="#b10">Lu et al., 2016</ref>) with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps 4 .</p><p>Comparison Metric: Rank Correlation. We first scale both the machine-generated and human atten- tion maps to 14x14, rank the pixels according to their spatial attention and then compute correlation between these two ranked lists. We choose an order- based metric so as to make the evaluation invariant to absolute spatial probability values which can be made peaky or diffuse by tweaking a 'temperature' parameter. <ref type="table">Table 1</ref> shows rank-order correlation averaged over all image-question pairs on the validation set. We compare with random attention maps and task- independent saliency maps generated by a model trained to predict human eye fixation locations where subjects are asked to freely view an image for 3 seconds ( <ref type="bibr" target="#b8">Judd et al., 2009</ref>). Both SAN-2 and HieCoAtt attention maps are positively corre- lated with human attention maps, but not as strongly as task-independent Judd saliency maps. Our find- ings lead to two take-away messages with signifi- cant potential impact on future research in this ac- tive field. First, current VQA attention models do not seem to be 'looking' at the same regions as hu- mans to produce an answer. Second, as attention- based VQA models become more accurate (58.9% SAN → 62.1% HieCoAtt), they seem to be (slightly) better correlated with humans in terms of where they</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Rank-correlation SAN-2 (  0.249 ± 0.004</p><p>HieCoAtt-W ( <ref type="bibr" target="#b10">Lu et al., 2016)</ref> 0.246 ± 0.004 HieCoAtt-P ( <ref type="bibr" target="#b10">Lu et al., 2016)</ref> 0.256 ± 0.004 HieCoAtt-Q ( <ref type="bibr" target="#b10">Lu et al., 2016)</ref> 0.264 ± 0.004  <ref type="table">Table 1</ref>: Mean rank-correlation coefficients (higher is better); error bars show standard error of means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head><p>We can see that both SAN-2 and HieCoAtt attention maps are positively correlated with human attention maps, but not as strongly as task-independent Judd saliency maps.  <ref type="table">Table 2</ref>: Correlation on the reduced set without cen- ter bias goes down significantly for Judd saliency since they have a strong center bias. Relative trends among SAN-2 &amp; HieCoAtt are similar to those over the whole validation set (reported in <ref type="table">Table 1</ref>).</p><p>look. Our dataset will allow for a more thorough val- idation of this observation as future attention-based VQA models are proposed. <ref type="figure" target="#fig_6">Figure 4</ref> shows ex- amples of human and machine-generated attention maps with their rank-correlation coefficients.</p><p>To put these numbers in perspective, we computed inter-human agreement on the validation set by col- lecting 3 human attention maps per image-question pair and computing mean rank-correlation, which is 0.623. Lastly, all reported correlations are aver- aged over 3 trials by adding random noise (order of 10 −14 ) to human attention maps to account for rank- ing variations in case of uniformly weighted regions. Center Bias. Judd saliency maps aim to predict hu- man eye fixations during natural visual exploration. These tend to have a strong center bias <ref type="bibr" target="#b13">(Tatler, 2007;</ref><ref type="bibr" target="#b8">Judd et al., 2009)</ref>. Although our human attention maps dataset is not an eye tracking study, the cen- ter bias still exists albeit not as severely as in eye- tracking. A potential source of center bias is the fact that the VQA dataset was human-generated by sub- jects looking at images. Thus, salient objects in the center of the image are likely to be potential subjects of questions. We compute rank-correlation of a syn- thetically generated central attention map with Judd saliency and human attention maps. Judd saliency maps have a mean rank-correlation of 0.877 and hu- man attention maps have a mean rank-correlation of 0.458 on the validation set.</p><p>To eliminate the effect of center bias in this evalua- tion, we removed human attention maps that have positive rank-correlation with the center attention map. We compute rank-correlation of machine- generated attention with human attention on this re- duced set. See <ref type="table">Table 2</ref>. Mean correlation goes down significantly for Judd saliency maps since they have a strong center bias. Relative trends among SAN-2 &amp; HieCoAtt are similar to those over the whole val- idation set (reported in <ref type="table">Table 1</ref>). HieCoAtt-Q now has higher correlation with human attention maps than Judd saliency. Thus discounting the center bias, VQA-specific machine attention maps correlate bet- ter with VQA-specific human attention maps than task-independent machine saliency maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion &amp; Discussion</head><p>We introduce and release the VQA-HAT dataset <ref type="bibr">1</ref> . This dataset can be used to evaluate attention maps generated in an unsupervised manner by attention-based VQA models, or to explicitly train models with attention supervision for VQA. We quantify whether current attention-based VQA models are 'looking' at the same regions of the image as humans do to produce an answer.</p><p>Necessary vs Sufficient Maps. Are human atten- tion maps 'necessary' and/or 'sufficient'? If regions highlighted by the human attention maps are suffi- cient to answer the question accurately, then so is any region that is a superset. For example, if atten- tion mass is concentrated on a 'cat' for 'What animal is present in the picture?', then an attention map that assigns weights to any arbitrary-sized region that in- cludes the 'cat' is sufficient as well. On the contrary, a necessary and sufficient attention map would be the smallest visual region sufficient for answering the question accurately. It is an ill-posed problem to define a necessary attention map in the space of pix- els; random pixels can be blacked out and chances are that humans would still be able to answer the question given the resulting subset attention map. Our work thus poses an interesting question for fu- ture work -what is the right semantic space in which it is meaningful to talk about necessary and suffi- cient attention maps for humans?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different human attention regions based on question. (best viewed in color)</figDesc><graphic url="image-1.png" coords="1,313.20,201.82,226.77,171.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a-c): Column 1 shows deblurred image, and column 2 shows human attention map.</figDesc><graphic url="image-2.png" coords="2,78.63,57.83,149.76,87.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>VQA Models.</head><label></label><figDesc>Attention-based models for VQA typically use convolutional neural networks to high-light relevant regions of image given a question. Stacked Attention Networks (SAN) proposed in (Yang et al., 2016) use LSTM encodings of ques- tion words to produce a spatial attention distribution over the convolutional layer features of the image. Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge 2 as of the time of this submission. Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (Andreas et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3</figDesc><graphic url="image-5.png" coords="3,313.20,510.21,226.71,111.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Random example of human attention (column 2) v/s machine-generated attention (columns 3-5)</figDesc><graphic url="image-6.png" coords="5,72.00,57.83,467.98,104.00" type="bitmap" /></figure>

			<note place="foot" n="1"> http://computing.ece.vt.edu/ ˜ abhshkdz/ vqa-hat</note>

			<note place="foot" n="2"> http://visualqa.org/challenge.html</note>

			<note place="foot" n="3"> https://github.com/zcyang/imageqa-san 4 https://github.com/jiasenlu/ HieCoAttenVQA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Jiasen Lu and Rama Vedantam for helpful sug-gestions. This work was supported in part by the National Science Foundation CAREER awards to DB &amp; DP, Army Research Office YIP awards to DB &amp; DP, ICTAS Junior Faculty awards at VT to DB &amp; DP, Army Research Lab grant W911NF-15-2-0080 to DP &amp; DB, Office of Naval Research (ONR) YIP award to DP, ONR grant N00014-14-1-0679 to DB, Alfred P. Sloan Fellowship to DP, Paul G. Allen Family Foundation Allen Distinguished Inves-tigator award to DP, Google Faculty Research award to DP &amp; DB, AWS in Education Research grant to DB, and NVIDIA GPU donation to DB. The views and conclu-sions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the US Government or any sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple Object Recognition With Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ba</surname></persName>
		</author>
		<idno>ICLR. 1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Describing Multimedia Content using Attention-based Encoder-Decoder Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<idno>abs/1507.01053. 1</idno>
		<editor>ICLR. 1 [Cho et al.2015] KyungHyun Cho, Aaron C. Courville, and Yoshua Bengio</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Neural Machine Translation by Jointly Learning to Align and Translate</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Leveraging the Wisdom of the Crowd for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno>abs/1505.04467. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<editor>Fei-Fei et al.2007] Li Fei-Fei, Asha Iyer, Christof Koch, and Pietro Perona</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Multi-way. multilingual neural machine translation with a shared attention mechanism. volume abs/1601.01073. 1</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">FineGrained Crowdsourcing for Fine-Grained Recognition</title>
		<idno>CVPR. 3</idno>
		<editor>Jia Deng and Jonathan Krause and Li Fei-Fei2013] Jia Deng and Jonathan Krause and Li Fei-Fei</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Saliency in Crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<title level="m">Salicon: Saliency in context. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Judd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical Question-Image CoAttention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 1, 2, 4</title>
		<imprint>
			<publisher>Alex Graves, and Koray Kavukcuoglu</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NIPS. 1</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<idno>abs/1412.7054. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Pierre Sermanet</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Andrea Frome, and Esteban Real</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tatler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI. 3</title>
		<editor>Luis von Ahn and Laura Dabbish</editor>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Labeling images with a computer game</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring questionguided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
		<idno>abs/1511.05234. 1 [Xu et al.2015</idno>
	</analytic>
	<monogr>
		<title level="m">and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML. 1</title>
		<editor>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICML. 1 [Xu and Saenko2015</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked Attention Networks for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 1</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Eye Movements and Vision. Plenum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yarbus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<pubPlace>New York. 2</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
