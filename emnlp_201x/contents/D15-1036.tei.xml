<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<address>
									<postCode>14853, 14853</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<address>
									<postCode>14853, 14853</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
							<email>mimno@cornell.edu,</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<address>
									<postCode>14853, 14853</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
								<address>
									<postCode>14853, 14853</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text. Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural word embeddings represent meaning via geometry. A good embedding provides vector rep- resentations of words such that the relationship be- tween two vectors mirrors the linguistic relation- ship between the two words. Despite the growing interest in vector representations of semantic in- formation, there has been relatively little work on direct evaluations of these models. In this work, we explore several approaches to measuring the quality of neural word embeddings. In particu- lar, we perform a comprehensive analysis of eval- uation methods and introduce novel methods that can be implemented through crowdsourcing, pro- viding better insights into the relative strengths of different embeddings.</p><p>Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic eval- uation, we use word embeddings as input features to a downstream task and measure changes in per- formance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>). Extrinsic evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures.</p><p>Intrinsic evaluations directly test for syntactic or semantic relationships between words ( <ref type="bibr" target="#b20">Mikolov et al., 2013a;</ref>). These tasks typi- cally involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory. Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality. Query inven- tories have so far been collected opportunistically from prior work in psycholinguistics, information retrieval ( <ref type="bibr">Finkelstein et al., 2002</ref>), and image anal- ysis ( <ref type="bibr" target="#b3">Bruni et al., 2014</ref>). Because these inventories were not constructed for word embedding evalu- ation, they are often idiosyncratic, dominated by specific types of queries, and poorly calibrated to corpus statistics.</p><p>To remedy these problems, this paper makes the following contributions. First, this is the first paper to conduct a comprehensive study cover- ing a wide range of evaluation criteria and popu- lar embedding techniques. In particular, we study how outcomes from three different evaluation cri- teria are connected: word relatedness, coherence, downstream performance. We show that using dif- ferent criteria results in different relative orderings of embeddings. These results indicate that embed- ding methods should be compared in the context of a specific task, e.g., linguistic insight or good downstream performance.</p><p>Second, we study the connections between di- rect evaluation with real users and pre-collected offline data. We propose a new approach to evalu- ation that focuses on direct comparison of embed- dings with respect to individual queries rather than overall summary scores. Because we phrase all tasks as choice problems rather than ordinal rel- evance tasks, we can ease the burden of the an-notators. We show that these evaluations can be gathered efficiently from crowdsourcing. Our re- sults also indicate that there is in fact strong corre- lation between the results of automated similarity evaluation and direct human evaluation. This re- sult justifies the use of offline data, at least for the similarity task.</p><p>Third, we propose a model-and data-driven ap- proach to constructing query inventories. Rather than picking words in an ad hoc fashion, we se- lect query words to be diverse with respect to their frequency, parts-of-speech and abstractness. To facilitate systematic evaluation and compar- ison of new embedding models, we release a new frequency-calibrated query inventory along with all user judgments at http://www.cs. cornell.edu/ ˜ schnabts/eval/.</p><p>Finally, we observe that word embeddings en- code a surprising degree of information about word frequency. We found this was true even in models that explicitly reserve parameters to com- pensate for frequency effects. This finding may explain some of the variability across embeddings and across evaluation methods. It also casts doubt on the common practice of using the vanilla co- sine similarity as a similarity measure in the em- bedding space.</p><p>It is important to note that this work is a survey of evaluation methods not a survey of embedding methods. The specific example embeddings pre- sented here were chosen as representative samples only, and may not be optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word embeddings</head><p>We refer to a word embedding as a mapping V → R D : w → w that maps a word w from a vocabu- lary V to a real-valued vector w in an embedding space of dimensionality D.</p><p>Following previous work <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b20">Mikolov et al., 2013a</ref>) we use the com- monly employed cosine similarity, defined as similarity(w 1 , w 2 ) = w 1 · w 2 w 1 w 2 , for all similar- ity computations in the embedding space. The list of nearest neighbors of a word w are all words v ∈ V \ {w}, sorted in descending order by similarity(w, v). We will denote w as the query word in the remainder of this paper.</p><p>All experiments in this paper are carried out on six popular unsupervised embedding meth- ods. These embeddings form a representative but incomplete subset; and since we are study- ing evaluation methods and not embeddings them- selves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of <ref type="bibr">word2vec (Mikolov et al., 2013a</ref>) and C&amp;W embeddings <ref type="bibr" target="#b7">(Collobert et al., 2011)</ref> both are motivated by a probabilistic predic- tion approach. Given a number of context words around a target word w, these models formulate the embedding task as that of finding a representa- tion that is good at predicting w from the context representations.</p><p>The second group of models, Hellinger PCA ( <ref type="bibr" target="#b13">Lebret and Collobert, 2014</ref>), <ref type="bibr">GloVe (Pennington et al., 2014</ref>), TSCCA ( <ref type="bibr" target="#b8">Dhillon et al., 2012)</ref> and Sparse Random Projections ( <ref type="bibr" target="#b17">Li et al., 2006</ref>) fol- low a reconstruction approach: word embeddings should be able to capture as much relevant infor- mation from the original co-occurrence matrix as possible.</p><p>Training corpus. We tried to make the compar- ison as fair as possible. As the C&amp;W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for train- ing the other models. We tokenized the data us- ing the Stanford tokenizer ( . Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros.</p><p>Details. All models embedded words into a 50- dimensional space (D = 50). As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relatedness</head><p>We begin with intrinsic evaluation of relatedness using both pre-collected human evaluations and a novel online user study. Section 3.1 introduces the list of datasets that is commonly used as a bench- mark for embedding methods. There, embeddings are evaluated individually and only their final scores are compared, hence we refer to this sce- nario as absolute intrinsic evaluation. We present a new scenario, comparative intrinsic evaluation, in which we ask people directly for their prefer- ences among different embeddings. We demon- strate that we can achieve the same results as of- fline, absolute metrics using online, comparative metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Absolute intrinsic evaluation</head><p>For the absolute intrinsic evaluation, we used the same datasets and tasks as . While we present results on all tasks for complete- ness, we will mainly focus on relatedness in this section. There are four broad categories:</p><p>• Relatedness: These datasets contain relat- edness scores for pairs of words; the cosine similarity of the embeddings for two words should have high correlation (Spearman or Pearson) with human relatedness scores.</p><p>• Analogy: This task was popularized by <ref type="bibr" target="#b20">Mikolov et al. (2013a)</ref>. The goal is to find a term x for a given term y so that x : y best resembles a sample relationship a : b.</p><p>• Categorization: Here, the goal is to re- cover a clustering of words into different cat- egories. To do this, the corresponding word vectors of all words in a dataset are clustered and the purity of the returned clusters is com- puted with respect to the labeled dataset.</p><p>• Selectional preference: The goal is to deter- mine how typical a noun is for a verb either as a subject or as an object (e.g., people eat, but we rarely eat people). We follow the pro- cedure that is outlined in .</p><p>Several important design questions come up when designing reusable datasets for evaluating relatedness. While we focus mainly on challenges that arise in the relatedness evaluation task, many of the questions discussed also apply to other sce- narios.</p><p>Query inventory. How we pick the word pairs to evaluate affects the results of the evalu- ation. The commonly-used WordSim-353 dataset ( <ref type="bibr">Finkelstein et al., 2002</ref>), for example, only tries to have word pairs with a diverse set of similarity scores. The more recent MEN dataset ( <ref type="bibr" target="#b3">Bruni et al., 2014</ref>) follows a similar strategy, but restricts queries to words that occur as annotations in an image dataset. However, there are more important criteria that should be considered in order to cre- ate a diverse dataset: (i) the frequency of the words in the English language (ii) the parts of speech of the words and (iii) abstractness vs. concreteness of the terms. Not only is frequency important be- cause we want to test the quality of embeddings on rare words, but also because it is related with distance in the embedding space as we show later and should be explicitly considered.</p><p>Metric aggregation. The main conceptual shortcoming of using correlation-based metrics is that they aggregate scores of different pairs - even though these scores can vary greatly in the embedding space. We can view the relatedness task as the task of evaluating a set of rankings, similar to ranking evaluation in Information Re- trieval. More specifically, we have one query for each unique query word w and rank all remaining words v in the vocabulary accordingly. The prob- lem now is that we usually cannot directly com- pare scores from different rankings <ref type="bibr" target="#b1">(Aslam and Montague, 2001</ref>) as their scores are not guaran- teed to have the same ranges. An even worse case is the following scenario. Assume we use rank correlation as our metric. As a consequence, we need our gold ranking to define an order on all the word pairs. However, this also means that we somehow need to order completely unrelated word pairs; for example, we have to decide whether (dog, cat) is more similar than (banana, apple). <ref type="table">Table 1</ref> presents the results on 14 different datasets for the six embedding models. We excluded ex- amples from datasets that contained words not in our vocabulary. For the relatedness and selective preference tasks, the numbers in the table indicate the correlation coefficient of human scores and the cosine similarity times 100. The numbers for the categorization tasks reflect the purities of the re- sulting clusters. For the analogy task, we report accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Absolute results</head><p>CBOW outperforms other embeddings on 10 of 14 datasets. CBOW especially excels at the relat- edness and analogy tasks, but fails to surpass other models on the selective preferences tasks. Ran- dom projection performs worst in 13 out of the 14 tasks, being followed by Hellinger PCA. C&amp;W and TSCCA are similar on average, but differ across datasets. Moreover, although TSCCA and GloVe perform similarly on most tasks, TSCCA suffers disproportionally on the analogy tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparative intrinsic evaluation</head><p>In comparative evaluation, users give direct feed- back on the embeddings themselves, so we do not have to define a metric that compares scored word pairs. Rather than defining both query and target words, we need only choose query words since the relatedness categorization sel. prefs analogy rg ws wss wsr men toefl ap esslli batt. up mcrae an ansyn ansem average CBOW 74.0 64.0 71.5 56.5 70.7 66.7 65.9 70.5 85.2 24.1 13.9 52.2 47.8 57.6 58.6 GloVe 63.7 54.8 65.8 49.6 64.6 69.4 64.1 65.9 77.8 27.0 18.4 42.2 44.2 39.7 53.4 TSCCA 57.8 54.4 64.7 43.3 56.7 58.3 57.5 70.5 64.2 31.0 14.4 15.5 19.0 11.1 44.2 C&amp;W 48.1 49.8 60.7 40.1 57.5 66.7 60.6 61.4 80.2 28.3 16.0 10.9 12.2 9.3 43.0 H-PCA 19.8 32.9 43.6 15.1 21.3 54.2 34.1 50.0 42.0 -2.5</p><p>3.2 3.0 2.4 3.7 23.1 Rand. Proj. 17.1 19.5 24.9 16.1 11.3 51.4 21.9 38.6 29.6 -8.5</p><p>1.2 1.0 0.3 1.9 16.2 <ref type="table">Table 1</ref>: Results on absolute intrinsic evaluation. The best result for each dataset is highlighted in bold. The second row contains the names of the corresponding datasets.</p><p>embeddings themselves will be used to define the comparable target words.</p><p>Query inventory. We compiled a diverse in- ventory of 100 query words that balance fre- quency, part of speech (POS), and concreteness. First, we selected 10 out of 45 broad categories from WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref>. We then chose an equal number of categories that mostly contained abstract concepts and categories that referred to concrete concepts. Among those categories, we had one for adjectives and adverbs each, and four for nouns and verbs each. From each category, we drew ten random words with the restriction that there be exactly three rare words (i.e., occur- ring fewer than 2500 times in the training corpus) among the ten.</p><p>Details. Our experiments were performed with users from Amazon Mechanical Turk (MTurk) that were native speakers of English with sufficient experience and positive feedback on the Amazon Mechanical Turk framework.</p><p>For each of the 100 query words in the dataset, the nearest neighbors at ranks k ∈ {1, 5, 50} for the six embeddings were retrieved. For each query word and k, we presented the six words along with the query word to the users. Each Turker was re- quested to evaluate between 25 and 50 items per task, where an item corresponds to the query word and the set of 6 retrieved neighbor words from each of the 6 embeddings. The payment was be- tween $0.01 and $0.02 per item. The users were then asked to pick the word that is most similar ac- cording to their perception (the instructions were almost identical to the WordSim-353 dataset in- structions). Duplicate words were consolidated, and a click was counted for all embeddings that returned that word. An option "I don't know the meaning of one (or several) of the words" was also provided as an alternative. <ref type="table" target="#tab_0">Table 2</ref> shows an exam- ple instance that was given to the Turkers.</p><p>Query: skillfully (a) swiftly (b) expertly (c) cleverly (d) pointedly The combination of 100 query words and 3 ranks yielded 300 items on which we solicited judgements by a median of 7 Turkers (min=5, max=14). We compare embeddings by average win ratio, where the win ratio was how many times raters chose embedding e divided by the number of total ratings for item i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparative results</head><p>Overall comparative results replicate previous re- sults. <ref type="figure" target="#fig_1">Figure 1(a)</ref> shows normalized win ratio scores for each embedding across 3 conditions corresponding to the frequency of the query word in the training corpus. The scores were normal- ized to sum to one in each condition to emphasize relative differences. CBOW in general performed the best and random projection the worst (p-value &lt; 0.05 for all pairs except H-PCA and C&amp;W in comparing un-normalized score differences for the ALL-FREQ condition with a randomized permuta- tion test). The novel comparative evaluations cor- respond both in rank and in relative margins to those shown in <ref type="table">Table 1</ref>.</p><p>Unlike previous results, we can now show differences beyond the nearest neighbors. <ref type="figure" target="#fig_1">Fig- ure 1(b)</ref>    broken up by the rank k of the neighbors that were compared. CBOW has its strengths especially at rank k = 1. For neighbors that appear after that, CBOW does not necessarily produce better em- beddings. In fact, it even does worse for k = 50 than GloVe. It is important to note, however, that we cannot make absolute statements about how performance behaves across different values of k since each assessment is always relative to the quality of all other embeddings.</p><p>We balanced our query inventory also with re- spect to parts of speech and abstractness vs. con- creteness. <ref type="figure" target="#fig_1">Figure 1(c)</ref> shows the relative per- formances of all embeddings for the four POS classes (adjectives, adverbs, nouns and verbs). While most embeddings show relatively homoge- neous behaviour across the four classes, GloVe suffers disproportionally on adverbs. Moving on to <ref type="figure" target="#fig_1">Figure 1(d)</ref>, we can see a similar behavior for TSCCA: Its performance is much lower on con- crete words than on abstract ones. This differ- ence may be important, as recent related work finds that simply differentiating between general and specific terms explains much of the observed variation between embedding methods in hierar- chical classification tasks ( <ref type="bibr" target="#b16">Levy et al., 2015b</ref>). We take the two observations above as evidence that a more fine-grained analysis is necessary in discern- ing different embedding methods.</p><p>As a by-product, we observed that there was no embedding method that consistently performed best on all of the four different absolute evaluation tasks. However, we would like to reiterate that our goal is not to identify one best method, but rather point out that different evaluations (e.g., changing the rank k of the nearest neighbors in the compar- ison task) result in different outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Coherence</head><p>In the relatedness task we measure whether a pair of semantically similar words are near each other in the embedding space. In this novel coherence task we assess whether groups of words in a small neighborhood in the embedding space are mutu- ally related. Previous work has used this property for qualitative evaluation using visualizations of 2D projections <ref type="bibr" target="#b26">(Turian et al., 2010</ref>), but we are not aware of any work using local neighborhoods for quantitative evaluation. Good embeddings should have coherent neighborhoods for each word, so inserting a word not belonging to this neighbor- hood should be easy to spot. Similar to <ref type="bibr" target="#b5">Chang et al. (2009)</ref>, we presented Turkers with four words, three of which are close neighbors and one of which is an "intruder." For each of the 100 words in our query set of Section 3.3, we retrieved the two nearest neighbors. These words along with the query word defined the set of (supposedly) good options. <ref type="table">Table 3</ref> shows an example instance that was given to the Turkers.</p><p>(a) finally (b) eventually (c) immediately (d) put <ref type="table">Table 3</ref>: Example instance of intrusion task. The query word is option (a), intruder is (d).</p><p>To normalize for frequency-based effects, we computed the average frequency avg of the three words in this set and chose the intruder word to be the first word that had a frequency of avg ± 500 starting at rank 100 of the list of nearest neighbors.</p><p>Results. In total, we solicited judgments on 600 items (100 query words for each of the 6 em- beddings) from a median of 7 Turkers (min=4, max=11) per item, where each Turker evaluated between 25 and 50 items per task. <ref type="figure" target="#fig_2">Figure 2</ref> shows the results of the intrusion experiment. The evalu- ation measure is micro-averaged precision for an embedding across 100 query words, where per- item precision is defined as the number of raters that discovered the intruder divided the total num- ber of raters of item i. Random guessing would achieve an average precision of 0.25.</p><p>All embeddings perform better than guessing, indicating that there is at least some coherent structure captured in all of them. However, the best performing embeddings at this task are TSCCA, CBOW and GloVe (the precision mean differences were not significant under a random permutation test), while TSCCA attains greater precision (p &lt; 0.05) in relation to C&amp;W, H-PCA and random projection embeddings. These re- sults are in contrast to the direct comparison study, where the performance of TSCCA was found to be significantly worse than that of CBOW. However, the order of the last three embeddings remains un- changed, implying that performance on the intru- sion task and performance on the direct compari-  son task are correlated. CBOW and C&amp;W seem to do equally well on rare and frequent words, whereas the other models' performance suffers on rare words.</p><p>Discussion. Evaluation of set-based properties of embeddings may produce different results from item-based evaluation: rankings we got from the intrusion task did not match the rankings we ob- tained from the relatedness task. Pairwise similar- ities seem to be only part of the information that is encoded in word embeddings, so looking at more global measures is necessary for a better under- standing of differences between embeddings.</p><p>We choose intruder words based on similar but lower-ranked words, so an embedding could score well on this task by doing an unusually bad job at returning less-closely related words. However, the results in <ref type="figure" target="#fig_1">Figure 1(b)</ref> suggest that there is lit- tle differences at higher ranks (rank 50) between embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Extrinsic Tasks</head><p>Extrinsic evaluations measure the contribution of a word embedding model to a specific task. There is an implicit assumption in the use of such eval- uations that there is a consistent, global ranking of word embedding quality, and that higher qual- ity embeddings will necessarily improve results on any downstream task. We find that this assumption does not hold: different tasks favor different em- beddings. Although these evaluations are useful in characterizing the relative strengths of different models, we do not recommend that they be used as a proxy for a general notion of embedding quality.    Noun phrase chunking. First we use a noun phrase chunking task similar to that used by <ref type="bibr" target="#b26">Turian et al. (2010)</ref>. The only difference is that we nor- malize all word vectors to unit length, rather than scaling them with some custom factor, before giv- ing them to the conditional random field (CRF) model as input. We expect that this task will be more sensitive to syntactic information than to se- mantic information.</p><p>Sentiment classification. Second we use a re- cently released dataset for binary sentiment clas- sification by <ref type="bibr" target="#b18">Maas et al. (2011)</ref>. The dataset con- tains 50K movie reviews with a balanced distribu- tion of binary polarity labels. We evaluate the rel- ative performance of word embeddings at this task as follows: we generate embedding-only features for each review by computing a linear combina- tion of word embeddings weighted by the num- ber of times that the word appeared in the review (using the same bag-of-words features as Maas et al. <ref type="formula">(2011)</ref>). A LIBLINEAR logistic regression model <ref type="bibr" target="#b9">(Fan et al., 2008</ref>) with the default parame- ters is trained and evaluated using 10 fold cross- validation. A vanilla bag of words feature set is the baseline (denoted as BOW here). We expect that this task will be more sensitive to semantic information than syntactic information.</p><p>Results. <ref type="table" target="#tab_4">Table 4</ref> shows the average F1-scores for the chunking task. The p-values were com- puted using randomization <ref type="bibr" target="#b28">(Yeh, 2000</ref>) on the sen- tence level. First, we can observe that adding word vectors as features results in performance lifts with all embeddings when compared to the baseline. The performance of C&amp;W and TSCCA is statis- tically not significant, and C&amp;W does better than all the remaining methods at the p = 0.05 level. Surprisingly, although the performance of Ran- dom Projections is still last, the gap to GloVe and CBOW is now very small. <ref type="table" target="#tab_5">Table 5</ref> shows results on the sentiment analysis task. We recover a sim- ilar order of embeddings as in the absolute intrin- sic evaluation, however, the order of TSCCA and GloVe is now reversed.</p><p>Discussion. Performance on downstream tasks is not consistent across tasks, and may not be con- sistent with intrinsic evaluations. Comparing per- formance across tasks may provide insight into the information encoded by an embedding, but we should not expect any specific task to act as a proxy for abstract quality. Furthermore, if good downstream performance is really the goal of an embedding, we recommend that embeddings be trained specifically to optimize a specific objective ( <ref type="bibr" target="#b13">Lebret and Collobert, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We find consistent differences between word em- beddings, despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions ( <ref type="bibr" target="#b24">Pennington et al., 2014;</ref><ref type="bibr" target="#b14">Levy and Goldberg, 2014</ref>). Recent work suggests that many apparent performance differ- ences on specific tasks are due to a lack of hyper- parameter optimization ( <ref type="bibr" target="#b15">Levy et al., 2015a</ref>). Dif- ferent algorithms are, in fact, encoding surpris- ingly different information that may or may not align with our desired use cases. For example, we find that embeddings encode differing degrees of information about word frequency, even after length normalization. This result is surprising for two reasons. First, many algorithms reserve dis- tinct "intercept" parameters to absorb frequency- based effects. Second, we expect that the ge- ometry of the embedding space will be primar- ily driven by semantics: the relatively small num- ber of frequent words should be evenly distributed through the space, while large numbers of rare, specific words should cluster around related, but more frequent, words.</p><p>We trained a logistic regression model to predict word frequency categories based on word vectors. The linear classifier was trained to put words ei- ther in a frequent or rare category, with thresholds varying from 100 to 50,000. At each threshold fre- quency, we sampled the training sets to ensure a consistent balance of the label distribution across all frequencies. We used length-normalized em- beddings, as rare words might have shorter vec- tors resulting from fewer updates during training ( <ref type="bibr" target="#b26">Turian et al., 2010)</ref>. We report the mean accuracy and standard deviation (1σ) using five-fold cross- validation at each threshold frequency in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>All word embeddings do better than random, suggesting that they contain some frequency in- formation. GloVe and TSCCA achieve nearly 100% accuracy on thresholds up to 1000. Unlike all other embeddings, accuracy for C&amp;W embed- dings increases for larger threshold values. Fur- ther investigation revealed that the weight vector direction changes gradually with the threshold fre- quency -indicating that frequency seems to be encoded in a smooth way in the embedding space.</p><p>Although GloVe and CBOW are the two best performing embeddings on the intrinsic tasks, they differ vastly in the amount of frequency informa- tion they encode. As a consequence, we can con- clude that most of the differences in frequency pre- diction are not due to intrinsic properties of natu- ral language: it is not the case that frequent words naturally have only frequent neighbors.  Word frequency information in the embedding space also affects cosine similarity. For each of the words in the WordSim-353 dataset, we queried for the k = 1000 nearest neighbors. We then looked up their frequency ranks in the training corpus and averaged those ranks over all the query words. We found a strong correlation between the frequency of a word and its position in the ranking of near- est neighbors in our experiments. <ref type="figure" target="#fig_5">Figure 4</ref> shows a power law relationship for C&amp;W embeddings between a word's nearest neighbor rank (w.r.t. a query) and the word's frequency rank in the train- ing corpus (nn-rank ∼ 1000 · corpus-rank 0.17 ). This is a concern: the frequency of a word in the language plays a critical role in word processing of humans as well <ref type="bibr" target="#b4">(Cattell, 1886)</ref>. As a conse- quence, we need to explicitly consider word fre- quency as a factor in the experiment design. Also, the above results mean that the commonly-used cosine similarity in the embedding space for the intrinsic tasks gets polluted by frequency-based effects. We believe that further research should address how to better measure linguistic relation- ships between words in the embedding space, e.g., by learning a custom metric. <ref type="bibr" target="#b21">Mikolov et al. (2013b)</ref> demonstrate that cer- tain linguistic regularities exist in the embedding space. The authors show that by doing simple vector arithmetic in the embedding space, one can solve various syntactic and semantic analogy tasks. This is different to previous work, which phrased the analogy task as a classification prob- lem <ref type="bibr" target="#b27">(Turney, 2008)</ref>. Surprisingly, word embed-dings seem to capture even more complex linguis- tic properties. <ref type="bibr" target="#b6">Chen et al. (2013)</ref> show that word embeddings even contain information about re- gional spellings (UK vs. US), noun gender and sentiment polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Previous work in evaluation for word embed- dings can be divided into intrinsic and extrin- sic evaluations. Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and ge- ometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures,  compare word embeddings against distributional word vectors on a variety of query inventories and tasks. <ref type="bibr" target="#b10">Faruqui and Dyer (2014)</ref> provide a website that allows the automatic evaluation of embeddings on a number of query inventories. <ref type="bibr" target="#b12">Gao et al. (2014)</ref> publish an improved query inventory for the analogical reasoning task. Finally, <ref type="bibr" target="#b25">Tsvetkov et al. (2015)</ref> propose a new in- trinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly lim- ited to local metrics like relatedness.</p><p>Extrinsic evaluations use embeddings as fea- tures in models for other tasks, such as semantic role labeling or part-of-speech tagging <ref type="bibr" target="#b7">(Collobert et al., 2011)</ref>, and improve the performance of ex- isting systems <ref type="bibr" target="#b26">(Turian et al., 2010)</ref>. However, they have been less successful at other tasks such as parsing ( <ref type="bibr" target="#b0">Andreas and Klein, 2014)</ref>.</p><p>More work has been done in unsupervised se- mantic modeling in the context of topic models. One example is the word intrusion task ( <ref type="bibr" target="#b5">Chang et al., 2009)</ref>, in which annotators are asked to iden- tify a random word inserted into the set of high probability words for a given topic. Word embed- dings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions <ref type="bibr" target="#b23">(Newman et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>There are many factors that affect word embed- ding quality. Standard aggregate evaluations, while useful, do not present a complete or con- sistent picture. Factors such as word frequency play a significant and previously unacknowledged role. Word frequency also interferes with the commonly-used cosine similarity measure. We present a novel evaluation framework based on di- rect comparisons between embeddings that pro- vides more fine-grained analysis and supports sim- ple, crowdsourced relevance judgments. We also present a novel Coherence task that measures our intuition that neighborhoods in the embedding space should be semantically or syntactically re- lated. We find that extrinsic evaluations, although useful for highlighting specific aspects of embed- ding performance, should not be used as a proxy for generic quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Direct comparison task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Intrusion task: average precision by global word frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>dev</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Embeddings can accurately predict whether a word is frequent or rare.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Avg. word rank by frequency in training corpus vs. nearest-neighbor rank in the C&amp;W embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example instance of comparative in-
trinsic evaluation task. The presented options in 
this example are nearest neighbors to the query 
word according to (a) C&amp;W, (b) CBOW, GloVe, 
TSCCA (c) Rand. Proj. and (d) H-PCA. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>presents the same results, but this time</figDesc><table>Rand. Proj H-PCA 

C&amp;W 
TSCCA GloVe CBOW 
0.00 

0.05 

0.10 

0.15 

0.20 

0.25 

0.30 

0.35 

0.40 

Score 

freq 2500 
freq &gt; 2500 
all freq 

(a) Normalized scores by global word frequency. 

Rand. Proj H-PCA 
C&amp;W 
TSCCA GloVe CBOW 
0.00 

0.05 

0.10 

0.15 

0.20 

0.25 

0.30 

0.35 

0.40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F1 chunking results using different word 
embeddings as features. The p-values are with re-
spect to the best performing method. 

test p-value 

BOW (baseline) 88.90 7.45·10 −14 
Rand. Proj. 62.95 7.47·10 −12 
GloVe 74.87 5.00·10 −2 
H-PCA 69.45 6.06·10 −11 
C&amp;W 72.37 1.29·10 −7 
CBOW 75.78 
TSCCA 75.02 7.28·10 −4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>F1 sentiment analysis results using differ-
ent word embeddings as features. The p-values are 
with respect to the best performing embedding. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded in part through NSF Award IIS-1513692. We would like to thank Alexandra Schofield, Adith Swaminathan and all other members of the NLP seminar for their help-ful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How much do word embeddings encode about syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Short Papers</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="822" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Models for metasearch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimodal distributional semantics. JAIR</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The time taken up by cerebral operations. Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mckeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cattell</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1886" />
			<biblScope unit="page" from="220" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<title level="m">The expressive power of word embeddings</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1408" to="3456" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JLMR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two step CCA: A new spectral method for estimating vector models of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1551" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JLMR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Community evaluation and exchange of word vectors at wordvectors.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<editor>Lev Finkelstein, Ehud Rivlin Zach Solan Gadi Wolfman Evgeniy Gabrilovich, Yossi Matias, and Eytan Ruppin</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">WordRep: A benchmark for research on learning word representations. ICML Workshop on KnowledgePowered Deep Learning for Text Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Word embeddings through Hellinger PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Do supervised distributional methods really learn lexical inference relations? In NAACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very sparse random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A uniform approach to analogies, synonyms, antonyms, and associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
