<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Inference of the Tense of Chinese Events Using Implicit Linguistic Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
							<email>yuchenz@brandeis.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Brandeis University</orgName>
								<address>
									<addrLine>415 South Street Waltham</addrLine>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
							<email>xuen@brandeis.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Brandeis University</orgName>
								<address>
									<addrLine>415 South Street Waltham</addrLine>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Inference of the Tense of Chinese Events Using Implicit Linguistic Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1902" to="1911"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We address the problem of automatically inferring the tense of events in Chinese text. We use a new corpus annotated with Chinese semantic tense information and other implicit Chinese linguistic information using a &quot;distant annotation&quot; method. We propose three improvements over a relatively strong baseline method-a statistical learning method with extensive feature engineering. First, we add two sources of implicit linguistic information as features eventuality type and modality of an event, which are also inferred automatically. Second, we perform joint learning on semantic tense, eventuality type, and modality of an event. Third, we train artificial neural network models for this problem and compare its performance with feature-based approaches. Experimental results show considerable improvements on Chinese tense inference. Our best performance reaches 68.6% in accuracy, out-performing a strong baseline method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a language with no grammatical tense, Chinese does not encode the temporal location of an event directly in a verb, while in English, the grammati- cal tense of a verb is a strong indicator of the tem- poral location of an event. In this paper we ad- dress the problem of inferring the semantic tense, or the temporal location of an event (e.g., present, past, future) in Chinese text. The semantic tense is defined relative to the utterance time or document creation time, and it does not always agree with the grammatical tense in languages like English where there is grammatical tense. Inferring se- mantic tense potentially benefits natural language processing tasks such as Machine Translation and Information Extraction <ref type="bibr" target="#b9">(Xue, 2008;</ref><ref type="bibr" target="#b5">Reichart and Rappoport, 2010;</ref><ref type="bibr" target="#b10">Ye et al., 2006;</ref><ref type="bibr" target="#b11">Ye, 2007;</ref><ref type="bibr" target="#b2">Liu et al., 2011</ref>), but previous work has shown that auto- matic inference of the semantic tense of events in Chinese is a very challenging task <ref type="bibr" target="#b9">(Xue, 2008;</ref><ref type="bibr" target="#b10">Ye et al., 2006</ref>; <ref type="bibr" target="#b2">Liu et al., 2011)</ref>.</p><p>There are at least two reasons why this is a dif- ficult problem. First, since Chinese does not have grammatical tense which could serve as an impor- tant clue when annotating the semantic tense of an event, generating consistent annotation for Chi- nese semantic tense has proved to be a challenge. <ref type="bibr" target="#b7">Xue and Zhang (2014)</ref> use a "distant annotation" method to address this problem. They take advan- tage of an English-Chinese parallel corpus with manual word alignments ( <ref type="bibr" target="#b1">Li et al., 2012)</ref> , and per- form annotation on the English side, which pro- vides more explicit information such as grammati- cal tense that helps annotators decide the appropri- ate semantic tense. The annotations are then pro- jected to the Chinese side via the word alignments. They show consistent annotation agreements on semantic tense. Second, the lack of grammatical tense also makes automatic inference of Chinese semantic tense challenging since the grammatical tense would be an important source of information for predicting the semantic tense. Previous work has shown that it is very difficult to achieve high accuracy using standard machine learning tech- niques such as Maximum Entropy and Conditional Random Field classifiers combined with extensive feature engineering.</p><p>We address these challenges in two ways. First of all, we take advantage of the newly annotated corpus described in <ref type="bibr" target="#b7">(Xue and Zhang, 2014</ref>) in which semantic tense is annotated together with eventuality type and modality using the distant an- notation method. This makes it possible to use these two additional sources of information to help predict tense. Eventuality type and modality are intricately tied to tense. For example, <ref type="bibr" target="#b6">Smith and Erbaugh (2005)</ref> show that states by default hold in the present but (episodic) events occur by default in the past. This means knowing the eventuality type of an event would help determine the tense. Eventuality type and modality are also annotated on the English side and then projected onto the Chinese side via manual word alignments, taking advantage of the rich morphosyntactic clues in En- glish. High inter-annotator agreement scores are also reported on eventuality type and modality.</p><p>We experimented with two ways of using even- tuality type and modality information. In the first approach, we first train statistical machine learn- ing models to predict eventuality type and modal- ity and then use these two sources of information as features to predict semantic tense. In the sec- ond approach we trained joint learning models be- tween semantic tense and eventuality type, and be- tween semantic tense and modality. We show both approaches improve the tense inference accuracy over a baseline where these two sources of infor- mation are not used. Second, in our statistical machine learning experiments on tense inference using feature engineering, we find that the design of feature templates has great influence on the re- sults. So in order to explore more possible feature combinations and mitigate the feature engineering work, we apply artificial neural network models to this problem. This shows improvements on tense inference accuracy as well in some of the experi- ment settings.</p><p>The rest of the paper is organized as follows. Section 2 discusses related work in automatic tense inference. Section 3 briefly introduces the distant annotation method. In section 4, we de- scribe our experiments and analyze the experimen- tal results. We conclude this paper in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Inferring the semantic tense of events in Chinese text is not a new topic. There have been several attempts at it, yet high accuracy in this task has proved to be elusive. Using a corpus with tense annotated directly in Chinese text, <ref type="bibr" target="#b9">Xue (2008)</ref> per- formed extensive feature engineering in a machine learning framework to address this problem. They used both local lexical features and structured fea- tures extracted from manually annotated syntactic parsing trees. In our baseline method, we adopt most of their features as the baseline, only on a new corpus in which semantic tenses are not an- notated directly on Chinese events but projected from annotations from the English side of a par- allel Chinese-English corpus. In our experiments, we also use structural features extracted from au- tomatic parse trees, so our experimental settings are more realistic. <ref type="bibr" target="#b10">Ye et al. (2006)</ref> took a similar approach in which they predict tense with feature engineering in a statistical learning framework. They also used a Chinese-English parallel corpus and projected tense for English events onto Chinese events via human alignments. The main difference between their data and ours is that they used the gram- matical tense of the English events, while we use human-annotated semantic tense which we believe are more "transferrable" across languages as it is free of the language-specific idiosyncrasies of grammatical tense. In addition, they also used hu- man annotated linguistic information as "latent" features in their work, which are similar to our implicit linguistic features. However, the "latent" features that they used in their system are human- annotated, while the eventuality type and modality features in our system are predicted automatically. Another difference is that they ignored events that are not verbs. For example, they excluded ver- bal expressions in Chinese that are translated into nominal phrases in English. In contrast, we kept all events in our data, and they can be realized as verbs, nouns, as well as words in other parts of speech. We performed separate experiments on events realized as verbal expressions and events not in verbal expressions to investigate their im- pact on semantic tense inference. <ref type="bibr" target="#b2">Liu et al. (2011)</ref> introduced more global fea- tures in a machine learning framework, and on top of that proposed an iterative learning algorithm which better handles noisy data, but they also ig- nored events that are not realized as verbal ex- pressions, or events that are verbal expressions but have more than one verb in them. They mainly focused on events that are one-verb expressions.</p><p>In a similar work on inferring tense in English text, <ref type="bibr" target="#b5">Reichart and Rappoport (2010)</ref> aimed at in- ferring fine-grained semantic tenses for events in English. They introduced a fine-grained sense tax- onomy for tense in a more general Tense Sense Disambiguation (TSD) task to annotate and dis- ambiguate semantic tenses. The underlying senses include "things that are always true", "general and repeated actions and habits", "plans, expectations and hopes", etc., which encode a combination of tense, eventuality type and modality. In the corpus that we use, the same information is organized in a more structured manner along three dimensions -semantic tense, eventuality type, and modality. <ref type="figure" target="#fig_0">Figure 1</ref> shows the distant annotation procedure from <ref type="bibr" target="#b7">(Xue and Zhang, 2014)</ref>. Starting with a word-aligned parallel English-Chinese corpus, all sentences are part-of-speech (POS) tagged first and then all verb instances in the English text as well as expressions aligned with verb instances on the Chinese side are targeted for annotation. As we will show in Section 4, these expressions include verbs as well as nouns, prepositions and word sequences "headed" by a verb. We consider those expressions as events. Annotators work only on the English side and tag every event with a pre-defined semantic tense label. These labels are then projected from the English side to the Chi- nese side via word alignments. The resulting cor- pus contains events annotated with semantic tense labels in both languages. Categories for seman- tic tense are "Past", "Present", "Future", "Relative Past", "Relative Present", "Relative Future", and "None".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distant Annotation</head><p>Events annotated with relative tenses are also linked to another event that serves as the tempo- ral reference for the event in question. In some cases the relative tense can be resolved to an ab- solute tense. For example, if an event is anno- tated with a "relative past" tense to a reference event that is annotated with a present tense, then the semantic tense of that event can resolve to an absolute "past" tense. In other cases, they can not be resolved. For example, if an event is la- beled with a "relative future" tense and the refer- ence event has a past tense, then its tense cannot be resolved to an absolute tense, which is defined with regard to the utterance time or document cre- ation time. In our work, where possible, we re- solve these links and keep only absolute tense la- bels. For events with relative tenses that can not be resolved (i.e. events which are "Relative Fu- ture" to "Past" events, or events which are "Rela- tive Past" to "Future" events), we use "None" as the default label.</p><p>Eventuality type and modality are labeled in the same way as auxiliary annotation that can help with the inference of tense. Labels for eventual- ity type include "Episodic", "Habitual", "State", "Progressive", "Completed", and "None". Labels for modality are "Actual", "Intended", "Hypothet- ical", "Modalized", and "None". Readers are ref- ered to <ref type="bibr" target="#b7">(Xue and Zhang, 2014</ref>) for detailed expla- nations of each label. As we mentioned in Section 2, in this corpus not only verbs but also their counterparts on the opposite language are considered as events, yield- ing events that may not be verbs. For example, in the following sentence pair (1), the Chinese verb (VV) "" is aligned with an English noun (NN) "use". In the sentence pair (2), the English verb (VBG) "opening" is aligned with an Chinese noun (NN) "".</p><p>(1) Statistics show that , in the past five years , Guangxi's foreign trade and its use of foreign investments has expanded rapidly.</p><p>(2) Beihai has already become a bright star aris- ing from China's policy of opening up to the outside world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(kai1fang4)</head><p>In this corpus, events could be either one verb, or a verb compound, or a verb sequence "headed" by a verb, or even nouns and words of other parts of speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Xue and Zhang (2014) annotated semantic tense, eventuality type and modality on top of the Par- allel Aligned Treebank ( <ref type="bibr" target="#b1">Li et al., 2012</ref>), a corpus of word-aligned Chinese-English sentences tree- banked based on the Penn TreeBank ( <ref type="bibr" target="#b3">Marcus et al., 1993</ref>) and the Chinese TreeBank ( <ref type="bibr" target="#b8">Xue et al., 2005</ref>) standards. Human annotation of tense is performed on the newswire and webblog sections of this corpus. They report that the average pair- wise agreement among three annotators consis- tently stays above 80% and the average Kappa score consistently exceeds 70%, indicating reli- able annotation.</p><p>Apart from using the entire corpus, we also con- ducted experiments on three different subsets of the corpus. An examination of the data indicates that newswire data is grammatically more for- mal and complete than webblog data, so we also conducted separate experiments on newswire data only. Considering that the diversity of the parts of speech of the events may affect the inference accuracy and that most of our features extracted from the parse trees assume that our events being verbs, we also conducted experiments exclusively on "v events". "v events" consist of two parts. One part is events that are realized as a single word and the word is a verb; the other one is events which have multiple words but there is only one verb among them. In the latter case, we stripped off words tagged with other parts of speech and only keep the verbs as events. This makes it more effective to use features from previous work that are designed for single verbs. One such feature is the aspect marker. Distinctions between newswire and webblog data and between v events and other events are further explored in Section 5.1 and Sec- tion 5.2. For each subset, randomly selected 80% were used as the training set, while 10% were used as the development set and 10% were used as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline</head><p>Based on previous approaches on Chinese tense inference, we used a Maximum Entropy model with extensive feature engineering as our baseline method. We use the implementation of the Maxi- mum Entropy algorithm in Mallet 1 for our exper- iments. The corpus is parsed using the Berkeley Parser for the purpose of extracting structure fea- tures. Since the Parallel Alignment TreeBank is a subset of the Chinese TreeBank (CTB) 8.0, we automatically parsed the CTB 8.0 by doing a 10- fold cross validation. The bracketing F-score is 80.5%. Feature extractions are performed on the automatic parse trees. Adopted features include previous word and its POS tag, next word and its POS tag, aspect marker following the event, following the event, the governing verb of the event, the character string of the main verb in the previous clause that is coordinated with the clause the event is in, whether the event is in quote, and left modifiers of the event including head of adver- bial phrases, temporal noun phrases, prepositional phrases, localizer phrases, as well as subordinat- ing clauses. Readers are referred to <ref type="bibr" target="#b9">(Xue, 2008)</ref> for details of these features. Since in this corpus an event can span over more than one verb, we also use the character string and the POS string of the entire event instead of one word and one POS tag as features.</p><p>• The character string of an event -it could be one or more words. In our corpus, only 69.7% events consist of single word (e.g. " ", "live"), the other 30.3% of the events are expressed with two or more words (e.g. " +", "have caused").</p><p>• The POS string of an event -it could be verbs, nouns, or POS sequences of other word sequences. <ref type="table" target="#tab_1">Table 2</ref> shows the top ten POS tag or POS tag sequences with example word or word sequences.</p><p>Other features that we used in the baseline sys- tem are as follows.</p><p>• DEC -if the word immediately following an event has the POS tag "DEC", use its charac- ter string as a feature. In most cases, "DEC" is the POS tag for "" when it used as a com- plementizer marking the boundary in a rela- tive clause. This feature implies that an event</p><formula xml:id="formula_1">POS freq examples VV 48.2% (live) NN 5.8% (opening) VC 5.2% (is) VV+AS 5.2% +(have caused) VV+DEC 3.2% +(isolated) AD+VV 3.0% +(is suggesting) VA 3.0% (is big) AD 2.0% (seemed) VE 1.9%</formula><p>(there is) P</p><p>1.8% (according to) is inside a relative clause modifying a noun phrase and it is more often stative than even- tive.</p><p>• Determiners -we find the subject of an event from its parse tree and extract the determiner of the subject, if there is one, as a fea- ture. This feature indicates different types of agents, and different types of agents of- ten signal different types of events. For ex- ample, individual agents tend to perform one- time episodic actions which are by default lo- cated in the past or described by a state in the present, while multiple agents tend to in- volved in habitual actions that spans over a long period of time.</p><p>Baseline results are reported in <ref type="table" target="#tab_3">Table 5</ref> and Ta- ble 6, in MaxEnt b rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Eventuality Type and Modality as Features</head><p>Xue and Zhang (2014) reports that gold eventual- ity type and modality labels significantly help the inference of tense in Chinese, improving the ac- curacy by more than 20%. However, it is unreal- istic to expect to have human annotated eventual- ity type and modality labels in a random new data set if we want to use these two sources of implicit linguistic information in any Chinese text. So we trained statistical learning models to automatically extract these two labels. We trained Maximum En- tropy models and ran a 10-fold cross validation on the entire corpus in order to get automatic labels for every event. Feature used for labeling modal- ity are as follows. <ref type="table">Table 3</ref> shows the average ac- curacies for automatic modality labeling.</p><p>• The character string of an event.</p><p>• The POS string of an event.</p><p>• The character string of an event's governing verb and its POS tag.</p><p>• Whether the event is in a conditional clause.</p><p>If an event is in a subtree with the func- tional tag "CND", return "True"; otherwise, return "False". This feature indicates that the event's modality label is "Hypothetical".</p><p>• Whether the event is in a purpose or reason clause. If an event is in a subtree with the functional tag "PRP", return "True"; other- wise, return "False" as a feature. This feature indicates the event's modality label is "In- tended".</p><p>• Whether the event string is the start of a sen- tence. If an event is the start of a sentence, re- turn "True"; otherwise, return "False". Sen- tences that start with an event is often impera- tive, and the event generally has "modalized" modality label.</p><p>dataset v events all events nw 81.1% 81.2% all 75.4% 76.4% <ref type="table">Table 3</ref>: Average modality labeling accuracy, us- ing a 10-fold cross validation.</p><p>Statistics show that the five labels for modality have a skewed distribution in this corpus. Among all events, 67.3% of them fall in the "Actual" cat- egory, while the events of all the other categories are around or less than 10%. Similar distributions are found in all four subsets of the data. Still, com- pared with always choosing the most frequent la- bel (around 67% accuracy), we still get a big im- provement from our statistical model, even though only a very simple set of features are used.</p><p>Features used for labeling eventuality type are as follows. <ref type="table">Table 4</ref> shows the average accuracies for automatic eventuality type labeling.</p><p>• The character string of an event.</p><p>• The POS string of an event.</p><p>• Adverbs on the left that modifies the event.</p><p>• Aspect marker following the event</p><p>• Whether the event is Inside a relative clause.</p><p>If an event is in a CP subtree with the word "" and POS tag "DEC" as its last node, return "True"; otherwise, return "False". Events in relative clauses modifying a noun phrase and tend to be more often stative than eventive.</p><p>dataset v events all events nw 68.7% 67.7% all 65.3% 65.1% <ref type="table">Table 4</ref>: Average automatic eventuality type label- ing accuracy using a 10-fold cross validation.</p><p>The six labels of eventuality type are also dis- tributed unevenly. The first group of columns in <ref type="figure">Figure 3</ref> shows the distribution of all events. Over 65% of events are either "Episodic" or "State", while the other types of events are less than 15%. There are two categories that are even less than 5%. However, even though we only use some sim- ple features, our model still beats the most fre- quent label baseline (around 35% accuracy) by a big margin, as shown in <ref type="table">Table 4</ref>.</p><p>Tense inference accuracies using automatic eventuality type and/or modality features are re- ported in <ref type="table" target="#tab_3">Table 5 and Table 6</ref>, in MaxEnt e, Max- Ent m, and MaxEnt em rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Joint Learning</head><p>Apart from using eventuality type and modality la- bels as features, we also conducted joint learning experiments on them. Joint learning are applied on 1) tense and eventuality type, and 2) tense and modality. Features used are the union of the two sets of features in inferring each single label. Max- Ent jle and MaxEnt jlm rows in <ref type="table" target="#tab_3">Table 5 and Table  6</ref> present the experimental results on joint learn- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Artificial Neural Network</head><p>For each of the experiments using the maximum entropy algorithm, we conducted a neural network experiment using the same setting in order to ex- plore more possible feature combinations and mit- igate the feature engineering work. We convert the features in each of our tense inference meth- ods into feature vectors. If a feature is not a word, we use a one-hot representation for that feature (a vector with all 0s except for a 1 at the place of the feature's index in our feature lexicon). If a feature is a word, we convert it into a word embedding. To get a dictionary of word embeddings, we use the word2vec tool 2 ( <ref type="bibr" target="#b4">Mikolov et al., 2013)</ref> and train it on the Chinese Gigaword corpus (LDC2003T09). For each word embedding, a 300-dimensional vec- tor is used. Artificial neural networks are built us- ing the theano package 3 <ref type="figure" target="#fig_0">(Bergstra et al., 2010)</ref>. We use 5000 hidden units for all networks and set the learning rate α = 0.01. Experimental results are presented in the ANN rows of <ref type="table" target="#tab_3">Tables 5 and 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results Analysis</head><p>A comparison of the baseline accuracy for the four different subsets of the data shows that (1) tense inference is slightly better on v events than on all events, but the difference is not substantial; and (2) tense inference on newswire data performs bet- ter than on all data by around 8% on v events and around 5% on all events, verifying our assump- tion that automatic tense inference is easier on newswire data than webblog data. Although our experiments are performed on different data sets from that of previous work, our baseline method still shows strong results compared with previous work <ref type="bibr" target="#b9">(Xue, 2008;</ref><ref type="bibr" target="#b10">Ye et al., 2006;</ref><ref type="bibr" target="#b2">Liu et al., 2011</ref>).</p><p>Adding automatic eventuality type and modal- ity labels as features for semantic tense inference leads to improvements over the baseline on all four data subsets. In fact they provide considerable improvements (around 2% increase) on newswire v events dataset. MaxEnt e rows report results when only automatic eventuality type is added as a feature, and MaxEnt m rows report results when only automatic modality is added as a fea- ture. They both outperform (or, in several datasets, match) the baseline results on all datasets. Max- Ent em rows report results when both automatic linguistic labels are added as features, and they show further improvements over when only one source of information is used. Analysis of the results shows again that tense inference accuracy is higher than webblog data under this experi- ment condition. The results also show that after adding eventuality type and modality as features,  the improvements on v events (0.7% and 1.8%) are much bigger than that on all events (0.2% and 0.4%), regardless of the data genre (newswire or weblog).</p><p>In order to test the potential for these two new features, we also conducted experiments using gold eventuality type and/or modality labels as features for the Maximum Entropy models <ref type="table" target="#tab_3">(Table  5 and Table 6</ref>, MaxEnt ge, MaxEnt gm, and Max- Ent gem rows.). They outperform our best Max- Ent results by around 10% on newswire data and around 15% on all data, indicating strong poten- tials for more accurately classified automatic even- tuality type and modality labels.</p><p>Results also show that joint learning with modality proves to be working better than the baseline <ref type="table" target="#tab_3">(Table 5 and Table 6</ref>, MaxEnt jle, Max- Ent jlm). In fact, on the datasets with all events, joint learning with modality produces the highest accuracy among all approaches. However, joint learning with eventuality is even worse than the baseline. One possible explanation is that the lower eventuality type classification accuracy af- fects the tense inference accuracy. We also believe there is still room for improvement with features tuned for the joint learning model. Simply adding the features may not be the best strategy.</p><p>On the entire dataset, regardless of v events or other events, results of the neural network models show improvements over the maximum entropy</p><note type="other">method all data nw data MaxEnt b 59.7% 65.1% MaxEnt e 59.9% 65.1% MaxEnt m 59.9% 65.4% MaxEnt em 59.9% 65.5% MaxEnt jle 59.7% 62.7% MaxEnt jlm 60.4% 65.6% MaxEnt ge 75.3% 76.1% MaxEnt gm 67.1% 69.0% MaxEnt gem 76.2% 75.9% ANN b 63.0% 64.0% ANN e 63.2% 66.9% ANN m 60.1% 64.7% ANN em 57.8% 66.1% ANN jle 61.4% 63.0% ANN jlm</note><p>62.9% 63.5% <ref type="table">Table 6</ref>: Accuracy of tense inference on all events. Best performances for each group of methods are in bold.</p><p>models under most experimental conditions. A clear trend is that artificial neural networks help more on all data than on newswire data only, in- dicating greater potentials of the neural network models to select and combine features with care- fully trained parameters, given noisier but larger training sets. Experimental results also show significant dif- ferences in accuracy between newswire data and webblog data, and smaller but still recognizable difference between v events and all events. There- fore, we specifically look into distinctions be- tween these data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Newswire Data vs. Webblog Data</head><p>Considering the big gap in accuracy between newswire and webblog data in our baseline results, we delve deeper into the data and found several major distinctions between these two domains that might have contributed to the rather significant dif- ference in performance on tense inference. First, we look into the word frequency distribution of the two datasets. Here by "word" we mean the char- acter string of an event. We find that both datasets have a small portion of words with high frequen- cies, but the webblog dataset contains much more low-frequent words than the newswire dataset. In <ref type="figure">Figure 2</ref>, the x-axis shows possible frequencies of words and the y-axis shows the number of words at a particular frequency. It can be seen that the num-ber of words that appear only once in the webblog dataset is about three times as large as that in the newswire dataset. The entire newswire dataset has a vocabulary of only 2671 entries, while the web- blog dataset has a vocabulary size of 6117. This greatly reduces the coverage of features extracted from the training dataset on the events in the test dataset.</p><p>Second, webblog data contains more events that are "inherently" ambiguous on temporal lo- cation. Among four possible labels for tense in this corpus, "None" is for events whose tem- poral locations are not clear even to human an- notators. Statistics show that in webblog data about 13.4% of the events are tagged as "None", while in newswire data only around 6.7% are "None". Another piece of evidence showing web- blog data is harder to process is the different inter- annotator agreement scores for tense annotation on newswire and webblog data reported by <ref type="bibr" target="#b7">(Xue and Zhang, 2014)</ref>. Newswire data has a 89.0% agreement score and a 84.9% kappa score, while webblog data only has a 81.0% agreement score and a 72.7% kappa. Third, automatic parse trees for newswire data is also more accurate than that for webblog data. The bracketing F-score of au- tomatically parsed newswire data is 83.0% while it is only 80.4% for weblog data. Moreover, sen- tences in newswire data are more grammatically complete. Analysis shows that webblog data has more dropped constituents in sentences. There are around 40.5% sentences in newswire data that have nominal empty categories, while in webblog data the number is 48.1%. Dropped constituents affect the structures of parse trees and some of the features, which can affect tense inference ac- curacy. Figure 2: Word frequency distribution in newswire and webblog datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">V events vs. All Events</head><p>In our definition, v events are (1) events that are single verbs (example 1, 3, 7, 9 in <ref type="table" target="#tab_1">Table 2)</ref>, and (2) events that are multi-word sequences but only one word among them is a verb and any non-verb words are stripped off (verbs in example 4, 5, 6 in <ref type="table" target="#tab_1">Table 2</ref>). Conversely, events that do not fall into this definition include (1) events that have no verbs in their surface form (example 2, 8, 10 in <ref type="table" target="#tab_1">Table 2)</ref>, and (2) events that have more than one verb in their surface form (e.g. "+(shi3+cheng2wei2)", VV+VV, "make it become"). So from the point of view of a statistical learning algorithm, ev- ery v event has one and only one verb. This makes sure that all features that we used are ap- plicable to v events. For other events, however, some features may be not applicable. For ex- ample, for an event which has a nominal expres- sion, aspect marker, DER, and DEC features are all "None" because these features are only appli- cable to verbs. Another major distinction between v events and "other events" is that the distributions of eventuality type labels on them are very differ- ent, presented in the second and third groups of columns in <ref type="figure">Figure 3</ref>. There is a rather high per- centage of "State" among "other events" and very low percentage of "Completed" and "None". The highly uneven distribution of eventuality type la- bels make it less effective as a feature for tense inference.  <ref type="figure">Figure 3</ref>: Statistics of eventuality types on differ- ent events.</p><p>We also find that, on newswire datasets, max- imum entropy models and neural network mod- els do not show much difference in performance. To understand this result better, we plot learn- ing curves of the artificial neural network model, trained and tested on newswire v events dataset. In <ref type="figure" target="#fig_2">Figure 4</ref>, the black line represents the error rate on training set, and the grey line represents the er-ror rate on test set. As the size of training data grows, the error rate on the training set gets larger because with more training examples the training set becomes noisier and it gets harder to model all samples with the same number of features; and the error rate on the test set gets smaller because a big- ger training set reduces the data sparsity and trains the parameters better. Both lines end at a rather high error rate (around 30%, i.e. only around 70% in accuracy) which means the current network is general enough to cover most cases in the test set, but it is under-fitting the training data. The cur- rent model is not specific enough to better cap- ture the fine distinctions between the tense cat- egories. The black line being not very smooth is also understandable, given that there are only around 6000 training examples in the newswire v events dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head><p>In order to get a better understanding of the use of eventuality type and modality, we look into the error rates for each error type in greater detail. In <ref type="table" target="#tab_6">Table 7</ref>, "Pa" stands for "Past", "Pre" is short for "Present", "Fu" is for "Future", and "No" is "None". For each error type, the left-hand side is the gold-standard tense, and the right-hand side is the wrongly assigned label. Statistics are col- lected on the newswire v events data test set. Ta- ble 7 compares the different error types between the baseline method and the MaxEnt em method, the best approach for this dataset. We can see that (1) "Present" and "Past" is the most frequently confused tense pair, and (2) eventuality type and modality information help disambiguate "Present" and "Past" events greatly, and reduce the errors due to mis-classifying "Past" as "Future", or "Fu- ture" as "Present", or "None" as "Present".  A closer examination of the sentences in which events are assigned the wrong tense reveals that "Pre → Pa" error is prone to occur on events in relative clauses. The Chinese verb implies a past episodic event, while the event is actually a present state or habitual event. As a good example, the "(sheng1chan3)" event in Sentence (3) is wrongly labeled as "Past" by MaxEnt b but cor- rectly classified as "Present" by MaxEnt em with eventuality type "Habitual" and modality tag "Ac- tual" (the underlined part in the Chinese sentence is the relative clause). It is also found that most "Pa → Pre" errors occur on events that are more stative. It is reasonable since classifiers tend to assign "Present" to states and "Past" to episodic events. MaxEnt em managed to correct some with "episodic" as their correct eventuality type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>error type MaxEnt b MaxEnt em</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) (sheng1chan3)</head><p>At present , the Pu Kang Company , which produces the vaccine in this zone , has already formed a production scale of 5 million doses per year , which has great sig- nificance in effectively controlling the hepati- tis A epidemic .</p><p>We are also surprised to see that over 2% "Past" events are classified as "Future" events, ranking third among all error types. This mistake seems very unlikely, but it is still possible when per- forming tense inference on a language with no grammatical tense at all. Take the following sen- tence pair (4) as an example. In the Chinese sen- tence, MaxEnt b classifies " (tao3lun4)" as "Future" because there is no grammatical indica- tor in the Chinese sentence implying that the "dis- cussion" has already happened and it is reason- able to assume the "discussion" is in the near fu- ture. However, with eventuality type "Episodic" and modality label "Actual", MaxEnt em classi- fies it as "Past" correctly, because episodic events tend to occur in the past and future events tend to get "Intended" or "Hypothetical" modality labels.</p><p>(4) " (tao3lun4) "</p><p>He also said, the French government "even directed its representative not to vote Yes when the Security Council discussed the res- olution on sanctions on Cuba".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we address the problem of automatic inference of Chinese semantic tense. We took ad- vantage of a new corpus annotated with rich lin- guistic information, and experimented with three approaches. In the first approach, we use two sources of implicit linguistic information, even- tuality type and modality, automatically derived, as features in tense inference. We then conducted joint learning on tense and each of these two infor- mation types. Finally, we experimented with using artificial neural networks to train models for tense prediction. All three approaches outperformed a strong baseline, a maximum entropy model with extensive engineering. Our future work will in- clude exploring ways to improve automatic even- tuality type and modality labeling accuracy to fur- ther improve tense inference accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distant annotation procedure.</figDesc><graphic url="image-1.png" coords="3,309.66,152.85,213.50,136.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning curves of the artificial neural network model, trained and tested on newswire v events dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 presents</head><label>1</label><figDesc></figDesc><table>the statistics for each 
subset of the experimental data. 

dataset # of v events # of all events 
nw 
6,686 
8,268 
all 
17,153 
20,885 

Table 1: Statistics of four subsets of the annotated 
corpus (Chinese side). "nw" denotes the newswire 
data. "v events" denotes events that consist of or 
can be reduced to only a single verb. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Frequencies and examples of the ten most 
frequent POS tag or POS tag sequences for events 
in our corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of tense inference on v events. Best performances for each group of methods are in bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Tense inference error rates for different 
error types on newswire v events test set. 

</table></figure>

			<note place="foot" n="1"> http://mallet.cs.umass.edu/</note>

			<note place="foot" n="2"> http://code.google.com/p/word2vec/ 3 http://deeplearning.net/software/theano/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the three anonymous re-viewers for their suggestions and comments. This work is supported by the National Science Foun-dation via Grant No. 0910532 entitled "Richer Representations for Machine Translation". All views expressed in this paper are those of the au-thors and do not necessarily represent the view of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallel Aligned Treebanks at LDC: New Challenges Interfacing Existing Infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safa</forename><surname>Ismael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC-2012</title>
		<meeting>LREC-2012<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from chinese-english parallel data for chinese tense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Natural Language Processing</title>
		<meeting>the 5th International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tense sense disambiguation: A new syntactic polysemy task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal interpretation in Mandarin Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlota</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Erbaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="756" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Buy one get one free: Distant annotation of chinese tense, event type, and modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC-2014</title>
		<meeting>LREC-2014<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Fu Dong Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic Inference of the Temporal Location of Situations in Chinese Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-2008</title>
		<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent features in automatic tense translation between Chinese and English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><forename type="middle">Li</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automatic Tense and Aspect Translation between Chinese and English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
