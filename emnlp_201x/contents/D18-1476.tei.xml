<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chargrid: Towards Understanding 2D Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><forename type="middle">R</forename><surname>Katti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SAP SE Machine Learning R&amp;D</orgName>
								<address>
									<country>Berlin Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Reisswig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SAP SE Machine Learning R&amp;D</orgName>
								<address>
									<country>Berlin Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordula</forename><surname>Guder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SAP SE Machine Learning R&amp;D</orgName>
								<address>
									<country>Berlin Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Brarda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SAP SE Machine Learning R&amp;D</orgName>
								<address>
									<country>Berlin Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SAP SE Machine Learning R&amp;D</orgName>
								<address>
									<country>Berlin Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Höhne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SAP SE Machine Learning R&amp;D</orgName>
								<address>
									<country>Berlin Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Baptiste</forename><surname>Faddoul</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SAP SE Machine Learning R&amp;D</orgName>
								<address>
									<country>Berlin Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chargrid: Towards Understanding 2D Documents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4459" to="4469"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4459</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a seg-mentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Textual information is often represented through structured documents which have an inherent 2D structure. This is even more so the case with the advent of new types of media and communica- tions such as presentations, websites, blogs and formatted notebooks. In such documents, the lay- out, positioning, and sizing might be crucial to un- derstand its semantic content and provide a strong guidance to the human perception.</p><p>NLP addresses the task of processing and un- derstanding natural language texts through sub- tasks like language modeling, classification, in- formation extraction, summarization, translation, and question answering among others. NLP meth- ods typically operate on serialized text, which is a 1D sequence of characters. Such methods have been proven very successful for various tasks on unformatted text (e.g. books, reviews, news arti- cles, short text snippets). However, when process- ing structured and formatted documents in which the relation between words is impacted not only by the sequential order, but also by the document layout, NLP can result in significant shortcomings. * Equal contribution Computer vision algorithms, on the other hand, are designed to exploit 2D information in the vi- sual domain. Images are commonly processed with convolutional neural networks ( <ref type="bibr" target="#b14">LeCun et al., 1998;</ref><ref type="bibr" target="#b12">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b21">Ren et al., 2015b;</ref><ref type="bibr" target="#b18">Pinheiro et al., 2016</ref>) (or likes) that preserve and exploit the 2D correlation between neighboring pixels. While it is feasible to convert structured documents into images and then apply computer vision algorithms, this approach is not optimal for understanding their semantics as it is driven mostly by the visual content and not by the textual content. As a result, a machine learning model would first need to extract the text from the image followed by learning the semantics. This purely visual approach requires a more complex model and significantly larger training data compared to text-based approaches.</p><p>We propose a novel paradigm for processing and understanding structured documents. Instead of serializing a document into a 1D text, the pro- posed method, named chargrid, preserves the spa- tial structure of the document by representing it as a sparse 2D grid of characters. We then formu- late the document understanding task as instance- level semantic segmentation on chargrid. More precisely, the model predicts a segmentation mask with pixel-level labels and object bounding boxes to group multiple instances of the same class. We apply the chargrid paradigm on an information ex- traction task from invoices and demonstrate that this method is superior to both, state-of-the-art NLP algorithms as well as computer vision algo- rithms.</p><p>The rest of the document is organized in three parts: we first introduce the chargrid paradigm; we then describe a specific application of infor- mation extraction from documents; finally, we present experimental results and conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>NLP focuses on understanding natural language text through tasks like classification <ref type="bibr" target="#b11">(Kim, 2014</ref>), translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>), summarization <ref type="bibr" target="#b23">(Rush et al., 2015)</ref>, and named entity recognition ( <ref type="bibr" target="#b13">Lample et al., 2016)</ref>. Such methods expect unfor- matted text as input and, therefore, do not assume any intrinsic 2D structure. Document Analysis, on the other hand, deals largely with problems such as recognizing printed/hand-written characters from a variety of documents ( <ref type="bibr" target="#b6">Graves and Schmidhuber, 2009)</ref>, pro- cessing document images for document localiza- tion (Javed and Shafait, 2018), binarization <ref type="bibr" target="#b25">(Tensmeyer and Martinez, 2017)</ref>, and layout segmenta- tion ( <ref type="bibr" target="#b2">Chen et al., 2015)</ref>. As a result, it does not fo- cus on understanding the character-and/or word- level semantics of the document the same way as NLP.</p><p>Within computer vision, problems such as scene text detection and recognition <ref type="bibr" target="#b5">(Goodfellow et al., 2013)</ref>, semantic segmentation ( <ref type="bibr" target="#b0">Badrinarayanan et al., 2017</ref>) as well as object detection <ref type="bibr" target="#b7">(Gupta et al., 2014;</ref><ref type="bibr">Lin et al., 2017)</ref> can be considered as related problems to ours, but applied on a different domain, i.e., processing natural images instead of documents as input.</p><p>The closest to our work is <ref type="bibr" target="#b28">Yang et al. (2017)</ref> that performs pixel-wise layout segmentation on a structured document, using sentence embeddings as additional input to an encoder-decoder network architecture. For each pixel inside the area of a sentence, the sentence embedding is appended to the visual feature embedding at the last layer of the decoder. The authors show that the layout seg- mentation accuracy can be significantly improved when using the textual features. Another related work is <ref type="bibr" target="#b16">Palm et al. (2017)</ref> which extracts key- value information from structured documents (in- voices) using a recurrent neural network (RNN). Their work addresses the problem of document understanding, however, the RNN operates on se- rialized 1D text.</p><p>Combining approaches from computer vision, NLP, and document analysis, our work is the first to systematically address the task of understanding 2D documents the same way as NLP while still retaining the 2D structure in structured documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Document Understanding with Chargrid</head><p>A human observer comprehends a document by understanding the semantic content of characters, words, paragraphs, and layout components. We encapsulate all such tasks under the common um- brella of document understanding. Therefore, we can formulate this problem as an instance segmen- tation task of characters on the page. In the fol- lowing sections, we describe a new approach for solving that task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Chargrid</head><p>Chargrid is a novel representation of a document that preserves its 2D layout. A chargrid can be constructed from character boxes, i.e., bounding boxes that each surround a single character some- where on a given document page. This positional information can come from an optical character recognition (OCR) engine, or can be directly ex- tracted from the layout information in the docu- ment as provided by, e.g., PDF or HTML. The coordinate space of a character box is defined by page height H and width W , and is usually mea- sured in units of pixels.</p><p>The complete text of a document page can thus be represented as a set of tuples D = {(c k , b k ) | k = 0, ..., n}, where c k denotes the k-th character in the page and b k the associated character box of the k-th character, which is for- malized by the top-left pixel position, width and height, thus</p><formula xml:id="formula_0">b k = (x k , y k , w k , h k ).</formula><p>We can now construct the chargrid g ∈ N H×W of the original document page, and its character- pixel g ij from the set D with</p><formula xml:id="formula_1">g ij = E(c k ) if (i, j) b k 0 (1)</formula><p>where means 'overlaps with', and where each point (i, j) corresponds to some pixel in the origi- nal document page pixel coordinate space defined by (H, W ). E(c k ) is some encoding of the char- acter in the k-th character box, i.e. the value of character c k may be mapped to a specific integer index. For instance, we may map the alphabet (or any character of interest) to non-zero indices {a, b, c, ...} → {1, 2, 3, ...}. Note that we assume that character boxes cannot overlap, i.e. each char- acter on a document page occupies a unique region on the page. In practice, it may happen that the corners and edges of a character box may overlap with other closeby characters. We solve such cor- ner cases by assigning the character-pixel to the box that has the closest box center. In other words, the chargrid representation is constructed as follows: for each character c k at location b k , the area covered by that character is filled with some constant index value E(c k ). All remaining character-pixels corresponding to empty regions on the original document page are initialized with 0. <ref type="figure" target="#fig_0">Figure 1</ref> visualizes the chargrid representation of an example input document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw document Chargrid</head><p>The advantage of the new chargrid representa- tion is twofold: (i) we directly encode a character by a single scalar value rather than by a granular collection of grayscale pixels as is the case for im- ages, thus making it easy for the subsequent doc- ument analysis algorithms to understand the doc- ument, and (ii), because the group of pixels that belonged to a given character are now all mapped to the same constant value, we can significantly downsample the chargrid representation without loss of any information. For instance, if the small- est character occupied a 10×10 pixel region in the original document, we can downsample the char- grid representation by a factor of 10×10. This sig- nificantly reduces the computational time of sub- sequent processing steps, such as training a ma- chine learning model on this data representation.</p><p>We point out that a character can occupy a re- gion spanning several character pixels. This is in contrast to traditional NLP where each character is represented by exactly one token. In our represen- tation, therefore, the larger a given character, the more character-pixels represent that single charac- ter. We do not find this to be a problem, instead, it even helps since it implicitly encodes additional information, for example, the font size, that would otherwise not be available.</p><p>Before the chargrid representation is used as in- put to, e.g., a neural network (see Sec. 3.2), we apply 1-hot encoding to the chargrid g. Thus, the original chargrid representation g ∈ N H×W be- comes a vector representatioñ g ∈ R H×W ×N C , where N C denotes the number of characters in the vocabulary including a padding/background character (in our case this is mapped to 0) and an unknown character (all characters that are not mapped by our encoding E will be mapped to this character).</p><p>We note that similar to using characters for con- structing the chargrid, one can also use words to construct a wordgrid in the same way. In that case, rather than using 1-hot encoding, one may use a word embedding like word2vec or GloVe. While the construction of a wordgrid seems straight- forward, we have not experimented with it in the present work as our dataset contains too many unusual words and spans multiple languages (see Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>We use the 1-hot encoded chargrid representatioñrepresentatioñ g as input to a fully convolutional neural network to perform semantic segmentation on the chargrid and predict a class label for each character-pixel on the document. As there can be multiple and an unknown number of instances of the same class, we further perform instance segmentation. This means, in addition to predicting a segmentation mask, we may also predict bounding boxes using the techniques from object detection. This allows the model to assign characters from the same seg- mentation class to distinct instances.</p><p>Our model is described in <ref type="figure">Figure 2</ref>. It is com- prised of two main parts: The encoder network and the decoder network. The decoder network is further made up of two branches: The seg- mentation branch and the bounding box regression branch. The encoder boils down to a VGG-type network <ref type="bibr" target="#b24">(Simonyan and Zisserman, 2014</ref>) with di- lated convolutions ( <ref type="bibr" target="#b29">Yu and Koltun, 2016)</ref>, batch normalization ( <ref type="bibr" target="#b9">Ioffe and Szegedy, 2015)</ref>, and spa- tial dropout ( <ref type="bibr" target="#b27">Tompson et al., 2015)</ref>. Essentially, the encoder consists of five blocks where each block consists of three 3 × 3 convolutions (which  Figure 2: Network architecture for document understanding, the chargrid-net. Each convolutional block in the network is represented as a box. The height of a box is a proxy for feature map resolution while the width is a proxy for the number of output channels. C corresponds to the number of 'base' channels, which in turns corresponds to the number of output channels in the first encoder block. d denotes dilation rate.</p><p>themselves are made of convolution, batch nor- malization, ReLU activation) followed by spatial dropout at the end of a block. The first convolution in a block is a stride-2 convolution to downsample the input to that block. Whenever we downsam- ple, we increase the number of output channels C of each convolution by a factor of two. We have found stride-2 convolutions to yield slightly bet- ter results compared to max pooling. In block four and five of the encoder, we do not apply any down- sampling, and we leave the number of channels at 512 (the first block has C = 64 channels). We use dilated convolutions in block three, four, five with rates d = 2, 4, 8, respectively.</p><p>The decoder for semantic segmentation and for bounding box regression are both made of con- volutional blocks which essentially reverse the downsampling of the encoder via stride-2 trans- posed convolutions. Each block first concatenates features from the encoder via lateral connections followed by 1×1 convolutions ( <ref type="bibr" target="#b22">Ronneberger et al., 2015;</ref><ref type="bibr" target="#b18">Pinheiro et al., 2016</ref>). Subsequently, we up- sample via a 3×3 stride-2 transposed convolution. This is followed by two 3 × 3 convolutions. Note that whenever we upsample, we decrease the num- ber of channels by a factor of two.</p><p>The two decoder branches are identical in archi- tecture up to the last convolutional block. The de- coder for semantic segmentation has an additional convolutional layer without batch normalization, but with bias and with softmax activation. The number of output channels of the last convolution corresponds to the number of classes. Together with the encoder, the decoder for the bounding box regression task forms a one-stage detector which makes use of focal loss ( <ref type="bibr">Lin et al., 2017</ref>). We also make use of the anchor box representation and corresponding bounding box regression targets as discussed in <ref type="bibr" target="#b20">Ren et al. (2015a)</ref>. The anchor-box representation allows us to handle bounding boxes that vary widely with respect to size and aspect ra- tios. Moreover, the anchor-box representation al- lows us to detect boxes of different classes. The number of output channels are 2N a for the box mask (foreground versus background) and 4N a for the four box coordinates, where N a is the num- ber of anchors per pixel. The weights of all layers are initialized following <ref type="bibr" target="#b8">He et al. (2015)</ref>, except for the last ones, which are initialized with a small constant value 1e − 3 for stabilization purposes.</p><p>In total, we have three equally contributing loss terms:</p><formula xml:id="formula_2">L total = L seg + L boxmask + L boxcoord ,<label>(2)</label></formula><p>where L seg is the cross entropy loss for segmen- tation, e.g. ( <ref type="bibr" target="#b22">Ronneberger et al., 2015)</ref>, L boxmask is the binary cross entropy loss for box masks, and L boxcoord is the Huber loss for box coordi- nate regression <ref type="bibr" target="#b20">(Ren et al., 2015a</ref>). Both cross en- tropy loss terms are augmented following the fo- cal loss idea ( <ref type="bibr">Lin et al., 2017</ref>). We also make use of aggressive static class weighting in both cross entropy loss terms mainly to counter the strong class imbalance between irrelevant and "easy" pixels (the "background" class) versus relevant and "hard" pixels (all other classes; see Sec. 4.2 for further details). We refer to the network depicted in <ref type="figure">Figure 2</ref> as the chargrid-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Information Extraction from Invoices</head><p>As a concrete example for understanding struc- tured 2D documents, we extract key-value infor- mation from invoices. We make no assumption on the format of the invoice, the country of origin (and consequently taxes, date formats, amount for- mats, currency etc.) or the language. In addition to that, real-world invoices often contain incomplete sentences, nouns, and abbreviations. We want our model to parse an invoice and to extract 5 header fields (i.e., Invoice Number, In- voice Date, Invoice Amount, Vendor Name and Vendor Address) as well as the list of product items purchased, referred to as the line-items. Line-items include details for each item such as Line-item Description, Line-item Quantity and Line-item Amount. Together with the background class, this yields 9 classes and each character on the invoice is associated to exactly one class. We note that while header fields may only appear once on an invoice (are unique), line-items may occur in multiple instances.</p><p>To extract the values for each field, we collect all characters that are classified as belonging to the corresponding class. For line-items, we further group the characters by the predicted item bound- ing boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>Our invoice dataset consists of 12k scanned sam- ple invoices from a large variety of different ven- dors and languages. We assign 10k samples for training, 1k for validation and 1k for test on which we report our results (see Sec. 5). We ensure that vendors that appear in one set do not appear in any other set. This is more restrictive than nec- essary but gives a good estimate on how well the model generalizes to unseen invoice layouts. <ref type="figure" target="#fig_2">Fig- ure 3</ref> shows the distribution over vendors and lan- guages. From the vendor distribution, it can be seen that most vendors contribute only one invoice to the dataset with at most 6 invoices coming from a single vendor. From the language distribution, it can be seen that while English is the predomi- nant language, there are large representations from French, Spanish, Norwegian, and German.</p><p>For all invoices, we collected manual annota- tions with bounding boxes around the fields of in- terest. Considerable efforts were spent to ensure that the labels are correct. In particular, each in- voice was analyzed by three annotators plus a re- viewer. Over 35k invoices were investigated and finally only clean set of 12k invoices were se- lected. Furthermore, detailed instructions were given to the annotators for each field. <ref type="figure" target="#fig_3">Figure 4</ref> visualizes the locations of annotated boxes on in- voices for Invoice Amount and Line-item Quan- tity. It can be seen that while some regions are denser, the occurrences are spread widely over the whole invoice thereby illustrating the diversity in the invoice format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The invoices were processed with an open-source engine for OCR, Tesseract (v4), to extract the text from the documents.</p><p>We limit the number of distinct characters in our encoding E (in Eq. 1) to the most frequent ones, which in our present case means N C = 54 differ- ent characters including the background/padding and unknown character. Thus, the 1-hot encoded chargrid˜gchargrid˜ chargrid˜g has 54 channels. Each page of an in- voice is processed independently.</p><p>Once we created the chargrid representation of a document page, we downsample to a fixed resolu- tion using nearest neighbor interpolation to ensure that all chargrid representations have the same res- olution for training (and inference). Note that this downsampling operation is still performed in the token space (i.e., before 1-hot encoding). To fur- ther ensure that no tokens in the chargrid repre- sentation are lost due to nearest neighbor interpo- lation, we downsample to twice the target resolu- tion. This resolution is determined by the smallest characters in the training set. A second downsam- pling step, now in the 1-hot encoding to the final target resolution of 336x256 in our case, is per- formed using bilinear interpolation and fed into the network. Note that we could have also first performed 1-hot encoding on the document and applied bilinear interpolation directly to the target resolution. Computationally, however, this two- stage downsampling is more efficient.</p><p>We handle landscape documents by simply squeezing all input pages into our target resolution using interpolation (similar to image re-sizing). We have found that this approach is not harmful and for simplicity stick to it.</p><p>Line-items can occur in an unknown number of distinct instances. Therefore, we require instance segmentation of characters on the document. To accomplish this, the model is trained to predict bounding boxes that span across the entire row of one instance of a line-item, while the segmen- tation mask classifies those characters belonging to given column classes (such as, e.g., Line-item Quantity, or Line-item Description) of that line- item instance.</p><p>We implemented our model in TensorFlow 1.4. We use SGD with momentum β = 0.9 and learn- ing rate α = 0.05. We used weight decay of λ = 10 −4 , and spatial dropout with probability P = 0.1. We perform random cropping of the chargrid for data augmentation (that is we pad by 16 character pixels in each direction after down- sampling and then crop with a random offset in range <ref type="figure" target="#fig_0">(16, 16)</ref>).</p><p>We use aggressive class weighting in the cross- entropy loss for semantic segmentation and for the bounding box mask. We have found this to be more effective than the focal loss (which can bee seen as a form of dynamic class weight- ing). We implement class weighting following <ref type="bibr" target="#b17">(Paszke et al., 2016</ref>) with a constant of c = 1.04. In early stages of our experiments not yet using class weights, we noted poor performance on the bounding box regression task as well as the seg- mentation task.</p><p>The distribution of line-items on invoices in our dataset reveals that around 50% of all in- voices only contain less than three line-items. We found that repeating those invoices with more than three line-items during training more often than those with only few line-items significantly boosts bounding box regression accuracy. With a mini batch size of 7, each model took around 360k it- erations and 3 days to fully converge on a single Nvidia Tesla V100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Measure</head><p>For evaluating our model, we would like to mea- sure how much work would be saved by using the extraction system, compared to performing the field extraction manually. To capture this, we use a measure similar to the word error rate <ref type="bibr" target="#b19">(Prabhavalkar et al., 2017</ref>) used in speech recognition or translation tasks. For a given field, we count the number of in- sertions, deletions, and modifications of the pre- dicted instances (pooled across the entire test set) to match the ground truth instances. Evaluations are made on the string level. We compute this measure as follows:</p><formula xml:id="formula_3">1 − #[insertions] + #[deletions] + #[modifications] N</formula><p>where N is the total number of instances occurring in the ground truth of the entire test set. This mea- sure can be negative, meaning that it would be less work to perform the extraction manually. In our present case, the error caused by the OCR engine does not affect this measure, because the same er- rors are present in the prediction and in the ground truth and are not considered a mismatch.  Figure 6: In example 1, chargrid-net attributes unrelated rows of text to the line-item's descrip- tion. Moreover, some character-pixels are mis- classified such that the predicted header field above the line-item is incorrect. Thus, we ob- serve errors in both, the segmentation mask and the bounding box predictions. In example 2, the model predicts the segmentation mask mostly cor- rect, but it fails to predict boxes for two of the four line-items. In example 3, the model fails to separate adjacent multi-row line-items from each other correctly. Please note that the ground truth is debatable in some cases; for example, it may be hard to decide which information belongs to a line-item's description and which does not. forms the sequential model on multi-instance or multi-word fields where 2D relationships between text entities are important. Examples of such fields are Line-item Description, Line-item Quantity and Line-item Amount, which are all grouped as sub- fields to a specific line-item. Each of those fields may span a varying number of rows per line-item, see <ref type="figure" target="#fig_4">Figure 5</ref>. The sequential model fails to cor- rectly identify those line-item fields which is man- ifested in a negative accuracy measure (Sec. 4.3). This implies that it is better to perform manual extraction over using automatic extraction. This is understandable since the line-item fields have a strong 2D structure that the sequential approach is not designed to capture.</p><p>In comparison with the image-only model, chargrid-net still performs much better. This is es- pecially true for smaller fields which need to be read to be accurately localized. On the other hand, for larger fields like Line-item Description, which can be localized by only vision, the gap is much smaller.</p><p>The values for the hybrid models are a bit more interesting. One could expect that combining two complementary inputs such as the chargrid repre- sentation and image -one capturing the content and the other capturing, e.g. table delineations - would boost the accuracy. It turns out, however, that at least in the case of our invoice dataset, the additional image encoder does not bring additional benefits. Model chargrid-hybrid-C64, where the chargrid encoder branch has the same number of base channels C = 64 as the original chargrid-net, is essentially as accurate as chargrid-net. Model chargrid-hybrid-C32, where the image and char- grid encoder combined have the same number of channels (that is the chargrid encoder branch only has C = 32 base channels), the accuracy is signif- icantly reduced.</p><p>We conclude that at least in our present extrac- tion problem, most of the discriminative informa-tion comes from the chargrid encoder branch and thus from the chargrid representation.</p><p>Error examples of the chargrid-net model are depicted in <ref type="figure">Figure 6</ref>. One frequent error-type is that the model may fail to disentangle line-items, if they have a peculiar structure. While this is also challenging for human experts, other erroneous predictions are observed in samples for which the ground truth annotations are debatable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We introduce a new way of modeling documents by using a character grid as document representa- tion. The chargrid allows models to capture 2D re- lationships between characters, words, and larger units of text. The idea of the chargrid paradigm is inspired by the human perception which is heav- ily guided by 2D shapes and structures for under- standing this type of documents. Therefore, the chargrid allows to encode the positioning, size and alignment for textual components in a meaningful manner. While the chargrid paradigm could be ap- plied to various kinds of NLP tasks, we demon- strate its potential on an information extraction task from invoices. We train a deep neural net- work with an encoder-decoder architecture and we show that the network computes accurate segmen- tation masks and bounding boxes, which pinpoint the relevant information on the invoice.</p><p>We evaluate the accuracy of the model and com- pare it to state-of-the-art NLP and computer vi- sion approaches. While those baseline models achieve accurate predictions for individual fields, only the chargrid performs well on all informa- tion extraction tasks. Some fields such as Invoice Number are relatively easy to detect for a model that operates on serialized text, as discriminative keywords are commonly preceding the words to be extracted. However, more complex extraction tasks (e.g. Vendor Address or Line-item Quantity) cannot be performed accurately as they require to exploit both, textual components and 2D layout structures. The traditional image-only computer vision model yields accurate predictions only for large visual columns (such as Line-item Amount) and it fails to locate extractions that require to un- derstand the text.</p><p>However, compared to traditional sequential neural NLP models, the benefits in accuracy come at a larger computational cost compared. Even though the proposed model is fully convolutional and parallelizes very well on a single (or even mul- tiple) GPU(s), the added complexity introduced by using a 2D data representation can significantly in- crease the total data dimensionality. In our cur- rent use case, a chargrid-net training requires up to three days until full convergence, whereas our sequential model converges after only a few hours.</p><p>Comparing chargrid to semantic segmentation on natural images, one should note that character- pixels, unlike pixels of a gray-scale image, are cat- egorical. This requires the character-pixels to be encoded as 1-hot. This yields a highly sparse data representation. While such representation is very common for NLP problems, it is new for segmen- tation networks that have previously only been ap- plied to image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Chargrid is a generic representation for 2D text. Using this as a base, one could solve any task on a 2D text such as document classification, named- entity recognition, information extraction, parts- of-speech tagging etc. Furthermore,chargrid is highly beneficial for scenarios where text and nat- ural images are blended. One could imagine per- forming all the above NLP tasks also on such in- puts. This work demonstrates the advantages of chargrid for an information extraction task but we believe that it is only the first step towards in- corporating the 2D document structure into doc- ument understanding tasks. Some follow-up di- rection could be solving other NLP tasks on struc- tured documents using chargrid or experimenting with other computer vision algorithms on char- grid. Furthermore, it may be interesting to use word embeddings rather than 1-hot encoded char- acters, i.e. a wordgrid, as 2D text input representa- tion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example for a document page (left) and corresponding chargrid representation g (right).</figDesc><graphic url="image-1.pbm" coords="3,166.20,40.13,430.28,612.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The left image shows a histogram of number of vendors over the number of contributing invoices in the dataset. Most vendors appear only once in the dataset. The right image shows a distribution over languages, illustrating the diversity of the invoice data.</figDesc><graphic url="image-16.png" coords="5,310.73,62.81,104.31,110.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Spatial distribution of Invoice Amount (left) and Line-item Quantity (right) over the invoice. This depicts the variation in the invoice layouts contained in our dataset.</figDesc><graphic url="image-18.png" coords="6,307.28,62.81,104.31,137.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sample invoices and their corresponding network predictions: the top row shows the chargrid input, the bottom row shows the predicted segmentation mask with overlaid bounding box predictions. Our model is able to handle a large diversity of layouts. The encoding of the characters on the chargrid has been scrambled to preserve privacy.</figDesc><graphic url="image-22.png" coords="7,226.86,150.66,62.79,81.25" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>In acknowledgement of the partnership between Nvidia and SAP this publication has been prepared using Nvidia DGX-1. We thank Zbigniew Jerzak and Markus Noga for their support and fruitful dis-cussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>Figure 5 shows some sample predictions. It can be seen that the model can successfully extract key-value information on sample invoices despite significant diversity and complexity in the invoice layouts.</p><p>We show the quantitative results in <ref type="table">Table 1</ref>. We compare the proposed model, chargrid-net (Sec. 3.2), against four other models. The first one is a sequential model based purely on text. It is a stack of bi-directional GRUs ( ) taking a sequence of characters as input, and pro- ducing a sequence of labels as output. This model is our implementation of <ref type="bibr" target="#b16">Palm et al. (2017)</ref> and serves as a baseline comparison against a more tra- ditional NLP approach which is based on sequen- tial input. We note that we have also experimented with a extension of this model that along with the characters also takes as input the position of each character on the document, however, with negli- gible returns. Therefore, we stick to this simpler model.</p><p>The second image-only model is identical to chargrid-net <ref type="figure">(Figure 2</ref>), except that we directly take the original image of the document page as network input rather than the chargrid˜gchargrid˜ chargrid˜g. This model serves as a baseline comparison to a purely image-based approach using directly the raw pixel information as input. We note that we downsam- ple the image to the same input resolution as the chargrid representation, that is 336x256.</p><p>The third and fourth models are both a hy- brid between the chargrid-net, and the image-only model. In both models, we replicate the encoder: one encoder for the chargrid input˜ginput˜ input˜g, and one en- coder for the image of the document page. Infor- mation from the two encoders is concatenated in the decoder: whenever a lateral connection from the encoder of the original chargrid-net is concate- nated in a decoder block, we now concatenate lat- eral connections from the two encoders in a de- coder block.</p><p>We distinguish two configurations of the hybrid model: model chargrid-hybrid-C64 where char- grid and image encoders both have C = 64 base channels, and model chargrid-hybrid-C32 where chargrid and image encoders have C = 32 base channels while the decoder still retains C = 64 base channels. The latter model is used to assess the influence on the number of encoder parame- ters since the hybrid model effectively has more parameters due to the two encoders.</p><p>It can be seen that compared to the purely text- based approach, the chargrid-net performs equiva- lently on single-instance single-word fields where the 2D structure is not as important. Examples of single-instance, single-word fields are 'Invoice Number', 'Invoice Amount', and 'Invoice Date'. The extraction of these fields could essentially also be easily tackled with named entity recognition approaches based on serialized text ( <ref type="bibr">Gillick et al., 2015;</ref><ref type="bibr" target="#b13">Lample et al., 2016</ref>).</p><p>The chargrid-net, however, significantly outper-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Page segmentation of historical document images with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hennebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ingold</surname></persName>
		</author>
		<idno type="doi">10.1109/ICDAR.2015.7333914</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2015.7333914" />
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1011" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gülcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="CoRRabs/1406.1078.http://arxiv.org/abs/1406.1078" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00103</idno>
		<title level="m">Oriol Vinyals, and Amarnag Subramanya. 2015. Multilingual language processing from bytes</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multidigit number recognition from street view imagery using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sacha</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><forename type="middle">D</forename><surname>Shet</surname></persName>
		</author>
		<idno>CoRR abs/1312.6082</idno>
		<ptr target="http://arxiv.org/abs/1312.6082" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber ; In</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time document localization in natural images by recursive application of a cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<idno type="doi">10.1109/ICDAR.2017.26</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2017.26" />
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno>CoRR abs/1408.5882</idno>
		<ptr target="http://arxiv.org/abs/1408.5882" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cloudscan-a configuration-free invoice analysis system using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Rasmus Berg Palm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laws</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07403</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Pedro O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Minimum word error rate training for attention-based sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kannan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01818</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1506.01497</idno>
		<ptr target="http://arxiv.org/abs/1506.01497" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>CoRR abs/1509.00685</idno>
		<ptr target="http://arxiv.org/abs/1509.00685" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Document image binarization with fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Martinez</surname></persName>
		</author>
		<idno>CoRR abs/1708.03276</idno>
		<ptr target="http://arxiv.org/abs/1708.03276" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tesseract</surname></persName>
		</author>
		<ptr target="https://github.com/tesseract-ocr/tesseract" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02337</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
