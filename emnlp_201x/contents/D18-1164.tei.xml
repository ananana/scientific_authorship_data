<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<postCode>100105</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1338" to="1346"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1338</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In Visual Question Answering, most existing approaches adopt the pipeline of representing an image via pre-trained CNNs, and then using the uninterpretable CNN features in conjunction with the question to predict the answer. Although such end-to-end models might report promising performance, they rarely provide any insight, apart from the answer, into the VQA process. In this work, we propose to break up the end-to-end VQA into two steps: explaining and reasoning, in an attempt towards a more explainable VQA by shedding light on the intermediate results between these two steps. To that end, we first extract attributes and generate descriptions as explanations for an image. Next, a reasoning module utilizes these explanations in place of the image to infer an answer. The advantages of such a breakdown include: (1) the attributes and captions can reflect what the system extracts from the image, thus can provide some insights for the predicted answer; (2) these intermediate results can help identify the inabilities of the image understanding or the answer inference part when the predicted answer is wrong. We conduct extensive experiments on a popular VQA dataset and our system achieves comparable performance with the baselines, yet with added benefits of ex-planability and the inherent ability to further improve with higher quality explanations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Answering textual questions from images, which is referred to as visual question answering, presents fundamental challenges to both computer vision and natural language processing communi- ties. Significant progress has been made on VQA in recent years ( <ref type="bibr" target="#b1">Antol et al., 2015;</ref><ref type="bibr" target="#b24">Zhu et al., 2016;</ref><ref type="bibr" target="#b18">Wu et al., 2016a;</ref><ref type="bibr" target="#b6">Goyal et al., 2017;</ref><ref type="bibr" target="#b23">Yu et al., 2017;</ref><ref type="bibr" target="#b15">Teney et al., 2017;</ref><ref type="bibr"></ref> Explainable VQA What is the woman doing sitting on the bench? talking on phone Answer Reasoning Attributes: sit, phone, bench, cell, talk, woman, chair, park Caption: a woman sitting on a bench talking on a cell phone.</p><p>Figure 1: An example of explanation and reasoning in VQA. We first extract attributes in the image such as "sit", "phone" and "woman." A caption is also generated to encode the relationship between these attributes, e.g. "woman sitting on a bench." Then a reasoning module uses these explanations to predict an answer "talking on phone."</p><p>A few ducks swim in the ocean near two ferries.</p><p>Is there a ferry in the picture?</p><p>A green fire hydrant sitting next to a street.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA</head><p>Yes (0.99)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA</head><p>Yes (0.99)</p><p>Figure 2: Two contrasting cases that show how the explanations can be used to determine if the sys- tem guesses the answer. <ref type="bibr" target="#b17">Wang et al., 2017;</ref><ref type="bibr" target="#b7">Gurari et al., 2018;</ref><ref type="bibr" target="#b0">Anderson et al., 2018;</ref>. A widely used pipeline is to first encode an im- age with Convolutional Neural Networks (CNNs) and represent associated questions with Recurrent Neural Networks (RNNs), and then formulate the vision-to-language task as a classification problem on a list of answer candidates. Although promis- ing performance has been reported, this end-to- end paradigm fails to provide any insight to illumi- nate the VQA process. In most cases, giving an- swers without any explanation cannot satisfy hu-man users, especially when the predicted answer is not correct. More frustratingly, the system gives no hint about which part of such systems is the culprit for a wrong answer.</p><p>To address the above issues, we propose to break up the popular end-to-end pipeline into two steps: explaining and reasoning. The philoso- phy behind such a break-up is to mimic the image question answering process of human beings: first understanding the content of the image and then performing inference about the answer according to the understanding. As is shown in <ref type="figure">Fig.1</ref>, we first generate two-level explanations for an image via pre-trained attribute detectors and image caption- ing model: 1). word-level: attributes, indicating individual objects and attributes the system learns from the image. 2). sentence-level: captions, rep- resenting the relationship between the objects and attributes. Then the generated explanations and question are infused to a reasoning module to pre- dict an answer. The reasoning module is mainly composed of LSTMs.</p><p>Our method has three benefits. First, these ex- planations are interpretable. According to the at- tributes and captions, we can tell what objects, at- tributes and their relationship the machine learns from the image as well as what information is lost during the image understanding step. In contrast, the fully-connected layer features of CNNs are usually uninterpretable to humans. When the pre- dicted answer is correct, these attributes and cap- tions can be provided for users as the supplemen- tary explanations to the answer. Second, the sep- aration of explaining and reasoning enables us to localize which step of the VQA process the error comes from when the predicted answer is wrong. If the explanations don't include key information to answer the question, the error is caused by miss- ing information during the explaining step. Oth- erwise, the reasoning module should be respon- sible for the wrong answer. Third, the explana- tions can also indicate whether the system really finds key information from the image to answer the question or merely guesses an answer. <ref type="figure">Fig.2</ref> presents two contrasting cases to illustrate this. In the first case, both the generated caption and the question include the key concept "ferry", so the answer "Yes" with a high probability is reliable. However, although the answer "Yes" has the same high probability in the second case, the caption is irrelevant to the question. The system sticks to a wrong answer even with the correct input from sentence generation. This is due to the training set bias that a large proportion of questions starting with "is there" in the training set have the answer "Yes".</p><p>To our knowledge, this is the first effort to break down the previous end-to-end pipeline to shed light on the VQA process. Our main contributions are summarized as follows:</p><p>• We propose to formulate VQA into two sepa- rate steps: explaining and reasoning. Our framework generate attributes and captions for images to shed light on why the system predicts any specific answer.</p><p>• We adopt several ways to measure the expla- nation quality and demonstrate strong corre- lation between explanation quality and VQA accuracy. The current system achieves com- parable performance to the baselines and can naturally improve with explanation quality.</p><p>• Extensive experiments are conducted on the popular VQA dataset ( <ref type="bibr" target="#b1">Antol et al., 2015)</ref>. We dissect all results according to the mea- surements of the quality of explanations to present a thorough analysis of the strength and weakness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a growing research interest in the task of visual question answering. In this section, we summarize recent advances from two directions.  regions still don't explicitly exhibit what the system learns from the image and it is also not explained why these regions should be attended to.</p><p>High-level Concepts. In the scenario of vision- to-language, high-level concepts exhibit superior performance than the low-level or middle-level vi- sual features of the image <ref type="bibr">Wu et al., 2016a,b)</ref>. ) first learn in- dependent detectors for visual words based on a multi-instance learning framework and then gen- erate descriptions for images based on the set of visually detected words via a maximum entropy language model. ( <ref type="bibr" target="#b18">Wu et al., 2016a</ref>,b) presents a thorough study on how much the high-level con- cepts can benefit the image captioning and visual question answering tasks. These work mainly uses high-level concepts to obtain a better performance. Different from these work, our paper is focused on fully exploiting the readability and understand- ability of attributes and captions to explain the pro- cess of visual question answering and use these explanations to analyze our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we introduce the proposed frame- work for the breakdown of VQA. As illustrated in <ref type="figure">Figure 3</ref>, the framework consists of three modules: word prediction, sentence generation, and answer reasoning. Next, we describe the three modules in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Prediction</head><p>From the work ( <ref type="bibr" target="#b18">Wu et al., 2016a</ref>), we have learned that explicit high-level attributes can ben- efit vision-to-language tasks. In fact, besides per- formance gain, the readability and understandabil- ity of attributes also makes them an intuitive way to explain what the model learns from images. We first build a word list based on MS COCO Captions ( <ref type="bibr" target="#b3">Chen et al., 2015)</ref>. We extract the most N frequent words in all captions and filter them by lemmatization and removing stop words to de- termine a list of 256 words, which cover over 90% of the word occurrences in the dataset. Our words are not tense or plurality sensitive, for example, "horse" and "horses" are considered as the same word. This significantly decreases the size of our word list. Given the word list, every image is paired with multiple labels (words) according to its captions. Then we formulate word prediction as a multi-label classification task and fine-tune the ResNet-152 ( ) on our image-words dataset by minimizing the element-wise sigmoid cross entropy loss:</p><formula xml:id="formula_0">J = 1 N N i=1 V j=1 −y ij log p ij −(1−y ij ) log(1−p ij )</formula><p>(1) where N is batch size, V is the size of word list, y i = [y i1 , y i2 , ..., y iV ], y ij ∈ {0, 1} is the label vector of the i th image, p i = [p i1 , p i2 , ..., p iV ] is the probability vector.</p><p>In the testing phase, instead of using region pro- posals like (Wu et al., 2016a), we directly feed the whole image into the word prediction CNN in or- der to keep simple and efficient. As a result, each image is encoded into a fixed-length vector, where each dimension represents the probability of the corresponding word occurring in the image. Word Quality Evaluation. We adopt two metrics to evaluate the predicted words. The first measures the accuracy of the predicted words by computing cosine similarity between the label vector y and the probability vector p:</p><formula xml:id="formula_1">a = y T p ||y|| · ||p|| (2)</formula><p>However, this metric disregards the extent to which the predicted words are relevant to the ques- tion. Intuitively speaking, question-relevant expla- nations for images should be more likely to help predict right answers than irrelevant ones. There- fore, we propose another metric to measure the relevance between the words and the question.</p><p>We first encode the question into a 0-1 vector q in terms of the word list. Then the relevance is computed as:</p><formula xml:id="formula_2">r = q T p ||q|| · ||p||<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Generation</head><p>This section we talk about generating sentence- level explanations for images by using a pre- trained image captioning model. Similar to ( <ref type="bibr" target="#b16">Vinyals et al., 2015)</ref>, we train an image cap- tioning model by maximizing the probability of the correct caption given an image. Suppose we have an image I to be described by a caption  <ref type="figure">Figure 3</ref>: An overview of the proposed framework for VQA with three modules: word prediction (upper left), sentence generation (lower left), answer reasoning (right). Explaining: in word prediction, the image is fed into pre-trained visual detectors to extract word-level explanation, which is represented by probability vector v w ; in sentence generation, we input the image to pre-trained captioning model to generate a sentence-level explanation. Reasoning: the caption and question are encoded by two different LSTMs into v s and v q , respectively. Then v q , v w and v s are concatenated and fed to a fully connected layer with softmax to predict an answer.</p><formula xml:id="formula_3">S = {s 1 , s 2 , ..., s L }, s t ∈ V,</formula><p>fully connected layer of ResNet-152 pre-trained on ImageNet, denoted as v i . The caption S can be represented as a sequence of one-hot vector S = {s 1 , s 2 , ..., s L }. Then we formulate the cap- tion generation problem as minimizing the cost function:</p><formula xml:id="formula_4">J(v i , S) = − log P (S|v i ) = − L t=0 log P (s t |v i , s 1 , ..., s t−1 )<label>(4)</label></formula><p>where P (s t |v i , s 1 , ..., s t−1 ) is the probability of generating the word s t given the image representa- tion v i and previous words {s 1 , ..., s t−1 }. We em- ploy a single-layer LSTM with 512-dimensional hidden states to model this probability. In the test- ing phase, the image is input to pre-trained image captioning model to generate sentence-level expla- nation. Sentence Quality Evaluation. Similar to word quality evaluation, we evaluate the quality of the generated sentence from two perspectives: accu- racy and relevance. The former one is an average fusion of four widely used metrics: BLEU@N, METEOR, ROUGE-L and CIDEr-D ( <ref type="bibr" target="#b3">Chen et al., 2015)</ref>, which try to consider the accuracy of the generated sentence from different perspectives. Note that we normalize all the metrics into [0, 1] before fusion. The latter metric is to measure the relevance between the generated sentence and the question. The binary TF weights are calculated over all words of the sentence to produce an in- tegrated representation of the entire sentence, de- noted by s. Likewise, the question can be encoded to q. The relevance is computed as:</p><formula xml:id="formula_5">r = q T s ||q|| · ||s||<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Reasoning</head><p>This section we discuss the reasoning module. Suppose we have an image I explained by the predicted words W and the generated sentence S, the question Q and the answer A. As shown in <ref type="figure">Fig.3</ref>, we denote the representations of the pre- dicted words W as v s . The caption S and ques- tion Q are encoded by two different LSTMs into v s and v q , respectively. What bears mention- ing is that these two LSTMs share a common word-embedding matrix, but not other parameters, because the question and caption have different grammar structures and similar vocabularies. At last, the v w , v s , and v q are concatenated and fed into a fully connected layer with softmax to pre- dict the probability on a set of candidate answers:</p><formula xml:id="formula_6">v = [v T w v T s v T q ] T<label>(6)</label></formula><formula xml:id="formula_7">p = sof tmax(Wv + b)<label>(7)</label></formula><p>where W, b are the weight matrix and bias vec- tor of the fully connected layer. The optimizing objective for the reasoning module is to minimize the cross entropy loss as:</p><formula xml:id="formula_8">J(I, Q, A) = J(W, S, Q, A) = − log p(A)<label>(8)</label></formula><p>where p(A) denotes the probability of the ground truth A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head><p>Dataset. We evaluate our framework on VQA- real ( <ref type="bibr" target="#b1">Antol et al., 2015)</ref> </p><note type="other">dataset. For each image in VQA-real, 3 questions are annotated by different workers and each question has 10 answers from different annotators. We follow the official split and report our results on the open-ended task. Metric. We use the accuracy: min( #humans giving that answer 3 , 1), i.e., an answer is deemed 100% accurate if at least three workers provided that exact answer. Ablation Models. To analyze the contribution of word-level and sentence-level explanations, we ablate the full model and evaluate several variants as:</note><p>• Word-based VQA: use the feature concate- nation of the predicted words and question in Eq.6.</p><p>• Sentence-based VQA: use the feature con- catenation of the generated sentence and question in Eq.6.</p><p>• Full VQA: use the feature concatenation of words, sentence, and question in Eq.6. An important characteristics of our framework is that the quality of explanations can influence the final VQA performance. In this section, we ana- lyze the impact of the quality of predicted words on the VQA accuracy. We measure the quality from two sides: word accuracy and word-question relevance. <ref type="table" target="#tab_1">Table 1a</ref> shows the relationship be- tween word accuracy and VQA performance. We can learn that the more accurate the predicted words, the better the VQA performance. Similar to word accuracy, the more relevant to the question the predicted words, the better the VQA perfor- mance. Particularly, when the word-question rele- vance exceeds 0.8, the predicted words are highly pertinent to the question, boosting the VQA accu- racy to 76.15%. This indicates high-quality word- level explanations can benefit the VQA perfor- mance a lot. As shown in <ref type="figure">Fig.4</ref>, word-question relevance has a bigger impact on the final VQA performance than word accuracy.  Similar to the quality measurements of predicted words, we focus on the accuracy of the generated sentence itself and the relevance between sentence and question. As shown in <ref type="table" target="#tab_2">Table 2a</ref>, the more ac- curate the generated sentence, the higher the VQA accuracy. The results suggest that the VQA per- formance can be further improved by a better im- age captioning model. From <ref type="table" target="#tab_2">Table 2b</ref>, we can see that the more relevant to the question the generated sentence, the better the VQA performance. Once the relevance reaches 0.8, the accuracy can sig- nificantly increase to 89.81%. This proves that a question-related sentence is more likely to contain the key information for the VQA module to an- swer the question. As shown in <ref type="figure">Fig. 5</ref>, sentence- question relevance has greater influence on VQA performance than sentence accuracy does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word-based VQA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence-based VQA</head><p>To further verify the causal relationship be- tween sentence quality and VQA performance, we conduct the following control experiments. First, we evaluate sentence-based VQA model when feeding different sources of captions with ascend- ing quality: null (only including an "#end" token), sentence generation and relevant groundtruth (se- lecting from the groundtruth captions the most rel- evant one to the question). As shown in <ref type="table" target="#tab_3">Table 3</ref>, sentence generation performs much better than null. And using relevant groundtruth captions, the accuracy can improve by another 1.2 percent. <ref type="figure">Fig- ure 6</ref> presents an example to illustrate the effect of the sentence quality on the accuracy. From the above analysis, we can safely reach the conclu- sion that the VQA performance can be greatly im- proved by generating sentence-level explanations of high quality, especially of high relevance to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>From the above evaluation of word-based and sentence-based VQA model, we conclude</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image and question Generated Caption Prediction (accuracy)</head><p>Q: what sport are they playing?</p><p>(Good) a group of people playing frisbee in a field.</p><p>frisbee (1.00) (Wrong) a group of people playing soccer in a field.</p><p>soccer (0.00) (Empty) NULL tennis (0.00) <ref type="figure">Figure 6</ref>: A control case for comparing the accu- racy when inputting captions of different quality.</p><p>When getting a caption of high quality (the first one), the system can answer the question correctly. If we manually change the "frisbee" to "soccer", a wrong answer is predicted. When using an empty sentence, the system predicts the most popular an- swer "tennis" for this question.</p><p>that the relevance between explanations (at- tributes/caption) and the question has a great impact on the final VQA performance. In this section we illustrate this conclusion by studying four possible types of cases: 1). high relevance and correct answer; 2). low relevance and wrong answer; 3). high relevance but wrong answer; 4). low relevance but correct answer. High relevance and correct answer. From the first case in <ref type="figure">Fig. 7</ref>, we can see that the explanations for the image are highly relevant to the question: both the predicted attributes and the generated sen- tence contain the words "man" and "racket" occur- ring in the question. And the explanations also has key information that can predict the answer "ten- nis court." In this type of case, the system success- fully extracts from the image the relevant informa- tion that covers the question, facilitating answer generation.</p><p>Low relevance and wrong answer. In the sec- ond case, although the attributes and caption can reflect part of the image content such as "man" and "food", they neglect the key information about the "glass" that is asked in the question. The absence of "glass" in the explanations produces a low explanation-question relevance score and leads the system to a wrong answer. In this type of case, two lessons can be derived from the low relevance: 1). as the explanations are irrelevant to the question, the system tends to predict the most frequent answer ("beer") for this question type ("what kind of drink ..."), which implies that the answer is actually guessed from the dataset bias; 2). the error comes from the image understanding part rather than the question answering module, because the system fails to extract from the image</p><note type="other">Image Explanations, Question and Answer ① High relevance and correct answer</note><p>Attributes: tennis, ball, man, racket, hit, court, play, player, swing, hold (0.87, 0.72) Caption: a man holding a tennis racket on a tennis court. (0.88, 0.68) Question: where is the man swinging the racket? Answer: tennis court (tennis court) ② Low relevance and wrong answer Attributes: bicycle, man, sit, eat, bike, look, outside, food, person, table (0.06, 0.48) Caption: a man sitting at a table with a plate of food. (0.00, 0.34) Question: what kind of drink is in the glass? Answer: beer (water) ③ High relevance but wrong answer Attributes: street, bus, cow, city, walk, car, drive, stand, road, white (0.51, 0.77) Caption: a cow that is walking in the street (0.51, 0.55) Question: what is walking next to the bus? Answer: car (cow) ④ Low relevance but correct answer Attributes: woman, bear, teddy, hold, sit, glass, animal, large, lady (0.00, 0.19) Caption: a woman holding a sandwich in her hands (0.00, 0.30) Question: does the man need a haircut? Answer: yes (yes) <ref type="figure">Figure 7</ref>: Four types of cases in our results: 1). high relevance and correct answer; 2). low relevance and wrong answer; 3). high relevance but wrong answer; 4). low relevance but correct answer. "(*,*)" behind the explanations (attributes/caption) denotes the explanation-question relevance score and explanation accuracy, respectively. Gray denotes groundtruth answers. enough information to answer the question in the first place. This error suggests that some improve- ments are needed in word prediction and sentence generation modules to generate more comprehen- sive explanations for the image.</p><p>High relevance but wrong answer. In the third case, we can see that although the system fails to predict the correct answer, the explanations for the image are indeed relevant to the question and the system also recognize the key information "cow." This indicates that the error is caused by the ques- tion answering module rather than the explanation generation part. The system can recognize that "a cow is walking in the street" and "a bus is in the street", but it fails to conclude that "the cow is next to the bus." This error may lie in the weakness of LSTM which struggles on such complex spatial relationship inference. In the following analysis, we would show that such cases only occupy a rel- atively small proportion of the whole dataset.</p><p>Low relevance but correct answer. In the last example of <ref type="figure">Fig. 7</ref>, we know from the explana- tions that the system mistakes the "man" in the image for "woman" and neglects the information about his "hair." The explanations, therefore, have a low relevance score, which indicates that the an- swer "yes" is guessed by the system. Although the guessed answer is correct, it cannot be cred- ited to the correctness of the system. In fact, for this particular answer type "yes/no", the system has at least 50% chance to hit the right answer. We dissect all the results in the dataset accord- ing to the above four types of cases, as shown in <ref type="figure">Fig. 8</ref>. Among the questions that the sys- tem answers correctly, nearly 30% are guessed. This discovery indicates that, buried in the seem- ingly promising performance, the system actually takes advantage of the dataset bias, rather than truly understands the image content. Over 65% of the answers that are correctly guessed belong to "yes/no", an answer type easier for the system to hit the right answer than other types. As for the questions to which the system predicts wrong answers, a large proportion (around 80%) has a low explanation-question relevance, which means that more efforts need to be put into improving the attributes detectors and image captioning model. Questions with other answer types account for more than 80% of the wrongly-guessed answers. This is not surprising because for these questions the system cannot rely on the dataset bias any- more, considering the great variety of the candi- date answers. In this section, we present the performance comparison between variants of our framework and the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance Comparison</head><p>From <ref type="table" target="#tab_4">Table 4</ref>, we can see that sentence-based VQA consistently out- performs word-based VQA, which indicates that sentence-level explanations are superior to word- level ones. The generated captions not only in- clude the objects in the image, but also encode the relationship between these objects, which is important for predicting the correct answer. Fur- thermore, full VQA model obtains a better perfor- mance by combining attributes and captions.</p><p>Compared with the baselines, our framework achieves better performance than LSTM Q+I <ref type="bibr" target="#b1">(Antol et al., 2015</ref>), Concepts ( <ref type="bibr" target="#b18">Wu et al., 2016a)</ref>, and ACK ( <ref type="bibr" target="#b19">Wu et al., 2016b)</ref>, which use CNN fea- tures, high-level concepts, and external knowl- edge, respectively. MCB without attention ( <ref type="bibr" target="#b5">Fukui et al., 2016</ref>) achieves better performance than ours and other methods, but it suffers from a high-dimensional feature <ref type="figure">(16,000 vs 1,280)</ref>, which poses a limitation on the model's efficiency. The main advantage of our framework over other methods is that it not only predicts an answer to the question, but also generates human-readable attributes and captions to explain the answer. These explanations can help us understand what the system extracts from an image and their rele- vance to the question. As explanations improve, so would our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions and Conclusions</head><p>In this work, we break up the end-to-end VQA pipeline into explaining and reasoning, and achieve comparable performance with the base- lines. Different from previous work, our method first generates attributes and captions as explana- tions for an image and then feed these explana- tions to a question answering module to infer an answer. The merit of our method lies in that these attributes and captions allow a peek into the pro- cess of visual question answering. Furthermore, the relevance between these explanations and the question can act as indication whether the system really understands the image content.</p><p>It is worth noting although we also use the CNN-RNN combination, we generate words and captions as the explanations of images, thus al- lowing the VQA system to perform reasoning on semantics instead of unexplainable CNN features. Since the effectiveness of CNN for generating at- tributes and captions is well established, the use of CNN as a component does not contradict our high- level objective for explainable VQA. Our goal is not to immediately make a big gain in perfor- mance, but to propose a more powerful frame- work for VQA. Our current implementation al- ready matches the baselines, but more importantly, provides the ability to explain and to improve.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Attention in VQA. The attention mechanism is firstly used in the machine translation task (Bah- danau et al., 2014) and then is brought into the vision-to-language tasks (Xu et al., 2015; You et al., 2016; Yang et al., 2016; Lu et al., 2016; Nam et al., 2017; Yu et al., 2017; Teney et al., 2017; Anderson et al., 2018; Teney et al., 2018; Liang et al., 2018). The visual attention in the vision-to- language tasks is used to address the problem of "where to look". In VQA, the question is used as a query to search for the relevant regions in the im- age. Yang et al. propose a stacked attention model which queries the image for multiple times to in- fer the answer progressively. Beyond the visual attention, Lu et al. exploit a hierarchical question- image co-attention strategy to attend to both re- lated regions in the image and crucial words in the question. Attention mechanism can find the question-related regions in the image, which ac- counts for the answer to some extent. But the at-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>tended</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: The comparison of the impact of word accuracy and word-question relevance on VQA performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>②</head><label></label><figDesc>Figure 8: Dataset dissection according to the four types of cases. We define that the answer is guessed when the explanations are irrelevant to the question and otherwise reliable. The case numbers in the third row correspond to these in Fig.7. QA: all questions and answers. CA: questions with correct answers. WA: questions with wrong answers. GA: questions with guessed answers. RA: questions with reliable answers. Y/N: answer type "yes/no". O: answer types other than "yes/no".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>where V is the vo- cabulary, L is the caption length. First the im- age I is represented by the activations of the first</figDesc><table>CNN 
LSTM 
LSTM 
LSTM 
LSTM 

Sentence Generation 

í µí°¯ í µí² 

Word Prediction 

… 

Three giraffes 
grass #end 

… 

í µí°¯ í µí± 

⊕ 

Q: what are the animals in the picture? 

A: giraffe 

LSTM 

Answer 
Reasoning 

Framework 

í µí°¯ í µí± 
LSTM 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The relationship between word quality 
and accuracy (%). 

(a) Word accuracy 

Word 
accuracy 

VQA 
accuracy 

[0.0, 0.2) 
46.30 
[0.2, 0.8) 
55.84 
[0.8, 1.0) 
58.52 

(b) W-Q Relevance 

W-Q 
Relevance 

VQA 
accuracy 

[0.0, 0.2) 
54.69 
[0.2, 0.8) 
60.23 
[0.8, 1.0) 
76.15 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The relationship between sentence quality 
and accuracy. 

(a) Sentence accuracy 

Sentence 
accuracy 

VQA 
accuracy 

[0.0, 0.2) 
51.29 
[0.2, 0.8) 
55.53 
[0.8, 1.0) 
61.33 

(b) S-Q Relevance 

S-Q 
Relevance 

VQA 
accuracy 

[0.0, 0.2) 
52.79 
[0.2, 0.8) 
62.34 
[0.8, 1.0) 
89.81 

In this section, we evaluate the sentence-based 
VQA model and analyze the relationship between 
the sentence quality and the VQA performance. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Performance comparison on the valida- tion split of VQA-real open-ended task when the sentence-based VQA model uses different sources of captions. (accuray in %)</head><label>3</label><figDesc></figDesc><table>Caption source 

validation 

All 
Y/N Num Others 

null 
46.21 73.82 34.98 25.63 
sentence generation 54.85 76.31 36.64 42.23 
relevant groundtruth 56.05 77.42 41.04 44.34 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance comparison with the base-
lines. We show the performance on both test-dev 
and test-standard splits of VQA-real open-ended 
task. Human is the human performance for ref-
erence. 

Method 

test-dev 
test-standard 

All 
Y/N Num Others 
All 
Y/N Num Others 

LSTM Q+I (Antol et al., 2015) 
53.74 78.94 35.24 36.42 54.06 79.01 35.55 36.80 
Concepts (Wu et al., 2016a) 
57.46 79.77 36.79 43.10 57.62 79.72 36.04 43.44 
ACK (Wu et al., 2016b) 
59.17 81.01 38.42 45.23 59.44 81.07 37.12 45.83 
MCB w/n attention (Fukui et al., 2016) 60.80 81.20 35.10 49.30 
-
-
-
-
Human (Antol et al., 2015) 
-
-
-
-
83.30 95.77 83.39 72.67 

Word-based VQA 
56.76 77.57 35.21 43.85 
-
-
-
-
Sentence-based VQA 
57.91 78.03 36.73 45.52 
-
-
-
-
Full VQA 
59.93 79.32 38.41 48.25 60.07 79.09 38.25 48.57 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Jiebo Luo would like to thank the support of Adobe and NSF Award #1704309.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyi</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7219" to="7228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Anton van den Hengel. 2018. Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The vqa-machine: Learning how to use existing vision algorithms to answer new questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-level attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
