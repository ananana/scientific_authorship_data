<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramidal Recurrent Unit for Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
							<email>mohammadr@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for AI and XNOR.AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramidal Recurrent Unit for Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4620" to="4630"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4620</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>LSTMs are powerful tools for modeling con-textual information, as evidenced by their success at the task of language modeling. However , modeling contexts in very high dimensional space can lead to poor generalizability. We introduce the Pyramidal Recurrent Unit (PRU), which enables learning representations in high dimensional space with more generalization power and fewer parameters. PRUs replace the linear transformation in LSTMs with more sophisticated interactions including pyramidal and grouped linear transformations. This architecture gives strong results on word-level language modeling while reducing the number of parameters significantly. In particular , PRU improves the perplexity of a recent state-of-the-art language model Merity et al. (2018) by up to 1.3 points while learning 15-20% fewer parameters. For similar number of model parameters, PRU outperforms all previous RNN models that exploit different gating mechanisms and transformations. We provide a detailed examination of the PRU and its behavior on the language modeling tasks. Our code is open-source and available at https: //sacmehta.github.io/PRU/.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Long short term memory (LSTM) units (Hochre- iter and Schmidhuber, 1997) are popular for many sequence modeling tasks and are used extensively in language modeling. A key to their success is their articulated gating structure, which al- lows for more control over the information passed along the recurrence. However, despite the so- phistication of the gating mechanisms employed in LSTMs and similar recurrent units, the input and context vectors are treated with simple linear transformations prior to gating. Non-linear trans- formations such as convolutions ( <ref type="bibr" target="#b13">Kim et al., 2016)</ref> have been used, but these have not achieved the performance of well regularized LSTMs for lan- guage modeling <ref type="bibr" target="#b25">(Melis et al., 2018)</ref>.</p><p>A natural way to improve the expressiveness of linear transformations is to increase the num- ber of dimensions of the input and context vec- tors, but this comes with a significant increase in the number of parameters which may limit gen- eralizability. An example is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, where LSTMs performance decreases with the in- crease in dimensions of the input and context vec- tors. Moreover, the semantics of the input and con- text vectors are different, suggesting that each may benefit from specialized treatment.</p><p>Guided by these insights, we introduce a new recurrent unit, the Pyramidal Recurrent Unit (PRU), which is based on the LSTM gating struc- ture. <ref type="figure" target="#fig_1">Figure 2</ref> provides an overview of the PRU. At . Blue, red, green (or orange), and purple signify the current input x t , output of the previous cell h t−1 , the output of transformations, and the fused output, respectively. The color intensity is used to represent sub-sampling and grouping operations.</p><p>the heart of the PRU is the pyramidal transforma- tion (PT), which uses subsampling to effect multi- ple views of the input vector. The subsampled rep- resentations are combined in a pyramidal fusion structure, resulting in richer interactions between the individual dimensions of the input vector than is possible with a linear transformation. Context vectors, which have already undergone this trans- formation in the previous cell, are modified with a grouped linear transformation (GLT) which al- lows the network to learn latent representations in high dimensional space with fewer parameters and better generalizability (see <ref type="figure" target="#fig_0">Figure 1)</ref>.</p><p>We show that PRUs can better model contextual information and demonstrate performance gains on the task of language modeling. The PRU im- proves the perplexity of the current state-of-the-art language model ( <ref type="bibr" target="#b26">Merity et al., 2018</ref>) by up to 1.3 points, reaching perplexities of 56.56 and 64.53 on the Penn Treebank and WikiText2 datasets while learning 15-20% fewer parameters. Replacing an LSTM with a PRU results in improvements in per- plexity across a variety of experimental settings. We provide detailed ablations which motivate the design of the PRU architecture, as well as detailed analysis of the effect of the PRU on other compo- nents of the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Multiple methods, including a variety of gating structures and transformations, have been pro- posed to improve the performance of recurrent neural networks (RNNs). We first describe these approaches and then provide an overview of recent work in language modeling.</p><p>Gating-based mechanisms: The performance of RNNs have been greatly improved by gat- ing mechanisms such as LSTMs <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>), <ref type="bibr">GRUs (Chung et al., 2014</ref>), peep-hole connections <ref type="bibr" target="#b7">(Gers and Schmidhuber, 2000</ref>), SRUs ( <ref type="bibr" target="#b20">Lei et al., 2018)</ref>, and RANs ( <ref type="bibr" target="#b19">Lee et al., 2017)</ref>. In this paper, we extend the gating architecture of <ref type="bibr">LSTMs (Hochreiter and Schmidhuber, 1997</ref>), a widely used recurrent unit across different domains.</p><p>Transformations: Apart from the widely used linear transformation for modeling the tempo- ral data, another transformation that has gained popularity is convolution ( <ref type="bibr" target="#b18">LeCun et al., 1995)</ref>. Convolution-based methods have gained attention in computer vision tasks ( <ref type="bibr" target="#b16">Krizhevsky et al., 2012)</ref> as well as some of the natural language process- ing tasks including machine translation <ref type="bibr" target="#b6">(Gehring et al., 2017)</ref>. Convolution-based methods for lan- guage modeling, such as CharCNN ( <ref type="bibr" target="#b13">Kim et al., 2016)</ref>, have not yet achieved the performance of well regularized LSTMs ( <ref type="bibr" target="#b25">Melis et al., 2018</ref>). We inherit ideas from convolution-based approaches, such as sub-sampling, to learn richer representa- tions ( <ref type="bibr" target="#b16">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b9">Han et al., 2017)</ref>.</p><p>Regularization: Methods such as dropout <ref type="bibr">(Srivastava et al., 2014</ref>), variational dropout ( <ref type="bibr" target="#b14">Kingma et al., 2015)</ref>, and weight dropout ( <ref type="bibr" target="#b26">Merity et al., 2018)</ref> have been proposed to regularize RNNs. These methods can be easily applied to PRUs.</p><p>Other efficient RNN networks: Recently, there has been an effort to improve the efficiency of RNNs. These approaches include quantization ( <ref type="bibr">Xu et al., 2018</ref>), skimming ( <ref type="bibr">Seo et al., 2018;</ref><ref type="bibr">Yu et al., 2017</ref>), skipping <ref type="bibr" target="#b3">(Campos et al., 2018)</ref>, and query reduction ( <ref type="bibr">Seo et al., 2017</ref>). These approaches extend standard RNNs and therefore, these approaches are complementary to our work.</p><p>Language modeling: Language modeling is a fundamental task for NLP and has garnered sig- nificant attention in recent years (see <ref type="table">Table 1</ref> for comparison with state-of-the-art methods). <ref type="bibr" target="#b26">Merity et al. (2018)</ref> introduce regularization techniques such as weight dropping which, coupled with a non-monotonically triggered ASGD optimization, achieves strong performance improvements. <ref type="bibr">Yang et al. (2018)</ref> extend <ref type="bibr" target="#b26">Merity et al. (2018)</ref> with the mixture of softmaxes (MoS) technique, which in- creases the rank of the matrix used to compute next-token probabilities. Further, <ref type="bibr" target="#b1">Merity et al. (2017)</ref> and <ref type="bibr" target="#b15">Krause et al. (2018)</ref> propose methods to improve inference by adapting models to recent sequence history. Our work is complementary to these recent softmax layer and inference proce- dure improvements.</p><p>We extend state-of-the-art language model in <ref type="bibr" target="#b26">Merity et al. (2018)</ref> by replacing the LSTM with the PRU. We show by experiments that the PRU improves the performance of <ref type="bibr" target="#b26">Merity et al. (2018)</ref> while learning fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pyramidal Recurrent Units</head><p>We introduce Pyramidal Recurrent Units (PRUs), a new RNN architecture which improves modeling of context by allowing for higher dimensional vec- tor representations while learning fewer parame- ters. <ref type="figure" target="#fig_1">Figure 2</ref> provides an overview of PRU. We first elaborate on the details of the pyramidal trans- formation and the grouped linear transformation. We then describe our recurrent unit, PRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pyramidal transformation for input</head><p>The basic transformation in many recurrent units is a linear transformation F L defined as:</p><formula xml:id="formula_0">y = F L (x) = W · x,<label>(1)</label></formula><p>where W ∈ R N ×M are learned weights that lin- early map x ∈ R N to y ∈ R M . To simplify nota- tion, we omit the biases. Motivated by successful applications of sub- sampling in computer vision (e.g., <ref type="bibr" target="#b2">(Burt and Adelson, 1987;</ref><ref type="bibr" target="#b22">Lowe, 1999;</ref><ref type="bibr" target="#b16">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b24">Mehta et al., 2018)</ref>), we subsample input vec- tor x into K pyramidal levels to achieve repre- sentation of the input vector at multiple scales. This sub-sampling operation produces K vectors, represented as x k ∈ R N 2 k−1 , where 2 k−1 is the sampling rate and k = {1, · · · , K}. We learn</p><formula xml:id="formula_1">scale-specific transformations W k ∈ R N 2 k−1 × M K for each k = {1, · · · K}.</formula><p>The transformed sub- samples are concatenated to produce the pyrami- dal analog to y, here denoted as ¯ y ∈ R M :</p><formula xml:id="formula_2">¯ y = F P (x) = W 1 · x 1 , · · · , W K · x K , (2)</formula><p>where [·, ·] indicates concatenation. We note that pyramidal transformation with K = 1 is the same as the linear transformation.</p><p>To improve gradient flow inside the recurrent unit, we combine the input and output using an element-wise sum (when dimension matches) to produce residual analog of pyramidal transforma- tion, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> ( <ref type="bibr" target="#b10">He et al., 2016</ref>).</p><p>Sub-sampling: We sub-sample the input vector x into K pyramidal levels using the kernel-based approach ( <ref type="bibr" target="#b18">LeCun et al., 1995;</ref><ref type="bibr" target="#b16">Krizhevsky et al., 2012)</ref>. Let us assume that we have a kernel κ with 2e + 1 elements. Then, the input vector x can be sub-sampled as:</p><formula xml:id="formula_3">x k = N/s i=1 e j=−e x k−1 [si]κ[j],<label>(3)</label></formula><p>where s represents the stride and k = {2, · · · , K}.</p><p>Reduction in parameters: The number of pa- rameters learned by the linear transformation and the pyramidal transformation with K pyramidal levels to map</p><formula xml:id="formula_4">x ∈ R N to ¯ y ∈ R M are N M and N M K K k=1 2 (1−k) respectively. Thus, pyramidal</formula><p>transformation reduces the parameters of a linear transformation by a factor of K(</p><formula xml:id="formula_5">K k=1 2 (1−k) ) −1 .</formula><p>For example, the pyramidal transformation (with K = 4 and N = M = 600) learns 53% fewer parameters than the linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Grouped linear transformation for context</head><p>Many RNN architectures apply linear transforma- tions to both the input and context vector. How- ever, this may not be ideal due to the differing se- mantics of each vector. In many NLP applications including language modeling, the input vector is a dense word embedding which is shared across all contexts for a given word in a dataset. In con- trast, the context vector is highly contextualized by the current sequence. The differences between the input and context vector motivate their sepa- rate treatment in the PRU architecture. The weights learned using the linear transfor- mation (Eq. 1) are reused over multiple time steps, which makes them prone to over-fitting ( <ref type="bibr" target="#b5">Gal and Ghahramani, 2016)</ref>. To combat over-fitting, var- ious methods, such as variational dropout ( <ref type="bibr" target="#b5">Gal and Ghahramani, 2016)</ref> and weight dropout <ref type="bibr" target="#b26">(Merity et al., 2018)</ref>, have been proposed to regularize these recurrent connections. To further improve generalization abilities while simultaneously en- abling the recurrent unit to learn representations at very high dimensional space, we propose to use grouped linear transformation (GLT) instead of standard linear transformation for recurrent con- nections ( <ref type="bibr" target="#b17">Kuchaiev and Ginsburg, 2017)</ref>. While pyramidal and linear transformations can be ap- plied to transform context vectors, our experimen- tal results in Section 4.4 suggests that GLTs are more effective.</p><p>The linear transformation F L : R N → R M maps h ∈ R N linearly to z ∈ R M . Grouped linear transformations break the linear interac- tions by factoring the linear transformation into two steps. First, a GLT splits the input vector</p><formula xml:id="formula_6">h ∈ R N into g smaller groups such that h = {h 1 , · · · , h g }, ∀ h i ∈ R N g . Second, a linear trans- formation F L : R N g → R M g is applied to map h i linearly to z i ∈ R M g , for each i = {1, · · · , g}.</formula><p>The g resultant output vectors z i are concatenated to produce the final output vector ¯ z ∈ R M .</p><formula xml:id="formula_7">¯ z = F G (h) = W 1 · h 1 , · · · , W g · h g<label>(4)</label></formula><p>GLTs learn representations at low dimensional- ity. Therefore, a GLT requires g fewer parame- ters than the linear transformation. We note that GLTs are subset of linear transformations. In a lin- ear transformation, each neuron receives an input from each element in the input vector while in a GLT, each neuron receives an input from a subset of the input vector. Therefore, GLT is the same as a linear transformation when g = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pyramidal Recurrent Unit</head><p>We extend the basic gating architecture of LSTM with the pyramidal and grouped linear transfor- mations outlined above to produce the Pyramidal Recurrent Unit (PRU), whose improved sequence modeling capacity is evidenced in Section 4. At time t, the PRU combines the input vector x t and the previous context vector (or previous hid- den state vector) h t−1 using the following trans- formation function as:</p><formula xml:id="formula_8">ˆ G v (x t , h t−1 ) = ˆ F P (x t ) + F G (h t−1 ),<label>(5)</label></formula><p>where v ∈ {f, i, c, o} indexes the various gates in the LSTM model, andˆFandˆ andˆF P (·) and F G (·) represents the pyramidal and grouped linear transformations defined in Eqns. 2 and 4, respectively.</p><p>We will now incorporatê G v (·, ·) into LSTM gating architecture to produce PRU. At time t, a PRU cell takes x t ∈ R N , h t−1 ∈ R M , and c t−1 ∈ R M as inputs to produce forget f t , input i t , output o t , and contentˆccontentˆcontentˆc t gate signals. The inputs are combined with these gate signals to produce context vector h t ∈ R M and cell state c t ∈ R M . Mathematically, the PRU with the LSTM gating architecture can be defined as:</p><formula xml:id="formula_9">f t = σ ˆ G f (x t , h t−1 ) i t = σ ˆ G i (x t , h t−1 ) ˆ c t = tanhˆG tanhˆ tanhˆG c (x t , h t−1 ) o t = σ ˆ G o (x t , h t−1 ) c t = f t ⊗ c t−1 + i t ⊗ ˆ c t h t = o t ⊗ tanh(c t )<label>(6)</label></formula><p>where ⊗ represents the element-wise multiplica- tion operation, and σ and tanh are the sigmoid and hyperbolic tangent activation functions. We note that LSTM is a special case of PRU when g=K=1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To showcase the effectiveness of the PRU, we evaluate the performance on two standard datasets for word-level language modeling and compare with state-of-the-art methods. Additionally, we provide a detailed examination of the PRU and its behavior on the language modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Set-up</head><p>Dataset: Following recent works, we compare on two widely used datasets, the Penn Tree- bank (PTB) ( <ref type="bibr" target="#b23">Marcus et al., 1993</ref>) as prepared by <ref type="bibr">Mikolov et al. (2010)</ref> and WikiText2 (WT-2) (Merity et al., 2017). For both datasets, we follow the same training, validation, and test splits as in <ref type="bibr" target="#b26">Merity et al. (2018)</ref>.</p><p>Language Model: We extend the language model, AWD-LSTM ( <ref type="bibr" target="#b26">Merity et al., 2018)</ref>, by re- placing LSTM layers with PRU. Our model uses 3-layers of PRU with an embedding size of 400.</p><p>The number of parameters learned by state-of-the- art methods vary from 18M to 66M with major- ity of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair com- parison with state-of-the-art methods, we fix the model size to 19M and vary the value of g and hidden layer sizes so that total number of learned parameters is similar across different configura- tions. We use 1000, 1200, and 1400 as hidden layer sizes for values of g=1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels K to two in our experiments and use average pooling for sub- sampling. These values are selected based on our ablation experiments on the validation set (Section 4.4). We measure the performance of our models in terms of word-level perplexity. We follow the same training strategy as in <ref type="bibr" target="#b26">Merity et al. (2018)</ref>.</p><p>To understand the effect of regularization meth- ods on the performance of PRUs, we perform ex- periments under two different settings: (1) Stan- dard dropout: We use a standard dropout <ref type="bibr">(Srivastava et al., 2014</ref>) with probability of 0.5 after em- bedding layer, the output between LSTM layers, and the output of final LSTM layer. (2) Advanced dropout: We use the same dropout techniques with the same dropout values as in <ref type="bibr" target="#b26">Merity et al. (2018)</ref>. We call this model as AWD-PRU. <ref type="table">Table 1</ref> compares the performance of the PRU with state-of-the-art methods. We can see that the PRU achieves the best performance with fewer pa- rameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Standard dropout: PRUs achieve either the same or better performance than LSTMs. In par- ticular, the performance of PRUs improves with the increasing value of g. At g = 4, PRUs out- perform LSTMs by about 4 points on the PTB dataset and by about 3 points on the WT-2 dataset. This is explained in part by the regularization ef- fect of the grouped linear transformation ( <ref type="figure" target="#fig_0">Figure  1</ref>). With grouped linear and pyramidal transfor- mations, PRUs learn rich representations at very high dimensional space while learning fewer pa- rameters. On the other hand, LSTMs overfit to the training data at such high dimensions and learn 1.4× to 1.8× more parameters than PRUs.</p><p>Advanced dropouts: With the advanced dropouts, the performance of PRUs improves by about 4 points on the PTB dataset and 7 points on the WT-2 dataset. This further improves with finetuning on the PTB (about 2 points) and WT-2 (about 1 point) datasets.</p><p>Comparison with state-of-the-art: For similar number of parameters, the PRU with standard dropout outperforms most of the state-of-the-art methods by large margin on the PTB dataset (e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAN (Lee et al., 2017) by 16 points with 4M less parameters, QRNN (Bradbury et al., 2017) by 16</head><p>points with 1M more parameters, and NAS (Zoph and Le, 2017) by 1.58 points with 6M less param- eters). With advanced dropouts, the PRU delivers the best performance. On both datasets, the PRU improves the perplexity by about 1 point while learning 15-20% fewer parameters.</p><p>Inference: PRU is a drop-in replacement for LSTM, therefore, it can improve language mod- els with modern inference techniques such as dy- namic evaluation ( <ref type="bibr" target="#b15">Krause et al., 2018</ref>). When we evaluate PRU-based language models (only with standard dropout) with dynamic evaluation on the PTB test set, the perplexity of PRU (g = 4, k = 2, M = 1400) improves from 62.42 to 55.23 while the perplexity of an LSTM (M = 1000) with simi- lar settings improves from 66.29 to 58.79; suggest- ing that modern inference techniques are equally applicable to PRU-based language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>It is shown above that the PRU can learn represen- tations at higher dimensionality with more gener- alization power, resulting in performance gains for language modeling. A closer analysis of the im- pact of the PRU in a language modeling system reveals several factors that help explain how the PRU achieves these gains. Confidence: As exemplified in <ref type="table" target="#tab_1">Table 2a</ref>, the PRU tends toward more confident decisions, plac- ing more of the probability mass on the top next- word prediction than the LSTM. To quantify this effect, we calculate the entropy of the next-token distribution for both the PRU and the LSTM using 3687 contexts from the PTB validation set. <ref type="figure" target="#fig_2">Fig- ure 3</ref> shows a histogram of the entropies of the dis- tribution, where bins of size 0.23 are used to effect categories. We see that the PRU more often pro- duces lower entropy distributions corresponding to higher confidences for next-token choices. This is evidenced by the mass of the red PRU curve lying in the lower entropy ranges compared to the blue LSTM's curve. The PRU can produce confident decisions in part because more information is en- coded in the higher dimensional context vectors.</p><p>Variance in word embeddings: The PRU has the ability to model individual words at differ- ent resolutions through the pyramidal transform; which provides multiple paths for the gradient to the embedding layer (similar to multi-task learn- ing) and improves the flow of information. When considering the embeddings by part of speech, we find that the pyramid level 1 embeddings exhibit higher variance than the LSTM across all POS cat- egories <ref type="figure" target="#fig_3">(Figure 4)</ref>, and that pyramid level 2 em- beddings show extremely low variance <ref type="bibr">1</ref> . We hy- pothesize that the LSTM must encode both coarse group similarities and individual word differences into the same vector space, reducing the space be- tween individual words of the same category. The PRU can rely on the subsampled embeddings to  account for coarse-grained group similarities, al- lowing for finer individual word distinctions in the embedding layer. This hypothesis is strengthened by the entropy results described above: a model which can make finer distinctions between indi- vidual words can more confidently assign proba- bility mass. A model that cannot make these dis- tinctions, such as the LSTM, must spread its prob- ability mass across a larger class of similar words.</p><p>Gradient-based analysis: Saliency analysis us- ing gradients help identify relevant words in a test sequence that contribute to the prediction ( <ref type="bibr" target="#b8">Gevrey et al., 2003;</ref><ref type="bibr" target="#b21">Li et al., 2016;</ref><ref type="bibr" target="#b0">Arras et al., 2017</ref>). These approaches compute the relevance as the squared norm of the gradients obtained through back-propagation. <ref type="table" target="#tab_1">Table 2a</ref> visualizes the heatmaps for different sequences. PRUs, in gen- eral, give more relevance to contextual words than LSTMs, such as southeast (sample 1), cost (sam- ple 2), face (sample 4), and introduced (sample 5), which help in making more confident deci- sions. Furthermore, when gradients during back- propagation are visualized ( <ref type="bibr">Selvaraju et al., 2017</ref>) <ref type="table" target="#tab_1">(Table 2b)</ref>, we find that PRUs have better gradient coverage than LSTMs, suggesting PRUs use more features than LSTMs that contributes to the deci- sion. This also suggests that PRUs update more parameters at each iteration which results in faster training. Language model in ( <ref type="bibr" target="#b26">Merity et al., 2018</ref>) takes 500 and 750 epochs to converge with PRU and LSTM as a recurrent unit, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies</head><p>In this section, we provide a systematic analysis of our design choices. Our training methodology is the same as described in Section 4.1 with the standard dropouts. For a thorough understanding of our design choices, we use a language model with a single layer of PRU and fix the size of em- bedding and hidden layers to 600. The word-level perplexities are reported on the validation sets of the PTB and the WT-2 datasets.</p><p>Pyramidal levels K and groups g: The two hyper-parameters that control the trade-off be- tween performance and number of parameters in PRUs are the number of pyramidal levels K and groups g. <ref type="figure">Figure 5</ref> provides a trade-off between perplexity and recurrent unit (RU) parameters 2 .</p><p>Variable K and fixed g: When we increase the number of pyramidal levels K at a fixed value of g, the performance of the PRU drops by about 1 to 4 points while reducing the total number of recur- rent unit parameters by up to 15%. We note that the PRU with K = 4 at g = 1 delivers similar per- formance as the LSTM while learning about 15% fewer recurrent unit parameters.</p><p>Fixed K and variable g: When we vary the value of g at fixed number of pyramidal levels K, the total number of recurrent unit parameters de- creases significantly with a minimal impact on the perplexity. For example, PRUs with K = 2 and g = 4 learns 77% fewer recurrent unit parameters while its perplexity (lower is better) increases by about 12% in comparison to LSTMs. Moreover, the decrease in number of parameters at higher value of g enables PRUs to learn the representa- tions in high dimensional space with better gener- alizability <ref type="table">(Table 1)</ref>.</p><p>Gradient-based sensitivity analysis heatmaps LSTM top-5 PRU top-5</p><p>Reference: the tremor was centered near &lt;unk&gt; southeast of san francisco</p><p>Reference: the massages last N minutes and typically cost about $ N.</p><p>Reference: he visits the same department every two or three weeks.</p><p>Reference: but pipeline companies estimate they still face $ N billion in liabilities from &lt;unk&gt; disputes including $ N billion.</p><p>Reference: chicken chains also are feeling more pressure from mcdonald's corp. which introduced its &lt;unk&gt; &lt;unk&gt; this year.</p><p>(a) Gradient-based saliency analysis. Salience score is proportional to cell coverage in red.  Qualitative comparison between the LSTM and the PRU: (a) Gradient-based saliency analysis along with top-5 predicted words. (b) Gradients during back-propagation. For computing the gradients for a given test sequence, the top-1 predicted word was used as the true predicted word. Best viewed in color.</p><p>(a) PTB (b) WT-2</p><p>Figure 5: Impact of number of groups g and pyramidal levels K on the perplexity. Reduction in recur- rent unit (RU) parameters is computed with respect to LSTM. Lower perplexity value represents better performance.</p><p>Transformations: <ref type="table" target="#tab_3">Table 3</ref> shows the impact of different transformations of the input vector x t and the context vector h t−1 . We make following ob- servations: (1) Using the pyramidal transforma- tion for the input vectors improves the perplex- ity by about 1 point on both the PTB and WT- 2 datasets while reducing the number of recur- rent unit parameters by about 14% (see R1 and R4). We note that the performance of the PRU drops by up to 1 point when residual connections are not used (R4 and R6). (2) Using the grouped linear transformation for context vectors reduces the total number of recurrent unit parameters by about 75% while the performance drops by about 11% (see R3 and R4). When we use the pyrami- dal transformation instead of the linear transfor- mation, the performance drops by up to 2% while there is no significant drop in the number of pa- rameters (R4 and R5).</p><p>Subsampling: We set sub-sampling kernel κ (Eq. 3) with stride s = 2 and size of 3 (e = 1) in four different ways: (1) Skip: We skip every other element in the input vector. (2) Convolution:</p><p>We initialize the elements of κ randomly from nor- mal distribution and learn them during training the model. We limit the output values between -1 and 1 using tanh activation function to make training stable. (3) Avg. pool: We initialize the elements of κ to 1 3 . (4) Max pool: We select the maximum value in the kernel window κ.   <ref type="table" target="#tab_2">Table 4</ref>: Impact of different sub-sampling methods on the word-level perplexity (lower is better). We used g=1 and K=4 in our experiments.</p><p>performance. Both of these methods enable the network to learn richer word representations while representing the input vector in different forms, thus delivering higher performance. Surprisingly, a convolution-based sub-sampling method does not perform as well as the averaging method. The tanh function used after convolution limits the range of output values which are further limited by the LSTM gating structure, thereby impeding in the flow of information inside the cell. Max pooling forces the network to learn representations from high magnitude elements, thus distinguish- ing features between elements vanishes, resulting in poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce the Pyramidal Recurrent Unit, which better model contextual information by admitting higher dimensional representations with good gen- eralizability. When applied to the task of language modeling, PRUs improve perplexity across several settings, including recent state-of-the-art systems. Our analysis shows that the PRU improves the flow of gradient and expand the word embedding subspace, resulting in more confident decisions. Here we have shown improvements for language modeling. In future, we plan to study the perfor- mance of PRUs on different tasks, including ma- chine translation and question answering. In ad- dition, we will study the performance of the PRU on language modeling with more recent inference techniques, such as dynamic evaluation and mix- ture of softmax.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of training (solid lines) and validation (dashed lines) perplexities on the Penn Treebank with standard dropout for pyramidal recurrent units (PRU) and LSTM. PRUs learn latent representations in very high-dimensional space with good generalizability and fewer parameters. See Section 3 for more details about PRUs. Best viewed in color.</figDesc><graphic url="image-1.png" coords="1,307.28,222.54,218.26,167.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Block diagram visualizing the transformations in pyramidal recurrent unit (left) and the LSTM (bottom right) along with the LSTM gating architecture (top right). Blue, red, green (or orange), and purple signify the current input x t , output of the previous cell h t−1 , the output of transformations, and the fused output, respectively. The color intensity is used to represent sub-sampling and grouping operations.</figDesc><graphic url="image-2.png" coords="2,80.50,62.81,436.52,181.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Histogram of the entropies of next-token distributions predicted by the PRU (mean 3.80) and the LSTM (mean 3.93) on the PTB validation set. Lower entropy values indicate higher confidence decisions, which is desirable if decisions are often correct.</figDesc><graphic url="image-3.png" coords="7,72.00,62.81,218.26,118.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Variance of learned word embeddings for different categories of words on the PTB validation set. We compute the variance of a group of embeddings as the average squared euclidean distance to their mean. Higher variance may allow for better intra-category distinctions. The PRU with pyramid levels 1 and 2 is shown.</figDesc><graphic url="image-4.png" coords="7,72.00,286.09,218.26,85.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Gradients during back-propagation for a test sequence (x-axis: dimensions of word vector, y-axis: test sequence)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 compares</head><label>4</label><figDesc></figDesc><table>the performance of the PRU 
with different sampling methods. Average pooling 
performs the best while skipping give comparable 

PTB 
WT-2 
Transformations PPL # Params 
PPL 
# Params 
Context Input 
(total/RU) 
(total/RU) 

R1 
LT 
LT 
74.80 
8.8/2.9 
89.30 
22.8/2.9 
R2 
GLT 
GLT 
84.38 
6.5/0.5 
104.13 20.46/0.5 
R3 
GLT 
PT 
82.67 6.6/0.64 
99.57 20.6/0.64 
R4 
LT 
PT 
74.18 
8.5/2.5 
88.31 
22.5/2.5 
R5 
PT 
PT 
75.80 
8.1/2.1 
90.56 
22.1/2.1 

R6 
LT 
PT  † 
75.61 
8.5/2.5 
89.27 
22.5/2.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Impact of different transformations used 
for processing input and context vectors (LT -lin-
ear transformation, PT -pyramidal transformation, 
and GLT -grouped linear transformation). Here, 
 † represents that PT was used without residual 
connection, PPL represents word-level perplexity 
(lower is better), and the number of parameters are 
in million. We used K=g=4 in our experiments. 

</table></figure>

			<note place="foot" n="1"> POS categories are computed using NLTK toolkit.</note>

			<note place="foot" n="2"> # total params = # embedding params + # RU params</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSF (IIS 1616112, III 1703166), Allen Distinguished In-vestigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Aaron Jaech, Hannah Rashkin, Man-dar Joshi, Aniruddha Kembhavi, and anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explaining recurrent neural network predictions in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th Workshop on Computational Approaches to Subjectivity</title>
		<imprint>
			<publisher>Sentiment and Social Media Analysis</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Computer Vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Review and comparison of methods to study the contribution of variables in artificial neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muriel</forename><surname>Gevrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Dimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sovan</forename><surname>Lek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological modelling</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factorization tricks for lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07393</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Recurrent additive networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training rnns as fast as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David G Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on Computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank. Computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
