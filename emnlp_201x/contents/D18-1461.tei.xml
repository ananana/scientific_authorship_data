<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Character-Based Neural Machine Translation with Capacity and Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><forename type="middle">Macherey</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
						</author>
						<title level="a" type="main">Revisiting Character-Based Neural Machine Translation with Capacity and Compression</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4295" to="4305"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4295</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architec-tures for handling character input are better viewed as methods for reducing computation time than as improved ways of model-ing longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) has largely re- placed the complex pipeline of Phrase-Based MT with a single model that is trained end-to-end. However, NMT systems still typically rely on pre- and post-processing operations such as tokeniza- tion and word fragmentation through byte-pair en- coding (BPE; <ref type="bibr" target="#b20">Sennrich et al., 2016)</ref>. Although these are effective, they involve hyperparameters * *Equal contributions that should ideally be tuned for each language pair and corpus, an expensive step that is frequently omitted. Even when properly tuned, the repre- sentation of the corpus generated by pipelined external processing is likely to be sub-optimal. For instance, it is easy to find examples of word fragmentations, such as fling → fl + ing, that are linguistically implausible. NMT systems are generally robust to such infelicities-and can be made more robust through subword regularization <ref type="bibr" target="#b12">(Kudo, 2018)</ref>-but their effect on performance has not been carefully studied. The problem of find- ing optimal segmentations becomes more complex when an NMT system must handle multiple source and target languages, as in multilingual translation or zero-shot approaches <ref type="bibr" target="#b8">(Johnson et al., 2017)</ref>.</p><p>Translating characters instead of word frag- ments avoids these problems, and gives the system access to all available information about source and target sequences. However, it presents sig- nificant modeling and computational challenges. Longer sequences incur linear per-layer cost and quadratic attention cost, and require information to be retained over longer temporal spans. Finer temporal granularity also creates the potential for attention jitter <ref type="bibr" target="#b7">(Gulcehre et al., 2017)</ref>. Perhaps most significantly, since the meaning of a word is not a compositional function of its characters, the system must learn to memorize many character sequences, a different task from the (mostly) com- positional operations it performs at higher levels of linguistic abstraction.</p><p>In this paper, we show that a standard LSTM sequence-to-sequence model works very well for characters, and given sufficient depth, consistently outperforms identical models operating over word fragments. This result suggests that a produc- tive line of research on character-level models is to seek architectures that approximate standard sequence-to-sequence models while being compu-tationally cheaper. One approach to this problem is temporal compression: reducing the number of state vectors required to represent input or output sequences. We evaluate various approaches for performing temporal compression, both accord- ing to a fixed schedule; and, more ambitiously, learning compression decisions with a Hierarchi- cal Multiscale architecture ( <ref type="bibr" target="#b5">Chung et al., 2017)</ref>. Following recent work by <ref type="bibr" target="#b14">Lee et al. (2017)</ref>, we fo- cus on compressing the encoder.</p><p>Our contributions are as follows:</p><p>• The first large-scale empirical investigation of the translation quality of standard LSTM sequence-to-sequence architectures operat- ing at the character level, demonstrating im- provements in translation quality over word fragments, and quantifying the effect of cor- pus size and model capacity.</p><p>• A comparison of techniques to compress character sequences, assessing their ability to trade translation quality for increased speed.</p><p>• A first attempt to learn how to compress the source sequence during NMT training by us- ing the Hierarchical Multiscale LSTM to dy- namically shorten the source sequence as it passes through the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early work on modeling characters in NMT fo- cused on solving the out-of-vocabulary and soft- max bottleneck problems associated with word- level models ( <ref type="bibr" target="#b15">Ling et al., 2015;</ref><ref type="bibr" target="#b6">Costa-jussà and Fonollosa, 2016;</ref><ref type="bibr" target="#b16">Luong and Manning, 2016)</ref>. These took the form of word-boundary-aware hi- erarchical models, with word-level models dele- gating to character-level models to generate repre- sentations in the encoder and words in the decoder. Our work will not assume fixed word boundaries are given in advance.</p><p>With the advent of word-fragment approaches, interest in character-level processing fell off, but has recently been reignited with the work of <ref type="bibr" target="#b14">Lee et al. (2017)</ref>. They propose a specialized character-level encoder, connected to an unmod- ified character-level RNN decoder. They address the modeling and efficiency challenges of long character sequences using a convolutional layer, max-pooling over time, and highway layers. We agree with their conclusion that character-level translation is effective, but revisit the question of whether their specific encoder produces a de- sirable speed-quality tradeoff in the context of a much stronger baseline translation system. We draw inspiration from their pooling solution for re- ducing sequence length, along with similar ideas from the speech community ( <ref type="bibr" target="#b3">Chan et al., 2016)</ref>, when devising fixed-schedule reduction strategies in Section 3.3.</p><p>One of our primary contributions is an ex- tensive invesigation of the efficacy of a typical LSTM-based NMT system when operating at the character-level. The vast majority of existing stud- ies compare a specialized character-level architec- ture to a distinct word-level one. To the best of our knowledge, only a small number of papers have explored running NMT unmodified on char- acter sequences; these include: Luong and Man- ning (2016) on WMT'15 English-Czech, <ref type="bibr">Wu et al. (2016) on WMT'14 English-German, and</ref><ref type="bibr" target="#b2">Bradbury et al. (2016)</ref> on IWSLT German-English. All report scores that either trail behind or reach par- ity with word-level models. Only <ref type="bibr">Wu et al. (2016)</ref> compare to word fragment models, which they show to outperform characters by a sizeable mar- gin. We revisit the question of character-versus fragment-level NMT here, and reach quite differ- ent conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Sequence-to-Sequence Model</head><p>We adopt a simplified version of the LSTM archi- tecture of <ref type="bibr" target="#b4">Chen et al. (2018)</ref> that achieves state-of- the-art performance on the competitive WMT14 English-French and English-German benchmarks. This incorporates bidirectional LSTM (BiLSTM) layers in the encoder, concatenating the output from forward and backward directions before feeding the next layer. Output from the top en- coder layer is projected down to the decoder di- mension and used in an additive attention mech- anism computed over the bottom decoder layer. The decoder consists of unidirectional layers, all of which use the encoder context vectors com- puted from attention weights over the bottom layer. For both encoder and decoder we use layer normalization ( <ref type="bibr">Ba et al., 2016</ref>) and residual con- nections beginning at the third layer. We do not apply a non-linearity to LSTM output. We regu- larize with dropout applied to embeddings and to the output of each LSTM layer.</p><p>In the interests of simplicity and reproducibil-ity, we depart from <ref type="bibr" target="#b4">Chen et al. (2018)</ref> in several ways: we do not use multi-headed attention, feed encoder context vectors to the softmax, regularize with label smoothing or weight decay, nor apply dropout to the attention mechanism.</p><p>Our baseline character models and BPE mod- els both use this architecture, differing only in whether the source and target languages are tok- enized into sequences of characters or BPE word fragments. We describe BPE briefly below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Byte-Pair Encoding</head><p>Byte-Pair Encoding (BPE) offers a simple inter- polation between word-and character-level rep- resentations ( <ref type="bibr" target="#b20">Sennrich et al., 2016)</ref>. It creates a vocabulary of frequent words and word fragments in an iterative greedy merging process that begins with characters and ends when a desired vocab- ulary size is reached. The source and target lan- guage are typically processed together in order to exploit lexical similarities. Given a vocabu- lary, BPE re-tokenizes the corpus into word frag- ments in a greedy left-to-right fashion, selecting the longest possible vocabulary match, and back- ing off to characters when necessary.</p><p>Since each BPE token consists of one or more characters, BPE-tokenized sequences will be shorter than character sequences. Viewed as a mechanism to reduce sequence length, BPE differs from the solutions we will discuss subsequently in that it increases the vocabulary size, delegat- ing the task of creating representations for word fragments to the embedding table. Also, despite being data-driven, its segmentation decisions are fixed before NMT training begins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fixed stride Temporal Pooling</head><p>We explore using fixed stride temporal pooling within the encoder to compress the source char- acter sequence. These solutions are characterized by pooling the contents of two or more contigu- ous timesteps to create a single vector that sum- marizes them, and will replace them to shorten the sequence in the next layer. These approaches can learn to interpret the raw character sequence in service to their translation objective, but any such interepretation must fit into the pooling schedule that was specified during network construction. We evaluate two methods in this family: a re- implementation of <ref type="bibr" target="#b14">Lee et al. (2017)</ref>, and a version of our baseline with interspersed pooling layers.</p><p>As mentioned earlier, <ref type="bibr" target="#b14">Lee et al. (2017)</ref> pro- pose a specialized character encoder that com- bines convolutional layers to accumulate local context, max-pooling layers to reduce sequence lengths, highway layers to increase network ca- pacity, followed by bidirectional GRU layers to generate globally aware contextual source repre- sentations. This strategy is particularly efficient because all reductions happen before the first re- current layer. We re-implement their approach faithfully, with the exceptions of using LSTMs in place of GRUs, 1 and modifying the batch sizes to accomodate our multi-GPU training scheme.</p><p>While pooling based approaches are typically employed in association with convolutional lay- ers, we can also intersperse pooling layers into our high capacity baseline encoder. This means that after each BiLSTM layer, we have the option to in- clude a fixed-stride pooling layer to compress the sequence before it is processed by the next BiL- STM layer. This is similar to the pyramidal LSTM encoders used for neural speech recognition <ref type="bibr" target="#b3">(Chan et al., 2016)</ref>. This general strategy affords consid- erable flexibility to the network designer, leaving the type of pooling (concatenation, max, mean), and the strides with which to pool as design deci- sions that can be tuned to fit the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learned Temporal Compression</head><p>It is unsatisfying to compress a sequence on a fixed schedule; after all, the characters in a sentence do not each carry an identical amount of information. The goal of this section is to explore data-driven reduction methods that are optimized to the NMT system's objective, and which learn to compress as a part of training.</p><p>Any strategy for performing temporal com- pression will necessarily make discrete decisions, since sentence length is discrete. Examples of such strategies include sparse attention ( <ref type="bibr" target="#b18">Raffel et al., 2017</ref>) and discrete auto-encoders ( <ref type="bibr" target="#b9">Kaiser et al., 2018</ref>). For our initial exploration, we chose the hierarchical multiscale (HM) architecture of <ref type="bibr" target="#b5">Chung et al. (2017)</ref>, which we briefly describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Hierarchical Multiscale LSTM</head><p>The HM is a bottom-up temporal subsampling ap- proach, with each layer selecting the timesteps that will survive to the layer above. At a given timestep t and layer , the network makes a binary decision, z t , to determine whether or not it should send its output up to layer + 1. The preactivation for this decision, ˜ z t , is a function of the current node's inputs from below and from the previous hidden state, similar to an LSTM gate. However, z t 's ac- tivation is a binary step function in the forward pass, to enable discrete decisions, and a hard sig- moid in the backward pass, to allow gradients to flow through the decision point. <ref type="bibr">2</ref> The z t decision affects both the layer above, and the next timestep of the current layer:</p><p>• z t = 1, flow up: the node above (t, +1) per- forms a normal LSTM update; the node to the right (t+1, ) performs a modified update called a flush, which ignores the LSTM inter- nal cell at (t, ), and redirects the incoming LSTM hidden state from (t, ) to (t, + 1).</p><p>• z t = 0, flow right: the node above (t, +1) simply copies the cell and hidden state values from (t−1, +1); the node to the right (t+1, ) performs a normal LSTM update.</p><p>Conceptually, when z t = 0, the node above it be- comes a placeholder and is effectively removed from the sequence for that layer. Shorter upper layers save computation and facilitate the left-to- right flow of information for the surviving nodes.</p><p>Typically, one uses the top hidden state h L t from a stack of L RNNs to provide the representation for a timestep t. But for the HM, the top layer may be updated much less frequently than the lay- ers below it. To enable tasks that need a distinct representation for each timestep, such as language modeling, the HM employs a gated output module to mix hidden states across layers. This learned module combines the states h 1 t , h 2 t , . . ., h L t using scaling and projection operators to produce a sin- gle output h t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Modifying the HM for NMT</head><p>We would like sequences to become progressively shorter as we move upward through the layers. As originally specified, the HM calculates z t indepen- dently for every t and , including copied nodes, meaning that a "removed" timestep could reappear in a higher layer when a copied node (t, ) sets z t = 1. This is easily addressed by locking z t = 0 for copied nodes, creating a hierarchical structure in which upper layers never increase the amount of computation.</p><p>We also found that the flush component of the original architecture, which modifies the LSTM update at (t+1, ) to discard the LSTM's inter- nal cell, provided too much incentive to leave z t at 0, resulting in degenerate configurations which collapsed to having very few tokens in their up- per layers. We addressed this by removing the no- tion of a flush from our architecture. The node to the right (t+1, ) always performs a normal LSTM update, regardless of z t . This modification is sim- ilar to one proposed independently by <ref type="bibr">Kádár et al. (2018)</ref>, who simplified the flush operation by re- moving the connection to (t, + 1).</p><p>We found it useful to change the initial value of the bias term used in the calculation of˜zof˜ of˜z t , which we refer to as the z-bias. Setting z-bias to 1, which is the saturation point for the hard sigmoid with slope 1, improves training stability by encourag- ing the encoder to explore configurations where most timesteps survive through all layers, before starting to discard them.</p><p>Even with these modifications, we observed de- generate behavior in some settings. To discour- age this, we added a compression loss component similar to that of <ref type="bibr">Ke et al. (2018)</ref> to penalize z activation rates outside a specified range α 1 , α 2 :</p><formula xml:id="formula_0">L c = l max(0, Z l − α 1 T, α 2 T − Z l ),</formula><p>where T is source sequence length and Z l = T t=1 z l t . To incorporate the HM into our NMT encoder, we replace the lowest BiLSTM layer with unidi- rectional HM layers. <ref type="bibr">3</ref> We adapt any remaining BiLSTM layers to copy or update according to the z-values calculated by the top HM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpora</head><p>We adopt the corpora used by <ref type="bibr" target="#b14">Lee et al (2017)</ref>, with the exception of WMT15 Russian-English. <ref type="bibr">4</ref> To measure performance on an "easy" language pair, and to calibrate our results against recent benchmarks, we also included WMT14 English- French. <ref type="table" target="#tab_0">Table 1</ref> gives details of the corpora used. All corpora are preprocessed using Moses tools. 5 <ref type="bibr">3</ref> The flush operation makes the original HM inherently left-to-right. Since we have dropped flushes from our current version, it should be straightforward to devise a bidirectional variant, which we leave to future work. <ref type="bibr">4</ref> Due to licence restrictions. 5 Scripts and arguments: remove-non-printing-char.perl Dev and test corpora are tokenized, but not filtered or cleaned. Our character models use only the most frequent 496 characters across both source and target languages; similarly, BPE is run across both languages, with a vocabulary size of 32k.</p><note type="other">corpus train dev test WMT15 Finnish-En 2.1M 1500 1370 WMT15 German-En 4.5M 3003 2169 WMT15 Czech-En 14.8M 3003 2056 WMT14 En-French 39.9M 3000 3003</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model sizes, training, and inference</head><p>Except where noted below, we used 6 bidirectional layers in the encoder, and 8 unidirectional layers in the decoder. All vector dimensions were 512. Models were trained using sentence-level cross- entropy loss. Batch sizes are capped at 16,384 to- kens, and each batch is divided among 16 NVIDIA P100s running synchronously.</p><p>Parameters were initialized with a uniform (0.04) distribution. We use the Adam optimizer, with β 1 = 0.9, β 2 = 0.999, and = 10 −6 ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>). Gradient norm is clipped to 5.0. The initial learning rate is 0.0004, and we halve it whenever dev set perplexity has not decreased for 2k batches, with at least 2k batches between successive halvings. Training stops when dev set perplexity has not decreased for 8k batches.</p><p>Inference uses beam search with 8 hypothe- ses, coverage penalty of 0.2 ( <ref type="bibr" target="#b21">Tu et al., 2016)</ref>, and length normalization of 0.2 (Wu et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tuning and Evalution</head><p>When comparing character-level and BPE models, we tuned dropout independently for each setting, greedily exploring increments of 0.1 in the range 0.1-0.5, and selecting based on dev-set BLEU. This expensive strategy is crucial to obtaining valid conclusions, since optimal dropout values tend to be lower for character models.</p><p>Our main evaluation metric is Moses-tokenized case-sensitive BLEU score. We report test-set scores on the checkpoints having highest dev-set BLEU. To facilitate comparison with future work tokenize.perl clean-corpus-n.perl -ratio 9 1 100   we also report SacreBLEU scores <ref type="bibr" target="#b17">(Post, 2018)</ref> for key results, using the Moses detokenizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Character-level translation</head><p>We begin with experiments to compare the stan- dard RNN architecture from Section 3.1 at the character and BPE levels, using our full-scale model with 6 bidirectional encoder layers and 8 decoder layers. The primary results of our experi- ments are presented in <ref type="table" target="#tab_2">Table 2, while Table 3</ref> posi- tions the same results with respect to recent points from the literature. There are a number of observations we can draw from this data. First, from the EnFr results in Ta- ble 3, we are in line with GNMT ( <ref type="bibr">Wu et al., 2016)</ref>, and within 2 BLEU points of the RNN and Trans- former models investigated by <ref type="bibr" target="#b4">Chen et al. (2018)</ref>. So, while we are not working at the exact state-of- the-art, we are definitely in a range that should be relevant to most practitioners.</p><p>Also from <ref type="table" target="#tab_3">Table 3</ref>, we compare quite favorably with <ref type="bibr" target="#b14">Lee et al. (2017)</ref>, exceeding their reported scores by 3-6 points, which we attribute to hav- ing employed much higher model capacity, as they use a single bidirectional layer in the encoder and a two-layer decoder. We investigate the impact of model capacity in Section 5.1.1.</p><p>Finally, <ref type="table" target="#tab_2">Table 2</ref> clearly shows the character- level systems outperforming BPE for all language pairs. The dominance of character-level methods in <ref type="table" target="#tab_2">Table 2</ref> indicates that RNN-based NMT archi- tectures are not only capable of translating charac-ter sequences, but actually benefit from them. This is in direct contradiction to the few previously re- ported results on this matter, which can in most cases be explained by our increased model capac- ity. The exception is GNMT ( <ref type="bibr">Wu et al., 2016)</ref>, which had similar depth. In this case, possible explanations for the discrepancy include our use of a fully bidirectional encoder, our translating into English instead of German, and our model- specific tuning of dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Effect of model capacity</head><p>Character-level NMT systems have a more diffi- cult sequence-modeling task, as they need to infer the meaning of words from their constituent char- acters, where models with larger tokens instead delegate this task to the embedding table. There- fore, we hypothesize that increasing the model's capacity by adding layers will have a greater im- pact on character-level models. <ref type="figure" target="#fig_0">Figure 1</ref> tests this hypothesis by measuring the impact of three model sizes on test BLEU score. For each of our four language pairs, the word-fragment model starts out ahead, and quickly loses ground as architecture size increases. For the languages with greater morphological complexity-German, Czech and Finnish-the slope of the character model's curve is notably steeper than that of the BPE system, indicating that these systems could benefit from yet more modeling capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Effect of corpus size</head><p>One of the most compelling arguments for work- ing with characters (and to a lesser extent, word- fragments) is improved generalization. Through morphological generalizations, the system can better handle low-frequency and previously un- seen words. It stands to reason that as the train- ing corpus increases in size, the importance of these generalization capabilities will decrease. We test this hypothesis by holding the language pair constant, and varying the training corpus size by downsampling the full training corpus. We choose EnFr because it has by far the most available data. We compare four sizes: 2M, 4M, 14M and 40M.</p><p>The results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. As expected, the gap between character and word-fragment modeling decreases as corpus size increases. From the slopes of the curves, we can infer that the ad- vantage of character-level modeling will disappear completely as we reach 60-70M sentence pairs. However, there is reason to expect this break-even point to be much higher for more morphologically complex languages. It is also important to re- call that relatively few language-pairs can assem- ble parallel corpora of this size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Speed</head><p>The performance advantage of working with char- acters comes at a significant computational cost. With our full-sized architecture, character models trained roughly 8x more slowly than BPE mod- els. 6 <ref type="figure" target="#fig_2">Figure 3</ref> shows that training time grows lin- early with number of layers in the model, and that character models have a much higher per-layer cost: roughly 0.38 msec/sentence versus 0.04 for BPE. We did not directly measure the difference in attention cost, but it cannot be greater than the difference in total cost for the smallest number of layers. Therefore, we can infer from <ref type="figure" target="#fig_2">Figure 3</ref> that processing 5 layers in a character model in- curs roughly the same time cost as attention. This is surprising given the quadratic cost of attention, and indicates that efforts to speed up character models cannot focus exclusively on attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Qualitative comparison</head><p>To make a qualitative comparison between word fragments (BPE) and characters for NMT, we ex- amined 100 randomly selected sentence pairs from the DeEn test set. One author examined the sen- tences, using a display that showed the source 7 and the reference, along with the output of BPE and character models. Any differences between the two outputs were highlighted. They then assigned tags to both system outputs indicating broad er- ror categories, such as lexical choice, word order and German compound handling. 8 Tags were re- stricted to cases where one system made a mistake that the other did not.</p><p>Of the 100 sentences, 47 were annotated as be- ing identical or of roughly the same quality. The remaining 53 exhibited a large variety of differ- ences. <ref type="table" target="#tab_5">Table 4</ref> summarizes the errors that were most easily characterized. BPE and character sys- <ref type="bibr">6</ref> Recall that we use batches containing 16,384 tokens- corresponding to a fixed memory budget-for both character and BPE models. Thus character models are slowed not only by having longer sentences, but also by parallelizing across fewer sentences in each batch. <ref type="bibr">7</ref> The annotating author does not speak German. 8 Our annotator also looked specifically for agreement and negation errors, as studied by <ref type="bibr" target="#b19">Sennrich (2017)</ref> for English-to- German character-level NMT. However, neither system ex- hibited these error types with sufficient frequency to draw meaningful conclusions.   tems differ most in the number of lexical choice errors, and in the extent to which they drop con- tent. The latter is surprising, and appears to be a side-effect of a general tendency of the character models to be more faithful to the source, verging on being overly literal. An example of dropped content is shown in <ref type="table" target="#tab_4">Table 5</ref> (top).</p><p>Regarding lexical choice, the two systems dif- fer not only in the number of errors, but in the nature of those errors. In particular, the BPE model had more trouble handling German com- pound nouns.  ing one where the character system is a strict im- provement, translating Bunsenbrenner into bunsen burner instead of bullets. The second error follows another common pattern, where both systems mis- handle the German compound (Chemiestunden / chemistry lessons), but the character system fails in a more useful way. We also found that both systems occasionally mistranslate proper names. Both fail by attempt- ing to translate when they should copy over, but the BPE system's errors are harder to understand as they involve semantic translation, rendering Britta Hermann as Sir Leon, and Esme Nussbaum as smiling walnut. <ref type="bibr">9</ref> The character system's one observed error in this category was phonetic rather than semantic, rendering Schotten as Scottland.</p><p>Interestingly, we also observed several in- stances where the model correctly translates the German 24-hour clock into the English 12-hour clock; for example, 19.30 becomes 7:30 p.m.. This deterministic transformation is potentially in reach for both models, but we observed it only for the character system in this sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Src</head><p>Für diejenigen, die in ländlichen und abgelegenen Regionen des Staates lebten, . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref</head><p>Those living in regional and remote areas of the state . . . BPE For those who lived in rural and remote regions, . . . Char For those who lived in rural and remote regions of the state, . . . Src¨Uberall Src¨ Src¨Uberall im Land, in Tausenden von Chemiestunden, haben Schüler ihre Bunsenbrenner auf Asbestmatten abgestellt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref</head><p>Up and down the country, in myriad chemistry lessons, pupils have perched their Bunsen burners on asbestos mats. BPE Across the country, thousands of chemists have turned their bullets on asbestos mats. Char Everywhere in the country, in thousands of chemical hours, students have parked their bunsen burners on asbestos mats. <ref type="table" target="#tab_4">Table 5</ref>: Examples of BPE and character outputs for two sentences from the DeEn test set, demonstrating dropped content (top) and errors with German compounds (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compressing the Source Sequence</head><p>At this point we have established that character- level NMT benefits translation quality, but incurs a large computational cost. In this section, we eval- uate the speed-quality tradeoffs of various tech- niques for reducing the number of state vectors re- quired to represent the source sentence. All exper- iments are conducted on our DeEn language pair, chosen for having a good balance of morphologi- cal complexity and training corpus size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Optimizing the BPE vocabulary</head><p>Recall that BPE interpolates between word-and character-level processing by tokenizing consecu- tive characters into word fragments; larger BPE vocabulary sizes result in larger fragments and shorter sequences. If character-level models out- perform BPE with a vocabulary size of 32k, then is there a smaller BPE vocabulary size that reaps the benefits of character-level processing, while still substantially reducing the sequence length?</p><p>To answer this question, we test a number of BPE vocabularies, as shown in <ref type="table" target="#tab_7">Table 6</ref>. For each vocabulary, we measure BLEU and sequence compression rate, defined as the average size of the source sequence in characters divided by its size in word fragments (the ratio for the target se- quence was similar). Unfortunately, even at just 1k vocabulary items, BPE has already lost a BLEU point with respect to the character model. When comparing these results to the other methods in this section, it is important to recall that BPE is compressing both the source and target sequence (by approximately the same amount), doubling its effective compression rate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Fixed Stride Compression</head><p>The goal of these experiments is to determine whether using fixed schedule compression is a feasible alternative to BPE. We evaluate our re- implementation of the pooling model of <ref type="bibr" target="#b14">Lee et al. (2017)</ref> and our pooled BiLSTM encoder, both de- scribed in Section 3.3. For the pooled BiLSTM encoder, development experiments led us to intro- duce two mean-pooling layers, a stride 3 layer af- ter the second BiLSTM, and a stride 2 layer after the third. Therefore, the final output of the encoder is compressed by a factor of 6.</p><p>The results are also shown in  points. We suspect most of these gains result from better optimization of the model with large batch training. However, our attempts to scale this en- coder to larger depths, and therfore to the level of performance exhibited by our other systems, did not result in any significant improvements. This is possibly due to difficulties with optimizing a deeper stack of diverse layers.</p><p>Comparing the performance of our Pooled BiL- STM model against BPE, we notice that for a com- parable level of compression (BPE size of 1k), BPE out-performs the pooled model by around 0.5 BLEU points. At a similar level of performance (BPE size of 4k), BPE has significantly shorter se- quences. Although fixed-stride pooling does not yet match the performance of BPE, we remain op- timistic about its potential. The appeal of these models derives from their simplicity; they are easy to optimize, perform reasonably well, and remove the complication of BPE preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Hierarchical Multiscale Compression</head><p>We experimented with using the Hierarchical Mul- tiscale (HM; Section 3.4.1) architecture to learn compression decisions for the encoder.</p><p>For initial exploration, we used a scaled-down architecture consisting of 3 unidirectional HM en- coder layers and 2 LSTM decoder layers, attend- ing over the HM's gated output module. Com- parisons to an equivalent LSTM are shown in ta- ble 7. The first two HM lines justify the no-flush and hierarchical modifications described in Sec- tion 3.4.1, yielding incremental gains of 27.3 (the flush variant failed to converge), and 1.2 respec- tively. Initializing z-bias to 1 and annealing the slope of the hard binarizer from 1.0 to 5.0 over 80k minibatches gave further small gains, bringing the HM to parity with the LSTM while saving approx- imately 35% of layer-wise computations. Interest- ingly, we found that, over a wide range of training conditions, each layer tended to reduce computa- tion by roughly 60% relative to the layer below. <ref type="bibr">10</ref> For full-scale experiments, we stacked 5 BiL- STM layers on top of 2 or 3 HM layers, as de- scribed in section 3.4.1, using only the top HM layer (rather than the gated output module) as in- put to the lowest BiLSTM layer. To stabilize the 3- HM configuration we used a compression penalty with a weight of 2, and α 1 and α 2 of 0.1 and 0.9. Given the tendency of HM layers to reduce com- putation by a roughly constant proportion, we ex- pect fewer z-gates to be open in the 3-HM con- figuration, but this is achieved at the cost of one extra layer relative to our standard 12-layer en- coder. As shown in table 6, the 3-HM configura- tion achieves much better compression even when this is accounted for, and also gives slightly better performance than 2-HM. In general, HM gating results in less compression but better performance than the fixed-stride techniques.</p><p>Although these preliminary results are promis- ing, it should be emphasized that the speed gains they demonstrate are conceptual, and that realizing them in practice comes with significant engineer- ing challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated the translation quality of standard NMT architectures operating at the character-level. Our experiments show the sur- prising result that character NMT can substan- tially out-perform BPE tokenization for all but the largest training corpora sizes, and the less surpris- ing result that doing so incurs a large computa- tional cost. To address this cost, we have ex- plored a number of methods for source-sequence compression, including the first application of the Hierarchical Multiscale LSTM to NMT, which allows us to learn to dynamically compress the source sequence.</p><p>We intend this paper as a call to action. Character-level translation is well worth doing, but we do not yet have the necessary techniques to benefit from this quality boost without suffering a disproportionate reduction in speed. We hope that these results will spur others to revisit the question of character-level translation as an interesting test- bed for methods that can learn to process, summa- rize or compress long sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Test BLEU for character and BPE translation as architectures scale from 1 BiLSTM encoder layer and 2 LSTM decoder layers (1×2+2) to our standard 6×2+8. The y-axis spans 6 BLEU points for each language pair.</figDesc><graphic url="image-3.png" coords="7,115.99,142.43,181.42,78.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU versus training corpus size in millions of sentence pairs, for the EnFr language-pair.</figDesc><graphic url="image-5.png" coords="7,79.09,268.96,204.10,113.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training time per sentence versus total number of layers (encoder plus decoder) in the model.</figDesc><graphic url="image-6.png" coords="7,79.09,424.39,204.10,113.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Corpora, with linecounts. Test sets are 
WMT14-15 newstest. Dev sets are newsdev 2015 (Fi) 
and newstest 2013 (De, Fr), and 2014 (Cs). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Character versus BPE translation.</head><label>2</label><figDesc></figDesc><table>Comparison Point 
Ref Ours 
Chen et al. (2018) BPE EnFr 41.0 38.8 
Wu et al. (2016) 
BPE EnFr 39.0 
Lee et al. (2017) 
Char CsEn 22.5 25.9 
DeEn 25.8 31.6 
FiEn 13.1 19.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with some recent points in the literature. Scores are tokenized BLEU.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 (bottom) shows an exam- ple which exhibits two compound errors, includ-</head><label>5</label><figDesc></figDesc><table>Error Type 
BPE Char 
Lexical Choice 
19 
8 
Compounds 
13 
1 
Proper Names 
2 
1 
Morphological 
2 
2 
Other lexical 
2 
4 
Dropped Content 
7 
0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Error counts out of 100 randomly sampled ex-
amples from the DeEn test set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Compression results on WMT15 DeEn. The 
Comp. column shows the ratio of total computations 
carried out in the encoder. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Note that 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>HM small-scale results on WMT15 DeEn. 
The Comp. column is the proportion of layer-wise 
computation relative to the full LSTM. 

</table></figure>

			<note place="foot" n="1"> Development experiments indicated that using LSTMs over GRUs resulted in a slight improvement.</note>

			<note place="foot" n="2"> This disconnect between forward and backward activations is known as a straight-through estimator (Bengio et al., 2013).</note>

			<note place="foot" n="9"> The BPE segmentations for these names were: _Britt a _Herr mann and _Es me _N uss baum</note>

			<note place="foot" n="10"> For instance, the 2nd and 3rd layer of the best configuration shown had on average 60% and 36% of z gates open, yielding the computation ratio of (1 + 0.6 + 0.36)/3 = 0.65.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01576</idno>
		<title level="m">Quasi-recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Mia Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Character-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="357" to="361" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Plan, attend, generate: Character-level neural machine translation with planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="228" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03382</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Niki Parmar, Samy Bengio, Jakob Uszkoreit, and Noam Shazeer</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Laurent Charlin, and Chris Pal. 2018. Focused hierarchical RNNs for conditional sequence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04342</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting the hierarchical multiscale LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">´ Akos</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04586</idno>
		<title level="m">Character-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1054" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771v1</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online and lineartime attention by enforcing monotonic alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How Grammatical is Characterlevel Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="376" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04811</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
