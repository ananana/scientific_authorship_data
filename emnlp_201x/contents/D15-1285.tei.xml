<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topic Identification and Discovery on Text and Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandler</forename><surname>May</surname></persName>
							<email>cjmay@jhu.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
							<email>ferraro@cs.jhu.edu, alan.mccree@jhu.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mccree</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Wintrode</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<email>vandurme@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Topic Identification and Discovery on Text and Speech</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We compare the multinomial i-vector framework from the speech community with LDA, SAGE, and LSA as feature learners for topic ID on multinomial speech and text data. We also compare the learned representations in their ability to discover topics, quantified by dis-tributional similarity to gold-standard topics and by human interpretability. We find that topic ID and topic discovery are competing objectives. We argue that LSA and i-vectors should be more widely considered by the text processing community as pre-processing steps for downstream tasks, and also speculate about speech processing tasks that could benefit from more interpretable representations like SAGE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The text processing and speech processing re- search communities have similar problems and goals, but the technical approaches in these two communities develop largely independently. In this paper we compare dimensionality reduction techniques on multinomial language data from the text and speech communities. We consider a multinomial formulation of the i-vector model (hereafter "mi-vector" model) from the speech community <ref type="bibr">(Soufifar et al., 2011</ref>), the sparse ad- ditive generative (SAGE) <ref type="bibr" target="#b10">(Eisenstein et al., 2011</ref>) and latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003b</ref>) topic models from the text community, and latent semantic analysis (LSA) <ref type="bibr" target="#b8">(Deerwester et al., 1990</ref>). Both the mi-vector model and the SAGE topic model represent a multinomial pa- rameter vector as the softmax of a sum of vectors, one of which is a background vector representing overall word usage in the corpus, and so we might expect mi-vectors and SAGE to produce similar results on real-world data. We evaluate these two recent models and two more conventional mod- els, LDA and LSA (a term describing a class of methods based on the singular value decomposi- tion, or SVD, which is used broadly in both re- search communities). We assess the similarity of mi-vectors and SAGE and expose the strengths and weaknesses of all four learned representations by evaluating them on the supervised task of topic identification (topic ID), depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. We also evaluate the representations on the unsuper- vised, less easily-measurable task of topic discov- ery. As a proxy for controlled human annotations, we quantify topic discovery performance by dis- tributional similarity to gold-standard topics.</p><p>We use the bag-of-words multinomial represen- tation of text data, i.e., each document is repre- sented by a vector of counts over the word vo- cabulary. For speech data, we use a modern au- tomatic speech recognition (ASR) system to pro- duce frame-wise triphone state cluster posteriors and we take the sum of these posteriors across all frames in a document to obtain a document- level vector of triphone state cluster soft counts. Modern topic ID systems for speech use ASR out- put instead of a lower-resource representation like these soft counts to improve performance <ref type="bibr" target="#b14">(Hazen et al., 2007)</ref>. ASR word counts are high-resource and can be viewed as a noisy version of word counts from text. We wish to assess the relative performance of our learned representations, not the quality of the data pre-processing scheme, and we desire to strengthen our results by evaluating performance on two distinct views of a corpus. Hence we break from convention and use triphone state cluster soft counts as speech data.</p><p>While previous work has juxtaposed the mi- vector model against LDA <ref type="bibr" target="#b3">(Chen et al., 2014;</ref><ref type="bibr" target="#b23">Morchid et al., 2014)</ref>, the current study is the first to provide cross-community evaluations of mi-vectors and a contemporaneous model from the text community on both text and speech data. This study is also novel in its direct application of the mi-vector model to topic ID and topic dis- covery, two separate tasks with different motiva- tions and preferring different types of models, and in its use of low-resource triphone state cluster soft counts as speech data for topic ID. The low- resource setting reflects constraints often faced in real-world applications, and we report topic ID performance under limited supervision to better il- luminate the practical strengths and weaknesses of the learned representations. Finally, we believe that the centralized comparison herein of several prominent learned representations on two comple- mentary tasks on both text and speech will provide a useful point of reference for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Previous work has compared and composed the mi-vector model with older dimensionality reduc- tion techniques, including LDA. <ref type="bibr" target="#b3">Chen et al. (2014)</ref> compared a mi-vector language model against LDA and other models on the task of spoken doc- ument retrieval, and found the mi-vector model to significantly outperform the other models on words, but not on subwords (syllable pairs), de- rived from ASR. The syllable pairs are similar in granularity to the triphone state clusters used as multinomial speech data in the current work. <ref type="bibr" target="#b23">Morchid et al. (2014)</ref> improved conversation theme identification by employing LDA and a Gaussian i-vector model in a pipeline. They learn LDA models of varying dimensions (numbers of topics) on ASR output and use them to gener- ate a suite of feature vectors. The feature vector for each document-dimension pair is created by marginalizing over topics according to the docu- ment's inferred topic proportions. A Gaussian i- vector model is then learned on those feature vec- tors; the i-vectors are normalized and used to iden- tify document themes via the Bayes decision rule.</p><p>Note that we have fundamentally different ap- proaches, goals and methodology from that of <ref type="bibr" target="#b23">Morchid et al. (2014)</ref>. First, in an effort to provide a scientific comparison of independently created models, we use multinomial i-vectors, whereas Morchid et al., focusing on a particular task set- ting, used traditional Gaussian i-vectors. Simi- larly, while we treat multiple types of topic mod- els as goals in their own rights, directly compar- ing SAGE and LDA, Morchid et al. use LDA as a pre-processing step to Gaussian i-vectors. Sec- ond, we use triphone state cluster soft counts in- stead of ASR word counts, hence our representa- tion of speech data is significantly lower-resource. Third, we also evaluate performance on text data, and where Morchid et al. limit their vocabulary (from ASR) to 166 task-specific words, we use all 26,606 words present in our training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Input Representations</head><p>Our data is drawn from Part 1 of the Fisher English speech corpus ( <ref type="bibr" target="#b6">Cieri et al., 2004c)</ref>, which contains audio recordings ( <ref type="bibr" target="#b4">Cieri et al., 2004a</ref>) and man- ual transcriptions <ref type="bibr" target="#b5">(Cieri et al., 2004b</ref>) of telephone conversations. Specifically, we use the topic ID training and evaluation test subsets defined in prior work ( <ref type="bibr" target="#b14">Hazen et al., 2007)</ref>. In each conversation in these subsets of the data, two study participants are prompted to speak on one of a predefined set of forty topics. There are 1374 training conver- sations and 686 test conversations. We represent each conversation by two documents, one for each side (speaker), resulting in a training set of 2748 documents and a test set of 1372 documents. The deep neural network (DNN) used to infer the tri- phone state cluster posteriors forming the basis of our speech data was trained on Parts 1 and 2 of the Fisher English speech corpus ( <ref type="bibr" target="#b4">Cieri et al., 2004a;</ref><ref type="bibr" target="#b7">Cieri et al., 2005)</ref>; see the supplement for further details about our dataset and ASR system.</p><p>To quantify the sparsity of the raw text (word count) and speech (triphone state cluster soft count) representations, we consider the represen- tation density (number of non-zero entries) on our training set. The text representation is sparse, with median density 292 and maximum 500 (out of 26,606 dimensions); the speech representation is dense, with median density 7586 and maximum 7591 (out of 7591 dimensions).</p><p>To assess approximate sparsity, we plot his- tograms of the entropy of the normalized multino- mial views of our training set in <ref type="figure" target="#fig_1">Figure 2</ref>. The me- dian entropy for speech is less than two bits away from the uniform entropy, so the speech data is neither sparse nor approximately sparse.</p><p>Finally, we note that topic occurrence in the Fisher English training set is unbalanced, with quartiles (including minimum and maximum) of 6, 18.75, 29.5, 50.25, and 87.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learned Representations</head><p>We consider four main dimensionality reduction models: the mi-vector model from the speech community, the SAGE and LDA topic models from the text community, and LSA. The learned representations we consider explain which words appear in a document d via a latent, lower- dimensional representation θ (d) . All representa- tions operate under a bag-of-words assumption. To compare mi-vectors, topic models and LSA, we find it useful to formulate each learned representa- tion as operating on different "areas" or "contexts" a of a document; such a formulation does not negate the fundamental bag-of-words assumption. The four models represent the words that appear in an area a-either the entire document or each token-via multinomial-style parameters φ (a) . 1,2 Each model consists of K components (e.g., a K- dimensional affine subspace), and shared param- eters H k,v prescribe the amount of weight each component k places on each vocabulary word v. The models construct φ (a) by combining H and θ (d) ; often empirical word statistics m are also used to stabilize the representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LSA</head><p>LSA ( <ref type="bibr" target="#b8">Deerwester et al., 1990</ref>) factorizes a term- document matrix by truncated SVD, learning the projection of the data onto a linear subspace of fixed rank such that the approximation error of the reconstructed term-document matrix (as measured by the Frobenius norm) is minimized. In the basic version of LSA, SVD is applied to the raw term counts, giving the low-dimensional representation</p><formula xml:id="formula_0">φ (d) = Hθ (d) ,<label>(1)</label></formula><p>where φ (d) is the vector of observed multinomial counts in document d, H is the matrix of left sin- gular vectors of the term-document count matrix, and</p><formula xml:id="formula_1">θ (d) is the inferred representation of φ (d) .</formula><p>In practice, LSA is often applied instead to the term- document matrix weighted by term frequency- inverse document frequency (tf-idf) in order to normalize terms by importance. We can also ap- ply further pre-processing steps, such as term-wise centering by subtracting the column-wise mean m of the data, in which case LSA finds an affine sub- space that approximates the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mi-vector Model</head><p>The original acoustic i-vector model represents continuous, high-dimensional ASR system state (namely, Gaussian mixture model supervectors) in an affine subspace <ref type="bibr" target="#b9">(Dehak et al., 2011</ref>). Prior work has found this dense, low-dimensional representa- tion to be effective for a number of tasks, including language recognition (Martínez et al., 2011) and speaker recognition (Dehak et al., 2011; Garcia- Romero and Espy-Wilson, 2011).</p><p>Recently the i-vector model was augmented for multinomial observations <ref type="bibr">(Soufifar et al., 2011)</ref> and applied accordingly to language recogni- tion <ref type="bibr">(Soufifar et al., 2011;</ref><ref type="bibr">McCree and GarciaRomero, 2015)</ref>, speaker recognition ( <ref type="bibr" target="#b15">Kockmann et al., 2010)</ref>, and spoken document retrieval <ref type="bibr" target="#b3">(Chen et al., 2014)</ref>. In this version of the i-vector model the observations are draws from a multinomial and the (unnormalized) natural parameters of that dis- tribution are represented in an affine subspace:</p><formula xml:id="formula_2">φ (d) = softmax m + Hθ (d) (2) θ (d) ∼ N (0, I).</formula><p>We call this multinomial version of the i-vector model the mi-vector model. The latent variable θ (d) is the multinomial i-vector, or mi-vector. H is an unconstrained linear transformation. The bias term m is computed as the log of the l 1 - normalized background word count vector. The Gaussian prior on the mi-vector θ (d) is effec- tively an l 2 regularizer; mi-vectors are neither non- negative nor sparse in general. Unlike many Bayesian topic models, word oc- currences in the mi-vector model are i.i.d. draws from a document-level multinomial φ (d) ; as in LSA, each latent component contributes equally to each word within a given document. Specifically, in the mi-vector model, the natural parameter vec- tor of the multinomial for all words in a given doc- ument is determined by an additive offset from a background parameter vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bayesian Discrete Topic Models</head><p>Bayesian topic models explain word occurrences via K latent components H k (topics) each drawn from some prior distribution G. Unlike mi-vectors and LSA, multinomial topic models are admixture models: each token n is drawn from a particular distribution H k . Latent token assignment vari- ables z</p><formula xml:id="formula_3">(d)</formula><p>n , taking integral values between 1 and K, dictate the token's topic choice. A document d controls how often each topic is chosen via the K-dimension multinomial distribution θ <ref type="bibr">(d)</ref> . In the parametric settings we consider, Dirichlet priors are often placed on θ (d) , allowing experimenta- tion with the topic representation H. 3 A mapping Q(H k ), possibly the identity, ensures φ (d,n) are probability vectors. A general formulation is then</p><formula xml:id="formula_4">φ (d,n) = Q H z (d) n (3) H k ∼ G (η) z (d) n ∼ Discrete θ (d) θ (d) ∼ Dirichlet (α) .</formula><p>The hyperparameters α and η dictate the infor- mativeness of the priors over H k and θ <ref type="formula">(</ref> </p><formula xml:id="formula_5">= softmax m + H z (d) n</formula><p>, and draws H k from some sparsity-inducing distribution G, e.g., the Laplace distribution. As m is a shared back- ground frequency vector, H k is the learned resid- ual frequency vector of topic k.</p><p>Replacing the topic assignment in SAGE by its conditional expectation gives˜φ</p><formula xml:id="formula_6">gives˜ gives˜φ (d,n) = softmax m + E z (d) n H z (d) n θ (d) , H = softmax m + Hθ (d) .<label>(4)</label></formula><p>This modification of the SAGE topic model is the same as the mi-vector model but with differ- ent regularization on the representation vector θ (d) and l 1 regularization on the basis vectors H k . This "marginal SAGE" model could be useful in fu- ture work: the marginalization may mitigate the problem of topic-switching, yielding a more iden- tifiable (but perhaps less interpretable) model and lending to downstream tasks such as topic ID.</p><p>LDA Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003b</ref>) is a generative Bayesian topic model similar to SAGE, but in which each topic is drawn from a Dirichlet prior G rather than a sparsity- inducing distribution. LDA does not explicitly ac- count for the background distribution; to account for this, it is common practice to threshold the vo- cabulary a priori to remove very common and very rare words (though in our experiments, we do not do this). Therefore,</p><formula xml:id="formula_7">φ (d,n) is exactly H z (d) n</formula><p>, and H k ∼ Dirichlet (η).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare these four models of learned repre- sentations empirically on two distinct tasks, topic ID and topic discovery. The essential imple- mentation details of the models are as follows; further details are provided in the supplement. We learn the mi-vector model in a maximum a posteriori framework as in McCree and <ref type="bibr">GarciaRomero (2015)</ref>. Our own C++ implementation of SAGE, available online, 4 uses approximate mean- field variational inference, as in <ref type="bibr" target="#b10">Eisenstein et al. (2011)</ref>. We learn the LDA model using Gibbs sampling, implemented in MALLET <ref type="bibr" target="#b20">(McCallum, 2002)</ref>. <ref type="bibr">5</ref> We perform LSA using centered tf-idf- weighted word counts and centered l 2 -normalized triphone state cluster soft counts. We implement tf-idf by scaling the raw term count by the log in- verse document frequency. We apply l 2 normal- ization rather than tf-idf weighting to the speech data because it is dense and tf-idf is thus inappro- priate. On both text and speech, mean-centering is performed after the respective normalization, as this pre-processing recipe performed best of all the variants we tried. <ref type="bibr">6</ref> For each of the four models, the low- dimensional real vector θ (d) represents a given document d in our experiments. We also con- sider two high-dimensional baseline representa- tions: raw (soft) counts on both the text and speech data, and, only on the text data, tf-idf- weighted word counts. These tf-idf weights con- stitute a high-dimensional learned representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Topic ID</head><p>In our first topic ID experiment we evaluate topic ID error on raw multinomial views of the data. To our knowledge, we are the first to adopt a multi-nomial view of triphone state clusters and apply it to topic ID. In subsequent experiments we explore the interaction of representation dimension with each model and dataset, and evaluate relative per- formance when the classifier is only given a frac- tion of the available data for training. This latter configuration is the most interesting, as it reflects the cost of obtaining supervised data in practice.</p><p>Given feature vectors for some representation of the documents in a corpus, topic ID is performed in a one-versus-all framework. We use logistic re- gression as the per-class binary classifier, imple- mented using LIBLINEAR <ref type="bibr" target="#b11">(Fan et al., 2008)</ref>. Re- sults were similar when logistic regression was re- placed by support vector machines. All document representations are length-normalized (divided by their l 2 norm) before they are input to the classi- fier. Performance is measured by topic ID error, the error of multi-class prediction where the class predicted for each document is that of the per-class classifier that gave it the highest weight. Baseline performance on the test set (where the baseline classifier chooses the most prevalent topic in the training set for all test examples) is 96.2% error. Note that this error rate differs from the uniform- at-random classification error rate of 97.5% be- cause of the uneven distribution of topics.</p><p>Document Construction Prior work <ref type="bibr" target="#b14">(Hazen et al., 2007;</ref><ref type="bibr" target="#b31">Wintrode and Khudanpur, 2014</ref>) treated whole conversations as documents in addition to separating each conversation into its two sides. We perform a small topic ID experiment in this configuration to probe the impact of this design choice. Ten-fold cross-validation (CV) is used to tune the logistic regression regularizers. On the test set, the classifier achieves topic ID error of 12.4% and 15.6% for whole-conversation and individual-side text data, respectively, and 20.1% and 29.5% for whole-conversation and individual- side speech data, respectively. These results cor- respond roughly to results listed in <ref type="table">Table 3</ref> of <ref type="bibr" target="#b14">Hazen et al. (2007)</ref>, specifically, the topic ID er- ror of 8.2% and 12.4% for whole-conversation and individual-side transcriptions, respectively, and 22.9% and 35.3% for whole-conversation and individual-side triphones derived from ASR lat- tices, respectively <ref type="bibr" target="#b14">(Hazen et al., 2007</ref>). However, we use logistic regression without feature selec- tion instead of Na¨ıveNa¨ıve Bayes with feature selection, and we apply our classifier to triphone state cluster soft counts inferred by a DNN instead of triphone counts from ASR lattices. We believe that the dis- crepancies in performance with respect to prior work are due to these differences in experimen- tal configuration. Our results and those of prior work show that using whole-conversation docu- ments instead of individual-side documents make the topic ID task easier. As a result, we expect that differences in performance between the different learned representations will be more clearly pro- nounced on individual conversation sides and we restrict the rest of our study to that setting.</p><p>Dimensionality Study We perform topic ID on learned representations at dimensions K ∈ {10, 50, 100, 200, 300, 600} on individual conver- sation sides, using ten-fold cross-validation to tune the logistic regression regularizers. <ref type="figure" target="#fig_2">Figure 3</ref> gives topic ID error results on the test set, vary- ing K. (Selected values are listed in <ref type="table">Table 1</ref>.) In both datasets, as the dimension K increases, topic ID error decreases, approaching (approxi- mately) the raw baseline. On text, tf-idf performs slightly better than the raw representation. LSA is marginally the best-performing lower-dimensional learned representation; LDA and mi-vectors per- form well at some representation sizes, depending on the data source, but their performance is less consistent. SAGE performs poorly overall.  <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limited Data Study</head><p>The raw text and speech representations (multinomial observations) are very high-dimensional, and the classifier is likely to overfit to specific components (words or tri- phone state clusters) in these representations. To measure this effect and attempt to separate the predictive power of logistic regression from the quality of the learned representations in our anal- ysis, we experiment with reducing the number of labeled training examples the classifier can use; we still learn representations on the full (unla- beled) training set. This experiment represents the limited-supervision setting in which supervised data is costly to obtain but unlabeled data abounds. We run this experiment twice, using = 2 and = 6 labeled examples per topic, for a total of 80 and 240 classifier training examples, respec- tively. Ten-fold cross-validation is used to fit the regularizer; per-class loss coefficients are set ac- cording to the class prior in the original training set in order to counteract the artificial balancing of the classes in the limited-supervision dataset. We report cross-validation estimates of the topic ID error on the training set for K = 10 <ref type="figure" target="#fig_3">(Figure 4</ref>), K = 100 ( <ref type="figure" target="#fig_4">Figure 5</ref>), and K = 600 ( <ref type="figure" target="#fig_5">Figure 6</ref>). For K = 100 and K = 600, LSA dominates in the limited-supervision setting. Mi-vectors perform as well as or better than other low-dimensional learned representations at K = 10, and exhibit mixed performance for larger K. SAGE performs poorly overall. <ref type="bibr">7</ref> LDA performs significantly bet- <ref type="bibr">7</ref> We believe that approximately sparse posterior θ (d) val- ues result in a kind of topic switching, contributing to the poor performance of SAGE. To examine this we "tested on train" and analyzed the top topics inferred for each document: while the highest-weighted topic tended to be consistent, SAGE infers approximately sparse θ (d) with large variation in the next four highest-weighted topics (the remaining topics are assigned trace mass). Second, a phenomenon known as con- versation drift, explained in Section 3 of the supplement, is so pronounced in Fisher that the first 25% percent of words of each conversation side are nearly as predictive as the entire ter than SAGE, but not as well as mi-vectors. Fi- nally, tf-idf-weighted word counts perform very well on text, often achieving the best performance of all representations, even under limited supervi- sion (but at the same dimension as the raw data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Topic Discovery</head><p>To quantitatively assess representations' potential for topic discovery we compute their V-measure against the gold-standard labels. V-measure is an unsupervised measure of similarity between two partitions <ref type="bibr" target="#b27">(Rosenberg and Hirschberg, 2007)</ref> and is equivalent to the mutual information normalized by the sum of the entropy <ref type="bibr" target="#b0">(Becker, 2011)</ref>. For all representations, we compute V-measure between a partition induced by that representation and the gold-standard topic labels on the test set. A partition is induced on a representation by assign- ing each document d to the cluster indexed by the coordinate of θ (d) with highest value (the argmax). Results of this analysis are displayed in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>(Selected values are listed in <ref type="table" target="#tab_3">Table 2</ref>.) On the text data, SAGE dominates the lower-dimensional rep- resentations, LSA is next best overall, and LDA and mi-vectors exhibit relatively low performance; document <ref type="bibr" target="#b32">(Wintrode, 2013)</ref>. All representations must con- tend with this drift, but θ (d) sparsity may make SAGE partic- ularly susceptible. These two issues may make the classifica- tion we use much less robust.    <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>the high-dimensional tf-idf weights are surpassed by SAGE for K &gt; 10 but beat other representa- tions by a significant margin. On speech, SAGE is best overall, mi-vectors exhibit similar but gener- ally lower performance, LDA performs worse, and LSA is worst. We also measure the topic discovery potential of the mi-vector and SAGE representations more directly. First, we provide a manual inspection of the learned topics: in <ref type="table">Table 3</ref> we show the top- five word lists for five random topics from the 600- dimensional mi-vector and SAGE models (respec- tively) learned on the text data. In both models, the top five words in a topic are selected accord- ing to the five largest positive values in the cor- responding vector H k . Qualitatively, the SAGE topics are considerably more interpretable than the mi-vector topics: the SAGE topics represent is- sues of censorship, foreign relations, coffee fran- chises, welfare, and professional basketball, while the mi-vector topics are less succinctly characteri- zable and more polluted by uninformative words.</p><p>We complement this qualitative analysis with <ref type="bibr" target="#b22">Mimno et al. (2011)</ref>'s intrinsic coherence mea- sure, a standard quantitative method. This scor- ing function, which correlates well with human quality judgments, averages estimates of the con- ditional log-likelihoods of each topic's M highest- weighted words across all topics. Using K = 600 models on text as before and picking M = 20, we compute mi-vector coherence as −453.34 and SAGE coherence (averaged over three runs) as −407.52, indicating that SAGE is more amenable to topic discovery and human interaction.</p><p>mi-vector you've, florida, each, a-, bit hours, never, couldn't, check, communicate pregnant, water, lifestyle, awful, called forgot, ran, social, topics, unique tough, way, let's, fifties, hand SAGE censor, books, censorship, neat, agree sanctions, siblings, democratic, rely, u. n. starbucks, franchise, coffee, franchising, studio welfare, wage, minimum, cents, tips team, role, professional, blazers, basketball <ref type="table">Table 3</ref>: Top five words in five random mi-vector and SAGE topics learned on text data at K = 600. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have theoretically and empirically compared several content-bearing representations of text and speech from prior work. We have measured the relative performance of these representations on topic ID, an easy-to-evaluate task familiar to both text and speech research communities. We have also assessed the representations in their ability to discover the topics inherent in a corpus, a task that is more prominent in the text community and more difficult to evaluate. On our subset of the Fisher English data, these tasks appear to have compet- ing objectives: the best representations in one task are not necessarily the best in the other. In partic- ular, while SAGE yields the worst performance as a feature learner for topic ID, it is demonstrably superior to other low-dimensional learned repre- sentations in topic discovery. We have evaluated performance in topic discovery by distributional similarity to gold-standard topics as a proxy for human-annotated judgments of topic quality, and briefly compared the interpretability of mi-vectors and SAGE; future work could pursue expert or crowd-sourced human evaluations.</p><p>In the full-supervision setting of topic ID, the lower-dimensional learned representations con- verge in performance to the raw representation as the dimension K increases. However, if only a couple of labeled examples per class are available, reflecting the expense of obtaining labels in prac- tice, then learned representations generally outper- form the raw representation, which is more prone to overfitting. It is surprising that tf-idf performs so well in the limited supervision setting; it is learned from the data, but it should be prone to overfitting due to its high dimensionality. It is also surprising that SAGE performance on text de- grades significantly at high dimensions; we sus- pect this is due to topic switching, but further in- vestigation is warranted. Overall, though, for topic ID on word counts or triphone state cluster soft counts, if labeled data is scarce, we benefit from training on unsupervised learned representations.</p><p>In the V-measure experiment, the documents were partitioned according to the heaviest coordi- nate in their representations. This choice of exper- imental protocol is a nuisance variable in our re- sults; other partition constructions may yield dif- ferent conclusions. In particular, the heaviest- coordinate partition may favor topic models, whose representations are probability vectors, and disfavor mi-vectors and LSA, whose representa- tions may have positive and negative coordinates encoding general linear combinations.</p><p>Within each task, the ranking of the represen- tations (by performance) is generally consistent between the text and speech data; however, mi- vectors often outperform LDA on the speech data, while LDA often outperforms mi-vectors on the text data. This may be evidence that the two com- munities have already independently identified ap- propriate dimensionality reduction techniques for their respective data sources. However, our re- sults support that the speech community can bene- fit from broader use of sparsity-inducing graphical models such as SAGE in tasks like spoken topic discovery and recommendation, in which human- interpretable representations are desired. The text community may similarly benefit from parsimo- nious models such as LSA or mi-vectors in down- stream tasks; underparametrized mi-vectors per- form particularly well on text, and future work may benefit from investigating this setting.</p><p>Word counts and triphone state cluster soft counts provide only one view of text and speech (respectively), and other input representations may yield different conclusions. The particular LSA approach we used for text, based on tf-idf weight- ing, is not as appropriate for our speech data, which is dense. Future work could evaluate other implementations of LSA or use a higher-level view of speech, such as triphone state cluster n- grams, that more naturally exhibits sparsity and lends to tf-idf weighting. In particular, weight- ing by a likelihood ratio test statistic and apply- ing a log transform has generated better perfor- mance in several other tasks <ref type="bibr" target="#b16">(Lapesa and Evert, 2014)</ref>. Future work could also test our conclusions on higher-resource views of speech, such as ASR word counts, or lower-resource views such as mel- frequency cepstral coefficients (MFCCs).</p><p>We have provided a brief cross-community evaluation of learned representations on multi- nomial text and speech data. Some prior work has evaluated related learned representations on text data alone, surveying parameters and tasks at greater breadth ( <ref type="bibr" target="#b16">Lapesa and Evert, 2014;</ref><ref type="bibr" target="#b17">Levy et al., 2015)</ref>. A similarly comprehensive evalua- tion spanning the text and speech research com- munities would demand great effort but provide a large and versatile resource. In complement, a de- tailed, case-by-case analysis of errors made by the models in our study could illuminate future model- ing efforts by exposing exactly how and why each model errs or excels in each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Topic ID and topic discovery are competing ob- jectives in our setting: we found that the best- performing representations per task were the same whether considering text-or speech-based com- munications. By evaluating learned representa- tions from both the text and speech communities on a common set of data and tasks, we have pro- vided a framework for better understanding the topic ID and topic discovery objectives, among others. More generally, we hope to encourage cross-community collaboration to accelerate con- vergence toward comprehensive models of lan- guage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Depiction of the topic ID pipeline. Raw text or speech data is processed into multinomial counts, which are then transformed into a learned representation, and a classifier then predicts the topic of each document based on its representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distributions of the empirical entropy (in bits) of documents under the two multinomial views of our corpus. The vertical lines are the respective upper bounds (entropy of the uniform distributions). The distribution of the entropy of the text documents has median 7.2, over seven bits away from the upper bound of 14.7, thus the text representation is approximately sparse. The speech distribution has median 11.6, within two bits of the upper bound of 12.9, thus the speech representation is nearly uniform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Topic ID error (%) on the test set for raw and tfidf representations and lower-dimensional learned representations at dimensions K ∈ {10, 50, 100, 200, 300, 600}. We see many of the learned representations approach the error rate of the raw representation, but at much lower dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CV topic ID error (%) for raw and tf-idf representations and lower-dimensional learned representations of size K = 10. Error bars denote plus and minus one standard deviation according to the CV empirical distribution. We see underparametrized mi-vectors excel at compressing the topic label information for text, particularly in the limitedsupervision settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: CV topic ID error (%) for raw and tf-idf representations and lower-dimensional learned representations of size K = 100. Error bars denote plus and minus one standard deviation according to the CV empirical distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: CV topic ID error (%) for raw and tf-idf representations and lower-dimensional learned representations of size K = 600. Error bars denote plus and minus one standard deviation according to the CV empirical distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: V-measure on the Fisher English text and speech data, respectively, for raw and tf-idf representations and lower-dimensional learned representations at selected dimensions. As in topic ID, we see underparametrized mi-vectors perform well on the text data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Selected V-measure values from</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Other efforts have modeled documents with intermediate granularity, e.g., sentence-level (Titov and McDonald, 2008) or entity-level (Newman et al., 2006) granularity. 2 For brevity, we use the multinomial distribution and its parameter interchangeably throughout.</note>

			<note place="foot" n="3"> There have been many efforts to provide or induce latent structure among the topics (Blei et al., 2003a; Li and McCallum, 2006; Wallach et al., 2009; Paul and Girju, 2010), but most models ground out to Dirichlet and discrete random variables.</note>

			<note place="foot" n="4"> https://github.com/fmof/sagepp 5 For Gibbs sampling, fractional counts are truncated. 6 Results for other versions of LSA are provided in the supplement. We did not present the conventional, uncentered tf-idf weighting scheme here because although it performs best in topic ID, it yields extremely variable V-measure.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the three anonymous re-viewers for their feedback. A National Science Foundation Graduate Research Fellowship, under Grant No. DGE-1232825, supported the second author. We would like to thank the Johns Hopkins HLTCOE for providing support. Any opinions ex-pressed in this work are those of the authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Identification and Characterization of Events in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Becker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 16 (NIPS)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">I-vector based language modeling for spoken document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Shin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="7083" to="7088" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Fisher english training speech part 1 speech LDC2004S13. DVD</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fisher english training speech part 1 transcripts LDC2004T19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Web Download</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The fisher corpus: a resource for the next generations of speech-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Fisher english training part 2, speech LDC2005S13. DVD</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society For Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réda</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse additive generative models of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis of i-vector length normalization in speaker recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">Y</forename><surname>Espy-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="page" from="249" to="252" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shared components topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topic identification from audio recordings using word and phone recognition lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Margolis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="659" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prosodic speaker verification using subspace multinomial models with intersession compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Kockmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciana</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1061" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A large scale evaluation of distributional semantic models: Parameters, interactions and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Lapesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="531" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pachinko allocation: Dag-structured mixture models of topic correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23th International Conference on Machine Learning (ICML)</title>
		<meeting>the 23th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language recognition in ivectors space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oldřich</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Matějka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="861" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://mallet.cs.umass.edu" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">DNN senone MAP multinomial i-vectors for phonotactic language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech. To appear</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An i-vector based approach to compact multi-granularity topic spaces representation of textual documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bouallegue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Driss</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato De</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="443" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical entity-topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="680" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sprite: Generalizing topic models with structured priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="43" to="57" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A twodimensional topic-aspect model for discovering multi-faceted topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 24th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 EMNLP-CoNLL Joint Conference</title>
		<meeting>the 2007 EMNLP-CoNLL Joint Conference</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Oldřich Plchot, Ondřej Glembek, and Torbjørn Svendsen. 2011. iVector approach to phonotactic language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Kockmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2913" to="2916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International World Wide Web Conference (WWW)</title>
		<meeting>the 17th International World Wide Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking lda: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22 (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Limited resource term detection for effective topic identification of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Wintrode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="7118" to="7122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging locality for topic identification of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Wintrode</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1579" to="1583" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
