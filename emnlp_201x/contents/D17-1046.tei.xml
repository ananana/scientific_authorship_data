<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why ADAGRAD Fails for Online Topic Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Lu</surname></persName>
							<email>you.lu@colorado.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lund</surname></persName>
							<email>jefflund@byu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>iSchool, LSC</roleName><forename type="first">Umiacs</forename><surname>Cs</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science (CS)</orgName>
								<orgName type="department" key="dep2">Computer Science (CS)</orgName>
								<orgName type="institution" key="instit1">University of Colorado Boulder Boulder</orgName>
								<orgName type="institution" key="instit2">Brigham Young University</orgName>
								<address>
									<settlement>Provo</settlement>
									<region>CO, UT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Why ADAGRAD Fails for Online Topic Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="446" to="451"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Online topic modeling, i.e., topic modeling with stochastic variational inference, is a powerful and efficient technique for analyzing large datasets, and ADAGRAD is a widely-used technique for tuning learning rates during online gradient optimization. However, these two techniques do not work well together. We show that this is because ADAGRAD uses accumulation of previous gradients as the learning rates&apos; denominators. For online topic modeling, the magnitude of gradients is very large. It causes learning rates to shrink very quickly, so the parameters cannot fully converge until the training ends. Probabilistic topic models (Blei, 2012) are popular algorithms for uncovering hidden thematic structure in text. They have been widely used to help people understand and navigate document collections (Blei et al., 2003), multilingual collections (Hu et al., 2014), images (Chong et al., 2009), networks (Chang and Blei, 2009; Yang et al., 2016), etc. Probabilistic topic modeling usually requires computing a posterior distribution over thousands or millions of latent variables, which is often intractable. Variational inference (Blei et al., 2016, VI) approximates posterior distributions. Stochas-tic variational inference (Hoffman et al., 2013, SVI) is its natural online extension and enables the analysis of large datasets. Online topic models (Hoffman et al., 2010; Bryant and Sudderth, 2012; Paisley et al., 2015) optimize the global parameters of interest using stochastic gradient ascent. At each iteration, they sample data points to estimate the gradient. In practice, the sample has only a small percentage of the vocabulary. The resulting sparse gradients hurt performance. ADAGRAD (Duchi et al., 2011) is designed for high dimensional online optimization problems and adjusts learning rates for each dimension, favoring rare features. This makes ADAGRAD well-suited for tasks with sparse gradients such as distributed deep networks (Dean et al., 2012), forward-backward splitting (Duchi and Singer, 2009), and regularized dual averaging methods (Xiao, 2010). Thus, it may seem reasonable to apply ADA-GRAD to optimize online topic models. However, ADAGRAD is not suitable for online topic models (Section 1). This is because to get a topic model, the training algorithm must break the symmetry between parameters of words that are highly related to the topic and words that are not related to the topic. Before the algorithm converges, the magnitude of gradients of the parameters are very large. Since ADAGRAD uses the accumulation of previous gradients as learning rates&apos; denominators, the learning rates shrink very quickly. Thus, the algorithm cannot break the symmetry quickly. We provide solutions for this problem. Two alternative learning rate methods, i.e., ADADELTA (Zeiler, 2012) and ADAM (Kingma and Ba, 2014), can address this incompatibility with online topic models. When the dataset is small enough, e.g., a corpus with only hundreds of documents, ADAGRAD can still work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Buridan's Optimizer</head><p>Latent Dirichlet allocation ( <ref type="bibr">Blei et al., 2003, LDA)</ref> is perhaps the most well known topic model. In this section, we analyze problems with ADAGRAD for online LDA <ref type="bibr" target="#b11">(Hoffman et al., 2010)</ref>, and provide some solutions. Our analysis is easy to generalize to other online topic models, e.g., online Hierarchi- cal Dirichlet Process ( <ref type="bibr">Wang et al., 2011, HDP)</ref>.  <ref type="figure">Figure 1</ref>: Illustration of ADAGRAD's problem. Ini- tially, the topic does not favor particular words over others, so the training algorithm incorrectly increases the parameters of bottom words. Then, ADAGRAD learning rates decrease too quickly, leav- ing the tie between top and bottom unbroken. Thus, the algorithm fails to form appropriate topics. A constant rate easily breaks the tie. When the tie is broken, the algorithm decreases the parameters of bottom words and increases the parameters of top words until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Online LDA</head><p>To train LDA, we want to compute the posterior</p><formula xml:id="formula_0">p(β, θ,z | w, α, η) ∝ K k=1 p(β k | η)· D d=1 p(θ d | α) N d n=1 p(z dn | θ d )p(w dn | β z dn ),</formula><p>where β k is the topic-word distribution for the k th of K topics, θ d is the document-topic distribution for the d th of D document, z dn is the topic assign- ment for the n th of N d words in in the d th document, w dn is the word type of the n th word in the d th doc- ument, with α and η the Dirichlet priors over the document-topic and topic-word distributions.</p><p>However, this is intractable. Stochastic varia- tional inference (SVI) is a popular approach for approximation. It first posits a mean field varia- tional distribution</p><formula xml:id="formula_1">q(β, θ, z | λ, γ, φ) = K k=1 q(β k | λ k )· D d=1 q(θ d | γ d ) N d n=1 q(z dn | φ dn ),</formula><p>where γ (Dirichlet) and φ (multinomial) are local parameters and λ (Dirichlet) is a global parame- ter. SVI then optimizes the variational parameters to minimize the KL divergence between the varia- tional distribution and the true posterior.</p><p>At iteration t, SVI samples a document d from the corpus and updates the local parameters:</p><formula xml:id="formula_2">φ d vk ∝ exp Ψ (γ dk ) + Ψ λ (t) kv − Ψ i λ (t) ki ,<label>(1)</label></formula><formula xml:id="formula_3">γ (t) k = α + v n v φ d vk ,<label>(2)</label></formula><p>where n v is the number of words v in d, and Ψ (.) is the digamma function. After finding φ d and γ d , SVI optimizes the global parameters using stochastic gradient ascent,</p><formula xml:id="formula_4">λ (t+1) kv = (1 − ρ (t) kv )λ (t) kv + ρ (t) kv (η + Dφ d vk n dv ) = (1 − ρ (t) kv )λ (t) kv + ρ (t) kvˆλ kvˆ kvˆλ (t) kv = λ (t) kv + ρ (t) kv g (t) kv ,<label>(3)</label></formula><p>where ρ (t) is the learning rate, ˆ λ</p><formula xml:id="formula_5">(t) kv = η +Dφ d vk n dv is the intermediate parameter and g (t) kv = −λ (t) kv + ˆ λ (t)</formula><p>kv is the gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">ADAGRAD for Online LDA</head><p>In general, ρ (t) kv = κ (t) , for all v ∈ 1, .., V and k ∈ 1, ..., K, where κ (t) can be a decreasing rate <ref type="bibr" target="#b12">(Hoffman et al., 2013</ref>), a small constant ( <ref type="bibr" target="#b7">Collobert et al., 2011</ref>) or an adaptive rate <ref type="bibr" target="#b16">(Ranganath et al., 2013</ref>). These three methods are all global learning rate methods, which cannot adaptively ad- just learning rate for each dimension of the pa- rameter, or address the problems caused by sparse gradients.</p><p>ADAGRAD is a popular learning rate method de- signed for online optimization problems with high dimension and sparse gradients. Thus, it seems reasonable to apply ADAGRAD to update learning rates for online topic models. When using ADA- GRAD ( <ref type="bibr" target="#b9">Duchi et al., 2011</ref>) with online LDA, the update rule for the each learning rate is</p><formula xml:id="formula_6">ρ (t) kv = ρ 0 + t i=0 g (i) kv 2 ,<label>(4)</label></formula><p>where ρ 0 is a constant, and a very small guaran- tees that the learning rates are non-zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">ADAGRAD's Indecision</head><p>A philosophical thought experiment provides us with the story of Buridan's ass (Bayle, 1826): sit- uated between two piles of equally tasty hay, the poor animal starved to death. ADAGRAD faces a similar problem in breaking the symmetries of common variational inference initializations. For convenience, we unfold an example with a single document at each iteration. Our analysis general- izes to mini-batches. Initially, the topics β 1:K do not favor particular words over others as inference cannot know a pri- ori which words will have high probability in a particular topic. The algorithm must break ties be- tween parameters of the top and bottom words in a topic. Unfortunately, the momentum of ADAGRAD fails for topic models. We now explain why this is.</p><p>ADAGRAD looks to the gradient for clues about what features will be important. This is because before the equilibrium is broken, the values of dif- ferent λ kv are close, so Equation 1 will be approx- imately seen as φ d vk ∝ exp {Ψ (γ dk )}, which im- plicates that λ has very small influence on the op- timization of φ. If some topics are prevalent in the sampled document d, large probability will be assigned to the corresponding φ .k , meaning that all words in document d are treated as top words. The initial clues are at best random and at words counter productive.</p><p>However, ADAGRAD uses these cues to prefer some dimensions over others. Let λ * be the opti- mum; the topic ADAGRAD should find at conver- gence:</p><formula xml:id="formula_7">λ * kv ≈ E ˆ λ (t) kv</formula><p>. By definition, once the algorithm converges, λ * kv for top words will have very large values while λ * kv for bottom words will be small. After using noisy momentum terms, it must overcome initial faulty signals.</p><p>We now show the lower and upper bounds of E ˆ λ (t) kv to show how big of an uphill battle ADA- GRAD faces. Expanding the update rule,</p><formula xml:id="formula_8">E ˆ λ (t) kv = E η + Dφ d vk n dv = η + D¯ n v E [φ vk ] ,</formula><p>where </p><formula xml:id="formula_9">¯ n v = D i=1 n iv /D,</formula><note type="other">and upper bounds of E ˆ λ (t) kv are</note><formula xml:id="formula_10">η + (1/K)D¯ n v ≤ E ˆ λ (t) kv ≤ η + D¯ n v .</formula><p>For a large datasets, D¯ n v should be large. Thus for top words, λ * kv will converge to a large value: quite a large hill to climb.</p><p>How quickly the algorithm climbs the hill is inversely proportional to the gradient size. We next show that the magnitude of gradients of top words are very large before the algorithm converges. Let g * be the gradient after convergence. We show the bounds of |g kv |, where |.| is the absolute value, in the following:</p><formula xml:id="formula_11">| g * kv | = | − λ * kv + η + Dφ d vk n dv | ≈ | − η − D¯ n v E[φ vk ] + η + Dφ d vk n dv | ≈ E [φ vk ] * D | n dv − ¯ n v | .</formula><p>Thus,</p><formula xml:id="formula_12">(D/K) | n dv − ¯ n v | ≤ | g * kv | ≤ D | n dv − ¯ n v | .</formula><p>Only when n dv = ¯ n v , does | g</p><p>kv | = 0. Otherwise, due to the large D, | g * kv | will be large. However, in practice, n dv varies largely from document to document, which leads to large values of | g * kv | . Based on the gradient's property, when λ kv is far away from the optimum, | g kv | for the top words are very large before convergence.</p><p>ADAGRAD uses the accumulations of previous gradients as learning rates' denominators. Because of these large gradients in the first several iterations, learning rates soon decrease to small values; even if a topic has gathered a few words, ADAGRAD lacks the momentum to move other words into the topic. These small learning rates slows the updates of λ.</p><p>In sum, the initial gradient signals confuse the algorithm, the gradients are large enough to impede progress later, and large datasets imply a very large hill the algorithm must climb. Since the update pro- gresses slowly, online LDA needs more iterations to break the equilibrium. Because the gradients of all words are still very large, the learning rates decrease quickly, which makes the update progress slower. When the update progresses more slowly, online LDA needs more iterations to break the tie. This cycle repeats, until some learning rates de- crease to zero and learning effectively stops. Thus, the algorithm will never break the tie or infer good topics. <ref type="figure">Figure 1</ref> illustrates the problem of online LDA with ADAGRAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Alternative Solutions</head><p>ADADELTA <ref type="bibr" target="#b20">(Zeiler, 2012)</ref> and ADAM <ref type="bibr" target="#b14">(Kingma and Ba, 2014</ref>) are extensions to ADAGRAD. ADADELTA does not have guaranteed convergence on con- vex optimization problems. Even though ADAM has a theoretical bound on its convergence rate, it is controlled by and sensitive to several learn- ing rate parameters. For good performance with ADAM, manual adjustment is necessary. In addi- tion, since ADADELTA computes the moving aver- age of updates, and ADAM needs to compute the bias-corrected gradient estimate, they require more intricate implementations. Consequently, these two methods are not as popular as ADAGRAD for begin- ners. However, for SVI latent variable models, they can address the problems with ADAGRAD.</p><p>ADADELTA updates the learning rates with the following rule:</p><formula xml:id="formula_14">ρ (t) kv = E (λ (t) kv − λ (t−1) kv ) + ε E g (t) kv + ε ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_15">E x (t) = ρ 0 E x (t−1) + (1 − ρ 0 )(x (t) ) 2 ,</formula><p>ρ 0 is a decay constant, and ε is for numerical sta- bility. ADAM's update rule is determined based on esti- mates of first and second moments of the gradients:</p><formula xml:id="formula_16">m (t) kv = b m m (t−1) kv + (1 − b m )g (t) kv , u (t) kv = b u u (t−1) kv + (1 − b u )(g (t) kv ) 2 , ˆ m (t) kv = m (t) kv 1 − b t m , ˆ u (t) kv = u (t) kv 1 − b t u , λ (t+1) kv = λ (t) kv + ρ 0 ˆ m (t) kv /( ˆ u (t)</formula><p>kv + ε), (6) where ρ 0 is a constant, b controls the decay rate.</p><p>Both ADADELTA and ADAM use the moving av- erage of gradients as the denominator of learn- ing rates. The learning rates will not monotoni- cally decrease, but vary in a certain range. This property prevents online topic models from being trapped and breaks the tie between top words and bottom topic words. ADAM in particular uses bias- corrected estimate of gradientˆmgradientˆ gradientˆm kv , rather than the original stochastic gradient g kv to guide direction for the optimization and therefore achieves better results.</p><p>In addition, the magnitude of gradients is propor- tional to the dataset's size. Thus, when the dataset is small enough, ADAGRAD will still work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Empirical Study</head><p>We study three datasets: synthetic data, Wikipedia and SMS spam corpus. <ref type="bibr">1</ref> We use the generative process of LDA to generate syn- thetic data. We vary the vocabulary size V ∈ {2, 10, 100, 1000, 5000}, and the number of doc- uments D ∈ {300, 500, 10 3 , 10 4 , 10 5 , 10 6 }. The Wikipedia dataset consists of 1M articles collected from Wikipedia. <ref type="bibr">2</ref> The vocabulary is the same as <ref type="bibr" target="#b11">(Hoffman et al., 2010)</ref>. The SMS corpus is a small corpus containing 1084 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Metrics and Settings</head><p>Error rate: For experiments on synthetic data set, we use error rate</p><formula xml:id="formula_17">Error( ˆ β) = 1 K K k=1 min i ||ˆβ||ˆ ||ˆβ i − β k || 1 (7)</formula><p>to measure the difference between the estimatedˆβestimatedˆ estimatedˆβ and the known β. The min greedily matches eachˆβ eachˆ eachˆβ k to its best fit. While an uncommon metric for unsupervised algorithms, on the synthetic data we have the true β.  Predictive likelihood: For experiments on real data sets, we use per-word likelihood <ref type="bibr" target="#b12">(Hoffman et al., 2013</ref>) to evaluate the model quality. We ran- domly hold out 10K documents and 100 documents on Wikipedia and SMS respectively.</p><p>Settings: In the experiments on synthetic data, we use online LDA <ref type="bibr" target="#b11">(Hoffman et al., 2010)</ref>, since the data is generated by LDA. In the experiments on real datasets, we use online LDA and online HDP ( <ref type="bibr" target="#b17">Wang et al., 2011</ref>). In the experiments on Wikipedia, we set the number of topics K = 100 and the mini-batch size M = 100. In the experi- ments on SMS corpus, we set K = 10 and M = 20. For ADAM, we use the default setting of b, and set ρ 0 = 10 and = 1000. For ADADELTA, we set = 1000. For ADAGRAD, we set ρ 0 = = 1. These are best settings for these three methods. The best constant rate is 10 −3 . <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the experimental results on syn- thetic datasets. ADAGRAD only works well with small datasets. When the number of documents increases, ADAGRAD performance degrades. Con- versely, other methods can handle more documents. <ref type="figure" target="#fig_4">Figure 3</ref> illustrates experimental results on real corpora. ADAGRAD gets competitive results to the other algorithms on the small SMS corpus. How- ever on very large Wikipedia corpus, ADAGRAD fails to infer good topics, and its predictive ability is worse than the other methods. While ADADELTA and ADAM work well on Wikipedia, ADAM is the clear winner between the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>ADAGRAD is a simple and popular technique for online learning, but is not compatible with tradi- tional initializations and objective functions for online topic models. We show that practitioners are best off using simpler online learning techniques or ADADELTA and ADAM, which are two variants of ADAGRAD, which use the moving average of gra- dients as denominator. These two methods avoid ADAGRAD's problem. In particular, ADAM per- forms much better for prediction.</p><p>We would like to build a deeper understanding of which aspects of an unsupervised objective, near- uniform initialization, and non-identifiability con- tribute to these issues and to discover other learning problems that may share these issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental results on synthetic data sets. We vary the vocabulary size V , and the number of documents D. ADADELTA, ADAM and constant rate perform better with more data, while ADAGRAD only does well with small values of D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results on real corpora. Larger predictive likelihood is better. On Wikipedia, ADAGRAD has does worse than other methods. On SMS corpus, ADAGRAD is competitive.</figDesc></figure>

			<note place="foot" n="1"> http://www.esp.uem.es/jmgomez/smsspamcorpus/ 2 http://www.wikipedia.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, Stephan Mandt, Alp Kucukelbir, Bill Foland, Forough Poursabzi-Sangdeh and Alvin Grissom II for their insightful comments. Boyd-Graber and Lu's contri-bution is supported by NSF grants NCSE-1422492 and IIS-1409287, (UMD). Boyd-Graber is also sup-ported by <ref type="bibr">IIS-1564275</ref> and IIS-1652666. Lund is supported by collaborative NSF Grant IIS-1409739 (BYU). Any opinions, findings, results, or recom-mendations expressed here are of the authors and do not necessarily reflect the view of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">1826. An historical and critical dictionary, selected and abridged. Number 1 in An historical and critical dictionary, selected and abridged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Bayle</surname></persName>
		</author>
		<ptr target="https://books.google.com/books?id=cDsN3xOyO-oC" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00670</idno>
		<title level="m">Variational inference: A review for statisticians</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Truly nonparametric online variational inference for hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational topic models for document networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics</title>
		<meeting>Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient online and batch learning using forward backward splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2899" to="2934" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online learning for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Matthew D Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Polylingual tree-based topic models for translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nested hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="256" to="270" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An adaptive learning rate for stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online variational inference for the hierarchical Dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">William</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics</title>
		<meeting>Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual averaging methods for regularized stochastic learning and online optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2543" to="2596" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discriminative topic model using document network structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
