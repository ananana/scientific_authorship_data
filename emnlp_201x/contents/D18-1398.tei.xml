<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Learning for Low-Resource Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CIFAR Azrieli Global Scholar</orgName>
								<orgName type="institution">The University of Hong Kong ‡ New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CIFAR Azrieli Global Scholar</orgName>
								<orgName type="institution">The University of Hong Kong ‡ New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CIFAR Azrieli Global Scholar</orgName>
								<orgName type="institution">The University of Hong Kong ‡ New York University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>‡ kyunghyun.cho@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">CIFAR Azrieli Global Scholar</orgName>
								<orgName type="institution">The University of Hong Kong ‡ New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CIFAR Azrieli Global Scholar</orgName>
								<orgName type="institution">The University of Hong Kong ‡ New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">CIFAR Azrieli Global Scholar</orgName>
								<orgName type="institution">The University of Hong Kong ‡ New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Learning for Low-Resource Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3622" to="3631"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3622</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen Euro-pean languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outper-forms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT&apos;16 by seeing only 16,000 translated words (⇠ 600 parallel sentences).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite the massive success brought by neural ma- chine translation <ref type="bibr">(NMT, Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b37">Vaswani et al., 2017)</ref>, it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation sys- tems (PBMT, <ref type="bibr" target="#b24">Koehn et al., 2003)</ref>, for low-resource language pairs (see, e.g., <ref type="bibr" target="#b23">Koehn and Knowles, 2017)</ref>. In the past few years, various approaches have been proposed to address this issue. The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre <ref type="bibr">* Equal contribution.</ref> et al., <ref type="bibr" target="#b32">Sennrich et al., 2015;</ref><ref type="bibr" target="#b40">Zhang and Zong, 2016)</ref>. It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource lan- guage pairs <ref type="bibr">(Firat et al., 2016a,b;</ref><ref type="bibr" target="#b27">Lee et al., 2016;</ref><ref type="bibr" target="#b21">Johnson et al., 2016;</ref><ref type="bibr" target="#b19">Ha et al., 2016b</ref>). Its variant, transfer learning, was also proposed by <ref type="bibr" target="#b42">Zoph et al. (2016)</ref>, in which an NMT system is pretrained on a high-resource language pair before being fine- tuned on a target low-resource language pair.</p><p>In this paper, we follow up on these latest ap- proaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation. We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, <ref type="bibr" target="#b10">Finn et al., 2017</ref>) could be ap- plied to low-resource machine translation by view- ing language pairs as separate tasks. This view en- ables us to use MAML to find the initialization of model parameters that facilitate fast adaptation for a new language pair with a minimal amount of training examples ( §3). Furthermore, the vanilla MAML however cannot handle tasks with mis- matched input and output. We overcome this limi- tation by incorporating the universal lexical repre- sentation ( <ref type="bibr" target="#b15">Gu et al., 2018b</ref>) and adapting it for the meta-learning scenario ( §3.3).</p><p>We extensively evaluate the effectiveness and generalizing ability of the proposed meta-learning algorithm on low-resource neural machine trans- lation. We utilize 17 languages from Europarl and Russian from WMT as the source tasks and test the meta-learned parameter initialization against five target languages (Ro, Lv, Fi, Tr and Ko), in all cases translating to English. Our experiments using only up to 160k tokens in each of the tar- get task reveal that the proposed meta-learning approach outperforms the multilingual translation approach across all the target language pairs, and the gap grows as the number of training examples decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Neural Machine Translation (NMT) Given a source sentence X = {x 1 , ..., x T 0 }, a neural ma- chine translation model factors the distribution over possible output sentences Y = {y 1 , ..., y T } into a chain of conditional probabilities with a left- to-right causal structure:</p><formula xml:id="formula_0">p(Y |X; ✓) = T +1 Y t=1 p(y t |y 0:t1 , x 1:T 0 ; ✓),<label>(1)</label></formula><p>where special tokens y 0 (hbosi) and y T +1 (heosi) are used to represent the beginning and the end of a target sentence. These conditional probabilities are parameterized using a neural network. Typi- cally, an encoder-decoder architecture <ref type="bibr" target="#b36">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b8">Cho et al., 2014;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2015</ref>) with a RNN-based decoder is used. More recently, architectures without any recurrent struc- tures ( <ref type="bibr" target="#b13">Gehring et al., 2017;</ref><ref type="bibr" target="#b37">Vaswani et al., 2017)</ref> have been proposed and shown to speed up train- ing while achieving state-of-the-art performance.</p><p>Low Resource Translation NMT is known to easily over-fit and result in an inferior performance when the training data is limited <ref type="bibr" target="#b23">(Koehn and Knowles, 2017)</ref>. In general, there are two ways for handling the problem of low resource translation:</p><p>(1) utilizing the resource of unlabeled monolin- gual data, and (2) sharing the knowledge between low-and high-resource language pairs. Many re- search efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning <ref type="bibr" target="#b17">(Gulcehre et al., 2015;</ref><ref type="bibr" target="#b40">Zhang and Zong, 2016)</ref>, back-translation <ref type="bibr" target="#b32">(Sennrich et al., 2015)</ref>, dual learning ( <ref type="bibr" target="#b20">He et al., 2016)</ref> and unsupervised machine translation with mono- lingual corpora only for both sides <ref type="bibr" target="#b2">(Artetxe et al., 2017b;</ref><ref type="bibr" target="#b26">Lample et al., 2017;</ref>. For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks. For instance, <ref type="bibr" target="#b7">Cheng et al. (2016)</ref>; ; <ref type="bibr" target="#b28">Lee et al. (2017)</ref>;  investigate the use of a pivot to build a translation path be- tween two languages even without any directed re- source. The pivot can be a third language or even an image in multimodal domains. When pivots are not easy to obtain, <ref type="bibr" target="#b11">Firat et al. (2016a)</ref>; <ref type="bibr" target="#b27">Lee et al. (2016)</ref>; <ref type="bibr" target="#b21">Johnson et al. (2016)</ref> have shown that the structure of NMT is suitable for multilingual ma- chine translation. <ref type="bibr" target="#b15">Gu et al. (2018b)</ref> also showed that such a multilingual NMT system could im- prove the performance of low resource translation by using a universal lexical representation to share embedding information across languages.</p><p>All the previous work for multilingual NMT as- sume the joint training of multiple high-resource languages naturally results in a universal space (for both the input representation and the model) which, however, is not necessarily true, especially for very low resource cases.</p><p>Meta Learning In the machine learning com- munity, meta-learning, or learning-to-learn, has recently received interests. Meta-learning tries to solve the problem of "fast adaptation on new train- ing data." One of the most successful applications of meta-learning has been on few-shot (or one- shot) learning ( <ref type="bibr" target="#b25">Lake et al., 2015)</ref>, where a neural network is trained to readily learn to classify in- puts based on only one or a few training examples. There are two categories of meta-learning:</p><p>1. learning a meta-policy for updating model parameters (see, e.g., <ref type="bibr" target="#b0">Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b18">Ha et al., 2016a;</ref><ref type="bibr" target="#b30">Mishra et al., 2017)</ref> 2. learning a good parameter initialization for fast adaptation (see, e.g., <ref type="bibr" target="#b10">Finn et al., 2017;</ref><ref type="bibr" target="#b38">Vinyals et al., 2016;</ref><ref type="bibr" target="#b35">Snell et al., 2017)</ref>.</p><p>In this paper, we propose to use a meta-learning algorithm for low-resource neural machine trans- lation based on the second category. More specifi- cally, we extend the idea of model-agnostic meta- learning (MAML, <ref type="bibr" target="#b10">Finn et al., 2017</ref>) in the multi- lingual scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Meta Learning for Low-Resource Neural Machine Translation</head><p>The underlying idea of MAML is to use a set of source tasks T 1 , . . . , T K to find the initializa- tion of parameters ✓ 0 from which learning a tar- get task T 0 would require only a small number of training examples. In the context of machine trans- lation, this amounts to using many high-resource language pairs to find good initial parameters and training a new translation model on a low-resource language starting from the found initial parame-  ters. This process can be understood as</p><formula xml:id="formula_1">✓ ⇤ = Learn(T 0 ; MetaLearn(T 1 , . . . , T K )).</formula><p>That is, we meta-learn the initialization from aux- iliary tasks and continue to learn the target task.</p><p>We refer the proposed meta-learning method for NMT to MetaNMT. See <ref type="figure" target="#fig_0">Fig. 1</ref> for the overall il- lustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learn: language-specific learning</head><p>Given any initial parameters ✓ 0 (which can be ei- ther random or meta-learned), the prior distribution of the parameters of a de- sired NMT model can be defined as an isotropic Guassian:</p><formula xml:id="formula_2">✓ i ⇠ N (✓ 0 i , 1/),</formula><p>where 1/ is a variance. With this prior distri- bution, we formulate the language-specific learn- ing process Learn(D T ; ✓ 0 ) as maximizing the log- posterior of the model parameters given data D T :</p><formula xml:id="formula_3">Learn(D T ; ✓ 0 ) = arg max ✓ L D T (✓) = arg max ✓ X (X,Y )2D T log p(Y |X, ✓) k✓ ✓ 0 k 2 ,</formula><p>where we assume p(X|✓) to be uniform. The first term above corresponds to the maximum likeli- hood criterion often used for training a usual NMT system. The second term discourages the newly learned model from deviating too much from the initial parameters, alleviating the issue of over- fitting when there is not enough training data. In practice, we solve the problem above by maximiz- ing the first term with gradient-based optimization and early-stopping after only a few update steps.</p><p>Thus, in the low-resource scenario, finding a good initialization ✓ 0 strongly correlates the final per- formance of the resulting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MetaLearn</head><p>We find the initialization ✓ 0 by repeatedly simulat- ing low-resource translation scenarios using auxil- iary, high-resource language pairs. Following Finn et al. <ref type="formula" target="#formula_0">(2017)</ref>, we achieve this goal by defining the meta-objective function as</p><formula xml:id="formula_4">L(✓) =E k E D T k ,D 0 T k (2) 2 6 4 X (X,Y )2D 0 T k log p(Y |X; Learn(D T k ; ✓)) 3 7 5 ,</formula><p>where k ⇠ U({1, . . . , K}) refers to one meta- learning episode, and</p><formula xml:id="formula_5">D T , D 0 T follow the uniform distribution over T 's data.</formula><p>We maximize the meta-objective function using stochastic approximation <ref type="bibr" target="#b31">(Robbins and Monro, 1951</ref>) with gradient descent. For each episode, we uniformly sample one source task at random, T k . We then sample two subsets of training ex- amples independently from the chosen task, D T k and D 0 T k . We use the former to simulate language- specific learning and the latter to evaluate its out- come. Assuming a single gradient step is taken only the with learning rate ⌘, the simulation is:</p><formula xml:id="formula_6">✓ 0 k = Learn(D T k ; ✓) = ✓ ⌘r ✓ L D T k (✓).</formula><p>Once the simulation of learning is done, we evalu- ate the updated parameters ✓ 0 k on D 0 T k , The gra- dient computed from this evaluation, which we refer to as meta-gradient, is used to update the Figure 2: An intuitive il- lustration in which we use solid lines to repre- sent the learning of ini- tialization, and dashed lines to show the path of fine-tuning.</p><p>meta model ✓. It is possible to aggregate multiple episodes of source tasks before updating ✓:</p><formula xml:id="formula_7">✓ ✓ ⌘ 0 X k r ✓ L D 0 T k (✓ 0 k ),</formula><p>where ⌘ 0 is the meta learning rate. Unlike a usual learning scenario, the resulting model ✓ 0 from this meta-learning procedure is not necessarily a good model on its own. It is however a good starting point for training a good model us- ing only a few steps of learning. In the context of machine translation, this procedure can be under- stood as finding the initialization of a neural ma- chine translation system that could quickly adapt to a new language pair by simulating such a fast adaptation scenario using many high-resource lan- guage pairs.</p><p>Meta-Gradient We use the following approxi- mation property</p><formula xml:id="formula_8">H(x)v ⇡ r(x + ⌫v) r(x) ⌫ to approximate the meta-gradient: 1 r ✓ L D 0 (✓ 0 ) = r ✓ 0 L D 0 (✓ 0 )r ✓ (✓ ⌘r ✓ L D (✓)) = r ✓ 0 L D 0 (✓ 0 ) ⌘r ✓ 0 L D 0 (✓ 0 )H ✓ (L D (✓)) ⇡ r ✓ 0 L D 0 (✓ 0 ) ⌘ ⌫  r ✓ L D (✓) ˆ ✓ r ✓ L D (✓) ✓ ,</formula><p>where ⌫ is a small constant andˆ✓</p><formula xml:id="formula_9">andˆ andˆ✓ = ✓ + ⌫r ✓ 0 L D 0 (✓ 0 ).</formula><p>In practice, we find that it is also possible to ignore the second-order term, ending up with the follow- ing simplified update rule:</p><formula xml:id="formula_10">r ✓ L D 0 (✓ 0 ) ⇡ r ✓ 0 L D 0 (✓ 0 ).<label>(3)</label></formula><p>1 We omit the subscript k for simplicity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work: Multilingual Transfer Learning</head><formula xml:id="formula_11">max ✓ L multi (✓) = E k 2 4 X (X,Y )2D k log p(Y |X; ✓) 3 5 ,</formula><p>where D k is the training set of the k-th task, or lan- guage pair. The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution ✓ 0 found from solving the above problem. The major difference between the proposed MetaNMT and these multilingual transfer ap- proaches is that the latter do not consider how learning happens with the target, low-resource lan- guage pair. The former explicitly incorporates the learning process within the framework by simulat- ing it repeatedly in Eq. (2). As we will see later in the experiments, this results in a substantial gap in the final performance on the low-resource task.</p><p>Illustration In <ref type="figure">Fig. 2</ref>, we contrast transfer learn- ing, multilingual learning and meta-learning us- ing three source language pairs (Fr-En, Es-En and Pt-En) and two target pairs (Ro-En and Lv-En). Transfer learning trains an NMT system specifi- cally for a source language pair (Es-En) and fine- tunes the system for each target language pair (Ro- En, Lv-En). Multilingual learning often trains a single NMT system that can handle many different language pairs (Fr-En, Pt-En, Es-En), which may or may not include the target pairs (Ro-En, Lv- En). If not, it finetunes the system for each target pair, similarly to transfer learning. Both of these however aim at directly solving the source tasks. On the other hand, meta-learning trains the NMT system to be useful for fine-tuning on various tasks including the source and target tasks. This is done by repeatedly simulating the learning process on low-resource languages using many high-resource language pairs (Fr-En, Pt-En, Es-En).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unified Lexical Representation</head><p>I/O mismatch across language pairs One ma- jor challenge that limits applying meta-learning for low resource machine translation is that the ap- proach outlined above assumes the input and out- put spaces are shared across all the source and tar- get tasks. This, however, does not apply to ma- chine translation in general due to the vocabulary mismatch across different languages. In multilin- gual translation, this issue has been tackled by us- ing a vocabulary of sub-words ( <ref type="bibr" target="#b32">Sennrich et al., 2015</ref>) or characters ( <ref type="bibr" target="#b27">Lee et al., 2016</ref>) shared across multiple languages. This surface-level sharing is however limited, as it cannot be applied to lan- guages exhibiting distinct orthography (e.g., Indo- Euroepan languages vs. Korean.)</p><p>Universal Lexical Representation (ULR) We tackle this issue by dynamically building a vo- cabulary specific to each language using a key- value memory network ( <ref type="bibr">Miller et al., 2016;</ref><ref type="bibr" target="#b16">Gulcehre et al., 2018)</ref>, as was done successfully for low-resource machine translation recently by <ref type="bibr" target="#b15">Gu et al. (2018b)</ref>. We start with multilingual word em- bedding matrices ✏ k query 2 R |V k |⇥d pretrained on large monolingual corpora, where V k is the vo- cabulary of the k-th language. These embedding vectors can be obtained with small dictionaries of seed word pairs ( <ref type="bibr" target="#b1">Artetxe et al., 2017a;</ref><ref type="bibr" target="#b34">Smith et al., 2017)</ref> or in a fully unsupervised manner ( <ref type="bibr" target="#b41">Zhang et al., 2017;</ref><ref type="bibr" target="#b9">Conneau et al., 2018</ref>). We take one of these languages k 0 to build universal lexical repre- sentation consisting of a universal embedding ma- trix ✏ u 2 R M ⇥d and a corresponding key matrix ✏ key 2 R M ⇥d , where M &lt; |V 0 k |. Both ✏ k query and ✏ key are fixed during meta-learning. We then com- pute the language-specific embedding of token x from the language k as the convex sum of the uni- versal embedding vectors by</p><formula xml:id="formula_12">✏ 0 [x] = M X i=1 ↵ i ✏ u [i],</formula><p>where </p><formula xml:id="formula_13">↵ i / exp 1 ⌧ ✏ key [i] &gt; A✏ k query [x]</formula><formula xml:id="formula_14">✏ k [x] = ✏ 0 [x] + ✏ k [x].</formula><p>During language-specific learning, the ULR ✏ 0 <ref type="bibr">[x]</ref> is held constant, while only ✏ k <ref type="bibr">[x]</ref> is updated, starting from an all-zero vector. On the other hand, we hold ✏ k [x]'s constant while updating ✏ u and A during the meta-learning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Target Tasks We show the effectiveness of the proposed meta-learning method for low resource NMT with extremely limited training examples on five diverse target languages: Romanian (Ro) from WMT'16, 2 Latvian (Lv), Finnish (Fi), Turk- ish (Tr) from WMT'17, <ref type="bibr">3</ref> and Korean (Ko) from Korean Parallel Dataset. <ref type="bibr">4</ref> We use the officially provided train, dev and test splits for all these lan- guages. The statistics of these languages are pre- sented in <ref type="table">Table 1</ref>. We simulate the low-resource translation scenarios by randomly sub-sampling the training set with different sizes.</p><p>Source Tasks We use the following languages from Swedish (Sv), in addition to Russian (Ru) 6 to learn the intilization for fine-tuning. In our exper- iments, different combinations of source tasks are explored to see the effects from the source tasks.</p><p>Validation We pick either Ro-En or Lv-En as a validation set for meta-learning and test the gener- alization capability on the remaining target tasks. This allows us to study the strict form of meta- learning, in which target tasks are unknown during both training and model selection.</p><p>Preprocessing and ULR Initialization As de- scribed in §3.3, we initialize the query embed- ding vectors ✏ k query of all the languages. For each language, we use the monolingual corpora built from Wikipedia 7 and the parallel corpus. The con- catenated corpus is first tokenized and segmented using byte-pair encoding <ref type="bibr">(BPE, Sennrich et al., 2016)</ref>, resulting in 40, 000 subwords for each lan- guage. We then estimate word vectors using fast- Text ( <ref type="bibr" target="#b4">Bojanowski et al., 2016)</ref> and align them across all the languages in an unsupervised way using MUSE ( <ref type="bibr" target="#b9">Conneau et al., 2018</ref>) to get mul- tilingual word vectors. We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix ✏ u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model and Learning</head><p>Model We utilize the recently proposed Trans- former ( <ref type="bibr" target="#b37">Vaswani et al., 2017)</ref> as an underlying NMT system. We implement Transformer in this paper based on (Gu et al., 2018a) 8 and mod- ify it to use the universal lexical representation from §3.3. We use the default set of hyperpa- rameters (d model = d hidden = 512, n layer = 6, n head = 8, n batch = 4000, t warmup = 16000) for all the language pairs and across all the experi- mental settings. We refer the readers to ( <ref type="bibr" target="#b37">Vaswani et al., 2017;</ref><ref type="bibr" target="#b14">Gu et al., 2018a</ref>) for the details of the model. However, since the proposed meta- learning method is model-agnostic, it can be eas- ily extended to any other NMT architectures, e.g. RNN-based sequence-to-sequence models with at- tention ( <ref type="bibr" target="#b3">Bahdanau et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Train</head><p>Ro <ref type="table">zero  finetune  zero  finetune  zero  finetune  zero  finetune</ref> zero finetune 00.00 ± .00 0.00 ± .00 0.00 ± .00 0.00 ± .00 0.00 ± .00 Es 9.20 15.71 ± .22 2.23 4.65 ± .12 2.73 5.55 ± .08 1.56 4.14 ± .03 0.63 1.40 ± .09 Es Fr 12.35 17.46 ± .41 2.86 5.05 ± .04 3.71 6.08 ± .01 2.17 4.56 ± .20 0.61 1.70 ± .14 Es Fr It Pt 13.88 18.54 ± .19 3.88 5.63 ± .11 4.93 6.80 ± .04 2.49 4.82 ± .10 0.82 1.90 ± .07 De Ru 10.60 16.05 ± .31 5.15</p><note type="other">-En Lv-En Fi-En Tr-En Ko-En</note><p>7.19 ± .17 6.62 7.98 ± .22 3.20 6.02 ± .11 1.19 2.16 ± .09 Es Fr It Pt De Ru 15.93 20.00 ± .27 6.33</p><p>7.88 ± .14 7.89 9.14 ± .05 3.72 6.02 ± .13 1.28 2.44 ± .11 All 18.12 22.04 ± .23 9.58 10.44 ± .17 11.39 12.63 ± .22 5.34 8.97 ± .08 1.96 3.97 ± .10</p><p>Full Supervised 31.76 15.15 20.20 13.74 5.97 <ref type="table">Table 2</ref>: BLEU Scores w.r.t. the source task set for all five target tasks. Learning We meta-learn using various sets of source languages to investigate the effect of source task choice. For each episode, by default, we use a single gradient step of language-specific learning with Adam ( <ref type="bibr" target="#b22">Kingma and Ba, 2014</ref>) per comput- ing the meta-gradient, which is computed by the first-order approximation in Eq. (3). For each target task, we sample training exam- ples to form a low-resource task. We build tasks of 4k, 16k, 40k and 160k English tokens for each lan- guage. We randomly sample the training set five times for each experiment and report the average score and its standard deviation. Each fine-tuning is done on a training set, early-stopped on a val- idation set and evaluated on a test set. In default without notation, datasets of 16k tokens are used.</p><p>Fine-tuning Strategies The transformer con- sists of three modules; embedding, encoder and decoder. We update all three modules during meta- learning, but during fine-tuning, we can selectively tune only a subset of these modules. Following <ref type="bibr" target="#b42">(Zoph et al., 2016)</ref>, we consider three fine-tuning strategies; (1) fine-tuning all the modules (all), <ref type="formula">(2)</ref> fine-tuning the embedding and encoder, but freez- ing the parameters of the decoder (emb+enc) and (3) fine-tuning the embedding only (emb).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vs. Multilingual Transfer Learning</head><p>We meta- learn the initial models on all the source tasks us- ing either Ro-En or Lv-En as a validation task. We also train the initial models to be multilin- gual translation systems. We fine-tune them us- ing the four target tasks (Ro-En, Lv-En, Fi-En and Tr-En; 16k tokens each) and compare the pro- posed meta-learning strategy and the multilingual, transfer learning strategy. As presented in <ref type="figure">Fig. 3</ref>, the proposed learning approach significantly out- performs the multilingual, transfer learning strat- egy across all the target tasks regardless of which target task was used for early stopping. We also notice that the emb+enc strategy is most effec- tive for both meta-learning and transfer learn- ing approaches. With the proposed meta-learning and emb+enc fine-tuning, the final NMT systems trained using only a fraction of all available train- ing examples achieve 2/3 (Ro-En) and 1/2 (Lv-En, Fi-En and Tr-En) of the BLEU score achieved by the models trained with full training sets.</p><p>vs. Statistical Machine Translation We also test the same Ro-En datasets with 16, 000 target tokens using the default setting of Phrase-based MT (Moses) with the dev set for adjusting the parameters and the test set for calculating the fi- nal performance. We obtain 4.79(±0.234) BLEU point, which is higher than the standard NMT per- formance (0 BLEU). It is however still lower than both the multi-NMT and meta-NMT.</p><p>Impact of Validation Tasks Similarly to train- ing any other neural network, meta-learning still requires early-stopping to avoid overfitting to a specific set of source tasks. In doing so, we ob- serve that the choice of a validation task has non- negligible impact on the final performance. For in- stance, as shown in <ref type="figure">Fig. 3</ref>, Fi-En benefits more when Ro-En is used for validation, while the oppo- site happens with Tr-En. The relationship between the task similarity and the impact of a validation task must be investigated further in the future.</p><p>Training Set Size We vary the size of the tar- get task's training set and compare the proposed meta-learning strategy and multilingual, transfer learning strategy. We use the emb+enc fine-tuning on Ro-En and Fi-En. <ref type="figure" target="#fig_4">Fig. 4</ref> demonstrates that the meta-learning approach is more robust to the drop in the size of the target task's training set. The gap between the meta-learning and transfer learning grows as the size shrinks, confirming the effective- ness of the proposed approach on extremely low- resource language pairs. Impact of Source <ref type="table">Tasks In Table 2</ref>, we present the results on all five target tasks obtained while varying the source task set. We first see that it is always beneficial to use more source tasks. Al- though the impact of adding more source tasks varies from one language to another, there is up to 2⇥ improvement going from one source task to 18 source tasks (Lv-En, Fi-En, Tr-En and Ko-En). The same trend can be observed even without any fine-tuning (i.e., unsupervised translation, <ref type="bibr" target="#b26">(Lample et al., 2017;</ref><ref type="bibr" target="#b2">Artetxe et al., 2017b)</ref>). In addi- tion, the choice of source languages has different implications for different target languages. For in- stance, Ro-En benefits more from {Es, Fr, It, Pt} than from {De, Ru}, while the opposite effect is observed with all the other target tasks.</p><p>Training Curves The benefit of meta-learning over multilingual translation is clearly demon- strated when we look at the training curves in <ref type="figure" target="#fig_5">Fig. 5</ref>. With the multilingual, transfer learning ap- proach, we observe that training rapidly saturates and eventually degrades, as the model overfits to the source tasks. MetaNMT on the other hand con- tinues to improve and never degrades, as the meta- objective ensures that the model is adequate for fine-tuning on target tasks rather than for solving the source tasks.</p><p>Sample Translations We present some sample translations from the tested models in <ref type="table">Table 3</ref>. Inspecting these examples provides the insight into the proposed meta-learning algorithm. For in- stance, we observe that the meta-learned model without any fine-tuning produces a word-by-word translation in the first example (Tr-En), which is due to the successful use of the universal lexcial representation and the meta-learned initialization. The system however cannot reorder tokens from Turkish to English, as it has not seen any train- ing example of Tr-En. After seeing around 600 sentence pairs (16K English tokens), the model rapidly learns to correctly reorder tokens to form a better translation. A similar phenomenon is ob- served in the Ko-En example. These cases could be found across different language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a meta-learning algo- rithm for low-resource neural machine translation that exploits the availability of high-resource lan- guages pairs. We based the proposed algorithm on the recently proposed model-agnostic meta- learning and adapted it to work with multiple lan- guages that do not share a common vocabulary us- ing the technique of universal lexcal representa- tion, resulting in MetaNMT. Our extensive evalu- ation, using 18 high-resource source tasks and 5 low-resource target tasks, has shown that the pro- posed MetaNMT significantly outperforms the ex- isting approach of multilingual, transfer learning in low-resource neural machine translation across all the language pairs considered.</p><p>The proposed approach opens new opportuni- ties for neural machine translation. First, it is a principled framework for incorporating various extra sources of data, such as source-and target- side monolingual corpora. Second, it is a generic framework that can easily accommodate existing and future neural machine translation systems. <ref type="bibr">Source (Tr)</ref> google mülteciler için 11 milyon dolar toplamaküzeretoplamak¨toplamaküzere ba˘ gıs¸es¸les¸tirmegıs¸gıs¸esgıs¸es¸lesgıs¸es¸les¸tirme kampanyasını bas¸lattıbas¸lattı . Target google launches donation-matching campaign to raise $ 11 million for refugees . Meta-0 google refugee fund for usd 11 million has launched a campaign for donation . <ref type="bibr">Meta-16k</ref> google has launched a campaign to collect $ 11 million for refugees .</p><p>Source (Ko) tà-¥Ï⇠¥ 0å⌧¨å‰0å⌧¨å‰ ⌘-î ÙÌ\ p ‡⌅¨,‡⌅¨, ∏`x , Xx , Ω⌧x Òt Ïh⇣‰ Target among the suspects are retired military officials , journalists , politicians , businessmen and others . Meta-0 last year , convicted people , among other people , of a high-ranking army of journalists in economic and economic policies , were included . Meta-16k the arrested persons were included in the charge , including the military officials , journalists , politicians and economists . <ref type="table">Table 3</ref>: Sample translations for Tr-En and Ko-En highlight the impact of fine-tuning which results in syntactically better formed translations. We highlight tokens of interest in terms of reordering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The graphical illustration of the training process of the proposed MetaNMT. For each episode, one task (language pair) is sampled for meta-learning. The boxes and arrows in blue are mainly involved in language-specific learning ( §3.1), and those in purple in meta-learning ( §3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The proposed MetaNMT differs from the existing framework of multilingual translation (Lee et al., 2016; Johnson et al., 2016; Gu et al., 2018b) or transfer learning (Zoph et al., 2016). The latter can be thought of as solving the following problem:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: BLEU scores reported on test sets for {Ro, Lv, Fi, Tr} to En, where each model is first learned from 6 source tasks (Es, Fr, It, Pt, De, Ru) and then fine-tuned on randomly sampled training sets with around 16,000 English tokens per run. The error bars show the standard deviation calculated from 5 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: BLEU Scores w.r.t. the size of the target task's training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The learning curves of BLEU scores on the validation task (Ro-En).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>and ⌧ is set to 0.05. This approach allows us to handle lan- guages with different vocabularies using a fixed number of shared parameters (✏ u , ✏ key and A.) Learning of ULR It is not desirable to update the universal embedding matrix ✏ u when fine-</figDesc><table># of sents. # of En tokens 
Dev 
Test 

Ro-En 
0.61 M 
16.66 M 
31.76 
Lv-En 
4.46 M 
67.24 M 
20.24 15.15 
Fi-En 
2.63 M 
64.50 M 
17.38 20.20 
Tr-En 
0.21 M 
5.58 M 
15.45 13.74 
Ko-En 
0.09 M 
2.33 M 
6.88 
5.97 

Table 1: Statistics of full datasets of the target lan-
guage pairs. BLEU scores on the dev and test sets 
are reported from a supervised Transformer model 
with the same architecture. 

tuning on a small corpus which contains a lim-
ited set of unique tokens in the target language, 
as it could adversely influence the other tokens' 
embedding vectors. We thus estimate the change 
to each embedding vector induced by language-
specific learning by a separate parameter ✏ k [x]: 

</table></figure>

			<note place="foot" n="2"> http://www.statmt.org/wmt16/translation-task.html 3 http://www.statmt.org/wmt17/translation-task.html 4 https://sites.google.com/site/koreanparalleldata/ 5 http://www.statmt.org/europarl/</note>

			<note place="foot" n="6"> A subsample of approximately 2M pairs from WMT&apos;17. 7 We use the most recent Wikipedia dump (2018.5) from https://dumps.wikimedia.org/backup-index.html.</note>

			<note place="foot" n="8"> https://github.com/salesforce/nonauto-nmt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported in part by the Face-book Low Resource Neural Machine Translation Award. This work was also partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recogni-tion to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure). KC thanks support by eBay, TenCent, NVIDIA and CIFAR.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A teacher-student framework for zeroresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00753</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Zeroresource neural machine translation with multiagent communication game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03116</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04928</idno>
		<title level="m">Neural machine translation with pivot languages</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-Decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<title level="m">Word translation without parallel data. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zero-resource translation with multi-lingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatos T Yarman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Vural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Universal neural machine translation for extremely low resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05368</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic neural turing machine with continuous and discrete addressing schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="857" to="884" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Toward multilingual neural machine translation with universal encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04798</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03872</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03017</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Emergent translation in multi-agent communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06922</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Karimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<title level="m">Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<title level="m">Meta-learning with temporal convolutions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02891</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03859</idno>
		<title level="m">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quôc</forename><surname>Lê</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation with weight sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09057</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s distance minimization for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Transfer learning for lowresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02201</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
