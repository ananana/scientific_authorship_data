<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Network Model for Low-Resource Universal Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
							<email>lduong@student.unimelb.edu.au {t.cohn,sbird}@unimelb.edu.au paul.cook@unb.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">University of New Brunswick</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Victoria Research Laboratory</orgName>
								<orgName type="institution">National ICT Australia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Network Model for Low-Resource Universal Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Accurate dependency parsing requires large treebanks, which are only available for a few languages. We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data. We propose a model for learning a shared &quot;univer-sal&quot; parser that operates over an inter-lingual continuous representation of language , along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is an important task for Nat- ural Language Processing (NLP) with application to text classification <ref type="bibr" target="#b21">( ¨ Ozgür and Güngör, 2010</ref>), relation extraction ( <ref type="bibr" target="#b1">Bunescu and Mooney, 2005</ref>), question answering ( <ref type="bibr" target="#b4">Cui et al., 2005</ref>), statistical machine translation ( <ref type="bibr" target="#b28">Xu et al., 2009)</ref>, and sen- timent analysis <ref type="bibr" target="#b23">(Socher et al., 2013)</ref>. A mature parser normally requires a large treebank for train- ing, yet such resources are rarely available and are costly to build. Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for low- resource languages.</p><p>In this paper we formalize the dependency pars- ing task for a low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain, while a much larger treebank in a high-resource lan- guage forms the out-of-domain data. In this way, we can apply well-understood domain adaptation techniques to the dependency parsing task. How- ever, a crucial requirement for domain adaptation is that the in-domain and out-of-domain data have compatible representations. In applying our ap- proach to data from several languages, we must learn such a cross-lingual representation. Here we frame this representation learning as part of a neural network training. The underlying hypoth- esis for the joint learning is that there are some shared-structures across languages that we can ex- ploit. This hypothesis is motivated by the excellent results of the cross-lingual application of unlexi- calised parsing <ref type="bibr" target="#b15">(McDonald et al., 2011</ref>), whereby a delexicalized parser constructed on one language is applied directly to another language.</p><p>Our approach works by jointly training a neu- ral network dependency parser to model the syn- tax in both a source and target language. Many of the parameters of the source and target language parsers are shared, except for a small handful of language-specific parameters. In this way, the in- formation can flow back and forth between lan- guages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one an- other's errors. We include some language-specific components, in order to better model the lexicon of each language and allow learning of the syntac- tic idiosyncrasies of each language. Our experi- ments show that this outperforms a purely super- vised setting, on both small and large data condi- tions, with a gain as high as 10% for small train- ing sets. Our proposed joint training method also out-performs the conventional cascade approach where the parameters between source and target languages are related together through a regular- ization term ( <ref type="bibr" target="#b7">Duong et al., 2015)</ref>.</p><p>Our model is flexible, allowing easy incorpora- tion of peripheral information. For example, as- suming the presence of a small bilingual dictio- nary is befitting of a low-resource setting, as this is prototypically one of the first artifacts gener- ated by field linguists. We incorporate a bilin- gual dictionary as a set of soft constraints on the model, such that it learns similar representations for each word and its translation(s). For example, the representation of house in English should be close to haus in German. We empirically show that adding a bilingual dictionary improves parser performance, particularly when target data is lim- ited.</p><p>The final contribution of the paper concerns the learned word embeddings. We demonstrate that these encode meaningful syntactic phenom- ena, both in terms of the observable clusters and through a verb classification task. The code for this paper is published as an open source project. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This work is motivated by the idea of delexical- ized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language ( <ref type="bibr" target="#b30">Zeman et al., 2008)</ref>. It is then applied directly to parse sentences in the target resource-poor languages. Delexical- ized parsing relies on the fact that identical part-of- speech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages.</p><p>Building a dependency parser for a resource- poor language usually starts with the delexical- ized parser and then uses other resources to refine the model. <ref type="bibr" target="#b15">McDonald et al. (2011)</ref> and <ref type="bibr" target="#b14">Ma and Xia (2014)</ref> exploited parallel data as the bridge to transfer constraints from the source resource- rich language to the target resource-poor lan- guages. <ref type="bibr">Täckström et al. (2012)</ref> also used par- allel data to induce cross-lingual word clusters which added as features for their delexicalized parser. <ref type="bibr" target="#b8">Durrett et al. (2012)</ref> constructed the set of language-independent features and used a bilin- gual dictionary as the bridge to transfer these fea- tures from source to target language. <ref type="bibr">Täckström et al. (2013)</ref> additionally used high-level linguis- tic features extracted from the World Atlas of Lan- guage Structures (WALS) <ref type="bibr" target="#b5">(Dryer and Haspelmath, 2013)</ref>.</p><p>For low-resource languages, no large paral- lel corpus is available.</p><p>Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California ( <ref type="bibr" target="#b9">Garrett et al., 2013)</ref>. Ac- cordingly, we adopt a different resource require- 1 http://github.com/longdt219/ universal_dependency_parser ment: a small treebank in the target low-resource language.</p><p>Domain adaptation or joint-training is a differ- ent branch of research, and falls outside the scope of this paper. Nevertheless, we would like to con- trast our work with <ref type="bibr">Senna (Collobert et al., 2011</ref>), a neural network framework to perform a vari- ety of NLP tasks such as part-of-speech (POS) tagging, named entity recognition (NER), chunk- ing, and so forth. Both approaches exploit com- mon linguistic properties of the data through joint learning. However, Collobert et al's goal is to find a single input representation that can work well for many tasks. Our goal is different: we allow the joint-training inputs to be different but con- strain the parameter weights in the upper layer to be identical. Consequently, our method ap- plies to the task where inputs are different, pos- sibly from different languages or domains. Their method applies for different tasks in the same lan- guage/domain where the inputs are fairly similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Neural Network Parser</head><p>This section describes the monolingual neural net- work dependency parser structure of <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>. This parser achieves excellent performance, and has a highly flexible formula- tion allowing auxilliary inputs. The model is based on a transition-based dependency parser <ref type="bibr" target="#b20">(Nivre, 2006</ref>) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration. <ref type="bibr">2</ref> That is, for each configura- tion, the selected list of words, POS tags and la- bels from the Stack, Queue and Arcs are extracted. Each word, POS and label is mapped into a low- dimension vector representation using an embed- ding matrix, which is then fed into a two-layer neural network classifier to predict the next pars- ing action. The set of parameters for the model is E = {E word , E pos , E arc } for the embedding layer, W 1 for the fully connected cubic hidden layer and W 2 for the softmax output layer. The model pre- diction function is</p><formula xml:id="formula_0">P (Y |X = x, W 1 , W 2 , E) = softmax W 2 × cube(W 1 × Φ [ x, E])<label>(1)</label></formula><p>2 Our approach is focused on a technique for transfer learning which can be more widely applied to other types of dependency parser (and models, generally) regardless of whether they are transition-based or graph-based.</p><p>where cube is a non-linear activation function, Φ is the embedding function that returns a vector rep- resentation of parsing state x using an embedding matrix E. We refer the reader to <ref type="bibr" target="#b2">Chen and Manning (2014)</ref> for a more detailed description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Joint Interlingual Model</head><p>We assume a small treebank in a target resource- poor language, as well as a larger treebank in the source language. Our objective is to learn a model of both languages, subject to the constraint that both models are similar overall, while allowing for some limited language variability. Instead of just training two different parsers on source and then on target, we train them jointly, in order to learn an interlingual parser. This allows the method to take maximum advantage of the limited treebank data available, resulting in highly accurate pre- dicted parses.</p><p>Training a monolingual parser as de- scribed in section 2.1 requires optimizing the simple cross-entropy learning objec- tive,</p><formula xml:id="formula_1">L = − |D| i=1 log P (Y = y (i) |X = x (i) )</formula><p>, where P (Y |X) is given by equation 1 and</p><formula xml:id="formula_2">D = { x (i) , y (i) } n i=1</formula><p>is the training data. Joint training of a parser over the source and target languages can be achieved by simply adding two such cross-entropy objectives, i.e.,</p><formula xml:id="formula_3">L joint = − |Ds| i=1 log P (Y s = y (i) s |X s = x (i) s ) − |Dt| i=1 log P (Y t = y (i) t |X t = x (i) t ) , (2)</formula><p>where the training data, D = D s ∪ D t , comprises data in both the source and target language. How- ever training the model according to equation 2 will result in two independent parsers. To enforce similarity between the two parsers, we adopt pa- rameter sharing: the neural network parameters, W 1 and W 2 , are identical in both parsers. Thereby</p><formula xml:id="formula_4">P (Y α |X α = x) = P (Y |X = x, W 1 , W 2 , E α ) ,</formula><p>where the subscript α ∈ {s, t} denotes the source or target language. We allow the embedding matrix E α to differ in order to accommodate language-specific features, in terms of the repre- sentations of lexical types, E word s , part-of-speech, E pos s and dependency arc labels E arc s . This reflects the fact that different languages have different lex- icon, parts-of-speech often exhibit different roles, and dependency edges serve different functions, e.g. in Korean a static verb can serve as an adjec- tive <ref type="bibr" target="#b11">(Kim, 2001)</ref>. During training, the language- specific errors are back propagated through dif- ferent branches according to the language, guid- ing learning towards an interlingual representa- tion that informs parsing decisions in both lan- guages. The set of parameters for the model is W 1 , W 2 , E s , E t where E s , E t are the embedding matrices for the source and target languages.</p><p>Generally speaking, we can understand the model as building the universal dependency parser that parses the universal language. Specifically, the model is the combination of two parts: the universal part (W 1 , W 2 ) that is shared between the languages, and the conversion part (E s , E t ) that maps a language-specific representation into the universal language. Naturally, we could stack sev- eral non-linear layers in the conversion compo- nents such that the model can better transform the input into the universal representation; we leave this exploration for future work. Currently, our cross-lingual word embeddings are meaningful for a pair of source and target languages. However, our model can easily be used for joint training over k &gt; 2 languages. We also leave this avenue of en- quiry for future work One concern from equation 2 is that when the source language treebank D s is much bigger than the target language treebank D t , it is likely to dominate, and consequently, learning will mainly focus on optimizing the source language parser. We adjust for this disparity by balancing the two datasets, D s and D t , during training. When select- ing mini-batches for online gradient updates, we select an equal number of classification instances from the source and target languages. Thus, for each step |D s | = |D t |, effectively reweighting the cross-entropy components in (2) to ensure parity between the languages. The other concern is over-fitting, especially when we only have a small treebank in the tar- get language. As suggested by Chen and Man- ning (2014), we apply drop-out, a form of reg- ularization for both source and target language. That is, we randomly drop some of the activa- tion units from both hidden layer and input layer. Following <ref type="bibr" target="#b24">Srivastava et al. (2014)</ref>, we randomly dropout 20% of the input layer and 50% of the hid-den layer. Empirically, we observe a substantial improvement applying dropout to the model over MLE or l 2 regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Incorporating a Dictionary</head><p>Our model is flexible, enabling us to freely add additional components. In this section, we assume the presence of a bilingual dictionary between the source and target language. We seek to incorpo- rate this dictionary as a part of model learning, to encode the intuition that if two lexical items are translations of one another, the parser should treat them similarly. <ref type="bibr">3</ref> Recall that the mapping layer is the combination of word, pos and arc embed- dings, i.e.,</p><formula xml:id="formula_5">E α = {E word α , E pos α , E arc α }.</formula><p>We can easily add bilingual dictionary constraints to the model in the form of regularization to minimize the l 2 distance between word representations, i.e.,</p><formula xml:id="formula_6">i,j∈D E word(i) s − E word(j) t 2</formula><p>F , where D com- prises translation pairs, word(i) and word(j).</p><p>When the languages share the same POS tagset and arc set, <ref type="bibr">4</ref> we can also add further constraints such as their language-specific embeddings be close together. This results a regularised training objective,</p><formula xml:id="formula_7">L dict = L joint −λ i,j∈D E word(i) s −E word(j) t 2 F + E pos s − E pos t 2 F + E arc s − E arc t 2 F ,<label>(3)</label></formula><p>where λ ∈ [0, ∞] controls to what degree we bind these words or pos tags or arc labels to- gether, with high λ tying the parameters and small λ allowing independent learning. We expect the best value of λ to fall somewhere between these extremes. Finally, we use a mini-batch size of 1000 instance pairs and adaptive learning rate trainer, adagrad (Duchi et al., 2011) to build our two separate models corresponding to equations 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We experiment with the Universal Dependency Treebank (UDT) V1.0 ( <ref type="bibr" target="#b19">Nivre et al., 2015)</ref>, sim- ulating low resource settings. <ref type="bibr">5</ref> This treebank has many desirable properties for our model: the de- pendency types (arc labels set) and coarse POS tagset are the same across languages. This re- moves the need for mapping the source and target language tagsets to a common tagset. Moreover, the dependency types are also common across languages allowing evaluation of the labelled at- tachment score (LAS). The treebank covers 10 languages, <ref type="bibr">6</ref> with some languages very highly resourced-Czech, French and Spanish have 400k tokens-and only modest amounts of data for other languages-Hungarian and Irish have only around 25k tokens. Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Cascade Model</head><p>We compare our approach to a baseline inter- lingual model based on the same parsing algo- rithm as presented in section 2.1, but with cas- caded training ( <ref type="bibr" target="#b7">Duong et al., 2015)</ref>. This works by first learning the source language parser, and then training the target language parser using a regularization term to minimise the distance be- tween the parameters of the target parser and the source parser (which is fixed). In this way, some structural information from the source parser can be used in the target parser, however it is likely that the representation will be overly biased to- wards the source language and consequently may not prove as useful for modelling the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Monolingual Word Embeddings</head><p>While the E pos and E arc are randomly initialized, we initialize both the source and target language word embeddings E word s , E word t of our neural net- work models with pre-trained embeddings. This is an advantage since we can incorporate the mono- lingual data which is often available, even for   <ref type="figure">Figure 1</ref>: Sensitivity of regularization parameter λ against the LAS measured on the Swedish devel- opment set trained on 1000 (tokens).</p><p>resource-poor languages. We collect monolingual data for each language from the Machine Trans- lation Workshop (WMT) data, 7 Europarl ( <ref type="bibr" target="#b12">Koehn, 2005)</ref> and EU Bookshop Corpus (Skadin¸ˇSkadin¸Skadin¸ˇ s et al., 2014). The size of monolingual data also varies significantly, with as much as 400 million tokens for English and German, and as few as 4 mil- lion tokens for Irish. We use the skip-gram model ( <ref type="bibr" target="#b18">Mikolov et al., 2013b</ref>) to induce 50-dimensional word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Bilingual Dictionary</head><p>For the extended model as described in section 3.1, we also need a bilingual dictionary. We extract dictionaries from PanLex ( <ref type="bibr" target="#b10">Kamholz et al., 2014</ref>) which currently covers around 1300 language va- rieties and about 12 million expressions. This dataset is growing and aims at covering all lan- guages in the world and up to 350 million expres- sions. The translations in PanLex come from var- ious sources such as glossaries, dictionaries, au- tomatic inference from other languages, etc. Nat- urally, the bilingual dictionary size varies greatly among resource-poor and resource-rich languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Regularization Parameter Tuning</head><p>Joint training with a dictionary (see equation 3) includes a regularization sensitivity parameter λ. This parameter controls to what extent we should bind the source words and their target translation, common POS tags and arcs together. In this sec- tion we measure the sensitivity of our approach with respect to this parameter. In a real world sce-nario, getting development data to tune this param- eter is difficult. Thus, we want a parameter that can work well cross-lingually. To simulate this, we only tune the parameter on one language and apply it directly to different languages. We trained on a small Swedish treebank with 1k tokens, test- ing several different values of λ. We evaluated on the Swedish development dataset. <ref type="figure">Figure 1</ref> shows the labelled attachment score (LAS) for different λ. It's clearly visible that λ = 0.0001 gives the maximum LAS on the development set. Thus, we use this value for all the experiments involving a dictionary hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results</head><p>For our initial experiments we assume that we have only a small target treebank with 3000 to- kens (around 200 sentences). Ideally the much larger source language (English) treebank should be able to improve parser performance versus sim- ple supervised learning on such a small collection. We apply the joint model (equation 2) and joint model with the dictionary constraints (equation 3) for each target language,</p><p>The results are reported in <ref type="table">Table 1</ref>. The su- pervised neural network dependency parser per- formed worst, as expected, and the baseline cas- cade model consistently outperformed the super- vised model on all languages by an average mar- gin of 5.6% (absolute). <ref type="bibr">8</ref> The joint model also consistently out-performed both baselines giving a further 1.9% average improvement over the cas- cade. This was despite the fact that the cascaded model had the benefit of tuning for the regulariza- tion parameters on a development corpus, while the joint model had no parameter tuning. Note that the improvement varies substantially across lan- guages, and is largest for Czech but is only minor for Swedish. The joint model with the bilingual dictionary outperforms the joint model, however, the improvement is modest (0.7%). Nevertheless, this model gives substantial improvements com- pared with the cascaded and the supervised model (2.6% and 8.2%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning Curve</head><p>In section <ref type="bibr">4</ref>  target language data? <ref type="figure" target="#fig_1">Figure 2</ref> shows the learn- ing curve with respect to various models on differ- ent data sizes averaged over all target languages. For small datasets of 1k training tokens, the cas- caded model, joint model and joint + dict model performed similarly well, out-performing the su- pervised model by about 10% (absolute). With more training data, we see interesting changes to the relative performance of the different mod- els. While the baseline cascade model still out- performs the supervised model, the improvement is diminishing, and by 15k, the difference is only 2.9%. On the other hand, compared with the su- pervised model, the joint and joint + dict models perform consistently well at all sizes, maintaining an 8% lead at 15k. This shows the superiority of joint training compared with single language train- ing.</p><p>To understand this pattern of performance dif- ferences for the cascade versus the joint model, one needs to consider the cascade model formu- lation. In this approach, the target language pa- rameters are tied (softly) with the source language parameters through regularization. This is a bene- fit for small datasets, providing a smoothing func- tion to limit overtraining. However, when we have more training data, these constraints limit the capacity of the model to describe the target data. This is compounded by the problem that the source representation may not be appropriate for modelling the target language, and there is no way to correct for this. In contrast the joint model learns a mutually compatible representation auto- matically during joint training.</p><p>The performance results for the joint model with and without the dictionary are similar over- all. Only on small datasets (1k, 3k), is the dif- ference notable. From 5k tokens, the bilingual dictionary doesn't confer additional information, presumably as there is sufficient data for learning syntactic word representations. Moreover, trans- lation entries exist between syntactically related word types as well as semantically related pairs, with the latter potentially limiting the beneficial effect of the dictionary.</p><p>When training on all the target language data, the supervised model does well, surpassing the cascade model. Surprisingly, the joint models out- perform slightly, yielding a 0.4% improvement. This is an interesting observation suggesting that our method has potential for use not only for low resource problems, but also high resource settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Different Tagsets</head><p>In the above experiments, we used the universal POS tagset for all the languages in the corpus. However, for some languages, 9 the UDT also pro- vides language specific POS tags. We use this data to test the relative performance of the model using a universal tagset cf. language specific tagsets. In this experiment, we applied the same joint model (see §3) but with a language specific tagset instead of UPOS for these languages. We expect the joint model to automatically learn to project the differ- ent tagsets into a common space, i.e., implicitly learn a tagset mapping between languages. <ref type="figure" target="#fig_2">Fig- ure 3</ref> shows the learning curve comparing the joint model with the two types of POS tagsets. For the small dataset, it is clear that the data is insuffi- cient for the model to learn a good tagset map- ping, especially for a morphologically rich lan- guage like Czech. However, with more data, the model is better able to learn the tagset mapping as part of joint training. Beyond 15k tokens, the joint model using the language specific POS tagset out- performs UPOS. Clearly there is some information lost in the UPOS tagset, although the UPOS map- ping simultanously provides implicit linguistic su- pervision. This explains why the UPOS might be useful in small data scenarios, but detrimental at scale. Using all the target data ("All") the language specific POS provides a 1% (absolute) gain over UPOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Universal Representation</head><p>As described in section 3, we can consider our joint model as the combination of two parts: a uni- versal parser and a language-specific embedding E s or E t that converts the source and target lan- guage into the universal representation. We now seek to analyse qualitatively this universal repre- sentation through visualization. For this purpose we use a joint model of English and French, using all the available French treebank (more than 350k This is partially under- standable since word embeddings for dependency parsing need to convey the dependency context rather than surrounding words, as in most distri- butional embedding models. Words having similar dependency relation should be grouped together as they are treated similarly by the parser. Some of the learned cross-lingual word- embeddings are shown in <ref type="table">Table 2</ref>, which includes the five nearest neighbours to selected English words according to the monolingual word embed- ding (section 4.3) and our cross-lingual depen- dency word embeddings, trained using PanLex. The monolingual sets appear to be strongly char- acterised by distributional similarity. The cross- lingual embeddings display greater semantic sim- ilarity, while being more variable morphosyntacti- cally. In many cases, the top five words of English and French are translations of each other, but with varying inflectional endings in the French forms. For example, "buy" vs "vendez" or "invest" vs "in-   <ref type="table">Table 2</ref>: Examples of 5 nearest neighbours with the target English word using the original mono- lingual word embedding and our cross-lingual de- pendency based word embedding.</p><p>vestir". This is a direct consequence of incorpo- rating the bilingual lexicon. Moreover, the top five closest words of both English and French mostly have the same part of speech. This is consistent with the finding in <ref type="figure" target="#fig_3">Figure 4</ref>. <ref type="bibr" target="#b13">Levin (1993)</ref> has shown that there is a strong connection between a verb's meaning and its syn- tactic behaviour. We compare the English side of our cross-lingual dependency based word em- beddings with various other pre-trained monolin- gual English word embeddings and our mono- lingual embedding (section 4.3) on Verb-143 dataset ( <ref type="bibr" target="#b0">Baker et al., 2014</ref>). This dataset con- tains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning sim- ilarity. <ref type="table">Table 3</ref> shows the Pearson correlation Correlation Senna <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> 0.36 Skip-gram ( <ref type="bibr" target="#b17">Mikolov et al., 2013a)</ref> 0.27 RNN ( <ref type="bibr" target="#b16">Mikolov et al., 2011)</ref> 0.31 Our monolingual embedding 0.39 Our crosslingual embedding 0.44 <ref type="table">Table 3</ref>: Compare the English side of our cross- lingual embeddings with various other embed- dings evaluated on Verb-143 dataset ( <ref type="bibr" target="#b0">Baker et al., 2014</ref>). We directly use the pre-trained models from corresponding papers.</p><p>with human judgment for our embeddings and other pre-trained embeddings. As expected, our cross-lingual embeddings out-perform others em- beddings on this dataset. This is partly because the syntactic behaviour is well encoded in our word embeddings through dependency relation. Our embeddings encode not just cross-lingual correspondences, but also capture dependency re- lations which we expect might be beneficial for other NLP tasks based on dependency parsing, e.g., cross-lingual semantic role labelling where long-distance relationship can be captured by word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a training method for building a dependency parser for a resource- poor language using a larger treebank in a high- resource language. Our approach takes advantage of the shared structure among languages to learn a universal parser and language-specific mappings to the lexicon, parts of speech and dependency arcs. Compared with supervised learning, our joint model gives a consistent 8-10% improvement over several different datasets in simulation low- resource scenarios. Interestingly, some small but consistent gains are still realised by joint cross- lingual training even on large complete treebanks. This suggests that our approach has utility not just in low resource settings. Our joint model is flexi- ble, allowing the incorporation of a bilingual dic- tionary, which results in small improvements par- ticularly for tiny training scenarios.</p><p>As the side-effect of training our joint model, we obtain cross-lingual word embeddings special- ized for dependency parsing. We expect these em- beddings to be beneficial to other syntatic and se-mantic tasks. In future work, we plan to extend joint training to several languages, and further ex- plore the idea of learning and exploiting cross- lingual embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning curve for Joint model, Joint + Dict model, Baseline cascaded and Supervised model: the x-axis is the size of data (number of tokens); the y-axis is the average LAS measured on 9 languages (except English).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curve for joint model using the UPOS tagset or language specific POS tagset: the x-axis is the size of data (number of tokens); the yaxis is the average LAS measured on 5 languages (except English).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Universal Language visualization according to language and POS. (This should be viewed in colour.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Words</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>.6, we used a 3k token treebank in the target language. What if we have more or less</figDesc><table>cs 

de 
es 
fi 
fr 
ga 
hu 
it 
sv 
µ 

Supervised 
43.1 47.3 60.3 46.4 56.2 59.4 48.4 65.4 52.6 53.2 
Baseline Cascaded 49.6 59.2 66.4 49.5 63.2 59.5 50.5 69.9 61.4 58.8 
Joint 
55.2 61.2 69.1 51.4 65.3 60.6 51.2 71.2 61.4 60.7 
Joint + Dict 
55.7 61.8 70.5 51.5 67.2 61.1 51.0 71.3 62.5 61.4 

Table 1: Labelled attachment score (LAS) for each model type trained on 3000 tokens for each target 
language (columns). All bar the supervised model also use a large English treebank. 

q 

q 

q 

q 

q 

q 

1k 
3k 
5k 
10k 
15k 
All 

Data Size (tokens) 

45 

55 

65 

75 

LAS (%) 

q 

Joint + Dict Model 
Joint Model 
Cascade Model 
Supervised Model 

</table></figure>

			<note place="foot" n="4"> Experiments In this section, we compare our joint training approach with baseline methods of supervised learning in the target language, and cascaded learning of source and target parsers. 3 However, this is not always the case. For example, modal or auxiliary verbs in English often have no translations in different languages or map to words with different syntactic functions. 4 As was the case for our experiments.</note>

			<note place="foot" n="5"> Evaluating on truly resource-poor languages would be preferable to simulation. However for ease of training and evaluation, which requires a small treebank in the target language, we simulate the low-resource setting using a small part of the UDT. 6 Czech (cs), English (en), Finnish (fi), French (fr), German (de), Hungarian (hu), Irish (ga), Italian (it), Spanish (es), Swedish (sv).</note>

			<note place="foot" n="7"> http://www.statmt.org/wmt14/</note>

			<note place="foot" n="8"> We use absolute percentage comparisons herein.</note>

			<note place="foot" n="9"> en, cs, fi, ga, it and sv.</note>

			<note place="foot" n="10"> We also visualized the cross-lingual word embeddings without the dictionary, however the results were rather odd. Although we saw coherent POS clusters, the two languages were largely disjoint. We speculate that many components of the embeddings are use for only one language, and these outnumber the shared components, and thus more careful projection is needed for meaningful visualisation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the University of Melbourne and National ICT Australia (NICTA). Trevor Cohn is the recipient of an Australian Re-search Council Future Fellowship (project number FT130101105).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An unsupervised model for instance level subcategorization acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="278" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question answering passage retrieval using dependency relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renxu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">WALS Online. Max Planck Institute for Evolutionary Anthropology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Leipzig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Syntactic transfer using a bilingual lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Developing the Karuk Treebank. Fieldwork Forum, Department of Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Sandy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Line</forename><surname>Mikkelsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Davidson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>UC Berkeley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Panlex: Building a resource for panlingual lexical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kamholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Colowick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3145" to="50" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Does korean have adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT Working Papers 43. Proceedings of HUMIT 2001</title>
		<imprint>
			<publisher>MIT Working Papers</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="71" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Machine Translation Summit (MT Summit X)</title>
		<meeting>the Tenth Machine Translation Summit (MT Summit X)<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">English Verb Classes and Alternations: A Preliminary Investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1337" to="1348" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-source transfer of delexicalized dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="62" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rnnlm-recurrent neural network language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukar</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Honza</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>IEEE Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Mariecatherine De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lenci</surname></persName>
		</author>
		<title level="m">Universal dependencies 1.0</title>
		<meeting><address><addrLine>Teresa Lynn, Christopher Manning, Ryan McDonald, Anna Missilä, Simonetta Montemagni, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Maria Simi, Aaron Smith, Reut Tsarfaty, Veronika Vincze, and Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Inductive Dependency Parsing (Text, Speech and Language Technology)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>SpringerVerlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Text classification with the support of pruned dependency patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent¨ozgürlevent¨</forename><surname>Levent¨ozgür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tunga</forename><surname>Güngör</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1598" to="1607" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Billions of parallel words for free: Building and using the eu bookshop corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raivis</forename><surname>Skadin¸ˇskadin¸skadin¸ˇs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberts</forename><surname>Rozis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiga</forename><surname>Deksne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014)</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation (LREC-2014)<address><addrLine>Reykjavik, Iceland, May</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="477" to="487" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Target language adaptation of discriminative transfer parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1061" to="1071" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using a dependency parser to improve smt for subject-object-verb languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="245" to="253" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-language parser adaptation between related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Univerzita</forename><surname>Karlova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP-08 Workshop on NLP for Less Privileged Languages</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
