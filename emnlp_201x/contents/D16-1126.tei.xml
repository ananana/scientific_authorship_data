<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Topical Poetry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute &amp; Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute &amp; Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute &amp; Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute &amp; Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Topical Poetry</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1183" to="1191"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe Hafez, a program that generates any number of distinct poems on a user-supplied topic. Poems obey rhythmic and rhyme constraints. We describe the poetry-generation algorithm, give experimental data concerning its parameters, and show its generality with respect to language and poetic form.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic algorithms are starting to generate in- teresting, creative text, as evidenced by recent dis- tinguishability tests that ask whether a given story, poem, or song was written by a human or a com- puter. <ref type="bibr">1</ref> In this paper, we describe Hafez, a program that generates any number of distinct poems on a user-supplied topic. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of the system, which sets out these tasks:</p><p>• Vocabulary. We select a specific, large vocabu- lary of words for use in our generator, and we compute stress patterns for each word.</p><p>• Related words. Given a user-supplied topic, we compute a large set of related words.</p><p>• Rhyme words. From the set of related words, we select pairs of rhyming words to end lines.</p><p>• Finite-state acceptor (FSA). We build an FSA with a path for every conceivable sequence of vocabulary words that obeys formal rhythm constraints, with chosen rhyme words in place.</p><p>• Path extraction. We select a fluent path through the FSA, using a recurrent neural network (RNN) for scoring. <ref type="bibr">1</ref> For example, in the 2016 Dartmouth test bit.ly/20WGLF3, no automatic sonnet-writing system passed indistinguishability, though ours was selected as the best of the submitted systems. Sections 3-7 describe how we address these tasks. After this, we show results of Hafez generating 14- line classical sonnets with rhyme scheme ABAB CDCD EFEF GG, written in iambic pentameter (ten syllables per line with alternating stress: "da-DUM da-DUM da-DUM . . . "). We then show experiments on Hafez's parameters and conclude by showing the generality of the approach with respect to language and poetic form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Automated poem generation has been a popular but challenging research topic ( <ref type="bibr" target="#b11">Manurung et al., 2000;</ref><ref type="bibr" target="#b4">Gervas, 2001;</ref><ref type="bibr" target="#b3">Diaz-Agudo et al., 2002;</ref><ref type="bibr" target="#b12">Manurung, 2003;</ref><ref type="bibr" target="#b23">Wong and Chun, 2008;</ref><ref type="bibr" target="#b9">Jiang and Zhou, 2008;</ref><ref type="bibr" target="#b16">Netzer et al., 2009)</ref>. Recent work attempts to solve this problem by applying grammatical and seman- tic templates <ref type="bibr" target="#b17">(Oliveira, 2009;</ref><ref type="bibr" target="#b18">Oliveira, 2012)</ref>, or by modeling the task as statistical machine trans- lation, in which each line is a "translation" of the previous line ( <ref type="bibr" target="#b27">Zhou et al., 2009;</ref><ref type="bibr">He et al., 2012)</ref>. <ref type="bibr">Yan et al. (2013)</ref> proposes a method based on sum- marization techniques for poem generation, retriev- ing candidate sentences from a large corpus of po- ems based on a user's query and clustering the con- stituent terms, summarizing each cluster into a line of a poem. <ref type="bibr" target="#b6">Greene et al. (2010)</ref> use unsupervised learning to estimate the stress patterns of words in a poetry corpus, then use these in a finite-state net- work to generate short English love poems.</p><p>Several deep learning methods have recently been proposed for generating poems. <ref type="bibr" target="#b26">Zhang and Lapata (2014)</ref> use an RNN model to generate 4-line Chi- nese poems. They force the decoder to rhyme the second and fourth lines, trusting the RNN to control rhythm. <ref type="bibr" target="#b25">Yi et al. (2016)</ref> also propose an attention- based bidirectional RNN model for generating 4- line Chinese poems. The only such work which tries to generate longer poems is from <ref type="bibr" target="#b22">Wang et al. (2016)</ref>, who use an attention-based LSTM model for gener- ation iambic poems. They train on a small dataset and do not use an explicit system for constraining rhythm and rhyme in the poem.</p><p>Novel contributions of our work are:</p><p>• We combine finite-state machinery with deep learning, guaranteeing formal correctness of our poems, while gaining coherence of long- distance RNNs.</p><p>• By using words related to the user's topic as rhyme words, we design a system that can gen- erate poems with topical coherence. This al- lows us to generate longer topical poems.</p><p>• We extend our method to other poetry formats and languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Vocabulary</head><p>To generate a line of iambic pentameter poetry, we arrange words to form a sequence of ten syllables alternating between stressed and unstressed. For ex- ample: Following <ref type="bibr" target="#b5">Ghazvininejad and Knight (2015)</ref>, we refer to unstressed syllables with 0 and stressed syl- lables with 1, so that the form of a Shakespearean sonnet is ((01) 5 ) 14 . To get stress patterns for in- dividual words, we use CMU pronunciation dictio- nary, 2 collapsing primary and secondary stresses. For example:</p><formula xml:id="formula_0">CAFETERIA K AE2 F AH0 T IH1 R IY0 AH0 becomes CAFETERIA 10100</formula><p>The first two columns of <ref type="table" target="#tab_0">Table 1</ref> show other ex- amples. From the 125,074 CMU dictionary word types, we can actually only use words whose stress pattern matches the iambic pattern (alternating 1s and 0s). However, we make an exception for words that end in ...100 (such as spatula). To mimic how human poets employ such words, we convert all "...100" patterns to "...101". This leaves us with a 106,019 word types.</p><p>Words with multiple syllable-stress patterns present a challenge. For example, our program may use the word record in a "...10..." context, but if it is a verb in that context, a human reader will pronounce it as "01", breaking the intended rhythm. To guarantee that our poems scan properly, we eject all ambiguous words from our vocabulary. This problem is especially acute with monosyllabic words, as most have a stress that depends on context. <ref type="bibr" target="#b6">Greene et al. (2010)</ref> apply the EM algorithm to align word stress pattern strict rhyme class slant rhyme class <ref type="table" target="#tab_0">(coarse version)  needing 10  IY1 D IH0 NG  IY1 * IH0 NG  ordinary 1010  EH1 R IY0  EH1 * IY0  obligate 101</ref> EY1 T last syllable stressed, no slant rhyme human-written sonnets with assumed meter, extract- ing P(0|word) and P(1|word) probabilities. Using their method, we eject all monosyllabic words ex- cept those with P(0|word) &gt; 0.9 or P(1|word) &gt; 0.9. A consequence is that our poetry generator avoids the words to, it, in, and is, which actually forces the system into novel territory. This yields 16,139 monosyllabic and 87,282 multisyllabic words. Because our fluency module (Section 7) is re- stricted to 20,000 word types, we further pare down our vocabulary by removing words that are not found in the 20k-most-frequent list derived from the song lyrics corpus we use for fluency. After this step, our final vocabulary contains 14,368 words (4833 monosyllabic and 9535 multisyllabic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topically Related Words and Phrases</head><p>After we receive a user-supplied topic, the first step in our poem generation algorithm is to build a scored list of 1000 words/phrases that are related to that topic. For example:</p><p>• User-supplied input topic: colonel • Output: colonel (1.00), lieutenant colonel (0.77), brigadier general (0.73), commander (0.67) ... army (0.55) ... This problem is different from finding synonyms or hypernyms in WordNet <ref type="bibr" target="#b15">(Miller, 1995)</ref>. For exam- ple, while <ref type="bibr" target="#b0">Banerjee and Pedersen (2003)</ref> use Word- Net to assign a 1.0 similarity score between car and automobile, they only give a 0.3 similarity between car and gasoline.</p><p>A second method is to use pointwise mutual in- formation (PMI). Let t be the topic/phrase, and let w be a candidate related word. We collect a set of sentences S that contain t, and sort candidates by Proportion of sentences in S containing w P(w) in general text <ref type="table" target="#tab_1">Table 2</ref> shows that PMI has a tendency to assign a high score to low frequency words <ref type="bibr" target="#b1">(Bouma, 2009;</ref><ref type="bibr" target="#b20">Role and Nadif, 2011;</ref><ref type="bibr" target="#b2">Damani, 2013)</ref>.</p><p>A third method is word2vec ( <ref type="bibr" target="#b13">Mikolov et al., 2013a)</ref>, which provides distributed word represen- tations. We train a continuous-bag-of-words model <ref type="bibr">3</ref> with window size 8 and 40 and word vector dimen- sion 200. We score candidate related words/phrases with cosine to topic-word vector. We find that a larger window size works best ( <ref type="bibr" target="#b19">Pennington et al., 2014;</ref><ref type="bibr" target="#b10">Levy and Goldberg, 2014)</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows examples. The training corpus for word2vec has a crucial effect on the quality of the re- lated words. We train word2vec models on the En- glish Gigaword corpus, 4 a song lyrics corpus, and the first billion characters from Wikipedia. <ref type="bibr">5</ref> The Gi- gaword corpus produces related words that are too newsy, while the song lyrics corpus does not cover enough topics. Hence, we train on Wikipedia. To obtain related phrases as well as words, we apply the method of <ref type="bibr" target="#b14">Mikolov et al. (2013b)</ref> to the Wikipedia corpus, which replaces collocations like Los Ange- les with single tokens like Los Angeles. Word2vec then builds vectors for phrases as well as words. When the user supplies a multi-word topic, we use its phrase vector if available. Otherwise, we cre- ate the vector topic by element wise addition of its words' vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Choosing Rhyme Words</head><p>We next fill in the right-hand edge of our poem by selecting pairs of rhyming words/phrases and as- signing them to lines. In a Shakespearean sonnet with rhyme scheme ABAB CDCD EFEF GG, there are seven pairs of rhyme words to decide on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Strict Rhyme</head><p>The strict definition of English rhyme is that the sounds of two words must match from the last stressed vowel onwards. In a masculine rhyme,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Window Corpus</head><p>Phrases? Related words PMI n/a Gigaword no croquet, Romai, Carisbo, NTTF, showcourts ... CBOW 8</p><p>Gigaword no squash, badminton, golf, soccer, racquetball ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CBOW 40</head><p>Gigaword no singles, badminton, squash, ATP, WTA ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CBOW 40</head><p>Song Lyrics no high-heel, Reebok, steel-toed, basketball, Polos ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CBOW 40</head><p>Wikipedia no volleyball, racquet, Wimbledon, athletics, doubles ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CBOW 40</head><p>Wikipedia yes singles titles, grass courts, tennis club, hardcourt ... the last syllable is stressed; in a feminine rhyme, the penultimate syllable is stressed. We collect phoneme and stress information from the CMU pro- nunciation dictionary. We pre-compute strict rhyme classes for words (see <ref type="table" target="#tab_0">Table 1</ref>) and hash the vocab- ulary into those classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Slant Rhyme</head><p>In practice, human poets do not always use strict rhymes. To give ourselves more flexibility in choos- ing rhyme pairs, we allow for slant (or half) rhymes. By inspecting human rhyming data, we develop this operational definition of slant rhyme:</p><p>1. Let s1 and s2 be two potentially-rhyming phoneme sequences. 2. Replace ER with UH R in both sequences. 3. Let v1 and v2 be the last stressed vowels in s1 and s2. 4. Let w1 and w2 be last vowels in s1 and s2. 5. Let s1 = a1 v1 x1 w1 c1. Likewise, let s2 = a2 v2 x2 w2 c2. 6. Output NO under any of these circumstances:</p><p>(a) v1 = v2, (b) w1 = w2, (c) c1 = c2, (d) a1 = NULL and a2 = NULL and a1 = a2. 7. If x1 and x2 are single phonemes:</p><p>(a) If x1 ∼ x2, then output YES. <ref type="bibr">6</ref> (b) Otherwise, output NO. 8. If x1 and x2 contain different numbers of vow- els, output NO. 9. Let p1 and q1 be the first and last phonemes of x1. Let p2 and q2 be the same for x2. 10. If (p1 = p2) and (q1 ∼ q2), output YES. 11. If (p1 ∼ p2) and (q1 = q1), output YES. 12. Otherwise, output NO. <ref type="bibr">6</ref> x ∼ y if phonemes x and y are similar. Two phonemes are similar if their pairwise score according to <ref type="bibr" target="#b7">(Hirjee and Brown, 2010</ref>) is greater than -0.6. This includes 98 pairs, such as L/R, S/SH, and OY/UH.</p><p>Words whose last syllable is stressed do not partici- pate in slant rhymes.</p><p>Example slant rhymes taken from our gener- ated poems include Viking/fighting, snoopy/spooky, baby/crazy and comic/ironic. We pre-compute a coarse version of slant rhyme classes <ref type="table" target="#tab_0">(Table 1</ref>) with the pattern "v i * w i c i ". If two words hash to the same coarse class, then we subsequently accept or reject depending on the similarity of the intermedi- ate phonemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Non-Topical Rhyming Words</head><p>For rare topics, we may not have enough related words to locate seven rhyming pairs. For exam- ple, we generate 1000 related words for the topic Viking, but only 32 of them are found in our 14,368- word vocabulary. To give a chance for all topical words/phrases to be used as rhyme words, for each strict rhyme class, we add the most common word in our song lyric corpus to the list of related words. In addition, we add words from popular rhyme pairs 7 (like do/you and go/know) to the list of related words with a low topic similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Rhyme word selection</head><p>We first hash all related words/phrases into rhyme classes. Each collision generates a candidate rhyme pair (s1, s2), which we score with the maximum of cosine(s1, topic) and cosine(s2, topic). So that we can generate many different sonnets on the same topic, we choose rhyme pairs randomly with prob- ability proportional to their score. After choosing a pair (s1, s2), we remove it, along with any other can- didate pair that contains s1 or s2. Because a poem's beginning and ending are more important, we assign the first rhyme pair to the last two lines of the sonnet, then assign other pairs from beginning of the sonnet towards the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Constructing FSA of Possible Poems</head><p>After choosing rhyme words, we create a large finite-state acceptor (FSA) that compactly encodes all word sequences that use these rhyme words and also obey formal sonnet constraints:</p><p>• Each sonnet contains 14 lines.</p><p>• Lines are in iambic pentameter, with stress pat- tern (01) <ref type="bibr">5</ref> . Following poetic convention, we also use (01) 5 0, allowing feminine rhyming.</p><p>• Each line ends with the chosen rhyme word/phrase for that line.</p><p>• Each line is punctuated with comma or period, except for the 4th, 8th, 12th, and 14th lines, which are punctuated with period. To implement these constraints, we create FSA states that record line number and syllable count. For example, FSA state L2-S3 <ref type="figure" target="#fig_2">(Figure 2</ref>) signifies "I am in line 2, and I have seen 3 syllables so far". From each state, we create arcs for each feasible word in the vocabulary. For example, we can move from state L1-S1 to state L1-S3 by consuming any word with stress pattern 10 (such as table or active). When moving between lines (e.g., from L1-S10 to L2-S1), we employ arcs labeled with punctuation marks.</p><p>To fix the rhyme words at the end of each line, we delete all arcs pointing to the line-final state, ex- cept for the arc labeled with the chosen rhyme word. For speed, we pre-compute the entire FSA; once we receive the topic and choose rhyme words, we only need to carry out the deletion step.</p><p>In the resulting FSA, each path is formally a son- net. However, most of the paths through the FSA are meaningless. One FSA generated from the topic nat- ural language contains 10 229 paths, including this randomly-selected one:</p><p>Of pocket solace ammunition grammar. An tile pretenders spreading logical. An stories Jackie gallon posing banner. An corpses Kato biological ...</p><p>Hence, we need a way to search and rank this large space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Path extraction through FSA with RNN</head><p>To locate fluent paths, we need a scoring function and a search procedure. For example, we can build a n-gram word language model (LM)-itself a large weighted FSA. Then we can take a weighted in- tersection of our two FSAs and return the highest- scoring path. While this can be done efficiently with dynamic programming, we find that n-gram models have a limited attention span, yielding poor poetry.</p><p>Instead, we use an RNN language model (LM). We collect 94,882 English songs (32m word tokens) as our training corpus, 8 and train 9 a two-layer recur- rent network with long short-term memory (LSTM) units <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>). <ref type="bibr">10</ref> When decoding with the LM, we employ a beam search that is further guided by the FSA. Each beam state C t,i is a tuple of (h, s, word, score), where h is the hidden states of LSTM at step t in ith state, and s is the FSA state at step t in ith state. The model generates one word at each step. At the beginning, h 0,0 is the initial hidden state of LSTM, s 0,0 is the start state of FSA, word 0,0 = &lt;START&gt; and score 0,0 = 0. To expand a beam state C t,i , we first feed h t,i and word into the LM and get an updated hidden state h next . The LM also returns a probability distribution P (V ) over the entire vocabulary V for next word. Then, for each succeeding state s suc of s t,i in the FSA and the word w next over each edge from s t,i to s suc , we form a new state (h next , s suc , w next , score t,i + log(P (w next ))) and push it into next beam.</p><p>Because we fix the rhyme word at the end of each line, when we expand the beam states immedi- ately before the rhyme word, the FSA states in those beam states have only one succeeding state-LN- S10, where N = <ref type="bibr">[1,</ref><ref type="bibr">14]</ref>, and only one succeeding word, the fixed rhyme word. For our beam size b = 50, the chance is quite low that in those b words there exists any suitable word to precede that rhyme word. We solve this by generating the whole sonnet in reverse, starting from the final rhyme word. Thus, when we expand the state L1-S8, we can choose from almost every word in vocabulary instead of just b possible words. The price to pay is that at the beginning of each line, we need to hope in those b words there exists some that are suitable to succeed comma or period.</p><p>Because we train on song lyrics, our LM tends to generate repeating words, like never ever ever ever ever. To solve this problem, we apply a penalty to those words that already generated in previous steps during the beam search.</p><p>To create a poem that fits well with the pre- determined rhyme words at the end of each line, the LM model tends to choose "safe" words that are fre- quent and suitable for any topic, such as pronouns, adverbs, and articles. During decoding, we apply a reward on all topically related words (generated in Section 4) in the non-rhyming portion of the poem.</p><p>Finally, to further encourage the system to follow the topic, we train an encoder-decoder sequence-to- sequence model <ref type="bibr" target="#b21">(Sutskever et al., 2014</ref>). For train- ing, we select song lyric rhyme words and assemble</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bipolar Disorder</head><p>Existence enters your entire nation. A twisted mind reveals becoming manic, An endless modern ending medication, Another rotten soul becomes dynamic.</p><p>Or under pressure on genetic tests. Surrounded by controlling my depression, And only human torture never rests, Or maybe you expect an easy lesson.</p><p>Or something from the cancer heart disease, And I consider you a friend of mine. Without a little sign of judgement please, Deliver me across the borderline.</p><p>An altered state of manic episodes, A journey through the long and winding roads. them in reverse order (encoder side), and we pair this with the entire reversed lyric (decoder side). At gen- eration time, we put all the selected rhyme words on the source side, and let the model to generate the poem conditioned on those rhyme words. In this way, when the model tries to generate the last line of the poem, it already knows all fourteen rhyme words, thus possessing better knowledge of the re- quested topic. We refer to generating poems using the RNN LM as the "generation model" and to this model as the "translation model".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results and Analysis</head><p>Sample outputs produced by our best system are shown in <ref type="figure" target="#fig_3">Figures 3 and 4</ref>. We find that they gen- erally stay on topic and are fairly creative. If we re- quest a poem on the topic Vietnam, we may see the phrase Honky Tonkin Resolution; a different topic leads the system to rhyme Dirty Harry with Bloody Mary. In this section, we present experiments we used to select among different versions of our poem generator.</p><p>The first experiment tests the effect of encourag- ing topical words in the body of the poem, via a direct per-word bonus. For 40 different topics, we generate 2 sonnets with and without encouragement, using the same set of rhyme words. Then we ask 23 human judges to choose the better sonnet. Each judge compares sonnets for 10 different topics. <ref type="table" target="#tab_2">Ta- ble 3</ref> shows that using topical words increases the Love at First Sight An early morning on a rainy night, Relax and make the other people happy, Or maybe get a little out of sight, And wander down the streets of Cincinnati.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Girlfriend</head><p>Another party started getting heavy. And never had a little bit of Bobby, Or something going by the name of Eddie, And got a finger on the trigger sloppy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noodles</head><p>The people wanna drink spaghetti alla, And maybe eat a lot of other crackers, Or sit around and talk about the salsa, A little bit of nothing really matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Civil War</head><p>Creating new entire revolution, An endless nation on eternal war, United as a peaceful resolution, Or not exist together any more. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preference Encourages Does Not Cannot</head><p>Encourage Decide Sonnets 54% 18% 28% quality of the sonnets. Next, we compare the translation model with gen- eration model. For each of 40 topics, we gener- ate one poem with generation model and one poem with translation model, using the same set of rhyme words. We ask 25 human judges to chose the bet- ter poem. Each judge compares sonnets for 10 dif- ferent topics. This experiment is run separately for sonnets and stanzas. <ref type="table" target="#tab_4">Table 4</ref> shows how the trans- lation model generates better poems, and <ref type="figure" target="#fig_5">Figure 5</ref> compares two stanzas.</p><p>We check for plagiarism, as it is common for optimal-searching RNNs to repeat large sections of the training data. We hypothesize that strong condi- tions on rhyme, meter, repetition, and ambiguously- stressed words will all mitigate against plagiarism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gen</head><p>Another tiny thousand ashes scattered. And never hardly ever really own, Or many others have already gathered, The only human being left alone. Trans Being buried under ashes scattered, Many faces we forgotten own, About a hundred thousand soldiers gathered, And I remember standing all alone.   We find that on average, each sonnet copies only 1.2 5-grams from the training data. If we relax the repeated-word penalty and the iambic meter, this number increases to 7.9 and 10.6 copied 5- grams, respectively. Considering the lack of copy- ing, we find the RNN-generated grammar to be quite good. The most serious-and surprisingly common-grammatical error is the wrong use of a and an, which we fix in a post-processing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Other Languages and Formats</head><p>To show the generality of our approach, we mod- ify our system to generate Spanish-language poetry from a Spanish topic. We use these resources:</p><p>• A song lyric corpus for training our RNN. We download 97,775 Spanish song lyrics from LyricWikia, 11 which amounts to 20m word to- kens and 219k word types.</p><p>• A Spanish Wikipedia dump 12 consisting of 885m word tokens, on which we run word2vec to find words and phrases related to the topic. Our vocabulary consists of the 20k most frequent lyric words. For each word, we compute its syllable- stress pattern and its rhyme class (see <ref type="figure" target="#fig_6">Figure 6</ref>). Be- cause Spanish writing is quite phonetic, we can re- trieve this from the letter strings of the vocabulary. word stress rhyme v- -v consultado 0010 -ado yes aduciendo 0010 -endo yes yes régimen 100 -egimen hospital 001 -al yes For any given vocabulary word: <ref type="bibr">13</ref> 1. We remove silent h, and convert y into i.</p><p>2. We count the number of syllables by isolat- ing vowel groups. In such groups, weak vow- els (i, u) attached to strong vowels (a, e, o) do not form separate syllables, unless they are ac- cented (dí-as versus dios). Strong clusters are broken into separate syllables (eg, ca-er). 3. We determine which vowel (and therefore syl- lable) is stressed. If any vowel is accented, it is stressed. If the word is accent-free, then the second-to-last syllable is stressed, unless the word ends in a consonant other than n or s, in which case the last syllable is stressed. 4. We form the word's rhyme class by breaking off a letter suffix starting at the last stressed vowel (as in English). Weak vowels do not par- ticipate (e.g., tienda → -enda, not -ienda). We remove h from the rhyme, so búho rhymes with continúo. Because rhyming is easier in Spanish than English, we do not need slant rhyme. Most Spanish poetic formats enforce some num- ber of syllables per line, without meter. However, there are two caveats when counting syllables:</p><p>1. Sinalefa merges vowels across word bound- aries. Thus, la obra is counted as two syllables instead of three, and va a hacer is counted as two syllables instead of four. A line may there- fore have more words than syllables. 2. For the last word of a line (only), we count up to its last stressed syllable, then add one. This means that even though iambic meter is not em- ployed, we still need stress patterns to correctly count syllables. We implement these constraints in the FSA framework, now with separate states for "I have seen M syllables, and the last word ended in a vowel sound" and "I have seen M syllables, and the last <ref type="bibr">13</ref> http://community.dur.ac.uk/m.p.thompson/verse.htm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mariposa</head><p>Quieres saber dónde está el escorpión, Ni ayer ni antes vos sos corona dorada. Ya os ves más tal cual tortuga pintada, A ´ el nos gusta andar con cola marrón.</p><p>Ella es quién son las alas de algún gorrión. Si al fin podés ver tu imagen manchada, O hoy vas bajo un cielo azul plateada, Por qué estás tan lejos del aguijón.</p><p>No hay luz que al sol se enreda en tus palmera. Ay por qué eres víbora venenosa, Sin querer igual a un enredadera.</p><p>Y si aún sueñas con ser mariposa, En vez de abrir los ojos y espera, Sabes muy bien que el amor no es gran cosa. word ended in a consonant sound." Technically speaking, the FSA includes single-state cycles for the Spanish word a, due to sinalefa. Line-ending states can only be reached by words that have their syllable count adjusted as in point 2 above. <ref type="figure" target="#fig_7">Figure 7</ref> shows a sample Spanish output. The for- mat is the classical Spanish soneta, which consists of 14 eleven-syllable lines under the rhyme scheme ABBA ABBA CDC DCD. This scheme requires us to choose up to four words with the same rhyme.</p><p>Overall, we also find Spanish outputs to be flu- ent, fairly creative, and on topic. Grammatical prob- lems are a bit more common than in our English generator-for example, adjacent words sometimes disagree in number or gender. The RNN generaliza- tions that permit these errors no doubt also permit creative phrasings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We have described Hafez, a poetry generation sys- tem that combines hard format constraints with a deep-learning recurrent network. The system uses special techniques, such as rhyme-word choice and encoder-decoder modeling, to keep the poem on topic. We hope that future work will provide more discourse structure and function to automatic poetry, while maintaining the syntax, semantics, and cre- ative phrasing we observe.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of Hafez converting a user-supplied topic word (wedding) into a four-line iambic pentameter stanza.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An FSA compactly encoding all word sequences that obey formal sonnet constraints, and dictating the right-hand edge of the poem via rhyming, topical words delight, chance, ... and joy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sample sonnet generated from the topic phrase bipolar disorder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sample stanzas generated from different topic phrases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Stanzas generated with and without a encoderdecoder translation model for topic death.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sample word analyses needed to construct Spanish Hafez. v-and-v indicate whether the word starts and/or ends with a vowel sound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sample Spanish poem generated in classical soneta form, on the topic mariposa (butterfly).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Sample word analyses.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Different methods for extracting words related to the topic tennis. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Users prefer the system that encourages the use of 

related words in the body (non-rhyme) portion of the poem. 40 

poems are tested with 23 judges. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Users prefer poems created with the encoder-decoder 

translation model over those that use only the RNN language 

model in generation mode. 40 poems are tested with 25 judges. 

</table></figure>

			<note place="foot" n="2"> http://www.speech.cs.cmu.edu/cgi-bin/cmudict</note>

			<note place="foot" n="3"> https://code.google.com/archive/p/word2vec/ 4 https://catalog.ldc.upenn.edu/LDC2011T07 5 http://mattmahoney.net/dc/enwik9.zip</note>

			<note place="foot" n="7"> http://slate.me/OhTKCA</note>

			<note place="foot" n="8"> http://www.mldb.org/ 9 We use the toolkit: https://github.com/isi-nlp/Zoph RNN 10 We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2. The output vocabulary size is 20,000. The learning rate is initially set as 0.7 and starts to decay by 0.83 once the perplexity on a development set starts to increase. All parameters are initialized within range [−0.08, +0.08], and the gradients are re-scaled when the global norm is larger than 5.</note>

			<note place="foot" n="11"> http://lyrics.wikia.com/wiki/Category:Language/Spanish 12 https://dumps.wikimedia.org/eswiki/20160305/eswiki20160305-pages-meta-current.xml.bz2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their helpful comments. This work was sup-ported by DARPA (W911NF-15-1-0543) and NSF (IIS-1524371).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extended gloss overlaps as a measure of semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Normalized (pointwise) mutual information in collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Biennial GSCL Conference</title>
		<meeting>Biennial GSCL Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving pointwise mutual information (PMI) by incorporating significant cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Om</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Poetry generation in COLIBRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Diaz-Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gervas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Gonzalezcalero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCBR</title>
		<meeting>ECCBR</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An expert system for the composition of formal Spanish poetry. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gervas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How to memorize a random 60-bit string</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic analysis of rhythmic poetry with applications to generation and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tugba</forename><surname>Bodrumlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP. Jing He, Ming Zhou, and Long Jiang</title>
		<meeting>EMNLP. Jing He, Ming Zhou, and Long Jiang</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Proc. AAAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using automated rhyme detection to characterize rhyming style in rap music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussein</forename><surname>Hirjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Musicology Review</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating Chinese couplets using a statistical MT approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards a computational model of poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisar</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISB Symposium on Creative and Cultural Aspects and Applications of AI and Cognitive Science</title>
		<meeting>AISB Symposium on Creative and Cultural Aspects and Applications of AI and Cognitive Science</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An evolutionary algorithm approach to poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisar</forename><surname>Manurung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gaiku: Generating haiku with word associations norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL Workshop on Computational Approaches to Linguistic Creativity</title>
		<meeting>NAACL Workshop on Computational Approaches to Linguistic Creativity</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic generation of poetry: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Seminar of Art, Music, Creativity and Artificial Intelligence</title>
		<meeting>1st Seminar of Art, Music, Creativity and Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PoeTryMe: a versatile platform for poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Creativity, Concept Invention, and General Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Handling the impact of low frequency events on co-occurrence based measures of word similarity-a case study of pointwise mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Role</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Nadif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Information Retrieval</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06274</idno>
		<title level="m">Chinese song iambics generation with neural attention-based model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic haiku generation using VSM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACACOS</title>
		<meeting>ACACOS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2013. I, Poet: Automatic Chinese poetry composition through a generative summarization framework under constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Shou-De Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generating chinese classical poems with RNN encoderdecoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01537</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating Chinese couplets and quatrain using a statistical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
