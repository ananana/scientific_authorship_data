<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Network Approach to Selectional Preference Acquisition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
							<email>tim.vandecruys@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">IRIT &amp; CNRS Toulouse</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Network Approach to Selectional Preference Acquisition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="26" to="35"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper investigates the use of neural networks for the acquisition of selectional preferences. Inspired by recent advances of neural network models for NLP applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is entirely un-supervised-preferences are learned from unannotated corpus data. We propose two neural network architectures: one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences. The model&apos;s performance is evaluated on a pseudo-disambiguation task, on which it is shown to achieve state of the art performance .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Predicates often have a semantically motivated pref- erence for particular arguments. Compare for ex- ample the sentences in (1) and (2).</p><p>(1)</p><p>The vocalist sings a ballad.</p><p>(2)</p><p>The exception sings a tomato.</p><p>Most language users would have no problems ac- cepting the first sentence as well-formed: a vocalist can be expected to sing, and a ballad is something that can be sung. The same language users, how- ever, would likely consider the second sentence to be ill-formed: an exception is not supposed to sing, nor is a tomato something that is typically sung. Within the field of natural language processing, this inclination of predicates to select for particular arguments is known as selectional preference.</p><p>The automatic acquisition of selectional prefer- ences has been a popular research subject within the field of natural language processing. An auto- matically acquired selectional preference resource is a versatile tool for numerous NLP applications, such as semantic role labeling ( <ref type="bibr" target="#b3">Gildea and Jurafsky, 2002</ref>), word sense disambiguation <ref type="bibr" target="#b9">(McCarthy and Carroll, 2003)</ref>, and metaphor processing <ref type="bibr" target="#b19">(Shutova et al., 2013)</ref>.</p><p>Models for selectional preference need to ade- quately deal with the consequences of Zipf's law: language is inherently sparse, and the majority of language utterances occur very infrequently. As a consequence, models that are based on corpus data need to properly generalize beyond the mere co-occurrence frequencies of sparse corpus data, taking into account the semantic similarity of both predicates and arguments. Researchers have come up with various approaches to this generalization step. Earlier approaches to selectional preference acquisition mostly rely on hand-crafted resources such as WordNet <ref type="bibr" target="#b16">(Resnik, 1996;</ref><ref type="bibr" target="#b6">Li and Abe, 1998;</ref><ref type="bibr">Clark and Weir, 2001</ref>), while later approaches tend to take advantage of unsupervised learning machin- ery, such as latent variable models ( <ref type="bibr" target="#b18">Rooth et al., 1999;</ref><ref type="bibr" target="#b14">´ O Séaghdha, 2010)</ref> and distributional simi- larity metrics <ref type="bibr" target="#b2">(Erk, 2007;</ref><ref type="bibr" target="#b15">Padó et al., 2007)</ref>. This paper investigates the use of neural net- works for the acquisition of selectional preferences. Inspired by recent advances of neural network mod- els for NLP applications <ref type="bibr">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013)</ref>, we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particu- lar predicate. The model is entirely unsupervised - preferences are learned from unannotated corpus data. Positive training instances are constructed from attested corpus data, while negative instances are constructed from randomly corrupted instances. We propose two neural network architectures: one that handles standard two-way selectional prefer- ences and one that is able to deal with multi-way selectional preferences, where the interaction be-tween multiple verb arguments is taken into ac- count. The model's performance is evaluated on a pseudo-disambiguation task, on which it is shown to achieve state of the art performance.</p><p>The contributions of this paper are twofold. First of all, we apply and evaluate a neural network ap- proach to the problem of standard (two-way) se- lectional preference acquisition. Selectional pref- erence acquisition using neural networks has not yet been explored in the literature. Secondly, we propose a novel network architecture and training objective for the acquisition of multi-way selec- tional preferences, where the interaction between a verb and its various arguments is captured at the same time.</p><p>The remainder of this paper is as follows. Sec- tion 2 first discusses related work with respect to se- lectional preference acquisition and neural network modeling. Section 3 describes our neural network architecture and its training procedure. Section 4 evaluates the model's performance, comparing it to other existing models for selectional preference acquisition. Finally, section 5 concludes and indi- cates a number of avenues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Selectional preferences</head><p>One of the first approaches to the automatic induc- tion of selectional preferences from corpora was the one by <ref type="bibr" target="#b16">Resnik (1996)</ref>. <ref type="bibr" target="#b16">Resnik (1996)</ref> relies on WordNet synsets in order to generate gener- alized noun clusters. The selectional preference strength of a specific verb v in a particular relation is calculated by computing the Kullback-Leibler divergence between the cluster distribution of the verb and the prior cluster distribution.</p><formula xml:id="formula_0">S R(v) = ∑ c p(c|v) log p(c|v) p(c)<label>(1)</label></formula><p>where c stands for a noun cluster, and R stands for a given predicate-argument relation. The selectional association of a particular noun cluster is then the contribution of that cluster to the verb's preference strength.</p><formula xml:id="formula_1">A R(v,c) = p(c|v) log p(c|v) p(c) S R(v)<label>(2)</label></formula><p>The model's generalization relies entirely on Word- Net, and there is no generalization among the verbs.</p><p>Other researchers have equally relied on Word- Net in order to generalize over arguments. <ref type="bibr" target="#b6">Li and Abe (1998)</ref> use the principle of Minimum Descrip- tion Length in order to find a suitable generalization level within the lexical WordNet hierarchy. A same intuition is used by <ref type="bibr">Clark and Weir (2001)</ref>, but they use hypothesis testing instead to find the appro- priate level of generalization. A recent approach that makes use of WordNet (in combination with Bayesian modeling) is the one by´Oby´ by´O <ref type="bibr" target="#b13">Séaghdha and Korhonen (2012)</ref>.</p><p>Most researchers, however, acknowledge the shortcomings of hand-crafted resources, and fo- cus on the acquisition of selectional preferences from corpus data. <ref type="bibr" target="#b18">Rooth et al. (1999)</ref> propose an Expectation-Maximization (EM) clustering algo- rithm for selectional preference acquisition based on a probabilistic latent variable model. The idea is that both predicate v and argument o are gen- erated from a latent variable c, where the latent variables represent clusters of tight verb-argument interactions.</p><formula xml:id="formula_2">p(v, o) = ∑ c∈C p(c, v, o) = ∑ c∈C p(c)p(v|c)p(o|c) (3)</formula><p>The use of latent variables allows the model to generalize to predicate-argument tuples that have not been seen during training. The latent variable distribution -and the probabilities of predicates and argument given the latent variables -are au- tomatically induced from data using EM. We will compare against their model for evaluation pur- poses. <ref type="bibr" target="#b2">Erk (2007)</ref> and <ref type="bibr" target="#b1">Erk et al. (2010)</ref> describe a method that uses corpus-driven distributional simi- larity metrics for the induction of selectional pref- erences. The key idea is that a predicate-argument tuple (v, o) is felicitous if the predicate v appears in the training corpus with arguments o that are similar to o, i.e.</p><formula xml:id="formula_3">S(v, o) = ∑ o ∈O v wt(v, o ) Z(v) · sim(o, o )<label>(4)</label></formula><p>where O v represents the set of arguments that have been attested with predicate v, wt(·) represents an appropriate weighting function (such as the fre- quency of the (v, o ) tuple), and Z is a normaliza- tion factor. We equally compare to their model for evaluation purposes. <ref type="bibr">Bergsma et al. (2008)</ref> present a discriminative approach to selectional preference acquisition. Pos- itive examples are taken from observed predicate-argument pairs, while negative examples are con- structed from unobserved combinations. An SVM classifier is used to distinguish the positive from the negative instances. The training procedure used in their model is based on an intuition that is similar to ours, although it is implemented using different techniques.</p><p>A number of researchers presented models that are based on the framework of topic modeling. ´ O Séaghdha (2010) describes three models for selec- tional preference induction based on Latent Dirich- let Allocation, which model the selectional pref- erence of a predicate and a single argument. <ref type="bibr" target="#b17">Ritter et al. (2010)</ref> equally present a selectional pref- erence model based on topic modeling, but they tackle multi-way selectional preferences (of transi- tive predicates, which take two arguments) instead.</p><p>Finally, in previous work (Van de Cruys, 2009) we presented a model for multi-way selectional preference induction based on tensor factorization. Three-way co-occurrences of subjects, verbs, and objects are represented as a three-way tensor (the generalization of a matrix), and a latent factoriza- tion model is applied in order to generalize to unseen instances. We will compare our neural network based-approach for multi-way selectional preference acquisition to this tensor-based factor- ization model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural networks</head><p>In the last few years, neural networks have become increasingly popular in NLP applications. In partic- ular, neural language models ( <ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr" target="#b11">Mnih and Hinton, 2007;</ref><ref type="bibr">Collobert and Weston, 2008)</ref> have demonstrated impressive performance at the task of language modeling. By incorporating distributed representations for words that model their similarity, neural language models are able to overcome the problem of data sparseness that standard n-gram models are confronted with. Also related to our work is the approach by <ref type="bibr" target="#b21">Tsubaki et al. (2013)</ref>, who successfully use a neural network to model co-compositionality.</p><p>Our model for selectional preference acquisition uses a network architecture that is similar to the abovementioned models. Its training objective is also similar to the ranking-loss training objective proposed by <ref type="bibr">Collobert and Weston (2008)</ref>, but we present a novel, modified version in order to deal with multi-way selectional preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural network architecture</head><p>Our model computes the score for a predicate i and an argument j as follows. First, the selectional preference tuple (i, j) is represented as the concate- nation of the vectors v i and o j , i.e.</p><formula xml:id="formula_4">x = [v i , o j ]<label>(5)</label></formula><p>Vectors v i and o j are extracted from two embedding matrices, V ∈ R N×I (the predicate matrix, where I represents the number of elements in the predicate vocabulary) and O ∈ R N×J (the argument matrix, where J represents the number of elements in the argument vocabulary). N is a parameter setting of the model, representing the vector size of the em- beddings. Matrices V and O will be automatically learned during training. Vector x then serves as input vector to our neural network. We use a feed-forward neural network architecture with one hidden layer:</p><formula xml:id="formula_5">a 1 = f (W 1 x + b 1 )<label>(6)</label></formula><formula xml:id="formula_6">y = W 2 a 1 (7)</formula><p>where x ∈ R 2N is our input vector, a 1 ∈ R H repre- sents the activation of the hidden layer with H hid- den nodes, W 1 ∈ R H×2N and W 2 ∈ R 1×H respec- tively represent the first and second layer weights, b 1 represents the first layer's bias, f (·) represents the element-wise activation function tanh, and y is our final selectional preference score. The left-hand picture of <ref type="figure">figure 1</ref> gives a graphical representation of our standard neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the network</head><p>A proper estimation of a neural network's param- eters requires a large amount of training data. To be able to use non-annotated corpus data for train- ing, we use the method proposed by <ref type="bibr">Collobert and Weston (2008)</ref>. The authors present a method for training a neural network language model from un- labeled data by corrupting actual attested n-grams with a random word. They then define a ranking- type cost function, which allows the network to learn to discriminate between good and bad word sequences. We adopt the same method for our se- lectional preference model as follows.</p><p>Let (i, j) be our proper, attested predicate- argument tuple. The goal of our model is to dis- criminate the correct tuple (i, j) from other, non- attested tuples (i, j ), in which the correct predicate</p><formula xml:id="formula_7">V i O j j W 2 W 1 a 1 x y V i O j k W 2 W 1 a 1 x y S j j</formula><p>Figure 1: Neural network architectures for selectional preference acquisition. The left-hand picture shows the architecture for two-way selectional preferences, the right-hand picture shows the architecture for three-way selectional preferences. In both cases, vector x is constructed from the appropriate predicate and argument vectors from the embedding matrices, and fed forward through the network to yield a preference score y. j has been replaced with a random predicate j . We require the score for the correct tuple to be larger than the score for the corrupt tuple by a margin of one. For one tuple (i, j), this corresponds to minimizing the objective function in (8)</p><formula xml:id="formula_8">∑ j ∈J max(0, 1 − g[(i, j)] + g[(i, j )])<label>(8)</label></formula><p>where J represents the predicate vocabulary, and g <ref type="bibr">[·]</ref> represents our neural network scoring function presented in the previous section.</p><p>In line with <ref type="bibr">Collobert and Weston (2008)</ref>, the gradient of the objective function is sampled by randomly picking one corrupt argument j from the argument vocabulary for each attested predicate- argument tuple (i, j). The derivative of the cost with respect to the model's parameters (weight ma- trices W 1 and W 2 , bias vector b 1 , and embedding matrices V and O) is computed, and the appropriate parameters are updated through backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-way selectional preferences</head><p>The model presented in the previous section is only able to deal with two-way selectional pref- erences. In this section, we present an extension of the model that is able to handle multi-way selec- tional preferences. <ref type="bibr">1</ref> In order to model the selectional preference of a transitive verb for its subject and direct object, we start out in a similar fashion to the two-way case. Instead of having only one embedding matrix, we now have two embedding matrices S ∈ R N×J and O ∈ R N×K , representing the two different argument slots of a transitive predicate. Our input vector can now be represented as</p><formula xml:id="formula_9">x = (v i , s j , o k )<label>(9)</label></formula><p>Note that x ∈ R 3N and W 1 ∈ R H×3N . The rest of our neural network architecture stays exactly the same. The right-hand picture of <ref type="figure">figure 1</ref> presents a graphical representation.</p><p>For the multi-way case, we present an adapted version of the training objective. Given an attested subject-verb-object tuple (i, j, k), the goal of our network is now to discriminate this correct tuple from other, corrupted tuples (i, j, k ), (i, j , k) and (i, j , k ), where the correct arguments have been replaced by random subjects j and random objects k . Note that we do not only want the network to learn the infelicity of tuples in which both the subject and object slot are corrupted; we also want our network to learn the infelicity of tuples in which either the subject or object slot is corrupt, while the other slot contains the correct, attested argument. This leads us to the objective function represented in (10).</p><formula xml:id="formula_10">∑ k ∈K max(0, 1 − g[(i, j, k)] + g[(i, j, k )]) + ∑ j ∈J max(0, 1 − g[(i, j, k)] + g[(i, j , k)]) + ∑ j ∈J k ∈K max(0, 1 − g[(i, j, k)] + g[(i, j , k )]) (10)</formula><p>As in the two-way case, the gradient of the objec- tive function is sampled by randomly picking one corrupted subject j and one corrupted object k for each tuple (i, j, k). All of the model's parameters are again updated through backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementational details</head><p>We evaluate our neural network approach to se- lectional preference acquisition using verb-object tuples for the two-way model, and subject-verb- object tuples for the multi-way model.</p><p>Our model has been applied to English, using the UKWaC corpus ( <ref type="bibr">Baroni et al., 2009)</ref>, which covers about 2 billion words of web text. The corpus has been part of speech tagged and lemmatized with Stanford Part-Of-Speech Tagger ( <ref type="bibr" target="#b20">Toutanova et al., 2003)</ref>, and parsed with MaltParser ( <ref type="bibr" target="#b12">Nivre et al., 2006</ref>), so that dependency tuples could be extracted.</p><p>For the two-way model, we select all verbs and objects that appear within a predicate-argument re- lation with a frequency of at least 50. This gives us a total of about 7K verbs and 30K objects. For the multi-way model, we select the 2K most fre- quent verbs, together with the 10K most frequent subjects and the 10K most frequent objects (that appear within a transitive frame).</p><p>All words are converted to lowercase. We use the lemmatized forms, and only keep those forms that contain alphabetic characters. Furthermore, we require each tuple to appear at least three times in the corpus.</p><p>We set N, the size of our embedding matrices, to 50, and H, the number of units in the hidden layer, to 100. Following <ref type="bibr" target="#b4">Huang et al. (2012)</ref>, we use mini-batch L-BFGS ( <ref type="bibr" target="#b8">Liu and Nocedal, 1989</ref>) with 1000 pairs of good and corrupt tuples per batch for training, and train for 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Task</head><p>Our models are quantitatively evaluated using a pseudo-disambiguation task ( <ref type="bibr" target="#b18">Rooth et al., 1999</ref>), which bears some resemblance to our training pro- cedure. The task provides an adequate test of the generalization capabilities of our models. For the two-way case, the task is to judge which object (o or o ) is more likely for a particular verb v, where (v, o) is a tuple attested in the corpus, and o is a di- rect object randomly drawn from the object vocab- ulary. The tuple is considered correct if the model prefers the attested tuple (v, o) over (v, o ). For the three-way case, the task is to judge which subject  <ref type="table" target="#tab_0">Tables 1 and 2</ref> respectively show a number of examples from the two-way and three-way pseudo-disambiguation task.   <ref type="table">Table 2</ref>: Pseudo-disambiguation examples for three-way subject-verb-object tuples</p><p>The models are evaluated using 10-fold cross validation. All tuples from our corpus are randomly divided into 10 equal parts. Next, for each fold, 9 parts are used for training, and the remaining part is used for testing. In order to properly test the generalization capability of our models, we make sure that all instances of a particular tuple appear in one part only. This way, we make sure that tuples used for testing are never seen during training.</p><p>For the two-way model, our corpus consists of about 70M tuple instances (1.9M types), so in each fold, about 63M tuple instances are used for train- ing and about 7M (190K types) are used for testing. For the three-way model, our corpus consists of about 5,5M tuple instances (750K types), so in each fold, about 5M tuples are used for training and about 500K (75K types) are used for testing. Note that our training procedure is instance-based, while our evaluation is type-based: during training, the neural network sees a tuple as many times as it appears in the training set, while for testing each individual tuple is only evaluated once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison models</head><p>We compare our neural network model to a number of other models for selectional preference acquisi- tion.</p><p>For the two-way case, we compare our model to the EM-based clustering technique presented by <ref type="bibr" target="#b18">Rooth et al. (1999)</ref>, <ref type="bibr">2</ref> and to <ref type="bibr">Erk et al.'s (2010)</ref> similarity-based model. For Rooth et al.'s model, we set the number of latent factors to 50. Us- ing a larger number of latent factors does not in- crease performance. For Erk et al.'s model, we create a dependency-based similarity model from the UKWaC corpus using our 30K direct objects as instances and 100K dependency relations as features. The resulting matrix is weighted using pointwise mutual information <ref type="bibr">(Church and Hanks, 1990)</ref>. Similarity values are computed using cosine. Furthermore, we use a sampling procedure in the testing phase: we sample 5000 predicate-argument pairs for each fold, as testing Erk et al.'s model on the complete test sets proved prohibitively expen- sive.</p><p>For the three-way case, we compare our model to the tensor factorization model we developed in previous work (Van de Cruys, 2009). We set the number of latent factors to 300. <ref type="bibr">3</ref>   accuracy (µ ± σ ) <ref type="bibr" target="#b18">Rooth et al. (1999)</ref> .720 ± .002 <ref type="bibr" target="#b1">Erk et al. (2010)</ref> .887 ± .004 2-way neural network .880 ± .001 <ref type="table" target="#tab_1">Table 3</ref>: Comparison of model results for two-way selectional preference acquisition -mean accuracy and standard deviations of 10-fold cross-validation results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Two-way model</head><p>The results indicate that our neural network ap- proach outperforms <ref type="bibr">Rooth et al.'s (1999)</ref> method by a large margin (16%). Clearly, the neural net- work architecture is able to model selectional pref- erences more profoundly than Rooth et al.'s latent variable approach. The difference between the models is highly statistically significant (paired t-test, p &lt; .01), as the standard deviations already indicate.</p><p>Erk et al.'s model reaches a slightly better score than our model, and this result is also statistically significant (paired t-test, p &lt; .01). However, Erk et al.'s model does not provide full coverage, whereas the other two models are able to compute scores for all pairs in the test set. In addition, Erk et al.'s model is much more expensive to compute. Our model computes selectional preference scores for the test set in a matter of seconds, whereas for Erk et al.'s model, we ended up sampling from the test set, as computing preference values for the complete test set proved prohibitively expensive. <ref type="table">Table 4</ref> compares the results of our neural network architecture for three-way selectional preference acquisition to the results of the tensor-based factor- ization method (Van de Cruys, 2009).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Three-way model</head><formula xml:id="formula_11">model accuracy (µ ± σ )</formula><p>Van de Cruys (2009) .874 ± .001 3-way neural network .889 ± .001 <ref type="table">Table 4</ref>: Comparison of model results for three-way selectional preference acquisition -mean accuracy and standard deviations of 10-fold cross-validation results</p><p>The results indicate that the neural network ap- proach slightly outperforms the tensor-based factor- ization method. Again the model difference is sta-tistically significant (paired t-test, p &lt; 0.01). Using our adapted training objective, the neural network is clearly able to learn a rich model of three-way selectional preferences, reaching state of the art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Examples</head><p>We conclude our results section by briefly present- ing a number of examples that illustrate the kind of semantics present in our models. Similar to neu- ral language models, the predicate and argument embedding matrices of our neural network con- tain distributed word representations, that capture the similarity of predicates and arguments to other words. <ref type="table" target="#tab_4">Tables 5 and 6</ref> contain a number of nearest neigh- bour similarity examples for predicate and argu- ments from our two-way neural network model. The nearest neighbours were calculated using stan- dard cosine similarity.  <ref type="table">Table 5</ref>: Nearest neighbours of 4 verbs, calculated using the distributed word representations of em- bedding matrix V from our two-way neural net- work model <ref type="table">Table 5</ref> indicates that the network is effectively able to capture a semantics for verbs. The first column -verbs similar to DRINK -all have to do with food consumption. The second column con- tains verbs related to computer programming. The third column is related to human communication; and the fourth column seems to illustrate the net- work's comprehension of FLOOD having to do with invasion and water.   <ref type="table" target="#tab_4">, table 6</ref> shows the network's ability to capture the meaning of nouns that appear as direct objects to the verbs. Column one contains things that can be read. Column two contains things that can be consumed. Column three seems to hint at supervising professions, while column four seems to capture creative professions.</p><p>A similar kind of semantics is present in the em- bedding matrices of the three-way neural network model. <ref type="table" target="#tab_6">Tables 7, 8, and 9</ref>   <ref type="table" target="#tab_6">Table 7</ref>: Nearest neighbours of 4 verbs, calculated using the distributed word representations of em- bedding matrix V from our three-way neural net- work model  <ref type="table">Table 8</ref>: Nearest neighbours of 4 subjects, calcu- lated using the distributed word representations of embedding matrix S from our three way neural network model <ref type="table">Table 8</ref> illustrates the semantics for the subject slot of our three-way model. The first column cap- tures nature terms, the second column contains university-related terms, the third column contains politicians/government terms, and the fourth col- umn contains art expressions.</p><p>Finally, table 9 demonstrates the semantics of our three-way model's object slot. Column one generally contains housing terms, column two con- tains various locations, column three contains din- ing occasions, and column four contains textual expressions. <ref type="table">PARK  LUNCH  THESIS   FLOOR  STUDIO  DINNER  QUESTIONNAIRE  CEILING  VILLAGE  MEAL  DISSERTATION  ROOF  HALL  BUFFET  PERIODICAL  METRE  MUSEUM  BREAKFAST  DISCOURSE   Table 9</ref>: Nearest neighbours of 4 direct objects, cal- culated using the distributed word representations of embedding matrix O from our three way neural network model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WALL</head><p>Note that the embeddings for the subject and the object slot is different, although they mostly contain the same words. This allows the model to capture specific semantic characteristics for words given their argument position. Virus, for example, is in subject position more similar to active words like animal, whereas in object position, it is more similar to passive words like cell, device. Similarly, mouse in subject position tends to be similar to words like animal, rat whereas in object position it is similar to words like web, browser.</p><p>These examples, although anecdotal, illustrate that our neural network model is able to capture a rich semantics for predicates and arguments, which subsequently allows the network to make accurate predictions with regard to selectional preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>In this paper, we presented a neural network ap- proach to the acquisition of selectional preferences. Inspired by recent work on neural language models, we proposed a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate. The model is entirely unsupervised, as preferences are learned from unannotated corpus data. Positive training instances are constructed from attested corpus data, while negative instances are constructed from ran- domly corrupted instances. Using designated net- work architectures, we are able to handle stan- dard two-way selectional preferences as well as multi-way selectional preferences. A quantitative evaluation on a pseudo-disambiguation task shows that our models achieve state of the art perfor- mance. The results for our two-way neural network are on a par with <ref type="bibr">Erk et al.'s (2010)</ref> similarity- based approach, while our three-way neural net- work slightly outperforms the tensor-based factor- ization model (Van de Cruys, 2009) for multi-way selectional preference induction.</p><p>We conclude with a number of issues for future work. First of all, we would like to investigate how our neural network approach might be improved by incorporating information from other sources. In particular, we think of initializing our embedding matrices with distributed representations that come from a large-scale neural language model ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>). We also want to further investigate the advantages and disadvantages of having dif- ferent embedding matrices for different argument positions in our multi-way neural network. In our results section, we demonstrated that such an ap- proach allows for more flexibility, but it also adds a certain level of redundancy. We want to inves- tigate the benefit of our approach, compared to a model that shares the distributed word representa- tion among different argument positions. Finally, we want to investigate more advanced neural net- work architectures for the acquisition of selectional preferences. In particular, neural tensor networks ( <ref type="bibr">Yu et al., 2013)</ref> have recently demonstrated im- pressive results in related fields like speech recogni- tion, and might provide the necessary machinery to model multi-way selectional preferences in a more profound way.</p><p>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 83- 90, Athens, Greece, March. Association for Compu- tational Linguistics.</p><p>Dong Yu, Li Deng, and Frank Seide. 2013. The deep tensor neural network with applications to large vocabulary speech recognition. IEEE Transac- tions on Audio, Speech, and Language Processing, 21(2):388-396.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(s or s ) and direct object (o or o ) are more likely for a particular verb v, where (v, s, o) is the attested tuple, and s and o are a random subject and object drawn from their respective vocabularies. The tu- ple is considered correct if the model prefers the attested tuple (v, s, o) over the alternatives (v, s, o ), (v, s , o), and (v, s , o ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Pseudo-disambiguation examples for two-
way verb-object tuples 

v 
s 
o 
s 
o 

win 
team 
game 
diversity 
egg 
publish government document grid 
priest 
develop company 
software 
breakfast landlord 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 compares</head><label>3</label><figDesc></figDesc><table>the results of our neural network 
architecture for two-way selectional preferences to 
the results of Rooth et al.'s (1999) model and Erk 
et al.'s (2010) model. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Nearest neighbours of 4 direct objects, cal-
culated using the distributed word representations 
of embedding matrix O from our two way neural 
network model 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>again illustrate this using word similarity calculations.</figDesc><table>SEARCH 
DIMINISH 
CONFIGURE 
PROSECUTE 

CLICK 
LESSEN 
AUTOMATE 
CRITICISE 
BROWSE 
DISTORT 
SCROLL 
URGE 
SCROLL 
HEIGHTEN 
PROGRAM 
DEPLORE 
UPLOAD 
DEGRADE 
INSTALL 
CONDEMN 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 shows</head><label>7</label><figDesc>the network's verb semantics for the three-way case. The first column is related to internet usage, the second column contains verbs of scalar change, column three is again related to computer usage, and column four seems to capture 'mending' verbs.</figDesc><table>FLOWER 
COLLEGE 
PRESIDENT 
SONG 

FISH 
UNIVERSITY 
BUSH 
FILM 
BIRD 
INSTITUTE 
BLAIR 
ALBUM 
SUN 
DEPARTMENT 
MP 
PLAY 
TREE 
CENTRE 
CHAIRMAN 
MUSIC 

</table></figure>

			<note place="foot" n="1"> We exemplify the model using three-way selectional preferences for transitive predicates, but the model can be straightforwardly generalized to other multi-way selectional preferences.</note>

			<note place="foot" n="2"> Our own implementation of Rooth et al.&apos;s (1999) algorithm is based on non-negative matrix factorization (Lee and Seung, 2000). Non-negative matrix factorization with Kullback-Leibler divergence has been shown to minimize the same objective function as EM (Li and Ding, 2006). 3 The best scoring model presented by Van de Cruys (2009) also uses 300 latent factors; using more factors does not improve the results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">networks with multitask learning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A flexible, corpus-driven model of regular and inverse selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="723" to="763" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple, similarity-based model for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="245" to="288" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalizing case frames using a thesaurus and the MDL principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="244" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The relationships among various nonnegative matrix factorization methods for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2006. ICDM&apos;06. Sixth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="654" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maltparser: A data-driven parser-generator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC-2006</title>
		<meeting>LREC-2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2216" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelling selectional preferences in a lexical hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´o</forename><surname>Diarmuid´odiarmuid´</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="170" to="179" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent variable models of selectional preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><surname>Diarmuid´o Séaghdha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flexible, corpus-based modelling of human plausibility judgements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selectional constraints: An information-theoretic model and its computational realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="127" to="159" />
			<date type="published" when="1996-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A latent dirichlet allocation method for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inducing a semantically annotated lexicon via em-based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Rooth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detlef</forename><surname>Prescher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
	<note>Glenn Carroll, and Franz Beil</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical metaphor processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="353" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling and learning semantic co-compositionality through prototype projections and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Tsubaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A non-negative tensor factorization model for selectional preference induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
