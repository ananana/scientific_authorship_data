<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Better Internal Structure of Words for Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Xin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Research</orgName>
								<address>
									<addrLine>2025 Hamilton Ave</addrLine>
									<postCode>95125</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Hart</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Research</orgName>
								<address>
									<addrLine>2025 Hamilton Ave</addrLine>
									<postCode>95125</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhuti</forename><surname>Mahajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Research</orgName>
								<address>
									<addrLine>2025 Hamilton Ave</addrLine>
									<postCode>95125</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-David</forename><surname>Ruvini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Research</orgName>
								<address>
									<addrLine>2025 Hamilton Ave</addrLine>
									<postCode>95125</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Better Internal Structure of Words for Sequence Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2584" to="2593"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2584</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Character-based neural models have recently proven very useful for many NLP tasks. However , there is a gap of sophistication between methods for learning representations of sentences and words. While, most character models for learning representations of sentences are deep and complex, models for learning representations of words are shallow and simple. Also, in spite of considerable research on learning character embeddings, it is still not clear which kind of architecture is the best for capturing character-to-word representations. To address these questions, we first investigate the gaps between methods for learning word and sentence representations. We conduct detailed experiments and comparisons on different state-of-the-art con-volutional models, and also investigate the advantages and disadvantages of their constituents. Furthermore, we propose IntNet, a funnel-shaped wide convolutional neural architecture with no down-sampling for learning representations of the internal structure of words by composing their characters from limited , supervised training corpora. We evaluate our proposed model on six sequence labeling datasets, including named entity recognition, part-of-speech tagging, and syntactic chunk-ing. Our in-depth analysis shows that IntNet significantly outperforms other character embedding models and obtains new state-of-the-art performance without relying on any external knowledge or resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence labeling is the task of assigning a label or class to each element of a sequence of data, and is one of the first stages in many natural language processing (NLP) tasks. For example, named en- tity recognition (NER) aims to classify words in a sentence into several predefined categories of in- terest such as person, organization, location, etc.</p><p>Part-of-speech (POS) tagging assigns a part of speech to each word in an input sentence. Syn- tactic chunking divides text into syntactically re- lated, non-overlapping groups of words. Sequence labeling is a challenging problem because human annotation is very expensive and typically only a small amount of tagging data is available.</p><p>Most traditional sequence labeling systems have been dominated by linear statistical models which heavily rely on feature engineering. As a result, carefully constructed hand-crafted fea- tures and domain-specific knowledge are widely used for solving these tasks. Unfortunately, it is costly to develop domain specific knowledge and hand-crafted features. Recently, neural networks using character-level information have been used successfully for minimizing the need of feature engineering. There are basically two threads of character-based modeling, one focuses on learn- ing representations of sentences for semantics and syntax <ref type="bibr" target="#b29">(Zhang et al., 2015;</ref><ref type="bibr" target="#b3">Conneau et al., 2017)</ref>; the other focuses on learning representa- tions of words for the purpose of eliminating hand- crafted features for word shape information <ref type="bibr" target="#b12">(Lample et al., 2016;</ref><ref type="bibr" target="#b15">Ma and Hovy, 2016)</ref>.</p><p>Two main state-of-the-art approaches of learn- ing character representations for sequence labeling emerged from the latter thread. One is based on RNNs and uses bidirectional LSTMs or GRUs to learn forward and backward character information ( <ref type="bibr" target="#b13">Ling et al., 2015;</ref><ref type="bibr" target="#b12">Lample et al., 2016;</ref><ref type="bibr" target="#b27">Yang et al., 2017)</ref>. The other approach is based on CNNs with a fixed-size window around each word to create character-level representations <ref type="bibr" target="#b21">(Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b1">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b15">Ma and Hovy, 2016)</ref>. However, there is a gap in the so- phistication between character-based methods for learning representations of sentences compared to that of words. We found that most of the state- of-the-art character-based CNN models for words use a convolution followed by max pooling as a shallow feature extractor, which is very different from the CNN models with deep and complex ar- chitecture for sentences. In spite of considerable research on learning character embeddings, it is still not clear which kind of architecture is the best for capturing character-to-word representations.</p><p>Therefore, a number of questions remain open:</p><p>• Why is there a gap between methods for learning representations of sentences and words? How can this gap be bridged?</p><p>• How do state-of-the-art character embedding models differ in term of performance?</p><p>• What kind of neural network architecture is better for learning the internal structure of a word? Deep or shallow? Narrow or wide?</p><p>To answer these questions, we first investigate the gap between learning word representations and sentence representations for convolutional ar- chitectures. The most straightforward idea is to add more convolutional layers which follows the approaches from learning representations of sen- tences. Interestingly, we observe the accuracy does not increase much and found that accuracy drops when we increased the depth of the net- work. This observation shows that learning char- acter representations for the internal structure of words is very different than sentences, and also might explain one of the reasons there has been a gap in character-based CNN models for repre- senting words and sentences.</p><p>In this paper, we present detailed experiments and comparisons across different state-of-the-art convolutional models from natural language pro- cessing and computer vision. We also investi- gate the advantages and disadvantages of some of their constituents on different convolutional architectures. Furthermore, we propose IntNet, a funnel-shaped wide convolutional neural net- work for learning the internal structure of words by composing their characters. Unlike previous CNN-based approaches, our funnel-shaped Int- Net explores deeper and wider architecture with no down-sampling for learning character-to-word representations from limited supervised training corpora. Lastly, we combine our IntNet model with LSTM-CRF, which captures both word shape and context information, and jointly decode tags for sequence labeling.</p><p>The main contributions of this paper are the fol- lowing:</p><p>• We conduct detailed studies on investigating the gap between learning word representa- tions and sentence representations.</p><p>• We provide in-depth experiments and empir- ical comparisons of different convolutional models and explore the advantages and dis- advantages of their components for learning character-to-word representations.</p><p>• We propose a funnel-shaped wide convo- lutional neural architecture with no down- sampling that focuses on learning a better in- ternal structure of words.</p><p>• Our proposed compositional character-to- word model combined with LSTM-CRF achieves state-of-the-art performance for var- ious sequence labeling tasks.</p><p>This paper is organized as follows: Section 2 describes multiple threads of related work. Sec- tion 3 presents the whole architecture of the neu- ral network. Section 4 provides details about ex- perimental settings and compared methods. Sec- tion 5 reports model results on different bench- marks with detailed analyses and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There exist three threads of related work regarding the topic of this paper: (i) different convolutional architectures from different domains; (ii) character embedding models for words; (iii) sequence label- ing with deep neural network.</p><p>CNN models across domains. Convolutional neural networks (CNNs) are very useful in extract- ing information from raw signals. In the area of NLP, <ref type="bibr" target="#b10">Kim (2014)</ref> was the first to propose shallow CNN with word embeddings for sentence classifi- cation. <ref type="bibr" target="#b29">Zhang et al. (2015)</ref> proposed CNN with 6 convolutional layers by directly extracting charac- ter level information for learning representations of semantic structure on sentences. Recently, Con- neau et al. (2017) proposed a VDCNN architec- ture with 29 convolutional layers using residual connections for text classification. However, one study on randomly dropping layers for training deep residual networks, ( <ref type="bibr" target="#b8">Huang et al., 2016</ref>), has shown that not all layers may be needed and high- lighted there is some amount of redundancy in ResNet ( <ref type="bibr" target="#b5">He et al., 2016)</ref>. Also, some research has shown promising results with wide architectures, for example, wide ResNet ( <ref type="bibr" target="#b28">Zagoruyko and Komodakis, 2016</ref>), Inception-ResNet ( <ref type="bibr" target="#b22">Szegedy et al., 2017</ref>) and DenseNet ( <ref type="bibr" target="#b7">Huang et al., 2017</ref>). These models use character-level information to learn representations are for sentences, not words.</p><p>Character embedding models. <ref type="bibr" target="#b21">Santos and Zadrozny (2014)</ref> proposed a CNN model to learn character representations of words to re- place hand-crafted features for part-of-speech tag- ging. <ref type="bibr" target="#b13">Ling et al. (2015)</ref> proposed a bidirectional LSTM over characters to use as input for learn- ing character-to-word representations. <ref type="bibr" target="#b1">Chiu and Nichols (2016)</ref> proposed a bidirectional LSTM- CNN with lexicons for named entity recognition by applying the CNN-based character embedding model from <ref type="bibr" target="#b21">Santos and Zadrozny (2014)</ref>. <ref type="bibr" target="#b19">Plank et al. (2016)</ref> proposed a bi-LSTM model with aux- iliary loss for multilingual part-of-speech tagging by following the LSTM-based character embed- ding model from <ref type="bibr" target="#b13">Ling et al. (2015)</ref>. <ref type="bibr" target="#b4">Cotterell and Heigold (2017)</ref> proposed a character-level transfer learning model for neural morphological tagging. To test the effectiveness of our proposed model, we use these two models as our baselines in the latter sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IntNet</head><p>Character embeddings. The first step is to ini- tialize the character embeddings for each word w in the input sequence. We define the finite set of characters V char . This vocabulary contains all the variations of the raw text, including upper- case and lowercase letters, numbers, punctuation marks, and symbols. Unlike some character-based approaches, we do not use any character-level pre- possessing which enables our model to learn and capture regularities from prefixes to suffixes to construct character-to-word representations. The input word w is decomposed into a sequence of characters {c 1 , ..., c n }, where n is the length of w. Character embeddings are encoded by col- umn vectors in the embedding matrix W char ∈ R d char ×|V char | , where d char is the number of pa- rameters for each character in V char . Given a character c i , its embedding r char i is obtained by the matrix-vector product:</p><formula xml:id="formula_0">r char i = W char v char i ,<label>(1)</label></formula><p>where v char i is defined as a one-hot vector for c i . We randomly initialize a look-up table with values drawn from a uniform distribution with</p><formula xml:id="formula_1">range [− 3 d char , + 3 d char ]</formula><p>, where d char is em- pirically chosen by users. The character set in- cludes all unique characters and the special tokens PADDING and UNKNOWN. We do not perform any character-level preprocessing, including case nor- malization, digit replacement (e.g. replacing all sequences of digits 0-9 with a single "0"), nor do we use any capitalization features (e.g. allCaps, upperInitial, lowercase, mixedCaps, noinfo).</p><p>Convolutional blocks. The input for the Int- Net is the sequence of character embeddings {r char 1 , ..., r char n }. First is the initial convolutional layer, which is a temporal convolutional mod- ule that computes 1-D convolutions. Let x i ∈ R d char ×r char be the concatenation of the charac- ter embeddings for each w. The initial convolu- tional layer applies a matrix-vector operation to each successive window of size k char . An input k-grams x i:i+k−1 is transformed through a convo- lution filter w c :</p><formula xml:id="formula_2">c i = f (w c · x i:i+k−1 + b c ),<label>(2)</label></formula><p>where c i is the feature map of 1-D convolution, f is the non-linear ReLU function, and b c is a bias term. Equation 2 produces m filters with different kernel sizes. The filters are computed with differ- ent kernels by the initial convolutional layers are concatenated:</p><formula xml:id="formula_3">g 0 = [c k 1 1 . . . c k 1 m ; c k 2 1 . . . c k 2 m ; c k h 1 . . . k h m ],<label>(3)</label></formula><p>where h is the number of kernels, g 0 is the output for the initial convolutional layer which feeds into the next convolutional block. We define F(·) as a function of several con- secutive operations within a convolutional block. Firstly, a N×1 convolution transforms the input. The output size is 4 × m × h feature maps, like a bottleneck layer. The next step consists of multiple 1-D convolutions with kernels of different sizes. Lastly, we concatenate all the feature maps from kernels of different size. In each convolution, we use a batch normalization, followed by a ReLU ac- tivation and N×k temporal convolution.</p><p>Funnel-shaped wide architecture. The net- work comprises of L convolutional layers, which implies ( L−1 2 ) convolutional blocks. We use direct connections from every other layer to all subse- quent layers, inspired by dense connection. There- fore, the l th layer has access to the feature maps of all the alternate layers:</p><formula xml:id="formula_4">g l = F l ([g 0 , g 2 , . . . , g l−2 ]).<label>(4)</label></formula><p>Equation 4 ensures maximum information flow between blocks in the network. Compared to residual connection F l (g l−1 ) + g l−1 , it can be viewed as an extreme case of residual connec- tion and makes feature reuse possible. Unlike DenseNet and ResNet, we concatenate feature maps by different kernels in every other convo- lutional layers, which captures different levels of features and makes our wide architecture possible, inspired by Inception. Different levels of concate- nation can help IntNet to learn different patterns of word shape information. We compare our ar- chitecture to residual connection and dense con- nection for learning character-to-word representa- tions in Section 5.</p><p>Without down-sampling. Compared to other CNN models like ResNet and DenseNet, our model does not contain any halve down-sampling layer or average pooling to reduce resolution. We did not find these operations to be helpful and, in   some cases, found them to be detrimental to per- formance. These operations are useful for sen- tences and images, but might break the internal structure of words, like the sequential patterns for prefixes and suffixes.</p><p>Character-to-word representations. In the last layer, we use a max-over-time pooling oper- ation:</p><formula xml:id="formula_5">ˆ c i = max(c i ),<label>(5)</label></formula><p>which takes the maximum value corresponding to a particular filter. The idea is to capture the most important feature with the highest value for each feature map. Finally, we concatenate all of salient features together as a representation for this word:</p><formula xml:id="formula_6">z = [ ˆ c 0 , ˆ c 1 , . . . ˆ c u ],<label>(6)</label></formula><p>where u is the number of salient features which is equal to the total number of output feature maps in the last layer. If each function F l produces p feature maps, we obtain (p 0 + p × L−1 2 ) represen- tations, where p 0 is the number of output feature maps in the initial convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bi-directional RNN</head><p>Given the character-to-word representations are computed by IntNet in Equation 6, we denote the input vector (z 1 , z 2 , . . . , z n ) for a sentence. LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>) re- turns the sequence (h 1 , h 2 , . . . , h n ) that repre- sents the sequential information at every step. We use the following implementation:</p><formula xml:id="formula_7">i t = σ(W zi z t + W hi h t−1 + W ci c t−1 + b i ) f t = σ(W zf z t + W hf h t−1 + W cf c t−1 + b f ) c t = tanh(W zc z t + W hc h t−1 + b c ) c t = f t c t−1 + i t c t o t = σ(W zo z t + W ho h t−1 + W co c t + b o ) h t = o t tanh(c t ),</formula><p>where σ is the element-wise sigmoid function and is the element-wise product. z t is the input vec- tor at time t and i t , f t , o t , c t are the input gate, forget gate, output gate, and cell vectors, all of which are the same size as the hidden vector h t . </p><formula xml:id="formula_8">W zi , W zf , W zo , W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scoring Function</head><p>Instead of predicting each label independently, we consider the correlations between labels in neigh- borhoods and jointly decode the best chain of la- bels for a given input sentence by leveraging a conditional random field ( <ref type="bibr" target="#b11">Lafferty et al., 2001</ref>). Formally, the sequence of labels is defined as:</p><formula xml:id="formula_9">y = (y 1 , y 2 , ..., y T ).<label>(7)</label></formula><p>To define the scoring function f (h, y) for each position t, we multiply the hidden state h w t with a parameter vector w yt that is indexed by the tag y t to obtain the matrix of scores output by the bidi- rectional LSTM network. Therefore, the function f can be written as:</p><formula xml:id="formula_10">f (h, y) = T t=1 w yt h w t + T t=1 A y t−1 ,yt .<label>(8)</label></formula><p>In Equation 8, A is a matrix of transition scores, A i,j represents the score of a transition from the tag i to tag j, y 1 is the start tag of a sentence. Let Y(h) denote the set of possible label sequences for h. A probabilistic model for a sequence defines a family of conditional probabilities p(y|h) over all possible label sequences y given h with the fol- lowing form:</p><formula xml:id="formula_11">p(y|h) = e f (h,y) y ∈Y(h) e f (h,y ) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective Function and Inference</head><p>For end-to-end network training, we use maxi- mum conditional likelihood estimation to max- imize the log probability of the correct tag se- quence:</p><formula xml:id="formula_12">log(p(y|h)) = f (h, y) − log   y ∈Y(h) e f (h,y )   .</formula><p>While decoding, we predict the label sequence that obtains the highest score given by:</p><formula xml:id="formula_13">y * = arg max y ∈Y(h) f (h, y ).<label>(10)</label></formula><p>The objective function and its gradients can be efficiently computed by dynamic programming; for inference, we use the Viterbi algorithm to find the best tag path which maximizes the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We performed experiments on six standard datasets for sequence labeling tasks, i.e. named entity recognition, part-of-speech tagging, and syntactic chunking. To test the effectiveness of our proposed model, we do not use language-specific resources (such as gazetteers), external knowledge  <ref type="bibr" target="#b23">(Tjong Kim Sang, 2002</ref>; Tjong Kim Sang and De Meulder, 2003) con- tain named entity labels for Spanish, Dutch, En- glish and German as separate datasets. These four datasets contain different types of named entities: locations, persons, organizations, and miscella- neous entities. Unlike some approaches, we do not combine the validation set with the training set. Although POS tags were made available for these datasets, we do not leverage those as addi- tional information which sets our approach apart from that of transfer learning.</p><p>Part-of-speech tagging. The Wall Street Jour- nal (WSJ) portion of Penn Treebank (PTB) <ref type="bibr" target="#b16">(Marcus et al., 1993</ref>) contains 25 sections and catego- rizes each word into one out of 45 POS tags. We adopt the standard split and use sections 0-18 as training data, sections 19-21 as development data, and sections 22-24 as test data.</p><p>Syntactic chunking. The CoNLL 2000 chunk- ing task <ref type="bibr" target="#b25">(Tjong Kim Sang and Buchholz, 2000</ref>) uses sections 15-18 from the Wall Street Journal corpus for training and section 20 for testing. It defines 11 syntactic chunk types (e.g., NP, VP, ADJP), we adopt the standard split and sample 1000 sentences from the training set as the devel- opment set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Settings</head><p>Initialization. The size of the dimensions of char- acter embeddings is 32 which are randomly ini- tialized using a uniform distribution. We adopt the same initialization method for randomly ini- tialized word embeddings that are updated during training. For IntNet, the filter size of the initial convolution is 32 and that of other convolutions is 16. We have used filters of size <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr">5]</ref> for all the kernels. The number of convolutional layers are 5 and 9 for IntNet-5 and IntNet-9, respectively, and we have adopted the same weight initialization as that of ResNet. We use pre-trained word embed- dings for initialization, <ref type="bibr">GloVe (Pennington et al., 2014</ref>) 100-dimension word embeddings for En- glish, and fastText ( <ref type="bibr" target="#b0">Bojanowski et al., 2017</ref>) 300- dimension word embeddings for Spanish, Dutch, and German. The state size of the bi-directional LSTMs is set to 256. We adopt standard BIOES tagging scheme for NER and Chunking.</p><p>Optimization. We employ mini-batch stochas- tic gradient descent with momentum. The batch size, momentum and learning rate are set to 10, 0.9 and η t = η 0 1+ρt , where η 0 is the initial learning rate 0.01 and ρ = 0.05 is the decay ratio, the value of gradient clipping is 5. Dropout is applied on the input of IntNet, LSTMs, and CRF, and its ratio 0.5 is fixed, but with no dropout inside of IntNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Methods</head><p>To address those open questions in Section 1, we conduct detailed experiments and empirical comparisons on different state-of-the-art charac- ter embedding models across different domains. Firstly, we use LSTM-CRF with randomly ini- tialized word embeddings as our initial baseline. We adopt two state-of-the-art methods in sequence labeling, denoted as char-LSTM ( <ref type="bibr" target="#b12">Lample et al., 2016</ref>) and char-CNN ( <ref type="bibr" target="#b15">Ma and Hovy, 2016)</ref>. We add more layers to the char-CNN model and re- fer to that as char-CNN-5 and char-CNN-9, re- spectively for 5 and 9 convolutional layers. Fur- thermore, we add residual connections to the char- CNN-9 and refer it as char-ResNet. Also, we ap- ply 3 dense blocks based on char-ResNet which we refer to as char-DenseNet, to compare the dif- ference between residual connection and dense connection. Lastly, we refer to our proposed  <ref type="bibr" target="#b1">Chiu and Nichols, 2016)</ref> - - 90.77 - - - LSTM-Softmax+char-LSTM ( <ref type="bibr" target="#b13">Ling et al., 2015)</ref> - - - - - 97.55 LSTM-CRF+char-LSTM ( <ref type="bibr" target="#b12">Lample et al., 2016)</ref> 85.75 81.74 90.94 78.76 - - LSTM-CRF+char-CNN ( <ref type="bibr" target="#b15">Ma and Hovy, 2016)</ref> - - 91.21 - - 97.55 GRM-CRF+char-GRU ( <ref type="bibr" target="#b27">Yang et al., 2017)</ref> 84   model, which uses different convolution layers, as char-IntNet-5 and char-IntNet-9. <ref type="table">Table 1</ref> presents the performance of differ- ent character-to-word models on six benchmark datasets. For sequence labeling, char-LSTM and char-CNN are current state-of-the-art character embedding models for learning character-to-word representations. We observe that char-LSTM per- forms better than char-CNN in most cases, how- ever, char-CNN uses a convolution layer followed by max pooling as a shallow feature extractor, that does not explore the full potential of CNNs. Therefore, we implement two variations based on char-CNN, referred to as char-CNN-5 and char- CNN-9. The result shows that for most of the datasets, the F1 score does not improve much when we directly add more layers. We also ob- serve some accuracy drop when we continuously increase the depth. This confirms why most CNN- based approaches for learning representations on words are shallow, which is very different from learning representations for sentences. Further- more, we add residual connections to char-CNN- 9 as char-ResNet-9, which confirms that residual connections can help train deep layers. We fur- ther improve char-ResNet-9 by changing residual connections into dense connection blocks as char- DenseNet-9, which shows that the dense connec- tions are better than residual connections for learn- ing word shape information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Character-to-word Models</head><p>Our proposed character-to-word model, char- IntNet-5 and char-IntNet-9 generally improves the results across all datasets. Our IntNet significantly outperforms other character embedding models, for example, the improvement is more than 2% in terms of F1 score for German and Dutch. Also, we observe that char-IntNet-5 is more ef- fective for learning character-to-word representa- tions than char-IntNet-9 in most of the cases. The only exception is German which seems to require a deeper and wider model for learning better rep- resentations. <ref type="table" target="#tab_5">Table 2</ref> presents our proposed model in com- parison with state-of-the-art results. LSTM-CRF is our baseline which uses fine-tuned pre-trained word embeddings. Its comparison with LSTM- CRF using random initializations for word em- beddings, as shown in <ref type="table">Table 1</ref>, confirms that pre-trained word embeddings are useful for se- quence labeling. Since the training corpus for sequence labeling is relatively small, pre-trained embeddings learned from a huge unlabeled cor- pus can help to enhance word semantics. Fur- thermore, we adopt and re-implement two state- of-the-art character models, char-LSTM and char- CNN, by combining with LSTM-CRF, which we <ref type="table">Model  English  German  Spanish  Dutch  IV  OOTV OOEV OOBV  IV  OOTV OOEV OOBV  IV  OOTV OOEV OOBV  IV  OOTV OOEV</ref>    refer to as LSTM-CRF-char-LSTM and LSTM- CRF-char-CNN. Lastly, we combine our proposed model with LSTM-CRF which we refer to as LSTM-CRF-char-IntNet-9 and LSTM-CRF-char- IntNet-5. These experiments show that our char-IntNet generally improves results across different mod- els and datasets. The improvement is more pro- nounced for non-English datasets, for example, IntNet improves the F-1 score over the state- of-the-art results by more than 2% for Dutch and Spanish. It also shows that the results of LSTM-CRF are significantly improved after adding character-to-word models, which confirms that word shape information is very important for sequence labeling. <ref type="figure" target="#fig_3">Figure 2</ref> presents the details of training epochs in comparison with other state-of- the-art character models for different languages. It shows that char-CNN and char-LSTM converge early whereas char-IntNet takes more epochs to converge and generally performs better. It alludes to the fact that IntNet is suitable for reducing over- fitting, since we have used early stopping while training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">State-of-the-art Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Rare and OOV Words Analysis</head><p>Another advantage of learning internal structure of words is that it can capture representations for out-of-vocabulary (OOV) words. To better un- derstand the behavior of IntNet, <ref type="table" target="#tab_7">Table 3</ref> presents error analysis on in-vocabulary words (IV), out- of-training-vocabulary words (OOTV), out-of- embedding-vocabulary words (OOEV), and out- of-both-vocabulary words (OOBV) compared to different character models. The result shows that our proposed model significantly outperforms other character models on OOV words includ- ing OOTV, OOEV, and OOBV. For example, in OOBV category, our IntNet outperforms other models by more than 3% in terms of F1 score for Dutch and German datasets.</p><p>Furthermore, we present comparisons of near- est neighbors with different models for frequent words, rare words, and OOV words. <ref type="table" target="#tab_8">Table 4</ref> shows the results of nearest neighbors for learn- ing word shape information, which gives insights on what kind of character-to-word representations can be learned by different models. For exam- ple, in OOV words, our IntNet model learns a bet- ter xx-month shape pattern when matching 11- month compared to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>In many situations, learning character-to-word representations of subword sequences that exceed the typical length of word shape pattern or mor- pheme sequences might result in noise. RNNs can capture longer sequences in theory, however, longer sequences do not guarantee better results when learning prefixes and suffixes. The funnel- shaped wide architecture of IntNet, uses different kernels with different levels of concatenation to capture patterns of different subword lengths and that is flexible than char-LSTM and char-CNN. For example, <ref type="table" target="#tab_8">Table 4</ref> shows T hursday in OOV words, our model learns a better word-shape struc- ture for character-to-word representations com- pared to other methods.</p><p>When considering training time, IntNet is only 20% slower than char-CNN for the whole training process. Also, learning word representations use fewer parameters than learning sentence represen- tations. Therefore, the impact of training speed for sequence labeling is limited. The inference time of IntNet is almost the same as char-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented empirical comparisons of differ- ent character embedding models for learning character-to-word representations and investigated the gaps between methods for learning repre- sentations of words and sentences. We con- ducted detailed experiments of different state-of- the-art convolutional models, and explored the ad- vantages and disadvantages of their components for learning word shape information. Further- more, we presented IntNet, a funnel-shaped wide convolutional neural architecture with no down- sampling that focuses on learning better inter- nal structure of words by composing their char- acters from limited supervised training corpora. Our in-depth analysis showed that a shallow wide architecture is better than a narrow deep archi- tecture for learning character-to-word representa- tions. Omitting down-sampling operations is use- ful for capturing the sequential patterns of pre- fixes and suffixes. Our proposed compositional character-to-word model does not leverage any ex- ternal resources, hand-crafted features, additional knowledge, joint training, or character-level pre- processing, and achieves new state-of-the-art per- formance for various sequence labeling tasks, in- cluding named entity recognition, part-of-speech tagging and syntactic chunking. In the future, we would like to explore using the IntNet model for other NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Sequence labeling.</head><label></label><figDesc>Collobert et al. (2011) first proposed a method based on CNN-CRF that learns important features from words and requires few hand-crafted features. Huang et al. (2015) pro- posed a bidirectional LSTM-CRF model by us- ing word embeddings and hand-crafted features for sequence tagging. Lample et al. (2016) applied the LSTM-based character embedding model from Ling et al. (2015) with bidirectional LSTM-CRF and obtained best results on NER for Spanish, Dutch, and German. Ma and Hovy (2016) ap- plied the CNN-based character embedding model from Chiu and Nichols (2016), but without us- ing any data preprocessing or external knowledge and achieved the best result on NER for English and part-of-speech tagging. Also, there have been some joint models which use additional knowl- edge, like transfer learning (Yang et al., 2017), pre-trained language models (Peters et al., 2017), language model joint training (Rei, 2017), and multi-task learning (Liu et al., 2018). Without any additional supervision or extra resources, LSTM- CRF (Lample et al., 2016) and LSTM-CNN-CRF (Ma and Hovy, 2016) are current state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The main architecture of IntNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training details of different models for English, German, Spanish, and Dutch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F1 score of our proposed models in comparison with state-of-the-art results. 

0 
20 
40 
60 

Epoch 

90.00 

90.25 

90.50 

90.75 

91.00 

91.25 

91.50 

91.75 

F1-percentage 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 3 : F1 score of different models for IV, OOTV, OOEV and OOBV.</head><label>3</label><figDesc></figDesc><table>Model 
Frequent Words 
Rare Words 
OOV Words 

char-LSTM 

newspapers 
slipped 
world 
Commerce 
youthf ul 
sessions 
11-month T hursdays undetermined 
enclosures 
stirred 
wolrd 
Committee 
luthier 
cessions 
19-month 
Thousands 
undereducated 
nelsonville 
clipped 
worde 
Computer 
loughmoe 
sensible 
10-month 
Tunbridge 
underpinned 
entrances 
snipped 
lowed 
Comments 
wrathful 
stepanos 
12-month 
Standings 
undermined 
newpapers 
striped 
wowed 
Corrects 
slothful 
stefanos 
14-month 
Torrance 
underlined 
necklaces 
stifled 
crowd 
Clippers 
ephorus 
constans 
11-inch 
Phillies 
underprepared 

char-CNN 

newspaper 
slipper 
worli 
Committee 
mouthful suppressions 31-month 
Thursday 
determined 
newspapermen 
slippy 
worle 
Community 
eeyou 
oppressions 51-month Wednesday overdetermined 
newpapers 
stripped 
worse 
Commodities mouthfeel digressions 
1-month 
Tuesday 
determinist 
nitrification 
shipped 
werle 
Communist 
motul 
confessions 21-month 
Ecuador 
determiners 
megaphones 
stopped 
wereld 
Comments 
yourself 
fissions 
41-month 
Windass 
determiner 

char-IntNet 

newpapers 
blipped 
eworld 
Commissioner mouthful 
recessions 
55-month 
Thursday 
undermined 
wallpapers 
unclipped 
offworld 
Commodities 
mirthful 
accessions 
51-month 
Saturday 
determined 
escapers 
tripped 
homeworld 
Clarence 
mouthfuls 
missions 
22-month 
thursdays 
overdetermined 
carcases 
dripped 
linuxworld 
Commission 
youths 
conversions 25-month 
Tuesday 
unexamined 
spacers 
slopped 
westworld 
Commons 
slothful 
possessions 12-month 
tuesdays 
predetermined 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Nearest neighbours of different models for frequent words, rare words and OOV words.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crosslingual character-level neural morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="748" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2016</title>
		<meeting>NAACL-HLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conll-2002</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="155" to="158" />
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2000 shared task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2000 and LLL-2000</title>
		<meeting>CoNLL-2000 and LLL-2000</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
