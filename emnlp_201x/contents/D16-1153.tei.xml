<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Bharadwaj</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mortensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1462" to="1472"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Named Entity Recognition is a well established information extraction task with many state of the art systems existing for a variety of languages. Most systems rely on language specific resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually. In this paper , we introduce an attentional neural model which only uses language universal phonolog-ical character representations with word em-beddings to achieve state of the art performance in a monolingual setting using supervision and which can quickly adapt to a new language with minimal or no data. We demonstrate that phonological character representations facilitate cross-lingual transfer, out-perform orthographic representations and incorporating both attention and phonological features improves statistical efficiency of the model in 0-shot and low data transfer settings with no task specific feature engineering in the source or target language.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) ( <ref type="bibr" target="#b28">Nadeau and Sekine, 2007;</ref><ref type="bibr" target="#b26">Marrero et al., 2013</ref>) is an informa- tion extraction task that deals with finding and clas- sifying entities in text into a fixed set of types of interest. It is challenging for a variety of reasons. Named Entities (NEs) can be arbitrarily synthesized (eg. people's/organization's names). NEs are often not subject to uniform cross-linguistic spelling con- ventions: compare France (English) and Frantsiya (Uzbek). NEs occur rarely in data which makes gen- eralization difficult. Skewed class statistics necessi- tate measures to prevent models from merely favor- ing a majority class.</p><p>Named entities must also be annotated in con- text (eg. " <ref type="bibr">[New York Times]</ref> ORG " vs. "[New York] LOC "). Lexical ambiguity (Turkey-country vs. bird), semantic ambiguity ("I work at the [New York Times] ORG " vs. "I read the New York Times") and sparsity induced by morphology add complex- ity.</p><p>Despite the challenges mentioned above, compe- tent monolingual systems that rely on having suffi- cient annotated data, knowledge and resources avail- able for engineering features have been developed. A more challenging task is to design a model that retains competence in monolingual scenarios and can easily be transferred to a low resource language with minimum overhead in terms of data annotation requirements and feature engineering. This trans- fer setting introduces additional challenges such as varying character usage conventions across lan- guages with same script, differing scripts, lack of NE transliteration, varying morphology, different lexicons and mutual non-intelligibility to name a few.</p><p>We propose the following changes over prior work (  to address the challenges of the low-resource transfer setting. We use:</p><p>1. Language universal phonological character representations instead of orthographic ones.</p><p>2. Attention over characters of a word while labeling it with an NE category. We show that using phonological character rep- resentations instead does not negatively impact per- formance on two languages: Spanish and Turkish. We then show that using global phonological repre- sentations enables model transfer from one or more source languages to a target language with no extra effort, even when the languages use different scripts. We demonstrate that while attention over characters of words has marginal utility in monolingual and high resource settings, it greatly improves the sta- tistical efficiency of the model in 0-shot and low resource transfer settings. We do require a map- ping from a language's script to phonological feature space which is script specific and not task specific. This presents little or no overhead due to existence of tools like PanPhon ( <ref type="bibr" target="#b24">Littell et al., 2016</ref>). <ref type="figure" target="#fig_0">Figure 1</ref> provides a high level overview of our model. We model the words of a sentence at the type level and the token level. At the type level (ig- norant of sentential context), we use bidirectional character LSTMs as in <ref type="figure" target="#fig_1">figure 2</ref> to compose charac- ters of a word to obtain its word representation and concatenate this with a word embedding that cap- tures distributional semantics. This can memorize entities or capture morphological and suffixal clues that can help at a discriminative task like NER. We compose type level word representations with bi- directional LSTMs to obtain token level (cognizant of sentential context) representations. Using token level word representations along with an attentional context vector for each word based on the sequence of characters it contains, we generate score functions used by a Conditional Random Field (CRF) for in- ference. To facilitate transfer across languages with different scripts, we use Epitran 1 and PanPhon <ref type="bibr" target="#b24">(Littell et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Epitran is a straightforward orthography-to-IPA (International Phonetic Alphabet [language univer- sal]) system including a collection of preprocessors and grapheme-to-phoneme mappings for a variety of language-script pairs. It converts a word from its na- tive script into a sequence of IPA characters, each of which approximately corresponds to a phoneme. PanPhon is a database of IPA-to-phonological fea- ture vector mappings and a library for querying, ma- nipulating, and exploiting this database. It consumes the output of Epitran and produces feature vectors (21 dimensions) of phonological characteristics such as whether a phoneme is articulated with (accom- panied by) vibration of the vocal folds (voiced), with the tongue in a high, low, back, or front po- sition, with the lips rounded or unrounded, with tongue tip or blade (coronal), etc. <ref type="figure" target="#fig_2">Figure 3</ref>   in Uyghur (Perso-Arabic script) and Turkish (Latin script), thus making the equivalence across scripts apparent. We concatenate the feature vectors from PanPhon and 1-hot encodings of the corresponding IPA characters and use these as inputs to the charac- ter bi-LSTMs.</p><p>This shift to IPA space is motivated by prior work ( <ref type="bibr" target="#b32">Tsvetkov et al., 2015;</ref><ref type="bibr" target="#b32">Tsvetkov and Dyer, 2015)</ref> which demonstrated the value of projecting ortho- graphic surface forms of words into a phonologi- cal space for detecting loan words, transliteration and cognates even in language pairs that exhibit sig- nificant typological, morphological and phonologi- cal differences. Our underlying assumption is that named entities are likely to be transliterated or re- tain pronunciation patterns across languages. Addi- tionally, phenomena such as vowel harmony mani- fest explicitly in IPA representation and can poten- tially be helpful for NER. Foreign named entities for example, need not obey vowel harmony rules preva- lent in languages like Turkish. A powerful sequence model such as a LSTM could be tolerant to the noise created by lexical aberrations, lack of spelling con- ventions, vowel raising etc. when given a phonolog- ical representation of an NE in different languages.</p><p>Our second proposed change is to incorporate at- tention over the sequence of IPA segments in a word when predicting scores for its possible labels. Atten- tion is an unsupervised alternative to convolution or feature engineering to capture helpful localized phe- nomena like capitalization of first letter, presence of case markers, special characters, helpful morpho- logical suffixes etc. or the conjunction of multiple such phenomena. Such features have been the main- stay of most prior work for NER. Most of these fea- tures are subtle and occur at the type level, whereas the CRF performs inference at the token level. We show (empirically) that attention makes the CRF more sensitive to such useful type level phenom- ena during inference and improves the statistical ef- ficiency of the model in certain scenarios. Having described our intuitions, we now provide mathemat- ical details of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">LSTM</head><p>Long Short Term Memory (LSTM) <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997</ref>) belongs to a special breed of neural networks called Recurrent Neural Net- works (RNNs) which are capable of processing se- quential input of unbounded and arbitrary length. This makes them suitable for a sequence labeling task. LSTMs incorporate gating functions at each time step to allow the network to forget, remember and update contextual memory and mitigate prob- lems like vanishing gradient. We use the following implementation:</p><formula xml:id="formula_0">i t =σ (W xi x t + W hi h t−1 + W ci c t−1 + b i ) c t = (1 − i t ) c t−1 + i t tanh(W xc x t + W hc h t−1 + b c ) o t =σ (W xo x t + W ho h t−1 + W co c t + b o ) h t =o t tanh(c t )</formula><p>where indicates element-wise product and σ indi- cates element-wise sigmoid function.</p><p>In practice we use bi-directional LSTMs (each with its own parameters) to mitigate cases where the LSTM may be biased towards the last few inputs it reads. This is done both at the word-level and the character level. Let the hidden state at time step t of the forward LSTM be denoted by − → h t and the cor- responding state of the backward LSTM be denoted by ← − h t . At the character level, for a word with L characters, we only take the final hidden states in</p><formula xml:id="formula_1">each direction i.e. [ − → h L ; ← − h 0 ]</formula><p>and concatenate them to get a representation of the word that comprises these characters. At the word level, we concatenate corresponding forward and backward LSTM states for each word</p><formula xml:id="formula_2">X t to get [ − → h t ; ← − h t ]</formula><p>which is the token level representation for the t th word in a sentence X. We use this to generate un-normalized energy/score functions for the CRF layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention</head><formula xml:id="formula_3">Let w t = [ − → h t ; ← − h t ]</formula><p>indicate the concatenated word bi- LSTM output (of dimension d 1 ) at step t correspond- ing to the t th word (X t ) in the input sequence X. Let M t be the matrix containing the concatenated bi- directional character LSTM outputs for each charac- ter of X t . It has dimensions (l t , d 2 ) where d 2 is the dimension of the concatenated bi-directional charac- ter LSTM hidden states and l t is the number of char- acters in X t . Let m i t denote the i th row of M t . Let P be a parameter matrix of dimension (d 1 , d 2 ) and p be a bias vector of length d 2 . We follow ( ) in the formulation of attention context vector a t :</p><formula xml:id="formula_4">w t = tanh(w t · P + p) α i = exp(w t · m i t ) lt j=1 exp(w t · m j t ) a t = lt i=1 (α i * m i t )</formula><p>The attentional context vector a t is then appended to w t to obtain concatenated vector u t = [a t ; w t ]. We apply a linear transform U (matrix of dimension</p><formula xml:id="formula_5">(d 1 + d 2 , k)</formula><p>where k is the number of unique NER tags). This gives us:</p><formula xml:id="formula_6">e t = u t · U (1)</formula><p>where e t is a vector of un-normalized energy/score functions indicating the compatibility between word X t and each of the k possible NER labels it can be given. Note that each word has a separate attention context vector obtained using the character LSTM hidden states generated by its constituent characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Conditional Random Field</head><p>Unlike Hidden Markov Models, CRFs do not en- force any independence assumptions among ob- served data and directly model the likelihood of a la- beling hypothesis discriminatively. They also model adjacency compatibility between NER tags which can capture strong constraints like an 'I-label' tag not following an O tag without a 'B-label' tag in be- tween them (see section 2.6). In our model, the CRF is parametrized as follows: For a word sequence X = (x 1 , x 2 , ...x N ), let E be a matrix of dimension (k,N) where k is the number of unique NER tags and N is the sequence length. The t th column is the vector e t obtained in equation 1. Let T be the square transition matrix of size (k+2, k+2) which captures transition score between the k NER tags, the start and the end symbols. Let Y = (y 1 , y 2 , ...y N ) be the label sequence associated with the input word sequence, each y i being an index into the ordered set of unique NER tags. Let y 0 be the start symbol and y N +1 be the end symbol. The score of this sequence is evaluated as:</p><formula xml:id="formula_7">S(X, Y ) = N i=1 E y i ,i + N j=0 T y j ,y j+1</formula><p>Let Y X indicate the exponential space of all possi- ble labelings of this sequence X. The partition func- tion associated with this CRF is then evaluated as:</p><formula xml:id="formula_8">Z = Y ∈Y X e S(X,Y )</formula><p>The probability of a specific labeling Y ∈ Y X :</p><formula xml:id="formula_9">P (Y |X) = e S(X,Y ) Z</formula><p>The training objective is to maximize conditional log probability of the correct labeling sequence Y * :</p><formula xml:id="formula_10">log(P (Y * |X)) = S(X, Y * ) − log (Z) (2)</formula><p>The decoding criteria for an input sequence X is:</p><formula xml:id="formula_11">Y * = arg max Y ∈Y X S(X, Y )<label>(3)</label></formula><p>Normally, evaluating the partition function over the exponential space of all possible labelings would be intractable. However, as described in ( <ref type="bibr" target="#b19">Lafferty et al., 2001</ref>), this can be done efficiently for linear chain CRFs using the forward backward algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Word Representations</head><p>The inputs to our model are in the form of type level word representations (figure 2). Motivated by the distributional hypothesis <ref type="bibr" target="#b14">(Harris, 1954;</ref><ref type="bibr" target="#b11">Firth, 1957)</ref> we use word embeddings as inputs. In the monolin- gual scenario, we use structured skipgram word em- beddings ( <ref type="bibr" target="#b22">Ling et al., 2015a</ref>). For the transfer sce- nario, embeddings can optionally be trained using techniques like multi CCA described in <ref type="bibr" target="#b0">(Ammar et al., 2016)</ref>. By learning a linear transformation from a shared vector space between languages, the model may acquire some transfer capability to the target language.</p><p>We use character bi-LSTMs to handle the Out Of Vocabulary (OOV) problem as in ( <ref type="bibr" target="#b23">Ling et al., 2015b</ref>). However, just as a distributional hypothe- sis exists for words, prior work ( <ref type="bibr" target="#b32">Tsvetkov and Dyer, 2015;</ref><ref type="bibr" target="#b32">Tsvetkov et al., 2015</ref>) suggests phonological character representations capture inherent similari- ties between characters that are not apparent from orthogonal one-hot orthographic character represen- tations and can serve as a language universal surro- gate for character representations. This is also use- ful for multi-lingual named entity recognition as ex- plained in section 2. Therefore we make use of Epi- tran and PanPhon as in <ref type="figure" target="#fig_2">figure 3</ref> to obtain both 1- hot IPA character encodings and phonological fea- ture vectors capturing similarity between IPA char- acters. These form the inputs to the character bi- LSTMs. The mapping from orthographic charac- ter segments to IPA is sometimes many to one (am- biguous) and unarticulated characters (like periods in NE abbreviations) and capitalization information is lost by default. Given the importance of such or- thographic features for NER and the ambiguity in- troduced, a drop in performance may be expected by using phonological character representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training</head><p>Our model parametrization is similar to ( . The weights to be trained include the the CRF transition matrix T, the projection parameters are used to generate matrix E (P and U), the LSTM parameters and word and character embedding ma- trices. The objective is to maximize the log prob- ability of the correct labeling sequence as given in equation 2. This objective is fully differentiable and standard back-propagation is used to learn weights. We use Stochastic Gradient Descent with a learning rate of 0.05 and gradients clipped at 5.0 providing best performance. Using Adadelta or Adam leads to faster convergence but slightly worse performance.</p><p>Word level LSTMs use a hidden layer size of 100, orthographic character LSTMs (if used) used a hid- den layer of size 25 while phonological character LSTMs used a hidden layer of size 15 due to the smaller phonetic alphabet. Tuning these did not have a major effect on performance. Dropout of 0.5 is applied after concatenation of the word embeddings and character LSTM outputs. Best dropout value was chosen through ablation studies. For Spanish, we use word embeddings pre-trained on the Span- ish Gigaword version 3. For transfer experiments, we use multilingual word embeddings trained using multi CCA described in <ref type="figure" target="#fig_0">(Ammar et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Entity Types and Tagging Schemes</head><p>In all of the datasets in table 1, 4 entity types are annotated:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Persons (PER)</head><p>Real Names of entities with organization and man- agerial structure. E.g. Democratic Party, Google, JetBlue, etc.</p><p>A BIO tagging scheme is used for all annotations. All non-entity tokens are annotated as 'O'. The first token of an entity span is annotated as 'B-label' and all remaining tokens in the entity span are annotated as 'I-label'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct four different experiments:</p><p>1. CoNLL 2002 Spanish NER (Sang., 2002) task.</p><p>This demonstrates the monolingual compe- tence of phonological character representations vs. orthographic representations.</p><p>2. Turkish NER using the Linguistic Data Con- sortium's LDC2014E115 BOLT Turkish Lan- guage Pack 2 . This demonstrates the utility of phonological character representations and at- tention in a morphologically rich, low resource language. We compare against a state-of-the- art monolingual model ( ) that uses orthographic character LSTMs. 3. Transfer Experiments from Uzbek to Turkish using LDC2014E112 BOLT 3 data pack for Uzbek and LDC2014E115 BOLT data pack for Turkish. These two languages have sim- ilar syntax and word order (Dryer, 2013) but vary significantly in morphology, vowel har- mony and phonology, can use different scripts (Uzbek-Latin/Cyrilic, Turkish-Latin) and are not mutually intelligible. <ref type="table" target="#tab_3">Tables 2 and 3</ref> report results from monolingual ex- periments in Spanish. In table 3, we report the per- formance of our best model against other state-of- the-art models for the Spanish <ref type="bibr">CoNLL 2002</ref><ref type="bibr">NER task (Sang., 2002</ref>). Our model performs marginally better than other benchmarks with the optimal con- figuration of hyper-parameters and using pre-trained word embeddings. <ref type="table" target="#tab_3">Table 2</ref> report ablation study re- sults, which reveal that using pre-trained word em- beddings without using character LSTMs yields a very strong baseline that already out-performs var- ious previous benchmarks. Using character LSTMs that compose orthographic character representations yields a +0. <ref type="bibr">91</ref>       <ref type="formula">(2016)</ref> 37.1 Our best transfer model* 51.2 <ref type="table">Table 6</ref>: NIST evaluations for Uyghur. * indicates transfer from Uzbek and Turkish +0.12 F1 with attention. Using phonological charac- ter representations instead yields an improvement of +0.47 F1 and a further +0.8 F1 with attention. Thus, phonological representations benefit more from at- tention applied over them to beat out orthographic representations in that scenario. Using sparse fea- tures indicating the character category (alphabet vs. number vs. punctuation/non-phonetic) and capital- ization in conjunction with with phonological char- acter representations and word embeddings with at- tention over phonological characters yields the best configuration that slightly outperforms the best pub- lished models so far. Given that many previous benchmarks used features that rely heavily on or- thography, this is an encouraging result since one would expect to lose some performance by using more abstract phonological representations as ex- plained in section 2. <ref type="bibr">4</ref>. <ref type="table" target="#tab_5">Tables 4 and 5</ref> highlight results from monolingual experiments on Turkish. This dataset is much more challenging since the annotated training courpus is significantly smaller than the CoNLL 2002 shared task dataset and because Turkish is an agglutinative language exhibiting sparsity inducing morphology which leads to huge vocabulary size. As a compet- itive baseline, we train the LSTM CRF described in ( ) due to its documented abil- ity to obtain state-of-the-art monolingual results for many languages with minimal feature engineering. Our best model from the Turkish ablation study out- performs this baseline. We also see a stark contrast between the ablation study results for Turkish com- pared to Spanish. Firstly, word embeddings alone perform rather poorly due to the challenges of reli- ably estimating them for a large vocabulary given a small dataset. Character composed representations of words provide a significant performance boost (+17.27 F1 for the best model). Secondly, usage of sparse character features (like capitalization) seems to hurt performance in all but the last model in table 4. Thirdly, phonological and orthographic charac- ter representations are complementary in the case of Turkish, unlike Spanish. This is not too surprising since Turkish exhibits phonological phenomena like vowel harmony. Lack of vowel harmony in a word could give-away a foreign word or a named entity for example. Results show that attention is helpful as well. We would also like to point out that the only models in the ablation studies eligible for trans- fer are those that do not use orthographic character representations. Among these, the model that uses phonological representation with attention and word vectors performs the best and also outperforms the baseline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transfer Experiments from Uzbek and Turk- ish into Uyghur using LDC2014E112 and LDC2014E115 BOLT data pack for Uzbek and Turkish respectively and Uyghur data provided as part of DARPA LORELEI 4 . These languages all have different scripts, lexicons, morphology and phonology. Results are reported by NIST 5 on an unseen test set.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Our next experiments on model transfer are ar- guably the most interesting. <ref type="table" target="#tab_8">Tables 7 and 8</ref> docu- ment single source (Uzbek to Turkish) transfer re- sults. We find that using sparse character category and capitalization features in conjunction with atten- tion and phonological features yields the best 0-shot transfer performance (no training labels in the tar- get language). Specifically, attention provides +6 F1 in 0-shot and 5% labeled-target language data scenarios. It is interesting to note that using mul- tilingual word embeddings for the source and tar- get languages alone accounts for very poor trans- fer. We also find that with as little as 20% of the data, we approach the performance of a monolingual target model that was trained on all the data. We also notice that the transfer models retain a consis- tent advantage over a monolingually trained target language model across all data availability scenar- ios. Lastly, we note that while attention provides a significant improvement in 0-shot and 5% data availability scenarios, a model without attention (or sparse features like capitalization) eventually does better with more data. This indicates that the model is competent enough to leverage transfer via phonol- ogy alone. This could also possibly be because at- tention patterns from Uzbek could bring in a bias that is eventually sub-optimal for Turkish due to dif-   ferent morphology and phonology. In future work, we shall perform more insightful error analysis to explain these trends. <ref type="table">Table 6</ref> documents NIST evaluation results on an unseen Uyghur test set (with gold annotations) for the best transfer model configuration jointly trained on Turkish and Uzbek gold annotations and Uyghur training annotations produced by a non-speaker lin- guist (non-gold). Since Uyghur lacks helpful type- level orthographic features such as capitalization, our transfer model in table 6 does not use any sparse features or attention but benefits from transfer via the phonological character representations we've proposed. Despite the noisy supervision provided in the target language, transferring from Turkish and Uzbek provides a +14.1 F1 improvement over a state of the art monolingual model trained on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prior Work</head><p>NER is a well-studied sequence-labeling problem for which many different approaches have been pro- posed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models ( <ref type="bibr" target="#b5">Chieu and Ng, 2003)</ref>, hierarchically smoothed tries <ref type="bibr" target="#b8">(Cucerzan and Yarowsky, 1999</ref>), decision trees ( <ref type="bibr" target="#b3">Carreras et al., 2002</ref>) and models incorporating syntactic, semantic and world knowledge ( <ref type="bibr" target="#b34">Wakao et al., 1996</ref>). Each of these models brings in a bias of its own. <ref type="bibr" target="#b12">Florian et al. (2003)</ref> successfully tried ensembling multiple classifiers and improved performance.</p><p>Since NER is a sequence labeling problem, there are local dependencies both among NE labels as- sociated with words and among the words them- selves, that could aid the labeling process. To explic- itly deal with these sequential characteristics, Hid- den Markov Models (HMMs) and Conditional Ran- dom Fields (CRFs) became very popular. ( <ref type="bibr" target="#b18">Klein et al., 2003;</ref><ref type="bibr" target="#b12">Florian et al., 2003;</ref><ref type="bibr" target="#b27">McCallum and Li, 2003;</ref><ref type="bibr" target="#b30">Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b4">Chandra et al., 1981;</ref><ref type="bibr" target="#b21">Lin and Wu, 2009;</ref><ref type="bibr" target="#b35">Yang et al., 2016;</ref><ref type="bibr" target="#b25">Ma and Hovy, 2016)</ref>. CRFs even- tually became more popular because they are dis- criminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem.</p><p>Most of the work cited so far makes use of hand engineered features. The following approaches min- imize the use of features while still maintaining a monolingual focus. The most recent work eliminates feature engi- neering altogether and combines CRFs with LSTMs which can model long sequences while remember- ing appropriate past context.  proposed an architecture that uses both character and word level LSTMs to produce score functions for CRF inference conditioned on global context. <ref type="bibr" target="#b25">Ma and Hovy (2016)</ref> replace the character LSTMs of  with a CNN instead. <ref type="bibr" target="#b35">Yang et al. (2016)</ref> follow a very similar architecture to , replacing the LSTMs with Gated Recurrent Units ( ). However, <ref type="bibr" target="#b35">Yang et al. (2016)</ref> also tackle multi task and multi-lingual joint training scenarios.</p><p>Most of the models cited so far are monolin- gual either because they use hand crafted features and language specific resources or because of deep- seated assumptions. For example a change in or- thography, lexicon, script, word order or addition of complex morphology makes transfer impossi- ble. This is the central challenge that we tackle. There has been much less work catering to this sce- nario. <ref type="bibr" target="#b17">Kim et al. (2012)</ref> use weak annotations from Wikipedia metadata and parallel data for multi lin- gual NER. <ref type="bibr" target="#b35">Yang et al. (2016)</ref> addresses the use case of multilingual joint training, which assumes there is sufficient data available in all languages. <ref type="bibr" target="#b29">Nothman et al. (2013)</ref> also operate under the assumption of availability of Wikipedia data.</p><p>To the best of our knowledge, a scenario involving transfer of a model trained in one (or more) source language(s) to another language with little or no la- beled data, different script, different morphology, different lexicon, lack of transliteration, non-mutual intelligibility etc. has not been addressed recently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented two improvements over a state-of-the-art monolingual model to address Named Entity Recognition in transfer settings. The first seeks to reconcile various dimensions of vari- ability between languages such as varying script, orthographic conventions, phonological phenomena etc. by representing words as sequences of IPA (In- ternational Phonetic Alphabet) segments consistent across all languages, rather than sequences of char- acters specific to a particular language. Secondly, we exploit the one-to-one mapping between input sequence words and output labels and advocate for the use of attention over the character/IPA sequence of a word when predicting its label. We show em- pirically that these two improvements 1) achieve at least state-of-the-art performance on a monolin- gual NER task in Spanish, 2) handle complex mor- phology in languages such as Turkish, Uzbek and Uyghur better than state of the art, 3) provide 0-shot performance in a transfer scenario to a related new language, well above that possible using multilin- gual word embeddings alone, and 4) are capable of generalizing to a new language with much less train- ing data than a monolingually trained model. More- over, all of this is achieved without any extra feature engineering specific to the task or language, apart from mapping characters in that language to IPA. We believe these results to be encouraging and look for- ward to replicating these results on more language pairs in different language families and further au- tomating the process of obtaining phonological char- acter representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This work is sponsored by Defense Advanced Re- search Projects Agency Information Innovation Of- fice (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C- 0114. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Govern- ment. The U.S. Government is authorized to re- produce and distribute reprints for Government pur- poses notwithstanding any copyright notation here on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attentional LSTM-CRF architecture. li denotes the encoding of a word and its left context (forward LSTM) while ri includes only right context (backward LSTM). Inputs to word LSTMs are obtained using character LSTMs and word-embeddings. ai denotes an attentional context vector concatenated with li and ri.</figDesc><graphic url="image-1.png" coords="2,76.68,64.31,229.32,177.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Type level word representations-l denotes a word prefix encoding (by forward char LSTM) while r denotes a word suffix encoding (by backward char LSTM).</figDesc><graphic url="image-2.png" coords="2,313.06,64.31,222.26,184.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Use of Epitran and PanPhon to enable transfer across orthographies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Collobert et al. (2011), Turian et al. (2010), and Ando and Zhang (2005) use unsuper- vised features in conjunction with engineered fea- tures capturing capitalization, character categories and gazetteer matches. Collobert et al. (2011) use a Convolutional Neural Network (CNN) over the se- quence of word embeddings. Huang et al. (2015) instead use bi-directional LSTMs over the sequence of words, along with spelling and orthographic fea- tures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>depicts the sequence of operations applied to the same NE orthographic representation</figDesc><table>Epitran 

IPA 
representation 

PanPhon 

feature 
vector 
representation 

&gt; 
� 
� 
� 
� 
� 
‫ڭ‬ 
&lt; 

/ʃind͡ ʒaŋ/ 

PanPhon 

feature 
vector 
representation 

orthographic 
representation 

Epitran 

IPA 
representation 

&lt;Şincan&gt; 

/ʃind͡ ʒan/ 

TURKISH 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Ablation Tests on Spanish CoNLL 2002. Bold indi-

cates the best model. 

Model 
F1 
Carreras et al. (2002)* 
81.39 
dos Santos et al. (2015) 82.21 
Gillick et al. (2015) 
81.83 
Gillick et al. (2015)* 
82.95 
Lample et al. (2016) 
85.75 
Yang et al. (2016) 
85.77 
Our Best 
85.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison with benchmarks. * indicates a model 

that uses external labeled data 

Phono 
Chars 

Ortho 
Chars 

Word 
vecs 

Cap+ 
Cat 

Ortho 
Attn 

Phono 
Attn 
F1 

No 
No 
Yes 
No 
No 
No 
49.2 
No 
Yes 
Yes 
No 
No 
No 
65.41 
No 
Yes 
Yes 
No 
Yes 
No 
64.76 
No 
Yes 
Yes 
Yes 
No 
No 
60.57 
No 
Yes 
Yes 
Yes 
Yes 
No 
60.87 
Yes 
No 
Yes 
No 
No 
No 
63.04 
Yes 
No 
Yes 
No 
No 
Yes 
66.07 
Yes 
No 
Yes 
Yes 
No 
No 
59.08 
Yes 
No 
Yes 
Yes 
No 
Yes 
62.46 
Yes 
Yes 
Yes 
No 
No 
Yes 
63.43 
Yes 
Yes 
Yes 
Yes 
No 
No 
63.46 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
66.47 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Ablation Tests on Turkish Bold indicates the best</head><label>4</label><figDesc></figDesc><table>transfer eligible (66.07) and transfer ineligible models (66.47) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with state-of-the-art monolingual Turkish 

model 

Model 
F1 
Lample et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Model Transfer from Uzbek (Source) to Turkish (Target) at different target data availability thresholds 

Model 
0-shot 
5% 
data 

20% 
data 

40% 
data 

60% 
data 

80% 
data 

All 
data 
LSTM-CRF (Lample et al., 2016) 0 
33.44 50.61 53.25 57.41 60 
61.11 
S-LSTM (Lample et al., 2016) 
0 
15.41 39.33 42.99 51.92 51.55 56.58 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 : Monolingual Turkish baseline at different data availability thresholds</head><label>8</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/dmort27/epitran</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly 1470 learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity extraction using adaboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 6th conference on Natural language learning</title>
		<meeting>the 6th conference on Natural language learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note>pages 1-4. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexter</forename><forename type="middle">C</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">J</forename><surname>Kozen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alternation. Journal of the Association for Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="133" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Named entity recognition with a maximum entropy approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Leong Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<editor>Walter Daelemans and Miles Osborne</editor>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language independent named entity recognition combining morphological and contextual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC</title>
		<meeting>the 1999 Joint SIGDAT Conference on EMNLP and VLC</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting named entity recognition with neural character embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Cıcero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Niterói</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janeiro</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NEWS 2015 The Fifth Named Entities Workshop</title>
		<meeting>NEWS 2015 The Fifth Named Entities Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Order of Subject, Object and Verb. Max Planck Institute for Evolutionary Anthropology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Leipzig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rupert Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Linguistic Analysis</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Philological Societ</publisher>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<editor>Walter Daelemans and Miles Osborne</editor>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00103</idno>
		<title level="m">Oriol Vinyals, and Amarnag Subramanya. 2015. Multilingual language processing from bytes</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual named entity recognition using parallel data and metadata from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="694" to="702" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named entity recognition with character-level models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Smarr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<editor>Walter Daelemans and Miles Osborne</editor>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="180" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Phrase clustering for discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<title level="m">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bridge-language capitalization inference in western iranian: Sorani, kurmanji, zazaki, and tajik</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC16)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Named entity recognition: fallacies, 1471 challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mónica</forename><surname>Marrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julián</forename><surname>Urbano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Sánchezcuadrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Morato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Miguel</forename><surname>Gómezberbís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Standards &amp; Interfaces</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="482" to="489" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A survey of named entity recognition and classification. Lingvisticae Investigationes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning multilingual named entity recognition from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Introduction to the conll2002 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-lingual bridges with models of lexical borrowing. JAIR. Yulia Tsvetkov, Waleed Ammar, and Chris Dyer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constraint-based models of lexical borrowing. NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of an algorithm for the recognition and classification of proper names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Wakao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="418" to="423" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
