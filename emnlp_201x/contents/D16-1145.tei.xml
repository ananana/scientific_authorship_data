<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining Inference Formulas by Goal-Directed Random Walks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mining Inference Formulas by Goal-Directed Random Walks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1379" to="1388"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually. Data-driven methods have been proposed to mine formulas from KBs automatically , where random sampling and approximate calculation are common techniques to handle big data. Among a series of methods , Random Walk is believed to be suitable for knowledge graph data. However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference. Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas. To this end, we propose a novel goal-directed inference formula mining algorithm, which directs random walks by the specific inference target at each step. The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously. The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, various knowledge bases (KBs), such as Freebase ( <ref type="bibr" target="#b2">Bollacker et al., 2008)</ref>, WordNet <ref type="bibr" target="#b14">(Miller, 1995)</ref>, <ref type="bibr">Yago (Hoffart et al., 2013)</ref>, have been built, and researchers begin to explore how to make use of structural information to promote performances of several inference-based NLP applications, such as text entailment, knowledge base completion, ques- tion and answering. Creating useful formulas is one of the most important steps in inference, and an ac- curate and high coverage formula set will bring a great promotion for an inference system. For ex- ample, Nationality(x, y) ∧ Nationality(z, y) ∧ Lan- guage(z, w) ⇒ Language(x, w) is a high-quality for- mula, which means people with the same nationality probably speak the same language. However, it is a challenge to create formulas for open-domain KBs, where there are a great variety of relation types and it is impossible to construct a complete formula set by hand.</p><p>Several data-driven methods, such as Induc- tive Logic Programming (ILP) <ref type="bibr" target="#b15">(Muggleton and De Raedt, 1994)</ref> and Markov Logic Network (MLN) ( <ref type="bibr" target="#b18">Richardson and Domingos, 2006</ref>), have been pro- posed to mine formulas automatically from KB da- ta, which transform frequent sub-structures of KBs, e.g., paths or loops, into formulas. <ref type="figure" target="#fig_0">Figure 1</ref>.a shows a sub-graph extracted from Freebase, and the for- mula mentioned above about Language can be gen- erated from the loop in <ref type="figure" target="#fig_0">Figure 1</ref>.d. However, the running time of these traditional probabilistic infer- ence methods is unbearable over large-scale KBs. For example, MLN needs grounding for each can- didate formula, i.e., it needs to enumerate all paths. Therefore, the computation complexity of MLN in- creases exponentially with the scale of a KB.</p><p>In order to handle large-scale KBs, the random walk is usually employed to replace enumerating al- l possible sub-structures. However, random walk is inefficient to find useful structures due to its com- pletely randomized mechanism. For example in Fig- ure 1.b, the target path (yellow one) has a small probability to be visited, the reason is that the algo- rithm may select all the neighboring entity to trans- fer with an equal probability. This phenomenon is very common in KBs, e.g., each entity in Freebase has more than 30 neighbors in average, so there will be about 810,000 paths with length 4, and only sev- eral are useful. There have been two types of meth- ods proposed to improve the efficiency of random walks, but they still meet serious problems, respec- tively. 1) Increasing rounds of random walks. More rounds of random walks will find more structures, but it will simultaneously introduce more noise and thus generate more false formulas. For example, the loop in <ref type="figure" target="#fig_0">Figure 1</ref>.c exists in Freebase, but it produces a false formula, Gender(x, y) ∧ Gender(z, y) ∧ Lan- guage(z, w) ⇒ Language(x, w), which means people with the same gender speak the same language. This kind of structures frequently occur in KBs even the formulas are mined with a high confidence, because there are a lot of sparse structures in KBs which will lead to inaccurate confidence. According to our s- tatistics, more than 90 percent of high-confidence formulas produced by random walk are noise. 2) Employing heuristic rules to direct random walks. This method directs random walks to find useful structures by rewriting the state transition probability matrix, but the artificial heuristic rules may only apply to a little part of formulas. For example, PRA <ref type="bibr" target="#b9">(Lao and Cohen, 2010;</ref><ref type="bibr" target="#b10">Lao et al., 2011</ref>) assumes the more narrow distributions of el- ements in a formula are, the higher score the for- mula will obtain. However, formulas with high s- cores in PRA are not always true. For example, the formula in <ref type="figure" target="#fig_0">Figure 1</ref>.c has a high score in PRA, but it is not true. Oppositely, formulas with low scores in PRA are not always useless. For exam- ple, the formula, F ather(x, y) ∧ F ather(y, z) ⇒ Grandf ather(x, t), has a low score when x and y both have several sons, but it obviously is the most effective to infer Grandf ather. According to our investigations, the situations are common in KBs.</p><p>In this paper, we propose a Goal-directed Ran- dom Walk algorithm to resolve the above problem- s. The algorithm employs the specific inference tar- get as the direction at each step in the random walk process. In more detail, to achieve such a goal- directed mechanism, at each step of random walk, the algorithm dynamically estimates the potentials for each neighbor by using the ultimate goal, and as- signs higher probabilities to the neighbors with high- er potentials. Therefore, the algorithm is more in- clined to visit structures which are beneficial to infer the target and avoid transferring to noise structures. For example in <ref type="figure" target="#fig_0">Figure 1</ref>, when the inference tar- get is what language a person speaks, the algorith- m is more inclined to walk along Nationality edge than Gender, because Nationality has greater poten- tial than Gender to infer Language. We build a re- al potential function based on low-rank distribution- al representations. The reason of replacing symbols by distributional representations is that the distribu- tional representations have less parameters and la- tent semantic relationship in them can contribute to estimate potentials more precisely. In summary, the contributions of this paper are as follows.</p><p>• Compared with the basic random walk, our ap- proach direct random walks by the inference target, which increases efficiency of mining useful formu- las and has a great capability of resisting noise.</p><p>• Compared with the heuristic methods, our ap- proach can learn the strategy of random walk au- tomatically and dynamically adjust the strategy for different inference targets, while the heuristic meth- ods need to write heuristic rules by hand and follow the same rule all the time.</p><p>• The experiments on link prediction task prove that our approach has a high efficiency on mining formu- las and has a good performance on both WN18 and FB15K datasets.</p><p>The rest of this paper is structured as follows, Sec- tion 2 introduces the basic random walk for mining formulas. Section 3 describes our approach in detail. The experimental results and related discussions are shown in Section 4. Section 5 introduces some relat- ed works, and finally, Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mining Formulas by Random Walk</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Frequent Pattern Mining</head><p>Mining frequent patterns from source data is a prob- lem that has a long history, and for different spe- cific tasks, there are different types of source data and different definitions of pattern. Mining formulas is more like frequent subgraph mining, which em- ploys paths or loops as frequent patterns and mines them from a KB. For each relation type R, the al- gorithm enumerates paths from entity H to entity T for each triplet R(H, T ). These paths are nor- malized to formulas by replacing entities to vari- ables. For example, the loop in <ref type="figure" target="#fig_0">Figure 1</ref>.d, National- ity(Bob, America) ∧ Nationality(Stewart, America) ∧ Language(Bob, English) ⇒ Language(Stewart, English), can be normalized to the formula, Nation- ality(x, y) ∧ Nationality(z, y) ∧ Language(z, w) ⇒ Language(x, w). Support and confidence are em- ployed to estimate a formula, where the support val- ue of a formula f : X ⇒ Y , noted as S f , is defined as the proportion of paths in the KB which contains the body X, and the confidence value of X ⇒ Y , noted as C f , is defined as the proportion of the paths that contains X which also meets X ⇒ Y . C f is calculated as follows,</p><formula xml:id="formula_0">C f = N f N X (1)</formula><p>where N f is the total number of instantiated formula f and N X is the total number of instantiated X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Random Walk on Knowledge Graph</head><p>Enumerating paths is a time consuming process and does not apply to large-scale KBs. Therefore, ran- dom walk on the graph is proposed to collect fre- quent paths instead of enumerating. Random walk randomly chooses a neighbor to jump unlike enu- merating which needs to search all neighbors. To es- timate a formula f , the algorithm employs f 's occur- rence number during random walks N f to approxi- mate the total number N f in Equation (1), and sim- ilarly employs N X to approximate N X . Therefore, f 's confidence C f can be approximatively estimated by N f and N X , noted as C f . Random walk maintains a state transition prob- ability matrix P , and P ij means the probability of jumping from entity i to entity j. To make the confi- dence C f as close to the true confidence C f as pos- sible, the algorithm sets P as follows,</p><formula xml:id="formula_1">P ij = 1/d i , j ∈ Adj(i) 0, j / ∈ Adj(i)<label>(2)</label></formula><p>where d i is the out-degree of the entity i, Adj(i) is the set of adjacent entities of i, and N j=1 P ij = 1. Such a transition matrix means the algorithm may jump to all the neighboring entities with an equal probability. Such a random walk is independen- t from the inference target, so we call this type of random walk as a goalless random walk. The goal- less mechanism causes the inefficiency of mining useful structures. When we want to mine paths for R(H, T ), the algorithm cannot arrive at T from H in the majority of rounds. Even though the algorith- m recalls several paths for R(H, T ), some of them may generate noisy formulas for inferring R(H, T ).</p><p>To solve the above problem, several methods di- rect random walks by statically modifying P . For example, PRA sets P r ij = P (j|i;r) |R i | , P (j|i; r) = r(i,j) r(i, * ) , where P (j|i; r) is the probability of reach- ing node j from node i under the specific relation r, r(i, * ) is the number of edges from i under r, and R i is the number of relation types from i. Such a transition matrix implies the more narrow distribu- tions of elements in a formula are, the higher score the formula will obtain, which can be viewed as the heuristic rule of PRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Goal-Directed Random Walk</head><p>We propose to use the inference target, ρ = R(H, T ), to direct random walks. When predict- ing ρ, our approach always directs random walks to find useful structures which may generate formulas to infer ρ. For different ρ, random walks are direct- ed by modifying the transition matrix P in differ- ent ways. Our approach dynamically calculates P r ij when jumping from entity i to entity j under relation r as follows,</p><formula xml:id="formula_2">P r ij =      Φ(r(i, j), ρ) k∈Adj(i) Φ(r(i, k), ρ) , j ∈ Adj(i) 0, j / ∈ Adj(i)<label>(3)</label></formula><p>where Φ(r(i, j), ρ) is the r(i, j)'s potential which measures the potential contribution to infer ρ after walking to j.</p><p>Intuitively, if r(i, j) exits in a path from H to T and this path can generate a benefic formula to in- fer R(H, T ), the probability of jumping from i to j should larger and thus Φ(r(i, j), ρ) also should be larger. Reversely, if we cannot arrive at T within the maximal steps after jumping to j, or if the path pro- duces a noisy formula leading to a wrong inference, P ij and Φ(r(i, j), ρ) should both be smaller.</p><p>To explicitly build a bridge between the potential Φ and the inference goal ρ, we maximize the like- lihood of paths which can infer ρ. First, we recur- sively define the likelihood of a path from H to t as P p Ht = P p Hs · P rst , where P rst is defined in E- quation (3). We then classify a path p Ht into three separate categories: a) t = T and p Ht can produce a benefic formula to infer R(H, T ); b) t = T ; c) t = T but p Ht may generate a noisy formula which misleads inference. Finally, we define the likelihood function as follows,</p><formula xml:id="formula_3">max P P = p Ht ∈P P a p Ht (1 − P p Ht ) b+c (4)</formula><p>where P is all paths found in the process of perform- ing random walks for R(H, T ), and t may be equal to T or not. a, b, c are three 0-1 variables corre- sponding to the above categories a), b), c). Only one in a, b, c can be 1 when P Ht belongs to the corre- sponding category. We then transform maximizing P P to minimizing L rw = − log P P and employ SGD to train it. In practice, there is not a clear-cut bound- ary between a) and c), so we divide the loss into two parts:</p><formula xml:id="formula_4">L rw = L t rw + λL inf rw . L t rw</formula><p>is the loss of that t = T , and L inf rw is the loss of that p HT generates a noisy formula leading to a wrong inference. λ is a super parameter to balance the two losses. L t rw and L inf rw have the same expression but are optimized in different stages. L t rw can be optimized during ran- dom walks, while L inf rw should be optimized in the inference stage. We rewrite L rw for a specific path p as follows,</p><formula xml:id="formula_5">L rw (p) = −y log P p − (1 − y) log (1 − P p ) (5)</formula><p>where y is the label of the path p and y = 1 if p is beneficial to infer ρ. To obtain the best Φ, we compute gradients of L rw as follows,</p><formula xml:id="formula_6">L rw (p) = (L rw (r 12 ), L rw (r 23 ), ...) L rw (r ij ) = ( ∂L rw (r ij ) ∂Φ r ij , ∂L rw (r ij ) ∂Φ r ik 1 , ∂L rw (r ij ) ∂Φ r ik 2 , ...) ∂L rw (r ij ) ∂Φ r ij = (P p − y) · (1 − P r ij ) Φ r ij · (1 − P p ) ∂L rw (r ij ) ∂Φ r ik = − (P p − y) · P r ij Φ r ij · (1 − P p )<label>(6)</label></formula><p>where L rw (r ij ) is the component of L rw (p) at r ij . Φ(r(i, j), ρ) and Φ(r(i, k), ρ) are the potentials for all triplets r(i, j) ∈ p and r(i, k) / ∈ p, and r ij is short for r(i, j). After iteratively updating Φ r ij and Φ r ik by the gradient of L t rw , the random walks can be directed to find more paths from H to T , and con- sequently it increases efficiency of the random walk. After updating Φ r ij and Φ r ik by the gradient of L inf rw , random walk is more likely to find high-quality path- s but not noise. Therefore, the goal-directed random walk increases efficiency of mining benefic formulas and has a great capability of resisting noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distributional Potential Function</head><p>The potential Φ(r(i, j), ρ) measures an implicit re- lationship between two triplets in the KB, so the total number of parameters is the square of the K- B size. It is hard to precisely estimate all Φ be- cause of the sparsity of training data. To reduce the number of parameters, we represent each en- tity or relation in the KB as a low-rank numeric vector which is called embeddings ( <ref type="bibr" target="#b3">Bordes et al., 2013)</ref>, and then we build a potential function Ψ on embeddings as Φ(r(i, j), ρ) = Ψ(E r(i,j) , E R(H,T ) ), where E r(i,j) and E R(H,T ) are the embeddings of triplets. In practice, we set E r(i,j) = [E r , E j ] and E R(H,T ) = [E R , E T ] because E i is the same for all triplets r(i, * ), where [] is a concatenation operator.</p><p>In the view of the neural network, our goal- directed mechanism is analogous to the attention mechanism. At each step, the algorithm estimates attentions for each neighboring edges by Ψ. There- fore, there are several existing expressions of Ψ, e.g., the dot product ( <ref type="bibr" target="#b20">Sukhbaatar et al., 2015</ref>) and the single-layer perceptron ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>). We will not compare different forms of Ψ, the detail comparison has been presented in the work ( <ref type="bibr" target="#b13">Luong et al., 2015)</ref>. We directly employ the simplest dot product for Ψ as follows,</p><formula xml:id="formula_7">Ψ(E r(i,j) , E R(H,T ) ) = σ(E r(i,j) · E R(H,T ) ) (7)</formula><p>where σ is a nonlinear function and we set it as an exponential function. Ψ has no parameters beside KB embeddings which are updated during the train- ing period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integrated Inference Model</head><p>To handle the dependence between goal-directed random walk and subsequent inference, we combine them into an integrated model and optimize them together. To predict ρ = R(H, T ), the integrated model first collects formulas for R(H, T ), and then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Train Integrated Inference Model</head><p>Input: KB, Ξ Output: Ψ, W , F 1: For ρ = R(H, T ) ∈ Ξ 2: Repeat ρ-directed Random Walk from H to t 3:</p><p>Update Ψ by L t rw 4: </p><formula xml:id="formula_8">If t = T , then F = F ∩ f p 5:</formula><formula xml:id="formula_9">χ(ρ) = f ∈Fρ δ(f )<label>(8)</label></formula><p>where F ρ is the formula set obtained by random walks for ρ, and δ(f ) is an estimation of formula f . The original frequent pattern mining algorithm employs formulas' confidence as δ(f ) directly, but it does not produce good results ( <ref type="bibr" target="#b4">Galárraga et al., 2013)</ref>. There are two ways to solve the problem: one is selecting another more suitable measure of f as δ(f ) ( <ref type="bibr" target="#b21">Tan et al., 2002</ref>); the other is attaching a weight to each formula and learning weights with supervision, e.g., MLN ( <ref type="bibr" target="#b18">Richardson and Domingos, 2006</ref>) . We employ the latter method and set δ(f ) = w f ·n f . Finally, we employ a logistic regres- sion classifier to predict R(H, T ), and the posterior probability of R(H, T ) is shown as follows,</p><formula xml:id="formula_10">P (ρ = y|χ) = F(χ) y (1 − F(χ)) 1−y F(χ) = 1 1 + e −χ (9)</formula><p>where y is a 0-1 label of ρ. Similar to L t rw in Equation <ref type="formula">(5)</ref>, we treat the negative logarithm of P (ρ = y|χ) as the loss of inference, L inf = − log P (ρ = y|χ), and turn to minimize it. More- over, the loss L inf rw of the above goal-directed ran- dom walk is influenced by the result of predicting R(H, T ), so Φ r ij and Φ r ik will be also updated. Al- gorithm 1 shows the main process of training, where Ξ is the triplet set for training, Ψ is the potential function in Equation <ref type="formula">(7)</ref>, F is the formula set, f p is Dataset Relation <ref type="table" target="#tab_1">Entity Train Valid Test  WN18 18</ref> 40,943 141,442 5,000 5,000 FB15K 1,345 14,951 483,142 50,000 59,071 a formula generated from the path p, and H, T, t are entities in the KB. To predict ρ = R(H, T ), the al- gorithm first performs multi rounds of random walk- s, and each random walk can find a path p Ht (at line 2). Then the algorithm decides to update Ψ by L t rw based on whether t is T (at line 3), and adds the for- mula p f into the formula set when t = T (at line 4). After random walks, the inference model pre- dicts ρ, and computes L inf and L inf rw according to the prediction result (at line 5). Finally W and Ψ are updated by L inf and L inf rw (at line 6-7), respective- ly. After training by all triplets in Ξ, the algorithm removes formulas with low weights from F (at line 8) and outputs the model (at line 9). When we infer a new triplet by this model, the process is similar to Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first compare our approach with several state-of- art methods on link prediction task to explore our approach's overall ability of inference. Subsequent- ly, we evaluate formulas mined by different random walk methods to explore whether the goal-directed mechanism can increase efficiency of mining useful structures. Finally, we dive deep into the formulas generated by our approach to analyze the characters of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Setup</head><p>We conduct experiments on both WN18 and FB15K datasets which are subsets sampled from WordNet <ref type="bibr" target="#b14">(Miller, 1995)</ref> and <ref type="bibr">Freebase (Bollacker et al., 2008)</ref>, respectively, and <ref type="table" target="#tab_1">Table 1</ref> shows the statistics of them. For the link prediction task, we predict the missing h or t for a triplet r(h, t) in test set. The de- tail evaluation method is that t in r(h, t) is replaced by all entities in the KB and methods need to rank the right answer at the top of the list, and so does h in r(h, t). We report the mean of those true an- swer ranks and the Hits@10 under both 'raw' and 'filter' as TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>) does, where Hits@10 is the proportion of correct entities ranked in the top 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We employ two types of baselines. One type is based on random walks including: a) the basic ran- dom walk algorithm whose state transition probabil- ity matrix is shown in Equation <ref type="formula" target="#formula_1">(2)</ref> ). These embedding-based methods have no explicit formulas, so we will not evaluate their performances on mining formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Settings</head><p>We implement three random walk methods under a unified framework. To predict r(h, * ) quickly, we first select Top-K candidate instances, t 1→K , by TransE as ( <ref type="bibr" target="#b26">Wei et al., 2015)</ref>, and then the algorith- m infers each r(h, t i ) and ranks them by inference results. We adjust parameters for our approach with the validate dataset, and the optimal configurations are set as follows. The rounds of random walk is 10, learning rate is 0.0001, training epoch is 100, the size of candidate set is 500 for WN18 and 100 for FB15K, the embeddings have 50 dimensionali- ties for WN18 and 100 dimensionalities for FB15K, and the embeddings are initialized by TransE. For some relations, random walk truly finds no practica- ble formulas, so we employ TransE to improve per-  formance for these relations. For embedding-based methods, we use reported results directly since the evaluation datasets are identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Link Prediction</head><p>We show the results of link prediction for our ap- proach and all baselines in <ref type="table" target="#tab_3">Table 2</ref> (* means the mean of ranks for random walk methods are eval- uated in the Top-K subset), and we can obtain the following observations: 1) Our approach achieves good performances on both WN18 and FB15K. On the FB15K, our ap- proach outperforms all baselines. It indicates that our approach is effective for inference. On the WN18, three random walk methods have similar performances. The reason is that most entities in WN18 only have a small number of neighbors, so RW and PRA can also find useful structures in a few rounds.</p><p>2) For FB15K, the performances of RW and PRA are both poor and even worse than a part of embedding-based methods, but the performance of our approach is still the best. The reason is that there are too many relation types in FB15K, so goalless random walks introduce lots of noise. Oppositely, our approach has a great capability of resisting noise for the goal-directed mechanism.</p><p>3) RW and PRA have similar performances on both datasets, which indicates the heuristic rule of PRA does not apply to all relations and formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Paths Recall by Random Walks</head><p>To further explore whether the goal-directed mech- anism can increase efficiency of mining paths, we compare the three random walk methods by the number of paths mined. For each triplet R <ref type="bibr">(H, T )</ref> in the training set, we perform 10 rounds of random walks from H and record the number of times which arrive at T, noted as Arr@10. We respectively select one relation type from WN18 and FB15K and show the comparison result in <ref type="figure" target="#fig_1">Figure 2</ref>. We can obtain the following observations:</p><p>1) With the increase of training epochs, Arr@10 of the goal-directed random walk first increases and then stays around a high value on both WN18 and FB15K, but the Arr@10 of RW and PRA always stay the same. This phenomenon indicates that the goal-directed random walk is a learnable model and can be trained to find more useful structures with epochs increasing, but RW and PRA are not.</p><p>2) RW and PRA always have similar Arr@10, which means PRA has not found more formulas. This indicates that the heuristic rule of PRA is not always be beneficial to mining more structures for all relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Example Formulas</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we show a small number of formulas mined by our approach from FB15K, and the formu- las represent different types. Some formulas contain clear logic, e.g, Formula 1 means that if the writer x contributes a story to the film y and y is adapted from the book z, x is the writer of the book z. Some formulas have a high probability of being satisfied, e.g., Formula 3 means the wedding place probably is also the burial place for some people, and Formu- la 7 means the parent of the person x died of the disease and thus the person x has a high risk of suf- fering from the disease. Some formulas depend on synonyms, e.g., story by and works written have the similar meaning in Formula 2. However, there are still useless formulas, e.g, Formula 8 is useless be-Relation Formula works written 1 film story contributor(x,y) ∧ adapted from(y,z) ⇒ works written(x,z) 2 story by(y,x) ⇒ works written(x,y) place of burial 3 place of death(x,y) ⇒ place of burial(x,y) 4 marriage type of union(x,y) ∧ marriage location of ceremony(y,z) ⇒ place of burial(x,z) service language 5 service location(x,y) ∧ imported from(y,z) ∧ official language(z,w) ⇒ service language(x,w) 6 service location(x,y) ∧ exported to(y,z) ∧ languages spoken(z,w) ⇒ service language(x,w) disease risk factors 7 parent cause of death(x,y) ∧ disease risk factors(y,z) ⇒ disease risk factors(x,z) 8 disease risk factors(x,y)∧ -disease risk factors(y,x) ⇒ disease risk factors(x,y) cause the body of the formula is same as the head. Such useless formula can be removed by a super- rule, which is that the head of a formula cannot oc- cur in its body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work has two aspects, which are related to min- ing formula automatically and inference on KBs, re- spectively. Inductive Logic Programming (ILP) <ref type="bibr" target="#b15">(Muggleton and De Raedt, 1994)</ref> and Association Rule Mining (ARM) ( <ref type="bibr" target="#b0">Agrawal et al., 1993)</ref> are both early work- s on mining formulas. FOIT <ref type="bibr" target="#b17">(Quinlan, 1990)</ref> and SHERLOCK ( <ref type="bibr" target="#b19">Schoenmackers et al., 2010</ref>) are typ- ical ILP systems, but the former one usually need a lot of negative facts and the latter one focuses on mining formulas from text. AMIE ( <ref type="bibr" target="#b4">Galárraga et al., 2013</ref>) is based on ARM and proposes a new mea- sure for formulas instead of the confidence. Several structure learning algorithms <ref type="bibr" target="#b6">(Kok and Domingos, 2005;</ref><ref type="bibr" target="#b7">Kok and Domingos, 2009;</ref><ref type="bibr" target="#b8">Kok and Domingos, 2010</ref>) based on Markov Logic Network (ML- N) ( <ref type="bibr" target="#b18">Richardson and Domingos, 2006</ref>) can also learn first order logic formulas automatically, but they are too slow to run on large KBs. ProPPR ( <ref type="bibr" target="#b23">Wang et al., 2013;</ref><ref type="bibr" target="#b24">Wang et al., 2014a</ref>) performs structure learn- ing by depth first searching on the knowledge graph, which is still not efficient enough to handle web- scale KBs. PRA ( <ref type="bibr" target="#b9">Lao and Cohen, 2010;</ref><ref type="bibr" target="#b10">Lao et al., 2011</ref>) is a method based on random walks and em- ploys heuristic rules to direct random walks. PRA is closely related to our approach, but unlike it, our ap- proach dynamically calculates state transition prob- abilities. Another method based on random walks ( <ref type="bibr" target="#b26">Wei et al., 2015</ref>) merges embedding similarities of candidates into the random walk as a priori, while our approach employs KB embeddings to calculate potentials for neighbors.</p><p>The majority of mining formula methods can per- form inference on KBs, and besides them, a dozen methods based KB embeddings can also achieve the inference goal, and the typical ones of them are TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>), <ref type="bibr">Rescal (Nickel et al., 2011</ref>), TransH ( <ref type="bibr" target="#b25">Wang et al., 2014b</ref>), TransR ( <ref type="bibr" target="#b12">Lin et al., 2015b</ref>). These embedding-based methods take advantage of the implicit relationship between ele- ments of the KB and perform inference by calcu- lating similarities. There are also methods which combine inference formulas and KB embeddings, such as PTransE ( <ref type="bibr" target="#b11">Lin et al., 2015a</ref>) and ProPPR+MF ( <ref type="bibr" target="#b22">Wang and Cohen, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>In this paper, we introduce a goal-directed random walk algorithm to increase efficiency of mining use- ful formulas and decrease noise simultaneously. The approach employs the inference target as the direc- tion at each steps in the random walk process and is more inclined to visit structures helpful to infer- ence. In empirical studies, we show our approach achieves good performances on link prediction task over large-scale KBs. In the future, we are interest- ed in exploring mining formulas directly in the dis- tributional spaces which may resolve the sparsity of formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1386</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: a) shows a subgraph extracted from Freebase. b) shows the searching space of finding the yellow path. c) shows a loop which can generate a false formula. d) shows a loop which can generate a true formula.</figDesc><graphic url="image-1.png" coords="2,75.60,57.82,460.80,244.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Arr@10 of three random walk algorithms and the horizontal axis represents epochs and the vertical axis represents Arr@10. Figure 2.a shows results on relation derivationally related form in WN18, and Figure 2.b shows results on relation form of government in FB15K.</figDesc><graphic url="image-3.png" coords="6,318.61,145.21,215.99,86.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>; b) PRA in (Lao et al., 2011) which is a typical heuristic random walk algorithm. The other type is based on KB embed- dings including TransE (Bordes et al., 2013), Rescal (Nickel et al., 2011), TransH (Wang et al., 2014b), TransR (Lin et al., 2015b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Statistics of WN18 and FB15K</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Link Prediction Results on both WN18 and FB15K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Example Formulas Obtained by Goal-directed Random Walk</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This work was supported by the Natural Sci-ence Foundation of China (No. 61533018), the National Basic Research Program of China (No. 2014CB340503) and the National Natural Science</head><p>Foundation of China (No. 61272332). And this work was also supported by Google through focused research awards program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Imieli´nskiimieli´nski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Amie: association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Antonio Galárraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Yago2: A spatially and temporally enhanced knowledge base from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning the structure of markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning markov logic network structure via hypergraph lifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning markov logic networks using structural motifs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="551" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Relational retrieval using a combination of path-constrained random walks. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="53" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00379</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inductive logic programming: Theory and methods. The Journal of Logic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>De Raedt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="629" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning logical definitions from relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="266" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning first-order horn clauses from web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1088" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selecting the right interestingness measure for association patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Pang-Ning Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaideep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning first-order logic embeddings via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence (IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Programming with personalized pagerank: a locally groundable first-order probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structure learning via parameter learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale knowledge base completion: Inferring via grounding network sampling over selected instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengya</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
