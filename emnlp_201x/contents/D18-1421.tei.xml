<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paraphrase Generation with Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Toutiao AI Lab</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Paraphrase Generation with Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3865" to="3878"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3865</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP). In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a generator and an evaluator, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Experimental results on two datasets demonstrate the proposed models (the generators) can produce more accurate paraphrases and outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Paraphrases refer to texts that convey the same meaning but with different expressions. For ex- ample, "how far is Earth from Sun", "what is the distance between Sun and Earth" are paraphrases. Paraphrase generation refers to a task in which given a sentence the system creates paraphrases of it. Paraphrase generation is an important task in NLP, which can be a key technology in many applications such as retrieval based question an- swering, semantic parsing, query reformulation in web search, data augmentation for dialogue sys- tem. However, due to the complexity of natural language, automatically generating accurate and diverse paraphrases is still very challenging. Tra- ditional symbolic approaches to paraphrase gen- eration include rule-based methods <ref type="bibr">(McKeown, 1983)</ref>, thesaurus-based methods ( <ref type="bibr">Bolshakov and Gelbukh, 2004;</ref><ref type="bibr">Kauchak and Barzilay, 2006</ref>), grammar-based methods <ref type="bibr" target="#b1">(Narayan et al., 2016)</ref>, and statistical machine translation (SMT) based methods ( <ref type="bibr" target="#b7">Quirk et al., 2004;</ref><ref type="bibr" target="#b29">Zhao et al., 2008</ref><ref type="bibr" target="#b28">Zhao et al., , 2009</ref>.</p><p>Recently, neural network based sequence-to- sequence (Seq2Seq) learning has made remark- able success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., <ref type="bibr">Cho et al. (2014)</ref>; <ref type="bibr" target="#b24">Wu et al. (2016)</ref>; <ref type="bibr" target="#b12">Shang et al. (2015)</ref>; <ref type="bibr" target="#b18">Vinyals and Le (2015)</ref>; <ref type="bibr" target="#b10">Rush et al. (2015)</ref>; <ref type="bibr" target="#b25">Yin et al. (2016)</ref>). Paraphrase generation can naturally be formulated as a Seq2Seq problem <ref type="bibr">(Cao et al., 2017;</ref><ref type="bibr" target="#b6">Prakash et al., 2016;</ref><ref type="bibr">Gupta et al., 2018;</ref><ref type="bibr" target="#b14">Su and Yan, 2017)</ref>. The main challenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated para- phrase and the given sentence. In a straightfor- ward application of Seq2Seq to paraphrase gen- eration one would make use of cross entropy as evaluation measure, which can only be a loose ap- proximation of semantic similarity. To tackle this problem, <ref type="bibr" target="#b8">Ranzato et al. (2016)</ref> propose employing reinforcement learning (RL) to guide the training of Seq2Seq and using lexical-based measures such as BLEU ( <ref type="bibr" target="#b4">Papineni et al., 2002</ref>) and ROUGE <ref type="bibr">(Lin, 2004</ref>) as a reward function. However, these lexi- cal measures may not perfectly represent semantic similarity. It is likely that a correctly generated sequence gets a low ROUGE score due to lexical mismatch. For instance, an input sentence "how far is Earth from Sun" can be paraphrased as "what is the distance between Sun and Earth", but it will get a very low ROUGE score given "how many miles is it from Earth to Sun" as a reference.</p><p>In this work, we propose taking a data-driven approach to train a model that can conduct evalu- ation in learning for paraphrasing generation. The framework contains two modules, a generator (for paraphrase generation) and an evaluator (for para- phrase evaluation). The generator is a Seq2Seq learning model with attention and copy mecha- nism ( <ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b11">See et al., 2017)</ref>, which is first trained with cross entropy loss and then fine-tuned by using policy gradient with su- pervisions from the evaluator as rewards. The evaluator is a deep matching model, specifically a decomposable attention model ( <ref type="bibr" target="#b5">Parikh et al., 2016)</ref>, which can be trained by supervised learn- ing (SL) when both positive and negative exam- ples are available as training data, or by inverse reinforcement learning (IRL) with outputs from the generator as supervisions when only positive examples are available. In the latter setting, for the training of evaluator using IRL, we develop a novel algorithm based on max-margin IRL prin- ciple ( <ref type="bibr" target="#b9">Ratliff et al., 2006</ref>). Moreover, the gener- ator can be further trained with non-parallel data, which is particularly effective when the amount of parallel data is small.</p><p>We evaluate the effectiveness of our approach using two real-world datasets (Quora question pairs and Twitter URL paraphrase corpus) and we conduct both automatic and human assessments. We find that the evaluator trained by our methods can provide accurate supervisions to the genera- tor, and thus further improve the accuracies of the generator. The experimental results indicate that our models can achieve significantly better per- formances than the existing neural network based methods.</p><p>It should be noted that the proposed approach is not limited to paraphrase generation and can be readily applied into other sequence-to-sequence tasks such as machine translation and generation based single turn dialogue. Our technical contri- bution in this work is of three-fold:</p><p>1. We introduce the generator-evaluator frame- work for paraphrase generation, or in general, sequence-to-sequence learning. 2. We propose two approaches to train the evalu- ator, i.e., supervised learning and inverse rein- forcement learning. 3. In the above framework, we develop several techniques for learning of the generator and evaluator. Section 2 defines the models of generator and evaluator. In section 3, we formalize the problem of learning the models of generator and evaluator. In section 4, we report our experimental results. In section 5, we introduce related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>This section explains our framework for para- phrase generation, containing two models, the generator and evaluator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem and Framework</head><p>Given an input sequence of words X = [x 1 , . . . , x S ] with length S, we aim to generate an output sequence of words Y = [y 1 , . . . , y T ] with length T that has the same meaning as X. We denote the pair of sentences in paraphrasing as (X, Y ). We use Y 1:t to denote the subsequence of Y ranging from 1 to t and usê Y to denote the sequence generated by a model. We propose a framework, which contains a generator and an evaluator, called RbM (Rein- forced by Matching). Specifically, for the gener- ator we adopt the Seq2Seq architecture with atten- tion and copy mechanism ( <ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b11">See et al., 2017)</ref>, and for the evaluator we adopt the decomposable attention-based deep matching model ( <ref type="bibr" target="#b5">Parikh et al., 2016)</ref>. We denote the gener- ator as G θ and the evaluator as M φ , where θ and φ represent their parameters respectively. <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of our framework. Basically the generator can generate a paraphrase of a given sentence and the evaluator can judge how seman- tically similar the two sentences are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generator: Seq2Seq Model</head><p>In this work, paraphrase generation is defined as a sequence-to-sequence (Seq2Seq) learning prob- lem. Given input sentence X, the goal is to learn a model G θ that can generate a sentence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3867ˆY</head><p>3867ˆ 3867ˆY = G θ (X) as its paraphrase. We choose the pointer-generator proposed by <ref type="bibr" target="#b11">See et al. (2017)</ref> as the generator. The model is built based on the encoder-decoder framework ( <ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b15">Sutskever et al., 2014</ref>), both of which are imple- mented as recurrent neural networks (RNN). The encoder RNN transforms the input sequence X into a sequence of hidden states H = [h 1 , . . . , h S ]. The decoder RNN generates an output sentence Y on the basis of the hidden states. Specifically it predicts the next word at each position by sam- pling fromˆyfromˆ fromˆy t ∼ p(y t |Y 1:t−1 , X) = g(s t , c t , y t−1 ), where s t is the decoder state, c t is the context vector, y t−1 is the previous word, and g is a feed-forward neural network. Attention mecha- nism ( <ref type="bibr">Bahdanau et al., 2015</ref>) is introduced to com- pute the context vector as the weighted sum of en- coder states:</p><formula xml:id="formula_0">c t = S i=1 α ti h i , α ti = exp η(s t−1 , h i ) S j=1 exp η(s t−1 , h j )</formula><p>, where α ti represents the attention weight and η is the attention function, which is a feed-forward neural network. Paraphrasing often needs copying words from the input sentence, for instance, named entities. The pointer-generator model allows either gener- ating words from a vocabulary or copying words from the input sequence. Specifically the proba- bility of generating the next word is given by a mixture model:</p><formula xml:id="formula_1">p θ (y t |Y 1:t−1 , X) = q(s t , c t , y t−1 )g(s t , c t , y t−1 ) + (1 − q(s t , c t , y t−1 )) i:yt=x i α ti ,</formula><p>where q(s t , c t , y t−1 ) is a binary neural classifier deciding the probability of switching between the generation mode and the copying mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluator: Deep Matching Model</head><p>In this work, paraphrase evaluation (identifica- tion) is casted as a problem of learning of sen- tence matching. The goal is to learn a real-valued function M φ (X, Y ) that can represent the match- ing degree between the two sentences as para- phrases of each other. A variety of learning tech- niques have been developed for matching sen- tences, from linear models (e.g., <ref type="bibr" target="#b23">Wu et al. (2013)</ref>) to neural network based models (e.g., <ref type="bibr" target="#b13">Socher et al. (2011)</ref>; <ref type="bibr">Hu et al. (2014)</ref>). We choose a simple yet effective neural network architecture, called the decomposable-attention model ( <ref type="bibr" target="#b5">Parikh et al., 2016)</ref>, as the evaluator. The evaluator can cal- culate the semantic similarity between two sen- tences:</p><formula xml:id="formula_2">M φ (X, Y ) = H( S i=1 G([e(xi), ¯ xi]), T j=1 G([e(yj), ¯ yj])),</formula><p>where e(·) denotes a word embedding, ¯ x i and ¯ y j denote inter-attended vectors, H and G are feed- forward networks. We refer the reader to <ref type="bibr" target="#b5">Parikh et al. (2016)</ref> for details. In addition, we add po- sitional encodings to the word embedding vectors to incorporate the order information of the words, following the idea in <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning</head><p>This section explains how to learn the generator and evaluator using deep reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning of Generator</head><p>Given training data (X, Y ), the generator G θ is first trained to maximize the conditional log like- lihood (negative cross entropy):</p><formula xml:id="formula_3">L Seq2Seq (θ) = T t=1 log p θ (y t |Y 1:t−1 , X). (1)</formula><p>When computing the conditional probability of the next word as above, we choose the previous word y t−1 in the ground-truth rather than the wordˆywordˆ wordˆy t−1 generated by the model. This technique is called teacher forcing. With teacher forcing, the discrepancy between training and prediction (also referred to as expo- sure bias) can quickly accumulate errors along the generated sequence ( <ref type="bibr">Bengio et al., 2015;</ref><ref type="bibr" target="#b8">Ranzato et al., 2016)</ref>. Therefore, the generator G θ is next fine-tuned using RL, where the reward is given by the evaluator.</p><p>In the RL formulation, generation of the next word represents an action, the previous words rep- resent a state, and the probability of generation p θ (y t |Y 1:t−1 , X) induces a stochastic policy. Let r t denote the reward at position t. The goal of RL is to find a policy (i.e., a generator) that maximizes the expected cumulative reward:</p><formula xml:id="formula_4">L RL (θ) = E p θ ( ˆ Y |X) T t=1 r t (X, ˆ Y 1:t ).<label>(2)</label></formula><p>We define a positive reward at the end of se- quence (r T = R) and a zero reward at the other positions. The reward R is given by the evalua- tor M φ . In particular, when a pair of input sen- tence X and generated paraphrasê Y = G θ (X) is given, the reward is calculated by the evaluator R = M φ (X, ˆ Y ). We can then learn the optimal policy by em- ploying policy gradient. According to the policy gradient theorem <ref type="bibr" target="#b22">(Williams, 1992;</ref><ref type="bibr" target="#b16">Sutton et al., 2000</ref>), the gradient of the expected cumulative re- ward can be calculated by</p><formula xml:id="formula_5">θ L RL (θ) = T t=1 [ θ log p θ (ˆ y t | ˆ Y 1:t−1 , X)]r t .</formula><p>(3) The generator can thus be learned with stochastic gradient descent methods such as Adam (Kingma and Ba, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning of Evaluator</head><p>The evaluator works as the reward function in RL of the generator and thus is essential for the task. We propose two methods for learning the evalua- tor in different settings. When there are both pos- itive and negative examples of paraphrases, the evaluator is trained by supervised learning (SL). When only positive examples are available (usu- ally the same data as the training data of the gener- ator), the evaluator is trained by inverse reinforce- ment learning (IRL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Learning</head><p>Given a set of positive and negative examples (paraphrase pairs), we conduct supervised learn- ing of the evaluator with the pointwise cross en- tropy loss:</p><formula xml:id="formula_6">J SL (φ) = − log M φ (X, Y )−log(1−M φ (X, Y − )),<label>(4)</label></formula><p>where Y − represents a sentence that is not a para- phrase of X. The evaluator M φ here is defined as a classifier, trained to distinguish negative example</p><formula xml:id="formula_7">(X, Y − ) from positive example (X, Y ).</formula><p>We call this method RbM-SL (Reinforced by Matching with Supervised Learning). The evalu- ator M φ trained by supervised learning can make a judgement on whether two sentences are para- phrases of each other. With a well-trained evalua- tor M φ , we further train the generator G θ by rein- forcement learning using M φ as a reward function. <ref type="figure" target="#fig_1">Figure 2a</ref> shows the learning process of RbM-SL. The detailed training procedure is shown in Algo- rithm 1 in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Reinforcement Learning</head><p>Inverse reinforcement learning (IRL) is a sub- problem of reinforcement learning (RL), about learning of a reward function given expert demon- strations, which are sequences of states and ac- tions from an expert (optimal) policy. More specifically, the goal is to find an optimal re- ward function R * with which the expert policy p θ * (Y |X) really becomes optimal among all pos- sible policies, i.e.,</p><formula xml:id="formula_8">E p θ * (Y |X) R * (Y ) ≥ E p θ ( ˆ Y |X) R * ( ˆ Y ), ∀θ.</formula><p>In the current problem setting, the problem becomes learning of an optimal reward function (evaluator) given a number of paraphrase pairs given by human experts (expert demonstrations).</p><p>To learn an optimal reward (matching) func- tion is challenging, because the expert demonstra- tions might not be optimal and the reward function might not be rigorously defined. To deal with the problem, we employ the maximum margin formu- lation of IRL inspired by <ref type="bibr" target="#b9">Ratliff et al. (2006)</ref>.</p><p>The maximum margin approach ensures the learned reward function has the following two desirable properties in the paraphrase generation task: (a) given the same input sentence, a reference from humans should have a higher reward than the ones generated by the model; (b) the margins be- tween the rewards should become smaller when the paraphrases generated by the model get closer to a reference given by humans. We thus specifi- cally consider the following optimization problem for learning of the evaluator:</p><formula xml:id="formula_9">J IRL (φ) = max(0, 1−ζ+M φ (X, ˆ Y )−M φ (X, Y )),<label>(5)</label></formula><p>where ζ is a slack variable to measure the agree- ment betweenˆYbetweenˆ betweenˆY and Y . In practice we set ζ = ROUGE-L( ˆ Y , Y ). Different from RbM-SL, the evaluator M φ here is defined as a ranking model that assigns higher rewards to more plausible para- phrases.</p><p>Once the reward function (evaluator) is learned, it is then used to improve the policy function (gen- erator) through policy gradient. In fact, the gen- erator G θ and the evaluator M φ are trained alter- natively. We call this method RbM-IRL (Rein- forced by Matching with Inverse Reinforcement Learning). <ref type="figure" target="#fig_1">Figure 2b</ref> shows the learning process of RbM-IRL. The detailed training procedure is shown in Algorithm 2 in Appendix A. We can formalize the whole learning procedure as the following optimization problem:</p><formula xml:id="formula_10">min φ max θ E p θ ( ˆ Y |X) J IRL (φ).<label>(6)</label></formula><p>RbM-IRL can make effective use of sequences generated by the generator for training of the eval- uator. As the generated sentences become closer to the ground-truth, the evaluator also becomes more discriminative in identifying paraphrases.</p><p>It should be also noted that for both RbM-SL and RbM-IRL, once the evaluator is learned, the reinforcement learning of the generator only needs non-parallel sentences as input. This makes it pos- sible to further train the generator and enhance the generalization ability of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward Shaping</head><p>In the original RL of the generator, only a positive reward R is given at the end of sentence. This pro- vides sparse supervision signals and can make the model greatly degenerate. Inspired by the idea of reward shaping <ref type="bibr" target="#b3">(Ng et al., 1999;</ref><ref type="bibr">Bahdanau et al., 2017)</ref>, we estimate the intermediate cumulative re- ward (value function) for each position, that is</p><formula xml:id="formula_11">Q t = E p θ (Y t+1:T | ˆ Y 1:t ,X) R(X, [ ˆ Y 1:t , Y t+1:T ]),</formula><p>by Monte-Carlo simulation, in the same way as in <ref type="bibr" target="#b14">Yu et al. (2017)</ref>:</p><formula xml:id="formula_12">Q t = 1 N n=N n=1 M φ (X, [ ˆ Y 1:t , Y n t+1:T ]), t &lt; T M φ (X, ˆ Y ), t = T,<label>(7)</label></formula><p>where N is the sample size and Y n t+1:T ∼ p θ (Y t+1:T | ˆ Y 1:t , X) denotes simulated sub- sequences randomly sampled starting from the (t + 1)-th word. During training of the generator, the reward r t in policy gradient (3) is replaced by Q t estimated in (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward Rescaling</head><p>In practice, RL algorithms often suffer from insta- bility in training. A common approach to reduce the variance is to subtract a baseline reward from the value function. For instance, a simple base- line can be a moving average of historical rewards. While in RbM-IRL, the evaluator keeps updating during training. Thus, keeping track of a base- line reward is unstable and inefficient. Inspired by <ref type="bibr">Guo et al. (2018)</ref>, we propose an efficient re- ward rescaling method based on ranking. For a batch of</p><formula xml:id="formula_13">D generated paraphrases { ˆ Y d } D d=1 , each associated with a reward R d = M φ (X d , ˆ Y d )</formula><p>, we rescale the rewards by</p><formula xml:id="formula_14">¯ R d = σ(δ 1 · (0.5 − rank(d) D )) − 0.5,<label>(8)</label></formula><p>where σ(·) is the sigmoid function, rank(d) is the rank of R d in {R 1 , ..., R D }, and δ 1 is a scalar con- trolling the variance of rewards. A similar strat- egy is applied into estimation of in-sequence value function for each word, and the final rescaled value function is</p><formula xml:id="formula_15">¯ Q d t = σ(δ 2 · (0.5 − rank(t) T )) − 0.5 + ¯ R d ,<label>(9)</label></formula><p>where rank(t) is the rank of</p><formula xml:id="formula_16">Q d t in {Q d 1 , ..., Q d T }.</formula><p>Reward rescaling has two advantages. First, the mean and variance of ¯ Q d t are controlled and hence they make the policy gradient more stable, even with a varying reward function. Second, when the evaluator M φ is trained with the ranking loss as in RbM-IRL, it is better to inform which paraphrase is better, rather than to provide a scalar reward in a range. In our experiment, we find that this method can bring substantial gains for RbM-SL and RbM- IRL, but not for RL with ROUGE as reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curriculum Learning</head><p>RbM-IRL may not achieve its best performance if all of the training instances are included in training at the beginning. We employ a curriculum learn- ing strategy ( <ref type="bibr">Bengio et al., 2009</ref>) for it. During the training of the evaluator M φ , each example k is associated with a weight w k , i.e.</p><formula xml:id="formula_17">J k IRL-CL (φ) = w k max(0,1 − ζ k + M φ (X k , ˆ Y k ) − M φ (X k , Y k ))<label>(10)</label></formula><p>In curriculum learning, w k is determined by the difficulty of the example. At the beginning, the training procedure concentrates on relatively sim- ple examples, and gradually puts more weights on difficult ones. In our case, we use the edit distance E(X, Y ) between X and Y as the mea- sure of difficulty for paraphrasing. Specifically, w k is determined by w k ∼ Binomial(p k , 1), and</p><formula xml:id="formula_18">p k = σ(δ 3 · (0.5 − rank(E(X k ,Y k )) K )</formula><p>), where K de- notes the batch size for training the evaluator. For δ 3 , we start with a relatively high value and grad- ually decrease it. In the end each example will be sampled with a probability around 0.5. In this manner, the evaluator first learns to identify para- phrases with small modifications on the input sen- tences (e.g. "what 's" and "what is"). Along with training it gradually learns to handle more compli- cated paraphrases (e.g. "how can I" and "what is the best way to").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines and Evaluation Measures</head><p>To compare our methods (RbM-SL and RbM- IRL) with existing neural network based meth- ods, we choose five baseline models: the at- tentive Seq2Seq model ( <ref type="bibr">Bahdanau et al., 2015)</ref>, the stacked Residual LSTM networks ( <ref type="bibr" target="#b6">Prakash et al., 2016)</ref>, the variational auto-encoder (VAE- SVG-eq) (Gupta et al., 2018) 1 , the pointer- generator ( <ref type="bibr" target="#b11">See et al., 2017)</ref>, and the reinforced pointer-generator with ROUGE-2 as reward (RL- ROUGE) ( <ref type="bibr" target="#b8">Ranzato et al., 2016)</ref>.</p><p>We conduct both automatic and manual eval- uation on the models.</p><p>For the automatic evaluation, we adopt four evaluation measures: ROUGE-1, ROUGE-2 (Lin, 2004), BLEU <ref type="bibr" target="#b4">(Papineni et al., 2002</ref>) (up to at most bi-grams) and METEOR ( <ref type="bibr">Lavie and Agarwal, 2007)</ref>. As pointed out, ideally it would be better not to merely use a lexical measure like ROUGE or BLEU for evalu- ation of paraphrasing. We choose to use them for <ref type="bibr">1</ref> We directly present the results reported in <ref type="bibr">Gupta et al. (2018)</ref> on the same dateset and settings. reproducibility of our experimental results by oth- ers. For the manual evaluation, we conduct evalu- ation on the generated paraphrases in terms of rel- evance and fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We evaluate our methods with the Quora ques- tion pair dataset 2 and Twitter URL paraphrasing corpus ( <ref type="bibr">Lan et al., 2017)</ref>. Both datasets contain positive and negative examples of paraphrases so that we can evaluate the RbM-SL and RbM-IRL methods. We randomly split the Quora dataset in two different ways obtaining two experimen- tal settings: Quora-I and Quora-II. In Quora-I, we partition the dataset by question pairs, while in Quora-II, we partition by question ids such that there is no shared question between the training and test/validation datasets. In addition, we sam- ple a smaller training set in Quora-II to make the task more challenging. Twitter URL paraphras- ing corpus contains two subsets, one is labeled by human annotators while the other is labeled auto- matically by algorithm. We sample the test and validation set from the labeled subset, while us- ing the remaining pairs as training set. For RbM- SL, we use the labeled subset to train the evalua- tor M φ . Compared to Quora-I, it is more difficult to achieve a high performance with Quora-II. The Twitter corpus is even more challenging since the data contains more noise. The basic statistics of datasets are shown in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Generator We maintain a fixed-size vocabulary of 5K shared by the words in input and output, and truncate all the sentences longer than 20 words. The model architecture, word embedding size and LSTM cell size are as the same as reported in <ref type="bibr" target="#b11">See et al. (2017)</ref>. We use Adadgrad optimizer <ref type="bibr">(Duchi et al., 2011</ref>) in the supervised pre-training and Adam optimizer in the reinforcement learning, with the batch size of 80. We also fine-tune the   Seq2Seq baseline models with Adam optimizer for a fair comparison. In supervised pre-training, we set the learning rate as 0.1 and initial accumu- lator as 0.1. The maximum norm of gradient is set as 2. During the RL training, the learning rate de- creases to 1e-5 and the size of Monte-Carlo sam- ple is 4. To make the training more stable, we use the ground-truth with reward of 0.1.</p><p>Evaluator We use the pretrained GoogleNews 300-dimension word vectors 3 in Quora dataset and 200-dimension GloVe word vectors 4 in Twit- ter corpus. Other model settings are the same as in <ref type="bibr" target="#b5">Parikh et al. (2016)</ref>. For evaluator in RbM- SL we set the learning rate as 0.05 and the batch size as 32. For the evaluator of M φ in RbM-IRL, the learning rate decreases to 1e-2, and we use the batch size of 80.</p><p>We use the technique of reward rescaling as mentioned in section 3.3 in training RbM-SL and RbM-IRL. In RbM-SL, we set δ 1 as 12 and δ 2 as 1. In RbM-IRL, we keep δ 2 as 1 all the time and de- crease δ 1 from 12 to 3 and δ 3 from 15 to 8 during curriculum learning. In ROUGE-RL, we take the exponential moving average of historical rewards as baseline reward to stabilize the training:</p><formula xml:id="formula_19">b m = λQ m−1 + (1 − λ)b m−1 , b 1 = 0</formula><p>where b m is the baseline b at iteration m, Q is the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head><p>Automatic evaluation <ref type="table" target="#tab_1">Table 2</ref> shows the per- formances of the models on Quora datasets. In both settings, we find that the proposed RbM- SL and RbM-IRL models outperform the baseline models in terms of all the evaluation measures. Particularly in Quora-II, RbM-SL and RbM-IRL make significant improvements over the baselines, which demonstrates their higher ability in learn- ing for paraphrase generation. On Quora dataset, RbM-SL is constantly better than RbM-IRL for all the automatic measures, which is reasonable because RbM-SL makes use of additional labeled data to train the evaluator. Quora datasets contains a large number of high-quality non-paraphrases, i.e., they are literally similar but semantically dif- ferent, for instance "are analogue clocks better than digital" and "is analogue better than digi- tal". Trained with the data, the evaluator tends to become more capable in paraphrase identification. With additional evaluation on Quora data, the eval- uator used in RbM-SL can achieve an accuracy of 87% on identifying positive and negative pairs of paraphrases. <ref type="table" target="#tab_2">Table 3</ref> shows the performances on the Twitter corpus. Our models again outperform the base- lines in terms of all the evaluation measures. Note that RbM-IRL performs better than RbM-SL in this case. The reason might be that the evaluator of RbM-SL might not be effectively trained with the relatively small dataset, while RbM-IRL can leverage its advantage in learning of the evaluator with less data.</p><p>In our experiments, we find that the training techniques proposed in section 3.3 are all neces- sary and effective. Reward shaping is by default employed by all the RL based models. Reward rescaling works particularly well for the RbM models, where the reward functions are learned from data. Without reward rescaling, RbM-SL can still outperform the baselines but with smaller margins. For RbM-IRL, curriculum learning is necessary for its best performance. Without cur- riculum learning, RbM-IRL only has comparable performance with ROUGE-RL. Human evaluation We randomly select 300 sen- tences from the test data as input and generate paraphrases using different models. The pairs of paraphrases are then aggregated and partitioned into seven random buckets for seven human asses- sors to evaluate. The assessors are asked to rate each sentence pair according to the following two criteria: relevance (the paraphrase sentence is se- mantically close to the original sentence) and flu- ency (the paraphrase sentence is fluent as a natural language sentence, and the grammar is correct). Hence each assessor gives two scores to each para- phrase, both ranging from 1 to 5. To reduce the evaluation variance, there is a detailed evaluation guideline for the assessors in Appendix B. Each paraphrase is rated by two assessors, and then av- eraged as the final judgement. The agreement be- tween assessors is moderate (kappa=0.44). <ref type="table" target="#tab_3">Table 4</ref> demonstrates the average ratings for each model, including the ground-truth references. Our models of RbM-SL and RbM-IRL get bet- ter scores in terms of relevance and fluency than the baseline models, and their differences are statistically significant (paired t-test, p-value &lt; 0.01). We note that in human evaluation, RbM-SL achieves the best relevance score while RbM-IRL achieves the best fluency score. Case study <ref type="figure">Figure 3</ref> gives some examples of gen- erated paraphrases by the models on Quora-II for illustration. The first and second examples show the superior performances of RbM-SL and RbM- IRL over the other models. In the third exam- ple, both RbM-SL and RbM-IRL capture accu- rate paraphrasing patterns, while the other models wrongly segment and copy words from the input sentence. Compared to RbM-SL with an error of repeating the word scripting, RbM-IRL generates a more fluent paraphrase. The reason is that the evaluator in RbM-IRL is more capable of measur- ing the fluency of a sentence. In the fourth ex- ample, RL-ROUGE generates a totally non-sense sentence, and pointer-generator and RbM-IRL just cover half of the content of the original sentence, while RbM-SL successfully rephrases and pre- serves all the meaning. All of the models fail in the last example, because the word ducking is a rare word that never appears in the training data. Pointer-generator and RL-ROUGE generate totally irrelevant words such as UNK token or vic- tory, while RbM-SL and RbM-IRL still generate topic-relevant words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Neural paraphrase generation recently draws at- tention in different application scenarios. The task is often formalized as a sequence-to-sequence (Seq2Seq) learning problem. <ref type="bibr" target="#b6">Prakash et al. (2016)</ref> employ a stacked residual LSTM network in the Seq2Seq model to enlarge the model capacity. <ref type="bibr">Cao et al. (2017)</ref> utilize an additional vocabu- lary to restrict word candidates during generation. <ref type="bibr">Gupta et al. (2018)</ref> use a variational auto-encoder framework to generate more diverse paraphrases. <ref type="bibr">Ma et al. (2018)</ref> utilize an attention layer instead of a linear mapping in the decoder to pick up word candidates. <ref type="bibr">Iyyer et al. (2018)</ref> harness syntac- tic information for controllable paraphrase gen- eration. <ref type="bibr" target="#b27">Zhang and Lapata (2017)</ref> tackle a simi- lar task of sentence simplification withe Seq2Seq model coupled with deep reinforcement learning, in which the reward function is manually defined for the task. Similar to these works, we also pre- train the paraphrase generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning.</p><p>There is also work on paraphrasing generation in different settings. For example, <ref type="bibr" target="#b21">Mallinson et al. (2017)</ref> leverage bilingual data to produce para- phrases by pivoting over a shared translation in an- other language. <ref type="bibr" target="#b21">Wieting et al. (2017)</ref>; <ref type="bibr" target="#b20">Wieting and Gimpel (2018)</ref> use neural machine translation to generate paraphrases via back-translation of bilin- gual sentence pairs. <ref type="bibr">Buck et al. (2018)</ref> and <ref type="bibr">Dong et al. (2017)</ref> tackle the problem of QA-specific paraphrasing with the guidance from an external <ref type="figure">Figure 3</ref>: Examples of the generated paraphrases by different models on Quora-II.</p><p>QA system and an associated evaluation metric.</p><p>Inverse reinforcement learning (IRL) aims to learn a reward function from expert demonstra- tions. <ref type="bibr" target="#b0">Abbeel and Ng (2004)</ref> propose apprentice- ship learning, which uses a feature based linear reward function and learns to match feature ex- pectations. <ref type="bibr" target="#b9">Ratliff et al. (2006)</ref> cast the problem as structured maximum margin prediction. <ref type="bibr" target="#b30">Ziebart et al. (2008)</ref> propose max entropy IRL in order to solve the problem of expert suboptimality. Recent work involving deep learning in IRL includes <ref type="bibr">Finn et al. (2016b)</ref> and <ref type="bibr">Ho et al. (2016)</ref>. There does not seem to be much work on IRL for NLP. In <ref type="bibr" target="#b2">Neu and Szepesvári (2009)</ref>, parsing is formalized as a feature expectation matching problem. <ref type="bibr" target="#b19">Wang et al. (2018)</ref> apply adversarial inverse reinforce- ment learning in visual story telling. To the best of our knowledge, our work is the first that applies deep IRL into a Seq2Seq task.</p><p>Generative Adversarial Networks (GAN) ( <ref type="bibr">Goodfellow et al., 2014</ref>) is a family of unsupervised generative models. GAN con- tains a generator and a discriminator, respectively for generating examples from random noises and distinguishing generated examples from real examples, and they are trained in an adversarial way. There are applications of GAN on NLP, such as text generation ( <ref type="bibr" target="#b14">Yu et al., 2017;</ref><ref type="bibr">Guo et al., 2018</ref>) and dialogue generation ( <ref type="bibr">Li et al., 2017)</ref>. <ref type="bibr">RankGAN (Lin et al., 2017</ref>) is the one most similar to RbM-IRL that employs a ranking model as the discriminator. However, RankGAN works for text generation rather than sequence- to-sequence learning, and training of generator in RankGAN relies on parallel data while the training of RbM-IRL can use non-parallel data.</p><p>There are connections between GAN and IRL as pointed by <ref type="bibr">Finn et al. (2016a)</ref>; <ref type="bibr">Ho and Ermon (2016)</ref>. However, there are significant differences between GAN and our RbM-IRL model. GAN employs the discriminator to distinguish gener- ated examples from real examples, while RbM- IRL employs the evaluator as a reward function in RL. The generator in GAN is trained to maxi- mize the loss of the discriminator in an adversarial way, while the generator in RbM-IRL is trained to maximize the expected cumulative reward from the evaluator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed a novel deep re- inforcement learning approach to paraphrase gen- eration, with a new framework consisting of a generator and an evaluator, modeled as sequence- to-sequence learning model and deep matching model respectively. The generator, which is for paraphrase generation, is first trained via sequence-to-sequence learning. The evaluator, which is for paraphrase identification, is then trained via supervised learning or inverse rein- forcement learning in different settings. With a well-trained evaluator, the generator is further fine-tuned by reinforcement learning to produce more accurate paraphrases. The experiment re- sults demonstrate that the proposed method can significantly improve the quality of paraphrase generation upon the baseline methods. In the fu- ture, we plan to apply the framework and training techniques into other tasks, such as machine trans- lation and dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Human Evaluation Guideline</head><p>Please judge the paraphrases from the following two criteria:</p><p>(1) Grammar and Fluency: the paraphrase is acceptable as natural language text, and the grammar is correct;</p><p>(2) Coherent and Consistent: please view from the perspective of the original poster, to what extent the answer of paraphrase is helpful for you with respect to the original question. Specifically, you can consider following as- pects:</p><p>-Relatedness: it should be topically rele- vant to the original question. -Type of question: the type of the original question remains the same in paraphrase. -Informative: no information loss in para- phrase.</p><p>For each paraphrase, give two separate score rank- ing from 1 to 5. The meaning of specific score is as following:</p><p>• Grammar and Fluency -5: Without any grammatical error; -4: Fluent and has one minor grammati- cal error that does not affect understand- ing, e.g. what is the best ways to learn programming; -3: Basically fluent and has two or more minor grammatical errors or one seri- ous grammatical error that does not have strong impact on understanding, e.g. what some good book for read; -2: Can not understand what it means but it is still in the form of human language, e.g. what is the best movie of movie; -1: Non-sense composition of words and not in the form of human language, e.g. how world war iii world war.</p><p>• Coherent and Consistent -5: Accurate paraphrase with exact the same meaning of the source sentence; -4: Basically the same meaning of the source sentence but does not cover some minor content, e.g. what are some good places to visit in hong kong during sum- mer → can you suggest some places to visit in hong kong;</p><p>-3: Cover part of the content of source sentence and has serious information loss, e.g. what is the best love movie by wong ka wai → what is the best movie; -2: Topic relevant but fail to cover most of the content of source sentence, e.g. what is some tips to learn english → when do you start to learn english; -1: Topic irrelevant or even can not un- derstand what it means.</p><p>There is token <ref type="bibr">[UNK]</ref> that stands for unknown token in paraphrase. Ones that contains <ref type="bibr">[UNK]</ref> should have both grammar and coherent score lower than 5. The grammar score should depend on other tokens in the paraphrase. The specific co- herent score depends on the impact of <ref type="bibr">[UNK]</ref> on that certain paraphrase. Here are some paraphrase examples given original question how can robot have human intelligence ?:</p><p>• paraphrase: how can [UNK] be intelligent ? coherent score: 1 This token prevent us from understanding the question and give proper answer. It causes serious information loss here;</p><p>• paraphrase: how can robot <ref type="bibr">[UNK]</ref> intelli- gent ? coherent score: 3 There is information loss, but the unknown token does not influence our understanding so much;</p><p>• paraphrase: how can robot be intelligent <ref type="bibr">[UNK]</ref> ? coherent score: 4</p><p>[UNK] basically does not influence under- standing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NOTED:</head><p>• Please decouple grammar and coherent as possible as you can. For instance, given a sentence is it true that girls like shopping, the paraphrase do girls like go go shopping can get a coherent score of 5 but a grammar score of only 3. But for the one you even can not understand, e.g., how is the go shopping of girls, you should give both of low grammar score and low coherent score, even it contains some topic-relevant words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Framework of RbM (Reinforced by Matching).</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,218.26,80.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning Process of RbM models: (a) RbM-SL, (b) RbM-IRL.</figDesc><graphic url="image-3.png" coords="5,298.77,93.10,226.77,70.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3</head><label></label><figDesc>https://code.google.com/archive/p/ word2vec/ 4 https://nlp.stanford.edu/projects/ glove/ mean value of reward, and we set λ as 0.1 by grid search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Statistics of datasets.</head><label>1</label><figDesc></figDesc><table>Generator 
Evaluator (RbM-SL) 

Dataset 
#Train #Test #Validation #Positive #Negative 

Quora-I 
100K 
30K 
3K 
100K 
160K 
Quora-II 
50K 
30K 
3K 
50K 
160K 
Twitter 
110K 
5K 
1K 
10K 
40K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Performances on Quora datasets.</head><label>2</label><figDesc></figDesc><table>Quora-I 
Quora-II 

Models 
ROUGE-1 ROUGE-2 BLEU METEOR 
ROUGE-1 ROUGE-2 BLEU METEOR 

Seq2Seq 
58.77 
31.47 
36.55 
26.28 
47.22 
20.72 
26.06 
20.35 
Residual LSTM 
59.21 
32.43 
37.38 
28.17 
48.55 
22.48 
27.32 
22.37 
VAE-SVG-eq 
-
-
-
25.50 
-
-
-
22.20 
Pointer-generator 
61.96 
36.07 
40.55 
30.21 
51.98 
25.16 
30.01 
24.31 
RL-ROUGE 
63.35 
37.33 
41.83 
30.96 
54.50 
27.50 
32.54 
25.67 

RbM-SL (ours) 
64.39 
38.11 
43.54 
32.84 
57.34 
31.09 
35.81 
28.12 
RbM-IRL (ours) 
64.02 
37.72 
43.09 
31.97 
56.86 
29.90 
34.79 
26.67 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Performances on Twitter corpus.</head><label>3</label><figDesc></figDesc><table>Twitter 

Models 
ROUGE-1 ROUGE-2 BLEU METEOR 

Seq2Seq 
30.43 
14.61 
30.54 
12.80 
Residual LSTM 
32.50 
16.86 
33.90 
13.65 
Pointer-generator 
38.31 
21.22 
40.37 
17.62 
RL-ROUGE 
40.16 
22.99 
42.73 
18.89 

RbM-SL (ours) 
41.87 
24.23 
44.67 
19.97 
RbM-IRL (ours) 
42.15 
24.73 
45.74 
20.18 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Human evaluation on Quora datasets. 

Quora-I 
Quora-II 

Models 
Relevance Fluency Relevance Fluency 

Pointer-generator 
3.23 
4.55 
2.34 
2.96 
RL-ROUGE 
3.56 
4.61 
2.58 
3.14 

RbM-SL (ours) 
4.08 
4.67 
3.20 
3.48 
RbM-IRL (ours) 
4.07 
4.69 
2.80 
3.53 

Reference 
4.69 
4.95 
4.68 
4.90 

</table></figure>

			<note place="foot" n="2"> https://www.kaggle.com/c/ quora-question-pairs</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by China National 973 Program 2014CB340301.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>Compute the value function by</p><p>Rescale the reward to ¯ Q t by (8);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head><p>Accumulate θ -gradient:</p><p>Update G θ using the gradient g θ with learning rate γ G :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Training Procedure of RbM- IRL</head><p>Input : A corpus of paraphrase pairs {(X, Y )}, a corpus of (non-parallel) sentences {X}. Output: Generator G θ , evaluator M φ 1 Pre-train the generator G θ with {(X, Y )}; 2 Init G θ := G θ and M φ ; 3 while not converge do <ref type="bibr">4</ref> while not converge do <ref type="bibr">5</ref> Sample a sentence X = [x 1 , . . . , x S ] from the paraphrase corpus; Train G θ with M φ as in Algorithm 1; 11 end <ref type="bibr">12</ref> Return G θ , M φ</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Paraphrase generation from latent-variable pcfgs for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training parsers by inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gergely</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="337" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daishi</forename><surname>Andrew Y Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural paraphrase generation with stacked residual lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monolingual machine translation for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum margin planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nathan D Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-domain semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">No metrics are perfect: Adversarial reward learning for visual storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Paranmt-50m: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning paraphrastic sentence embeddings from back-translated bitext</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning bilinear model for matching queries and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2519" to="2548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural generative question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Application-driven statistical paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combining multiple resources to improve smt-based paraphrasing model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Brian D Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anind K</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">• Do a Google search when you see any strange entity name such that you can make more appropriate judgement</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
