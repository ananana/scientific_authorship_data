<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3940" to="3949"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3940</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing text generation methods tend to produce repeated and &quot;boring&quot; expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for &quot;novel&quot; and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text generation is an important task in Natu- ral Language Processing (NLP) as it lays the foundation for many applications, such as dia- logue generation, machine translation ( <ref type="bibr" target="#b22">Ma et al., 2018b;</ref>), text summarization ( <ref type="bibr" target="#b21">Ma et al., 2018a)</ref>, and table summarization ( <ref type="bibr" target="#b18">Liu et al., 2017)</ref>. In these tasks, most of the systems are built upon the sequence-to-sequence paradigm <ref type="bibr" target="#b29">(Sutskever et al., 2014)</ref>, which is an end-to-end model that encodes a source sentence to a dense vector and then decodes the vector to a target sen- tence. The standard training method is based on Maximum Likelihood Estimation (MLE).</p><p>Although being widely applied, the conven- tional MLE training causes systems to repeatedly generate "boring" sentences, which usually are ex- pressions with high frequency (e.g., "I am sorry" in dialogue generation ( <ref type="bibr" target="#b13">Li et al., 2016)</ref>). The ma- jor reason is that MLE encourages the model to overproduce high-frequency words. <ref type="bibr">2</ref> The over- estimation of high-frequency words discourages the model from generating low-frequency but meaningful words in real data, which makes gen- erated text tend to be repeated and "boring".</p><p>To tackle this problem, we propose a new model for diversified text generation, called DP-GAN.</p><p>The key idea is to build a discriminator that is re- sponsible for giving reward to the generator based on the novelty of generated text. We consider the text that is frequently generated by the generator as the low-novelty text and the text that is uncom- mon in the generated data as the high-novelty text. Considering most of the real-world sentences are novel and fluent, we treat the real-world text as the positive example and the generated text as the negative example to train the discriminator. Such training mechanism encourages the discriminator to give higher reward for the text that looks like real-world data. The reward is fed back to the generator, which promotes the generator to gen- erate diverse and fluent text via policy gradient. In this framework, a good discriminator that can as- sign reasonable reward for the generator is a criti- cal component.</p><p>However, directly applying a classifier as the discriminator like most existing GAN models (e.g., <ref type="bibr">SeqGAN (Yu et al., 2017)</ref>) cannot achieve satisfactory performance. The main problem is that the reward given by the classifier cannot re- flect the novelty of text accurately. First, most existing classifier-based discriminators take the probability of a sequence being true as the reward. When a sentence fits the distribution of real-world text and is far from the generated data, the reward saturates and scarcely distinguishes the difference between these novel sentences. For example, for a sentence A with mildly high novelty and a sen- tence B with extremely high novelty, the classifier cannot tell the difference and gives them saturated reward: 0.997 and 0.998. Second, in our tasks, we find that a simple classifier can reach very high accuracy (almost 99%), which makes most gener- ated text receive reward around zero because the discriminator can identify them with high confi- dence. It shows that the classifier also cannot dis- tinguish the difference between low-novelty text. The reason for this problem is that the training ob- jective of the classifier-based GAN is in fact mini- mizing the Jensen-Shannon Divergence (JSD) be- tween the distributions of the real data and the gen- erated data <ref type="bibr" target="#b24">(Nowozin et al., 2016)</ref>. If the accuracy of classifier is too high, JSD fails to measure the distance between the two distributions, and cannot give reasonable reward to the model for generating real and diverse text ( .</p><p>Instead of using a classifier, we propose a novel language-model based discriminator and use the output of the language model, cross-entropy, as the reward. The main advantage of our model lies in that the cross-entropy based reward for novel text is high and does not saturate, while the reward for text with low novelty is small but discriminative. The analysis of the experimental results shows that our discriminator can better distinguish novel text compared with traditional classifier-based dis- criminators.</p><p>Our contributions are listed as follows:</p><p>• We propose a new model, called DP-GAN, for diversified text generation, which assigns low reward for repeated text and high reward for novel and fluent text.</p><p>• We propose a novel language-model based discriminator that can better distinguish novel text from repeated text without the sat- uration problem.</p><p>• The experimental results on review genera- tion and dialogue generation tasks show that our method can generate substantially more diverse and informative text than existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A great deal of attention has been paid to devel- oping data-driven methods for natural language dialogue generation. Conventional statistical ap- proaches tend to rely extensively on hand-crafted rules and templates, require interaction with hu- mans or simulated users to optimize parameters, or produce conversation responses in an information retrieval fashion. Such properties prevent training on the large corpora that are becoming increas- ingly available, or fail to produce novel natural language responses.</p><p>Currently, a popular model for text generation is the sequence-to-sequence model <ref type="bibr" target="#b29">(Sutskever et al., 2014;</ref>). However, the sequence- to-sequence model tends to generate short, repet- itive ( , and dull text ( <ref type="bibr" target="#b19">Luo et al., 2018)</ref>. Recent researches have focused on devel- oping methods to generate informative ( ) and diverse text ( <ref type="bibr" target="#b15">Li et al., 2017</ref><ref type="bibr" target="#b13">Li et al., , 2016</ref><ref type="bibr" target="#b12">Guu et al., 2017;</ref><ref type="bibr" target="#b28">Shao et al., 2017)</ref>. Reinforce- ment learning is incorporated into the model of conversation generation to generate more human- like speeches ( <ref type="bibr" target="#b15">Li et al., 2017)</ref>. Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, pro- totype editing, and self attention ( <ref type="bibr" target="#b13">Li et al., 2016;</ref><ref type="bibr" target="#b12">Guu et al., 2017;</ref><ref type="bibr" target="#b28">Shao et al., 2017)</ref>.</p><p>In this paper, to handle this problem, we pro- pose to use adversarial training ( <ref type="bibr" target="#b10">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b8">Denton et al., 2015;</ref><ref type="bibr" target="#b15">Li et al., 2017</ref>), which has achieved success in image generation <ref type="bibr" target="#b25">(Radford et al., 2015;</ref><ref type="bibr" target="#b11">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b5">Berthelot et al., 2017)</ref>. However, training GAN is a non-trivial task and there are some previ- ous researches that investigate methods to improve training performance, such as Wasserstein GAN (WGAN) (  and Energy- based GAN (EGAN) ( <ref type="bibr" target="#b27">Salimans et al., 2016;</ref><ref type="bibr" target="#b11">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b35">Zhao et al., 2017;</ref><ref type="bibr" target="#b5">Berthelot et al., 2017)</ref>. GAN in text generation has not shown significant improvement as it has in com- puter vision. This is partially because text gen- eration is a process of sampling in discrete space where the normal gradient descent solution is not available, which makes it difficult to train. There are some researches that focus on tackling this problem. <ref type="bibr">SeqGAN (Yu et al., 2017)</ref> incorpo- rates the policy gradient into the model by treating the procedure of generation as a stochastic policy in reinforcement learning. <ref type="bibr" target="#b26">Ranzato et al. (2016)</ref>  trains the sequence-to-sequence model with pol- icy gradient for neural machine translation. <ref type="bibr">Bahdanau et al. (2017)</ref> applies the actor-critic model on the same task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Diversity-Promoting GAN</head><p>The basic structure of our DP-GAN contains a generator that is responsible for generating text and a discriminator that discriminates between the generated text and the real text. The sketch of DP- GAN is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The generator G θ is based on a sequence-to- sequence structure. Given a sentence as input, the generator is capable of generating long text, which contains multiple sentences of various lengths. To put it formally, given the input sentence x 1:m = (x 1 , x 2 , x 3 , ..., x m ) of m words from Γ, the vo- cabulary of words, the model generates the text of T sentences Y 1:T = (y 1 , ..., y t , ..., y T ), where y t from Λ, the set of candidate sentence. The term y t = (y t,1 , ..., y t,K ) is the t th sentence, where y t,K is the K th word.</p><p>The discriminator D φ is a language model. The output of the language model, cross entropy, is de- fined as the reward to train the generator. Our re- ward consists of two parts, the reward at the sen- tence level and that at the word level. With the discriminator and the reward function, we train the generator by reinforcement learning. A sketch of training DP-GAN is shown in Algorithm 1. The details are described as follows.</p><p>Algorithm 1 The adversarial reinforcement learn- ing algorithm for training the generator G θ and the discriminator D φ .</p><p>1: Initialize G θ , D φ with random weights θ, φ 2: Pre-train G θ using MLE on a sequence dataset D = (X, Y ) 3: Generate samples using G θ for training D φ 4: Pre-train D φ by Eq. <ref type="formula" target="#formula_1">(1)</ref> 5: N = number of training iterations 6: M = number of training generator 7: K = number of training discriminator 8: for each i = 1, 2, ..., N do 9:</p><p>for each j = 1, 2, ..., M do 10:</p><p>Generate a sequence Y1:T ∼ G θ 11:</p><p>Compute rewards by Eq. <ref type="formula">(2)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generator</head><p>For the concern of real-world applications, this pa- per assumes that the output of the model can be long text made up of multiple sentences. In order to generate multiple sentences, we build a standard hierarchical LSTM decoder ( <ref type="bibr" target="#b14">Li et al., 2015</ref>). The two layers of the LSTM are structured hierarchi- cally. The bottom layer decodes the sentence rep- resentation and the top layer decodes each word based on the output of the bottom layer. The atten- tion mechanism is used for word decoding ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discriminator</head><p>Most existing GAN models use a binary classi- fier as the discriminator. The probability of be- ing true is regarded as the reward ( <ref type="bibr" target="#b13">Li et al., 2016;</ref><ref type="bibr" target="#b34">Yu et al., 2017)</ref>. Different from that, we pro- pose a language-model based discriminator D φ that builds on a unidirectional LSTM. We use the output of the language model, cross-entropy, as the reward. Specifically, given a sentence y t , the cross-entropy based reward for the k th word is cal- culated as</p><formula xml:id="formula_0">R(y t,k ) = − log D φ (y t,k |y t,&lt;k )</formula><p>We maximize the reward of real-world text and minimize the reward of generated text to train the discriminator. The reason of minimizing the re- ward of generated text is that, we expect the text that is repeatedly generated by the generator can be identified by the discriminator and get lower reward. The motivation of maximizing the reward of real-world data lies in that, we expect not only the uncommon text in the generated data can get high reward, but also low-quality text can be pun- ished to some extend. Considering the real-world text is diverse and fluent, we maximize the reward of real-world text to encourage the discriminator to give high reward for the text that looks like the real-world data. Therefore, such training mecha- nism avoids the problem of novel but low-quality text getting high reward. The loss function of the discriminator is formulated as follows:</p><formula xml:id="formula_1">J(φ) = − (E Y ∼p data [R(Y )] − E Y ∼G θ [R(Y )])<label>(1)</label></formula><p>where R(Y ) stands for the averaged reward of Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reward</head><p>Our reward function consists of two parts, the sentence-level reward and the word-level reward, which are illustrated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Sentence-Level Reward</head><p>For a sentence y t of K words, the reward at the sentence level is the averaged reward of each word:</p><formula xml:id="formula_2">R(y t ) = − 1 K K k=1 log D φ (y t,k |y t,&lt;k ) (2)</formula><p>In contrast, the reward of the existing classifier- based discriminators ( <ref type="bibr" target="#b13">Li et al., 2016;</ref><ref type="bibr" target="#b34">Yu et al., 2017</ref>) is calculated as follows:</p><formula xml:id="formula_3">R(y t ) = D φ (true|y t )</formula><p>where D φ is a binary classifier judging how likely y t is from the real-world data.</p><p>The major problem of the classifier-based discriminator is that the reward cannot reflect the novelty of text accurately. First, the re- ward for high-novelty text is easy to saturate, which scarcely distinguishes the difference be- tween novel text. Second, we find that the discrim- inator can easily achieve very high accuracy on identifying the generated text, which makes most of them get reward around zero. It shows that the classifier still cannot tell the difference between the text with low novelty.</p><p>On the contrary, the analysis of experimen- tal result shows that our proposed discriminator can better distinguish high-novelty text from low- novelty text without the saturation problem. The reward for high-novelty text is high and does not saturate while the reward for low-novelty text is small but discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Word-Level Reward</head><p>Considering that the reward for different words in a sentence y t should be different, we further pro- pose to use the reward at the word level as follows:</p><formula xml:id="formula_4">R(y t,k |y t,&lt;k ) = − log D φ (y t,k |y t,&lt;k ) (3)</formula><p>It can be found that the classifier-based discrim- inator only provides reward for the finished se- quence. Thus, for a sequence of length T , to eval- uate the action-value for a word at the time step t, Monte Carlo Search (MCS) with a roll-out pol- icy G θ is usually applied to sample the unknown last T − t tokens ( <ref type="bibr" target="#b34">Yu et al., 2017)</ref>. However, this could be computationally expensive because the time complexity is O(T 2 ). On the contrary, our discriminator can calculate the reward of all words with the time complexity of O(T ), which is more computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Policy Gradient Training</head><p>The loss function of the generator (policy) is to maximize the reward from the start state s 0 to the end state ( <ref type="bibr" target="#b30">Sutton et al., 1999</ref>):</p><formula xml:id="formula_5">J(θ) = T t=1 E[R t,K |s t−1 , θ] = T t=1 y t,1 G θ (y t,1 |s t−1 )Q G θ D φ (s t−1 , y t,1 )<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">R t,K = K k=1 γ k−1 R(y t )R(y t,k )</formula><p>is the total reward for a complete sentence, including both the sentence-level and the word-level re- wards. The term Q G θ D φ (s t−1 , y t,1 ) is estimated by R t,1 . The term γ is the discount rate and s t is the initial state.</p><p>In this paper, we use the policy gradient method <ref type="bibr" target="#b31">(Williams, 1992)</ref>. The gradient of Eq. <ref type="formula" target="#formula_5">(4)</ref> is approximated as follows:</p><formula xml:id="formula_7">θ J(θ) T t=1 K k=1 γ k−1 R t,k θ log G θ (y t,k |y t,&lt;k )<label>(5)</label></formula><p>where R t,k = K i=k γ i−1 R(y t )R(y t,i ) is the total reward starting from step k.</p><p>Following previous work ( <ref type="bibr" target="#b15">Li et al., 2017</ref>), we also use teacher forcing ( <ref type="bibr" target="#b4">Bengio et al., 2015)</ref> to train the generator. In teacher forcing, the decoder receives the real-world text as input at each time step. The loss function of teacher forcing is the same with that of policy gradient training. The only difference is that the text is generated from G θ in policy gradient training but from the real data in teacher forcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We evaluate DP-GAN on two real-world natu- ral language generation tasks, review generation and dialogue generation. We first introduce the dataset, the training details, the baselines, and the evaluation metrics. Then, we compare our model with the state-of-the-art models. Finally, we show the experimental results and provide the detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Yelp Review Generation Dataset (Yelp): This dataset is provided by Yelp Dataset Challenge. <ref type="bibr">3</ref> In our version of review generation, the model should generate a paragraph based on a given sentence. We build a new dataset for this task by splitting the data into two parts. In each review, we take the first sentence as the input text, and the following sentences as the target text. The processed Yelp dataset contains 1,400K, 400K, and 12K pairs for training, validation, and testing, respectively. Amazon Review Generation Dataset (Ama- zon): This dataset is provided by <ref type="bibr" target="#b23">McAuley and Leskovec (2013)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.6K DP-GAN(W)</head><p>79.4K 1.9K 7.7K 11.4K 6.0K DP-GAN(SW) 97.3K 2.1K 10.8K 19.1K 8.0K <ref type="table">Table 1</ref>: Performance of the DP-GAN and three base- lines on review generation and dialogue generation tasks. Higher is better. DP-GAN(S), DP-GAN(W), and DP-GAN(SW) represent DP-GAN with only sentence- level reward, only word-level reward, and combined reward, respectively. Token represents the number of generated words. Dist-1, Dist-2, Dist-3, and Dist-S are respectively the number of distinct unigrams, bigrams, trigrms, and sentences in the generated text. For exam- ple, 1.2K in Dist-1 means 1200 distinct unigrams. whose response is shorter than 5 words. We ran- domly sample 1,800K, 500K, and 12K turns for training, validation, and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare the proposed DP-GAN with the fol- lowing baseline models: MLE: The generator is a sequence-to-sequence model. The generator is trained with traditional MLE. PG-BLEU: The generator is a sequence-to- sequence model. It is trained by policy gradient with the BLEU score of the generated text as the reward ( <ref type="bibr">Bahdanau et al., 2017</ref>). The advantage is that this model can directly optimize the task- specific score: BLEU. SeqGAN: Sequence GAN ( <ref type="bibr" target="#b34">Yu et al., 2017</ref>) uses a binary classifier as the discriminator. Since it is originally for unconditional generation, for a fair comparison, we expand it to the version of condi- tional generation. We re-implement the generator by replacing a language model with a sequence- to-sequence model.  <ref type="table">Table 2</ref>: Results of human evaluation on the three datasets. The score represents the averaged ranking of each model and lower is better. All represents the rank- ing given by annotators based on a comprehensive con- sideration. It can be seen that DP-GAN results in the largest improvement in terms of diversity and relevance while slightly reducing fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details</head><p>For review generation, we set the number of gen- erated sentences to 6 with the maximum length of 40 words for each generated sentence. Based on the performance on the validation set, we set the hidden size to 256, embedding size to 128, vocab- ulary size to 50K, and batch size to 64 for the pro- posed model and the baselines. We use the Ada- grad ( <ref type="bibr" target="#b9">Duchi et al., 2011</ref>) optimizer with the ini- tial learning rate 0.1. In adversarial training, the step for training the generator is 1K, the step for training the discriminator is 5K. Both the gener- ator and the discriminator are pre-trained for 10 epochs before adversarial learning. In particular, for PG-BLEU and SeqGAN, before reinforcement learning or adversarial learning, we pre-train the sequence-to-sequence model for 10 epochs like DP-GAN. For dialogue generation, the settings are the same with review generation, except that we set the number of generated sentences to 1 with the maximum length of 40 words because there is only one sentence in the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>We conduct two kinds of evaluations in this work, automatic evaluation and human evaluation. The details of evaluation results are shown as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Automatic Evaluation</head><p>We evaluate the proposed model in terms of sev- eral metrics that can reflect the diversity. The results are shown in <ref type="table">Table 1</ref>. Token represents the total number of generated words. Dist- 1, Dist-2, Dist-3, and Dist-S are respectively the number of distinct unigrams, bigrams, tri- grms, and sentences. DP-GAN(S), DP-GAN(W), and DP-GAN(SW) represent DP-GAN with only sentence-level reward, only word-level reward, and combined reward, respectively. From the re- sults, it is obvious that the proposed model sub- stantially outperforms the existing models. PG- BLEU achieves slightly weaker results compared with MLE. The reason is that PG-BLEU uses BLEU score as the reward for reinforcement learn- ing. However, the BLEU score is low for most of the generated text. The low reward makes it hard to learn from the real data. SeqGAN does not achieve better results, which suggests that the classifier-based discriminator fails to encourage the generator to produce diverse text.</p><p>In terms of the total number of generated words, DP-GAN(S) achieves better results than DP- GAN(W). Since the sentence-level reward reflects the novelty of the whole sentence, it gives repeated and short text low reward while novel and longer text high reward. Thus, the generator is encour- aged to generate novel text. In terms of the number of distinct n-grams, DP-GAN(W) achieves better results than DP-GAN(S). It is because the word- level reward gives each word more precise score and novel n-grams could be better encouraged. As we can see, DP-GAN(SW), which combines the advantages of sentence-level and word-level re- wards, generates not only more diverse n-grams than DP-GAN(S) but also longer text than DP- GAN(W). Since combining the word-level and sentence-level rewards achieves better results than using just one of them, we focus more on the com- bined reward in the following parts.</p><p>In review generation and dialogue generation tasks, it is a widely debated question how well the BLEU score against a single reference can reflect the quality of the generated text <ref type="figure" target="#fig_0">(Liu et al., 2016)</ref>. Thus, although the proposed model achieves better BLEU scores compared with baselines, we omit the detailed comparisons in terms of BLEU for space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Human Evaluation</head><p>We conduct a human evaluation on the test set. For all tasks, we randomly extract 200 samples from the test sets. Each item contains the input text and the text generated by the different sys- tems. The items are distributed to three anno- Generated Data: Love it ! Generated Data: Both had the brisket and it was delicious.</p><p>True Data: Lyons Roofing needs to spend less effort on charming their customers and concentrate on their lack of business ethics and skill.</p><p>True Data: Now after a revamp, redecoration, renaming and it becomes another southeast asian restaurant.</p><p>Figure 2: Distribution of rewards between SeqGAN and DP-GAN. The upper two sentences are sampled from the real-world data and the lower two sentences are sampled from the generated data. It is important to note that the sentence-level reward of DP-GAN is averaged word-level reward and a long sentence does not indicate a high score. As we can see, the reward distribution of SeqGAN saturates and cannot distinguish the novelty of the text accurately. In contrast, DP-GAN has a strong ability of resisting reward saturation and can give more precise reward for text in terms of novelty. tators who have no knowledge about which sys- tem the text is from. Following the work of <ref type="bibr" target="#b15">Li et al. (2017)</ref>, we require them to rank the gener- ated text considering relevance, diversity, and flu- ency. It is important to note that all the annotators have linguistic background. Relevance means that how likely the generated text is related to the input text. Diversity means that how much the gener- ated text provides specific information, rather than "dull" and repeated information. Fluency means that how likely the generated text is produced by human. All represents the ranking given by anno- tators based on a comprehensive consideration of all human evaluation metrics. The results of hu- man evaluation are shown in <ref type="table">Table 2</ref>. It needs to be mentioned that in the special case that several pieces of generated text are exactly the same, they are given the same ranking. The inter-annotator agreement is satisfactory considering the difficulty in the human evaluation. The Pearson's correla- tion coefficient is 0.76 on diversity, 0.59 on flu- ency and 0.27 on relevance, with p &lt; 0.0001.</p><p>The p-value is all below 0.001, indicating high correlation and agreement. <ref type="table">Table 2</ref> shows that DP-GAN brings the largest improvement in terms of diversity and relevance while scarcely reducing fluency. First, DP-GAN significantly outperforms baselines in term of diversity. Second, DP-GAN achieves the best performance in terms of rele- vance. Diverse text generated by DP-GAN brings more specific expressions, increasing the perfor- mance of relevance. For example, given input sentence"food is good", the relevance of "love it, beef is the best" is obviously better than that of "love it". Third, the fluency of DP-GAN is slightly worse than that of baselines. It is mainly due to the fact that the text generated by DP-GAN is much longer. From <ref type="table">Table 1</ref>, we can see that the number of words generated by DP-GAN is almost three times than that of baselines in the Yelp dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Analysis: Why It Works</head><p>In this section, we provide detailed analysis to see why our proposed method works better. In <ref type="figure">Figure 2</ref>, we demonstrate the reward distri- butions of our model and SeqGAN. It can be seen that the reward of SeqGAN cannot reflect the nov- elty of text accurately. First, when the novelty of text is relatively high, the reward given by Seq- GAN saturates and cannot tell the difference be- tween high-novelty text. Even with different de- grees of novelty, the two high-novelty sentences sampled from the true data in the figure get nearly Input: One of my favorite places to eat. MLE: Service is great. PG-BLEU: Service is always good. SeqGAN: Love the chicken and waffles. Service is always great. DP-GAN: Love the fact that they have a large selection of food. Service is always great and the food is always fresh. I've been to this place a few times and have never been disappointed. Input: Excellent service, clean, nice ambience. MLE: Food is very good. PG-BLEU: Food is very good. SeqGAN: Food is very good. DP-GAN: I had the chicken and beef and it was delicious. Food is good and the service is great. I would recommend this place to anyone who wants to eat here. Input: I brought friends -seven of them -who'd never been to a Bar Louie before. MLE: Food was mediocre, but the service was great. I will not be returning. PG-BLEU: Food was terrible. Wouldn't recommend this place to anyone. SeqGAN: Food was just okay. I wouldn't recommend this place. DP-GAN: Ordered a chicken salad with a side of fries, and it was just ok. Food was good, but the service was slow. I had to ask for a refill on my drink, and the bartender was rude. I wouldn't recommend this place to anyone. Input: I don't think it's too spicy, so i add a little hot curry spice to the sauce. MLE: It's great for cooking. I have to say, I'm not sure if it is the same. PG-BLEU: Love it! SeqGAN: Love it! Love this stuff. I have used it for years and it is very good. DP-GAN:I've tried many different brands of hot sauces but this one is the best. I've also used it in soups and stews. I also like that it is organic and has a great flavor. <ref type="table">Table 3</ref>: Examples generated by DP-GAN and the state-of-the-art models on the Yelp dataset. It can be found that the text generated by baselines is more generic and repeated, while our model generates text with more specific details and higher diversity. the same reward in SeqGAN. Second, most gen- erated text receives reward around zero because of the high accuracy of classifier. It is hard for such reward to distinguish the difference between low- novelty text. For example, as shown in the fig- ure, "Both had the brisket and it was delicious" is much more informative than "Love it! ". The discriminator of SeqGAN gives them practically the same reward, while the proposed discriminator can better distinguish the two sentences in terms of novelty. In fact, the classifier in SeqGAN trained for 10 epochs can reach very high accuracy, that is, 98.35% and 99.63% for Yelp and Amazon, re- spectively. If the accuracy of classifier is too high, the classifier cannot give reasonable reward to the generator for generating real and diverse text ).</p><p>In contrast, the language-model based reward given by DP-GAN better reflect the novelty of the text. The novel text is given high reward that does not saturate. The generated data, which can be less novel, is given relatively low but nonzero re- ward that can encourage the generator to generate diverse expressions. The refined reward leads to more efficient training, thus resulting in better per- formance.</p><p>We also compare the cosine similarity between the real-world data distribution and the generated data distributions of various models. <ref type="figure" target="#fig_1">Figure 3</ref> shows the results. We calculate the cosine distance between two vectors, where each element is the frequency of a word indexed by its rank in real- world data. For example, the first element in the vector means the frequency of the word that ranks first in real-world data. The word frequency vec- tor is divided into 4 vectors to show the similarity of words of different frequencies. The distribution of words are more similar when they occur more frequently in real-world data. As DP-GAN pro- motes diversity, words of low frequency in real- world data are better learned and the similarity is much better than that of MLE. In all, the generated data distribution of DP-GAN is closer to the real- world data distribution in all intervals, especially considering the words of low frequency. <ref type="table">Table 3</ref> presents the examples generated by dif- ferent models on the Yelp dataset. It can be found that the text generated by MLE is more generic and repeated, while PG-BLEU and SeqGAN do not perform obviously better than MLE. More- over, it can be clearly seen that our model gen- erates text with more specific details and higher diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a new model, called DP- GAN, to promote the diversity of the generated text. DP-GAN assigns low reward for repeated text and high reward for novel and fluent text, en- couraging the generator to produce novel and di- verse text. We evaluate DP-GAN on two tasks and the findings are concluded as follows: First, the proposed method substantially outperforms the baseline methods in automatic and human evalu- ations. It shows that DP-GAN is capable of pro- ducing more diverse and informative text. Second, the proposed discriminator can better distinguish novel text from repeated text with the saturation problem compared without traditional classifier- based discriminators. Third, with the improve- ment of diversity, the generated data distribution of DP-GAN is closer to the real-world data distri- bution compared with that of MLE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of DP-GAN. Lower: The generator is trained by policy gradient where the reward is provided by the discriminator. Upper: The discriminator is based on the language model trained over the real text and the generated text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Cosine similarity between the real-world data distribution and the generated data distributions of various models. For example, the first column represents the cosine similarity on top 500 words with the highest frequencies in real-world data. As we can see, the generated data distribution of DP-GAN is closer to the real-world data distribution, especially considering the words of low frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>. It consists of review information of fine foods from Amazon. Like Yelp, we process this dataset by extracting the first sentence as the source text and the rest as the target text. The pro- cessed Amazon dataset contains 400K, 100K, and 12K pairs for training, validation, and testing, re- spectively. OpenSubtitles Dialogue Dataset (Dialogue): This dataset 4 is used for dialogue generation. Fol- lowing previous work, we treat each turn in the dataset as the target text and the two previous sen- tences as the source text. We remove the pairs</figDesc><table>3 https://www.yelp.com/dataset/ 
challenge 
4 http://opus.lingfil.uu.se/ 
OpenSubtitles.php 

Yelp 
Token Dist-1 Dist-2 Dist-3 Dist-S 
MLE 
151.2K 1.2K 3.9K 6.6K 3.9K 
PG-BLEU 
131.1K 1.1K 3.3K 5.5K 3.1K 
SeqGAN 
140.5K 1.1K 3.5K 6.1K 3.6K 
DP-GAN(S) 
438.6K 1.7K 7.5K 15.7K 10.6K 
DP-GAN(W) 
271.9K 2.8K 14.8K 29.0K 12.6K 
DP-GAN(SW) 406.8K 3.4K 22.3K 49.6K 17.3K 
Amazon 
Token Dist-1 Dist-2 Dist-3 Dist-S 
MLE 
176.1K 0.6K 2.1K 3.5K 2.6K 
PG-BLEU 
124.5K 0.6K 1.9K 3.5K 2.3K 
SeqGAN 
217.3K 0.7K 2.6K 4.6K 3.2K 
DP-GAN(S) 
467.6K 0.8K 3.6K 7.6K 7.0K 
DP-GAN(W) 
279.4K 1.6K 8.9K 18.4K 9.6K 
DP-GAN(SW) 383.6K 1.9K 11.7K 26.3K 13.6K 
Dialogue 
Token Dist-1 Dist-2 Dist-3 Dist-S 
MLE 
81.1K 1.4K 4.4K 6.3K 4.1K 
PG-BLEU 
97.9K 1.2K 3.9K 5.5K 3.3K 
SeqGAN 
83.4K 1.4K 4.5K 6.5K 4.5K 
DP-GAN(S) 
112.2K 1.5K 5.2K 8.5K 5</table></figure>

			<note place="foot" n="1"> The code is available at https://github.com/ lancopku/DPGAN</note>

			<note place="foot" n="2"> For example, the frequency ratios of &quot;the&quot;, &quot;and&quot;, &quot;was&quot; are 4.2%, 3.2%, 1.5% in real data, and they go up to 7.1%, 4.6%, 5.3% in the MLE generated data on our review generation task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natu-ral Science Foundation of China (No. 61673028). We thank Wei Li and Bingzhen Wei for providing the thoughtful suggestions. Xu Sun is the corre-sponding author of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<pubPlace>Aaron C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BEGAN: boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno>abs/1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno>abs/1709.08878</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2157" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<idno>abs/1805.03989</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Table-to-text generation by structure-aware seq2seq learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno>abs/1711.09724</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An auto-encoder matching model for learning utterance-level semantic dependency in dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Autoencoder as assistant supervisor: Improving text representation for chinese social media text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1805.04869</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bag-of-words as target for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1805.04871</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="897" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generating high-quality and informative conversation responses with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2210" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 1999</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A skeleton-based model for promoting coherence among sentences in narrative story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
