<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a Deep Hybrid Model for Semi-Supervised Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G Ororbia</forename><surname>Ii</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Sciences and Technology</orgName>
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Sciences and Technology</orgName>
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Sciences and Technology</orgName>
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a Deep Hybrid Model for Semi-Supervised Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel fine-tuning algorithm in a deep hybrid architecture for semi-supervised text classification. During each increment of the online learning process , the fine-tuning algorithm serves as a top-down mechanism for pseudo-jointly modifying model parameters following a bottom-up generative learning pass. The resulting model, trained under what we call the Bottom-Up-Top-Down learning algorithm , is shown to outperform a variety of competitive models and baselines trained across a wide range of splits between supervised and unsupervised training data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent breakthroughs in learning expressive neu- ral architectures have addressed challenging prob- lems in domains such as computer vision, speech recognition, and natural language processing. This success is owed to the representational power af- forded by deeper architectures supported by long- standing theoretical arguments <ref type="bibr" target="#b11">(Hastad, 1987)</ref>. These architectures efficiently model complex, highly varying functions via multiple layers of non-linearities, which would otherwise require very "wide" shallow models that need large quan- tities of samples <ref type="bibr" target="#b1">(Bengio, 2012)</ref>. However, many of these deeper models have relied on mini-batch training on large-scale, labeled data-sets, either us- ing unsupervised pre-training ( <ref type="bibr" target="#b0">Bengio et al., 2007)</ref> or improved architectural components (such as ac- tivation functions) <ref type="bibr" target="#b30">(Schmidhuber, 2015)</ref>.</p><p>In an online learning problem, samples are pre- sented to the learning architecture at a given rate (usually with one-time access to these data points), and, as in the case of a web crawling agent, most of these are unlabeled. Given this, batch training and supervised learning frameworks are no longer applicable. While incremental approaches such as co-training have been employed to help these models learn in a more update-able fashion <ref type="bibr" target="#b2">(Blum and Mitchell, 1998;</ref><ref type="bibr" target="#b10">Gollapalli et al., 2013)</ref>, neural architectures can naturally be trained in an online manner through the use of stochastic gradient de- scent (SGD).</p><p>Semi-supervised online learning does not only address practical applications, but it also reflects some challenges of human category acquisition <ref type="bibr" target="#b34">(Tomasello, 2001)</ref>. Consider the case of a child learning to discriminate between object categories and mapping them to words, given only a small amount of explicitly labeled data (the mother pointing to the object), and a large portion of un- supervised learning, where the child comprehends an adult's speech or experiences positive feedback for his or her own utterances regardless of their correctness. The original argument in this respect applied to grammar (e.g., <ref type="bibr" target="#b5">Chomsky, 1980;</ref><ref type="bibr" target="#b25">Pullum &amp; Scholz, 2002</ref>). While neural networks are not necessarily models of actual cognitive processes, semi-supervised models can show learnability and illustrate possible constraints inherent to the learn- ing process.</p><p>The contribution of this paper is the develop- ment of the Bottom-Up-Top-Down learning al- gorithm for training a Stacked Boltzmann Ex- perts Network (SBEN) <ref type="bibr" target="#b24">(Ororbia II et al., 2015)</ref> hybrid architecture. This procedure combines our proposed top-down fine-tuning procedure for jointly modifying the parameters of a SBEN with a modified form of the model's original layer-wise bottom-up learning pass <ref type="bibr" target="#b24">(Ororbia II et al., 2015)</ref>. We investigate the performance of the constructed deep model when applied to semi-supervised text classification problems and find that our hybrid ar- chitecture outperforms all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>471</head><p>Recent successes in the domain of connection- ist learning stem from the expressive power af- forded by models, such as the Deep Belief Net- work (DBN) ( <ref type="bibr" target="#b12">Hinton et al., 2006;</ref><ref type="bibr" target="#b0">Bengio et al., 2007)</ref> or Stacked Denoising Autoencoder ( <ref type="bibr" target="#b36">Vincent et al., 2010)</ref>, that greedily learn layers of stacked non-linear feature detectors, equivalent to levels of abstraction of the original representation. In a va- riety of language-based problems, deep architec- tures have outperformed popular shallow models and classifiers <ref type="bibr" target="#b26">(Salakhutdinov and Hinton, 2009;</ref><ref type="bibr" target="#b19">Liu, 2010;</ref><ref type="bibr" target="#b32">Socher et al., 2011;</ref><ref type="bibr" target="#b9">Glorot et al., 2011b;</ref><ref type="bibr" target="#b20">Lu and Li, 2013;</ref><ref type="bibr" target="#b21">Lu et al., 2014</ref>). How- ever, these architectures often operate in a multi- stage learning process, where a generative archi- tecture is pre-trained and then used to initialize pa- rameters of a second architecture that can be dis- criminatively fine-tuned (using back-propagation of errors or drop-out: <ref type="bibr" target="#b13">Hinton et al., 2012)</ref>. Sev- eral ideas have been proposed to help deep mod- els deal with potentially uncooperative input dis- tributions or encourage learning of discriminative information earlier in the process, many leverag- ing auxiliary models in various ways <ref type="bibr" target="#b0">(Bengio et al., 2007;</ref>). A few methods for adapting deep architecture con- struction to an incremental learning setting have also been proposed ( <ref type="bibr" target="#b3">Calandra et al., 2012;</ref><ref type="bibr" target="#b38">Zhou et al., 2012</ref>). Recently, it was shown in <ref type="bibr" target="#b24">(Ororbia II et al., 2015</ref>) that deep hybrid architectures, or multi-level models that integrate discriminative and generative learning objectives, offer a strong viable alternative to multi-stage learners and are readily usable for categorization tasks.</p><p>For text-based classification, a dominating model is the support vector machine (SVM) <ref type="bibr" target="#b6">(Cortes and Vapnik, 1995</ref>) with many useful in- novations to yet further improve its discrimina- tive performance <ref type="bibr" target="#b33">(Subramanya and Bilmes, 2008)</ref>. When used in tandem with prior human knowl- edge to hand-craft good features, this simple ar- chitecture has proven effective in solving practical text-based tasks, such as academic document clas- sification ( <ref type="bibr" target="#b4">Caragea et al., 2014)</ref>. However, while model construction may be fast (especially when using a linear kernel), this process is costly in that it requires a great deal of human labor to an- notate the training corpus. Our approach, which builds on that of <ref type="bibr" target="#b24">(Ororbia II et al., 2015</ref>), provides a means for improving classification performance when labeled data is in scarce supply, learning structure and regularity within the text to reduce classification error incrementally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Deep Hybrid Model for Semi-Supervised Learning</head><p>To directly handle the problem of discriminative learning when labeled data is scarce, <ref type="bibr" target="#b24">(Ororbia II et al., 2015)</ref> proposed deep hybrid architectures that could effectively leverage small amounts of labeled and large amounts of unlabeled data. In particular, the best-performing architecture was the Stacked Boltzmann Experts Network (SBEN), which is a variant of the DBN. In its construction and training, the SBEN design borrows many re- cent insights from efficiently learning good DBN models ( <ref type="bibr" target="#b12">Hinton et al., 2006</ref>) and is essentially a stack of building block models where each layer of model parameters is greedily modified while freezing the parameters of all others. In con- trast to the DBN, which stacks restricted Boltz- mann machines (RBM's) and is often used to ini- tialize a deep multi-layer perceptron (MLP), the SBEN model is constructed by composing hybrid restricted Boltzmann machines and can be directly applied to the discriminative task in a single learn- ing phase. The hybrid restricted Boltzmann machine (HRBM) ( <ref type="bibr" target="#b29">Schmah et al., 2008;</ref><ref type="bibr" target="#b15">Larochelle and Bengio, 2008;</ref><ref type="bibr" target="#b16">Larochelle et al., 2012</ref>) building block of the SBEN is itself an extension of the RBM meant to ultimately perform classification. The HRBM graphical model is defined via pa- rameters Θ = (W, U, b, c, d) (where W is the input-to-hidden weight matrix, U the hidden-to- class weight matrix, b is the visible bias vector, c is the hidden unit bias vector, and d is the class unit bias vector), and is a model of the joint distribu- tion of a binary feature vector x = (x 1 , · · · , x D ) and its label y ∈ {1, · · · , C} that makes use of a latent variable set h = (h 1 , · · · , h H ). The model assigns a probability to the triplet (y,x,h) using:</p><formula xml:id="formula_0">p(y, x, h) = e −E(y,x,h) Z ,<label>(1)</label></formula><formula xml:id="formula_1">p(y, x) = 1 Z h e −E(y,x,h)<label>(2)</label></formula><p>where Z is known as the partition function. The model's energy function is defined as</p><formula xml:id="formula_2">E(y, x, h) = −h T Wx−b T x−c T h−d T e y −h T Ue y . (3) where e y = (1 i=y ) C i=1</formula><p>is the one-hot vector en- coding of y. It is often not possible to compute p(y, x, h) or the marginal p(y, x) due to the in- tractable normalization constant. However, ex- ploiting the model's lack of intra-layer connec- tions, block Gibbs sampling may be used to draw samples of the HRBM's latent variable layer given the current state of the visible layer and vice versa. This yields the following equations:</p><formula xml:id="formula_3">p(h|y, x) = j p(h j |y, x), p(h j = 1|y, x) = σ(c j + U jy + i W ji x i )<label>(4)</label></formula><formula xml:id="formula_4">p(x|h) = i p(x i |h), p(x i = 1|h) = σ(b i + j W ji h j )<label>(5)</label></formula><formula xml:id="formula_5">p(y|h) = e dy+ j U jy h j y e d y + j U jy h j<label>(6)</label></formula><p>where σ(v) = 1/(1 + e −v ). Classification may be performed directly with the HRBM by using its free energy function F (y, x) to compute the con- ditional distribution</p><formula xml:id="formula_6">p(y|x) = e −F (y,x) y ∈{1,··· ,C} e −F (y ,x) (7)</formula><p>where the free energy is formally defined as</p><formula xml:id="formula_7">−F (y, x) = (d y + j ψ(c j + U jy + W ji x i )) (8) and ψ is the softplus activation function ψ(v) = log(1 + e v ).</formula><p>To construct an N-layer SBEN (or N-SBEN), as was shown in <ref type="bibr" target="#b24">(Ororbia II et al., 2015)</ref>, one may learn a stack of HRBMs in one of two ways: (1) in a strict greedy, layer-wise manner, where lay- ers are each trained in isolation on all of the data samples one at a time from the bottom-up; or (2) in a more relaxed disjoint fashion, where all layers are trained together on all of the data but still in a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ensembling of Layer-Wise Experts</head><p>The SBEN may be viewed as a natural vertical en- semble of layer-wise "experts", where each layer maps latent representations to predictions, which differs from standard methods such as boosting <ref type="bibr" target="#b28">(Schapire, 1990)</ref>. Traditional feedforward neural models propagate data through the final network to obtain an output prediction y t from a penulti- mate layer for a given x t . In contrast, this hybrid model is capable of a producing a label y n t at each level n for x t .</p><p>To vertically aggregate layer-wise expert out- puts, we compute a simple mean predictor, p(y|x) ensemble , as follows:</p><formula xml:id="formula_8">p(y|x) ensemble = 1 N N n=1 p(y|x) n (9)</formula><p>This ensembling scheme provides a simple way to incorporate acquired discriminative knowledge of different levels of abstraction into the model's fi- nal prediction. We note that the SBEN's inherent layer-wise discriminative ability stands as an alter- native to coupling helper classifiers ( <ref type="bibr" target="#b0">Bengio et al., 2007)</ref> or the "companion objectives" ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Bottom-Up-Top-Down Learning Algorithm</head><p>With the SBEN architecture defined, we next present its simple two-step training algorithm, or the Bottom-Up-Top-Down procedure (BUTD), which combines a greedy, bottom-up pass with a subsequent top-down fine-tuning step. At every iteration of training, the model makes use of a sin- gle labeled sample (taken from an available, small labeled data subset) and an example from either a large unlabeled pool or a data-stream. We describe each of the two phases in Sections 3.2.1 and 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Bottom-Up Layer-wise Learning (BU)</head><p>The first phase of N-SBEN learning consists of a bottom-up pass where each layerwise HRBM can be trained using a compound objective func- tion. Data samples are propagated up the model to the layer targeted for layer-wise training using the feedforward schema described above. Each HRBM layer of the SBEN is greedily trained us- ing the frozen latent representations of the one be- low, which are generated by using the lower level expert's input and prediction. The loss function for each layer balances a discriminative objective L disc , a supervised generative objective L gen , and an unsupervised generative objective L unsup , fully defined as follows:</p><formula xml:id="formula_9">L semi (D train , D unlab ) = γL disc (D train ) +αL gen (D train ) +βL unsup (D unlab )<label>(10)</label></formula><p>Unlike generative pre-training of neural architec- tures ( <ref type="bibr" target="#b0">Bengio et al., 2007)</ref>, the additional free pa- rameters γ, α, and β offer explicit control over the extent to which the final parameters discovered are influenced by generative learning ( <ref type="bibr" target="#b16">Larochelle et al., 2012;</ref><ref type="bibr" target="#b24">Ororbia II et al., 2015</ref>). More im- portantly, the generative objectives may be viewed as providing data-dependent regularization on the discriminative learning gradient of each layer. The objectives themselves are defined as:</p><formula xml:id="formula_10">L disc (D train ) = − |D train | t=1 log p(y|x t ),<label>(11)</label></formula><formula xml:id="formula_11">L gen (D train ) = − |D train | t=1</formula><p>log p(y t , x t ), and (12)</p><formula xml:id="formula_12">L unsup (D unlab ) = − |D unlab | t=1 log p(x t )<label>(13)</label></formula><p>where D train = {(x t , y)} is the labeled training data-set and D unlab = {(u t )} is the unlabeled training data-set. The gradient for L disc may be computed directly, which follows the general form</p><formula xml:id="formula_13">∂ log p(y t |x) ∂θ = −E h|yt,xt ∂ ∂θ (E(y t , x t , h)) +E y,h|,x ∂ ∂θ (E(y, x, h))<label>(14)</label></formula><p>and can be calculated directly (see Larochelle et al., <ref type="bibr">2012</ref> , for details) or through a form of Drop- ping, such as Drop-Out or Drop-Connect <ref type="bibr" target="#b35">(Tomczak, 2013</ref>). The generative gradients themselves follow the form</p><formula xml:id="formula_14">∂ log p(y t , x) ∂θ = −E h|yt,xt ∂ ∂θ (E(y t , x t , h)) +E y,x,h ∂ ∂θ (E(y, x, h))<label>(15)</label></formula><p>and, despite being intractable for any sample (x t , y t ), may be approximated via the contrastive divergence procedure <ref type="bibr" target="#b14">(Hinton, 2002</ref>). The in- tractable second expectation is replaced with a point estimate using a single Gibbs sampling step. To calculate the generative gradient for an unla- beled sample u, a pseudo-label must be obtained by using a layer-wise HRBM's current estimate of p(y|u), which can be viewed as a form of self- training or Entropy Regularization (Lee, 2013). The online procedure for computing the genera- tive gradient (either labeled or unlabeled example) for a single HRBM can be found in Ororbia et al.,</p><p>. Setting the coefficients that control learning ob- jective influences can lead to different model con- figurations (especially with respect to γ) as well as impact the gradient-based training of each model layer (i.e., α and β). In this paper, we shall ex- plore two particular configurations, namely 1) by setting γ = 0 and α = 1, which leads to con- structing a purely generative model of D train and Algorithm 1 Top-down fine-tuning of an N-SBEN (ensemble back-propagation). Note that "·" indicates a Hadamard product, ξ is an error signal vector, the prime superscript indicates a derivative (i.e., σ means derivative function of the sigmoid), and z is the symbol for linear pre-activation values.</p><p>Input: (x t , y t ) ∈ D, learning rate λ and model parameters Θ = {Θ 1 , Θ 2 , ..., Θ N } function FINETUNEMODEL((x t , y t ), λ, Θ) Ω ← ∅, x n ← x t , y n ← ∅ Initialize list of layer-wise model statistics &amp; variables // Conduct feed-forward pass to collect layer-wise statistics for Θ n ∈ Θ do (h n , z n , y h n , x n ) ← COMPUTELAYERWISESTATISTICS(x n , Θ n ) Ω n ← (h n , z n , y h n , x n ), x n ← h n , y n ← y h n // Conduct error back-propagation pass to adjust layer-wise parameters</p><formula xml:id="formula_16">ξ l ← ∅ for l ← N, l−−, while l ≥ 1 do (h l , z l , y h l , x l ) ← Ω[l] Grab relevant statistics for layer l of model if i = N then ( disc , ξ l ) ← COMPUTEDISCIMINATIVEGRADIENT(y t , x l , ∅, h n , z, Θ l ) else ξ l ← ξ l · σ ( z l ) ( disc , ξ l ) ← COMPUTEDISCIMINATIVEGRADIENT(y t , x l , ξ l , h n , z, Θ l ) Θ n ← Θ n − λ( disc ) function COMPUTELAYERWISESTATISTICS(x t , Θ n ) y h t ← p(y t |x t , Θ n ) Equation 7 under the layerwise model z ← c + W x t + U e yt</formula><p>Can re-use z to perform next step</p><formula xml:id="formula_17">h t ∼ p(h|y h t , x t , Θ n ) Equation 4 under the layerwise model return (h t , z, y h t , x t ) function COMPUTEDISCIMINATIVEGRADIENT(y t , x l , ξ l , h n , z, Θ l ) o ← p(y|h n , Θ l ), ξ ← sof tmax (o) · −(y t /o) U ← ξh T n , d ← ξ, ξ ← U ξ, ξ ← ξ · σ ( z) if ξ l = ∅ then ξ ← ξ · ξ l W ← ξx T l , c ← ξ, b ← 0, U ← U + (ξe T yt ), ξ ← W T ξ return ( ← ( W , U , b , c , d ), ξ)</formula><p>D unsup , and 2) by setting γ = 1 with α freely varying (which recovers the model of <ref type="bibr" target="#b24">Ororbia et al., 2015</ref>). In both scenarios, β is allowed to vary as a user-defined hyper-parameter. The second set- ting of γ allows for training the SBEN directly with only the bottom-up phase defined in this sec- tion. However, if the first setting is used, a sec- ond phase may be used to incorporate a top-down fine-tuning phase. A bottom-up pass simply en- tails computing this compound gradient for each layer of the model for 1 or 2 samples per training iteration. Notice that the first scenario reduces the number of hyper-parameters to explore in model selection, requiring only an appropriate value for β to be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Top-Down Fine-tuning (TD)</head><p>Although efficient, the bottom-up procedure de- scribed above is greedy, which means that the gra- dients are computed for each layer-wise HRBM independent of gradient information from other layers of the model. One way we propose to introduce a degree of joint training of param- eters is to incorporate a second phase that ad- justs the SBEN parameters via a modified form of back-propagation. Such a routine can further exploit the SBEN's multiple predictors (or entry points) where additional error signals may be com- puted and aggregated while signals are reverse- propagated down the network. We hypothesize that holistic fine-tuning ensures that discrimina- tive information is incorporated into the generative Algorithm 2 The Bottom-Up-Top-Down training procedure for learning an N-SBEN.</p><p>Input:</p><formula xml:id="formula_18">(x t , y t ) ∈ D train , (u t ) ∈ D unlab , rates λ &amp; β, ¯ p, &amp; parameters Θ = {Θ 1 , Θ 2 , ..., Θ N } function BOTTOMUPTOPDOWN((y t , x t , u t , λ, β, Θ)</formula><p>APPLYBOTTOMUPPASS(y t , x t , u t , λ, γ = 0, α = 1, β, Θ) See (Ororbia II et al., 2015) // Up to two calls can be made to the top-down tuning routine FINETUNEMODEL(x t , y t , λ, Θ)</p><p>See Algorithm 1 for details v t ← p ensemble (y|x, Θ n )</p><p>Calculate pseudo-label probability using Equation</p><formula xml:id="formula_19">9 if max[v t ] &gt; ¯ p then v t ← TOONEHOT(v t )</formula><p>Convert to 1-hot vector using argmax of model conditionals FINETUNEMODEL(u t , v t , λ, Θ) features being constructed in the bottom-up learn- ing step. Furthermore, errors from experts above are propagated down to lower layers, which were initially frozen during the greedy, bottom-up train- ing phase.</p><p>Fine-tuning in the context of training an SBEN is different from using a pre-trained MLP that is subsequently fine-tuned with back-propagation. First, since the SBEN is a more complex architec- ture than an MLP, pre-initializing an MLP would be insufficient given that one would be tossing po- tentially useful information stored in the SBEN's class filters (and corresponding class bias vectors) of each layer-wise expert (i.e., U and d). Second, merely using the SBEN as an intermediate model ignores the fact the SBEN can already perform classification directly. To avoid losing such infor- mation and to fully exploit the model's predictive ability, we adapt the back-propagation algorithm for training MLP's to operate on the SBEN, which we shall call ensemble back-propagation since the fine-tuning method propagates error deriva- tives down the network from many points of entry. Ensemble back-propagation is described in Algo- rithm 1.</p><p>With this second online training step, the Bottom-Up-Top-Down (BUTD) training algorithm for fully training an SBEN proceeds with a sin- gle bottom-up modification step followed by a single top-down joint fine-tuning step using the ensemble back-propagation procedure defined in Algorithm 1 for each training time step. A full top-down phase can consist of up to two calls to the ensemble back-propagation procedure. One is used to jointly modify the SBEN's parame- ters with respect to the sample taken from D train . A second one is potentially needed to tune pa- rameters with respect to the sample drawn from D unlab . For the unlabeled sample, if the high- est class probability assigned by the SBEN (us- ing Equation 9) is greater than a pre-set threshold (i.e., max[p ensemble (y|u)] &gt; ¯ p), a pseudo-label is created for that sample by converting the model's mean vector to a 1-hot encoding. The probability threshold ¯ p for the potential second call to the en- semble back-propagation routine allows us to in- corporate a tunable form of pseudo-labeling <ref type="bibr" target="#b18">(Lee, 2013)</ref> into the Bottom-Up-Top-Down learning al- gorithm.</p><p>The high-level view of the BUTD learning algo- rithm is depicted in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We investigate the viability of our deep hybrid ar- chitecture for semi-supervised text categorization. Model performance was evaluated on the WebKB data-set 1 and a small-scale version of the 20News- Group data-set 2 .</p><p>The original WebKB collection contains pages from a variety of universities (Cornell, Texas, Washington, and Wisconsin as well as miscella- neous pages from others). The 4-class classifica- tion problem we defined using this data-set was to determine if a web-page could be identified as one belonging to a Student, Faculty, Course, or a Project, yielding a subset of usable 4,199 sam- ples. We applied simple pre-processing to the text, namely stop-word removal and stemming, chose to leverage only the k most frequently occurring terms (this varied across the two experiments), and binarized the document low-level representation (only 1 page vector was discarded due to pres- ence of 0 terms). The 20NewsGroup data-set, on the other hand, contained 16242 total samples and was already pre-processed, containing 100 terms, binary-occurrence low-level representation, with tags for the four top-most highest level domains or meta-topics in the newsgroups array.</p><p>For both data-sets, we evaluated model gen- eralization performance using a stratified 5-fold cross-validation (CV) scheme. For each possible train/test split, we automatically partitioned the training fold into separate labeled, unlabeled, and validation subsets using stratified random sam- pling without replacement. Generalization perfor- mance was evaluated by estimating classification error, average precision, average recall, and av- erage F-Measure, where F-Measure was chosen to be the harmonic mean of precision and recall, F 1 = 2(precision · recall)/(precision + recall).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Designs</head><p>We evaluated the BUTD version of our model, the 3-SBEN,BUTD, as described in Algorithm 2. For simplicity, the number of latent variables at each level of the SBEN was held equal to the di- mensionality of the data (i.e., a complete repre- sentation). We compared this model trained with BUTD against a version utilizing only the bottom- up phase (3-SBEN,BU) as in <ref type="bibr" target="#b24">Ororbia et al. (2015)</ref>. Both SBEN models contained 3 layers of latent variables.</p><p>We compared against an array of baseline clas- sifiers. We used our implementation of an incre- mental version of Maximum Entropy, or MaxEnt- ST (which, as explained in <ref type="bibr" target="#b27">Sarikaya et al., 2014</ref>, is equivalent to a softmax classifier). Further- more, we used our implementation of the Pega- sos algorithm (SVM-ST) <ref type="bibr" target="#b31">(Shalev-Shwartz et al., 2011</ref>) which was extended to follow a proper multi-class scheme <ref type="bibr" target="#b7">(Crammer and Singer, 2002</ref>). This is the online formulation of the SVM, trained via sub-gradient descent on the primal objective followed by a projection step (for simplicity, we opted to using a linear-kernel). Additionally, we implemented a semi-supervised Bernoulli Naive Bayes classifier (NB-EM) trained via Expectation- Maximization as in ( <ref type="bibr" target="#b23">Nigam et al., 1999</ref>). We also compared our model against the HRBM (Larochelle and Bengio, 2008) (effectively a sin- gle layer SBEN), which serves as a powerful, non- linear shallow classifier in of itself, as well as a 3-layer sparse deep Rectifier Network ( <ref type="bibr" target="#b8">Glorot et al., 2011a</ref>), or Rect, composed of leaky rectifier units.</p><p>All shallow classifiers (except NB-EM and the HRBM) were extended to the semi-supervised set-  ting by leveraging a simple self-training scheme in order to learn from unlabeled data samples. The self-training scheme entailed using a classifier's estimate of p(y|u) for an unlabeled sample and, if max[p(y|u)] &gt; ¯ p, we created a 1-hot proxy encoding using the argmax of model's predictor, where ¯ p is a threshold meta-parameter. Since we found this simple pseudo-labeling approach, sim- ilar in spirit to <ref type="bibr" target="#b18">(Lee, 2013)</ref>, to improve the results for all classifiers, and thus we report all results uti- lizing this scheme. <ref type="bibr">3</ref> All classes of models (SBEN, HRBM, Rect, SVM-ST, MaxEnt-ST, NB-ST) were subject to the same model selection procedure de- scribed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Selection</head><p>Model selection was conducted using a paral- lelized multi-setting scheme, where a configura- tion file for each model was specified, describing a set of hyper-parameter combinations to explore (this is akin to a course-grained grid search, where the points of model evaluation are set manually a priori). For the SBEN's, we varied the learning rate ([0.01, 0.25]) and β coefficient ([0.1, 1.0]) and <ref type="table">Table 1</ref>: WEBKB categorization results on 1% of the training data labeled (8 examples per class), rest unlabeled (i.e., 5-fold means with standard error of the mean, 250 features).</p><p>Error Precision Recall F1-Score NB-EM 0.369 ± 0.039 0.684 ± 0.022 0.680 ± 0.028 0.625 ± 0.043 MaxEnt-ST 0.402 ± 0.026 0.623 ± 0.025 0.593 ± 0.015 0.583 ± 0.020 SVM-ST 0.342 ± 0.020 0.663 ± 0.010 0.665 ± 0.014 0.644 ± 0.015 HRBM 0.252 ± 0.023 0.740 ± 0.019 0.765 ± 0.016 0.741 ± 0.021 3-Rect 0.328 ± 0.020 0.673 ± 0.017 0.680 ± 0.021 0.654 ± 0.023 3-SBEN,BU 0.239 ± 0.015 0.754 ± 0.014 0.780 ± 0.016 0.754 ± 0.015 3-SBEN,BUTD 0.210 ± 0.011 0.786 ± 0.009 0.784 ± 0.014 0.777 ± 0.012  experimented with stochastic and mean-field ver- sions of the models 4 (we found that mean-field did slightly better for this experiment and thus report the performance of this model in this paper). The HRBM's meta-parameters were tuned using a sim- ilar set-up to ( <ref type="bibr" target="#b16">Larochelle et al., 2012</ref>) with learn- ing rate varied in <ref type="bibr">([0.01, 0.25]</ref>), α in ([0.1, 0.5]), and β in ({0.01, 0.1}). For the SVM-ST algo- rithm, we tuned its slack variable λ, searching in the interval [0.0001, 0.5], for MaxEnt-ST its learn- ing rate in [0.0001, 0.1], and for ¯ p of all models (shallow and deep) that used pseudo-labeling we searched the interval [0.1, 1.0]. All models of all configurations were trained for a 10,000 iteration sweep incrementally on the data and the model state with lowest validation error for that partic- ular run was used. The SBEN, HRBM, and Rect models were also set to use a momentum term of 0.9 (linearly increased from 0.1 in the first 1000 training iterations) and the Rect model made use of a small L1 regularization penalty to encourage additional hidden sparsity. For a data-set like the 20NewsGroup, which contained a number of unla- beled samples greater than training iterations, we view our schema as simulating access to a data-stream, since all models had access to any given unlabeled example only once during a training run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Performance</head><p>We first conducted an experiment, using the We- bKB data-set, exploring classification error as a function of labeled data subset cardinality <ref type="figure" target="#fig_1">(Fig- ure 2)</ref>. In this setup, we repeated the strati- fied cross-fold scheme for each possible labeled data subset size, comparing the performance of the SVM model against 3-SBEN,BU (blue dot- ted curve) and 3-SBEN,BUTD (green dash-dotted curve). We see that as the number of labeled ex- amples increases (which entails greater human an- notation effort) all models improve, nearly reach- ing 90% accuracy. However, while the perfor- mance difference between models becomes negli- gible as the training set becomes more supervised, as expected, it is in the less scarce regions of the plot we are interested in. We see that for small proportions, both variants of the SBEN outper- form the SVM, and furthermore, the SBEN trained via full BUTD can reach lower error, especially for the most extreme scenario where only 8 la- beled examples per class are available. We no- tice a bump in the performance of BUTD as nearly the whole training set becomes labeled and posit that since the BUTD involves additional pseudo- <ref type="table">Table 3</ref>: Top-most words that the SBEN (BUTD) model associates with the 4 NewsGroup meta-topics.</p><p>Meta-Topic Associated Terms comp.* windows, graphics, card, driver, scsi, dos, f iles, display rec.* players, hockey, season, nhl, team, league, baseball, games sci.* orbit, shuttle, space, earth, mission, nasa, moon, doctor talk.* jews, christian, religion, jesus, bible, war, israel, president labeling steps (as in the top-down phase), there is greater risk of reinforcing incorrect predictions in the pseudo-joint 5 tuning of layerwise expert pa- rameters. For text collections where most of the data is labeled and unlabeled data is minimal, only a simple bottom-up pass is needed to learn a good hybrid model of the data. The next set of experiments was conducted with only 1% of the training sets labeled. We observe ( <ref type="table">Tables 1 and 2</ref>) that our deep hybrid architec- ture trained via BUTD outperforms all other mod- els with respect to all performance metrics. While the SBEN trained with simply an online bottom-up performs significantly better than the SVM model, we note a further reduction of error using our pro- posed BUTD training procedure. The additional top-down phase serves as a mechanism for uni- fying the layer-wise experts, where error signals for both labeled and pseudo-labeled examples in- crease agreement among all model layer experts.</p><p>For the 20NewsGroup data-set, we conducted a simple experiment to uncover some of the knowl- edge acquired by our model with respect to the tar- get categorization task. We applied the mechanism from ( <ref type="bibr" target="#b16">Larochelle et al., 2012</ref>) to extract the vari- ables that are most strongly associated with each of the clamped target variables in the lowest layer of a BUTD-trained SBEN. The top-scored terms associated with each class variable are shown in <ref type="table">Table 3</ref>, using the 10 hidden nodes most highly triggered by the clamped class node, in a model trained on all of the 20NewsGroup data using a model configuration determined from CV results for the 20NewsGroup data-set reported in the pa- per. Since the SBEN is a composition of layer- wise experts each capable of classification, we note that this procedure could be applied to each level to uncover which unobserved variables are most strongly associated with each class target. We speculate that this could serve the basis for un-covering the model's underlying learnt hierarchy of the data and be potentially used for knowledge extraction, a subject for future work in analyzing black box neural models such as our own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented the Bottom-Up-Top-Down proce- dure for training the Stacked Boltzmann Experts Network, a hybrid architecture that balances both discriminative and generative learning goals, in the context of semi-supervised text categorization. It combines a greedy, layer-wise bottom-up ap- proach with a top-down fine-tuning method for pseudo-joint modification of parameters.</p><p>Models were evaluated using two text corpora: WebKB and 20NewsGroup. We compared re- sults against several baseline models and found that our hybrid architecture outperformed the oth- ers in all settings investigated. We found that the SBEN, especially when trained with the full Bottom-Up-Top-Down learning procedure could in some cases improve classification error by as much 39% over the Pegasos SVM, and nearly 17% over the HRBM, especially when data is in very limited supply. While we were able to demon- strate the viability of our hybrid model when using only simple surface statistics of text, future work shall include application of our models to more semantic-oriented representations, such as those leveraged in building log-linear language models ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the SBEN model. The model in feedforward mode can be viewed as a directed model, however, during training, connections are bi-directional.</figDesc><graphic url="image-1.png" coords="3,308.41,62.81,216.00,145.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean CV generalization performance as a function of labeled sample subset size (using 200 features).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2: 20NewsGroup data-set categorization results on 1% of the training data labeled (8 examples per class), rest unlabeled (i.e., 5-fold means with standard error of the mean)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The exact data-set we used can be found and downloaded at http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/ 2 The exact data-set we used can be found and downloaded at http://www.cs.nyu.edu/˜ roweis/data.html.</note>

			<note place="foot" n="3"> All model implementations were computationally verified for correctness when applicable. Since most discriminative objectives followed a gradient descent optimization scheme and could be realized in an automatic differentiation framework, we checked gradient validity via finite difference approximation.</note>

			<note place="foot" n="4"> Mean-field simply means no sampling steps were taken after computing probability vectors, or &quot;means&quot; in any stage of the computation.</note>

			<note place="foot" n="5"> We use the phrase &quot;pseudo-joint&quot; to differentiate a model that has all its parameters trained jointly from our own, where only the top-down phase of BUTD introduces any form of joint parameter modification.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>A.G.O. acknowledges support from The Penn-sylvania State University and the National Sci-ence Foundation <ref type="bibr">(DGE-1144860)</ref>. D.R. acknowl-edges support from the National Science Founda-tion (SES-1528409).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research-Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference on Computational Learning Theory</title>
		<meeting>the Eleventh Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Deep Belief Networks from Nonstationary Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Peter Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><forename type="middle">Montesino</forename><surname>Pouzols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2012, number 7553 in Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic identification of research articles from crawled documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujatha</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Teregowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop: Web-Scale Classification: Classifying Big Data from the Web</title>
		<meeting>the Workshop: Web-Scale Classification: Classifying Big Data from the Web<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<title level="m">Rules and representations. Behavioral and brain sciences</title>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Supportvector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>14th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Large-scale Sentiment Classification: A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Researcher Homepage Classification Using Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Sujatha Das Gollapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on World Wide Web, WWW &apos;13</title>
		<meeting>the 22Nd International Conference on World Wide Web, WWW &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="471" to="482" />
		</imprint>
	</monogr>
	<note>ternational World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Computational limitations of small-depth circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hastad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training Products of Experts by Minimizing Contrastive Divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification using Discriminative Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Algorithms for the Classification Restricted Boltzmann Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="643" to="669" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">DeeplySupervised Nets</note>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pseudo-label: The Simple and Efficient Semi-supervised Learning Method for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Novel Text Classification Approach Based on Deep Belief Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Neural Information Processing: Theory and Algorithms-Volume Part I, ICONIP&apos;10</title>
		<meeting>the 17th International Conference on Neural Information Processing: Theory and Algorithms-Volume Part I, ICONIP&apos;10</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Deep Architecture for Matching Short Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1367" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning new semi-supervised deep auto-encoder features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using maximum entropy for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-99 workshop on Machine Learning for Information Filtering</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online learning of deep hybrid architectures for semi-supervised categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<meeting><address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Empirical assessment of stimulus poverty arguments. The linguistic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">C</forename><surname>Pullum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scholz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Application of Deep Belief Networks for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="778" to="784" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Strength of Weak Learnability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative versus Discriminative Training of RBMs for classification of fMRI images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Schmah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Strother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pegasos: Primal Estimated Sub-gradient Solver for SVM. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="3" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Softsupervised learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1090" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceiving intentions and learning words in the second year of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tomasello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language acquisition and conceptual development</title>
		<editor>Melissa Bowerman and Stephen Levinson</editor>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="132" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Prediction of Breast Cancer Recurrence using Classification Restricted Boltzmann Machine with Dropping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomczak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.6324</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supervised Deep Learning with Auxiliary Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangjian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York City; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="353" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online Incremental Feature Learning with Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1453" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
