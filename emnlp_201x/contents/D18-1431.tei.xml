<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Baheti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ohio State University</orgName>
								<address>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ohio State University</orgName>
								<address>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiweil@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Shannon.AI</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3970" to="3980"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3970</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural conversation models tend to generate safe, generic responses for most inputs. This is due to the limitations of likelihood-based decoding objectives in generation tasks with diverse outputs, such as conversation. To address this challenge, we propose a simple yet effective approach for incorporating side information in the form of distri-butional constraints over the generated responses. We propose two constraints that help generate more content rich responses that are based on a model of syntax and topics (Griffiths et al., 2005) and semantic similarity (Arora et al., 2016). We evaluate our approach against a variety of competitive base-lines, using both automatic metrics and human judgments, showing that our proposed approach generates responses that are much less generic without sacrificing plausibility. A working demo of our code can be found at https://github.com/abaheti95/ DC-NeuralConversation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen growing interest in neural generation methods for data-driven conversation. This approach has the potential to leverage mas- sive conversational datasets on the web to learn open-domain dialogue agents, without relying on hand-written rules or manual annotation. Such re- sponse generation models could be combined with traditional dialogue systems to enable more natu- ral and adaptive conversation, in addition to new applications such as predictive response sugges- tion ( <ref type="bibr">Kannan et al., 2016)</ref>, however many chal- lenges remain.</p><p>A major drawback of neural conversation gener- ation is that it tends to produce too many "safe" or generic responses, for example: "I don't know" or "What are you talking about ?". This is a perva- sive problem that has been independently reported he 's EOS <ref type="figure">Figure 1</ref>: Illustration of the dull response problem in maximum likelihood neural conversation genera- tion using an example from the OpenSubtitles corpus. Function (stop) words tend to receive higher log prob- abilities than content (topic) words. The highest like- lihood stop words and topic words in this context are listed.</p><p>by multiple research groups ( <ref type="bibr" target="#b12">Li et al., 2016a;</ref><ref type="bibr" target="#b15">Li et al., 2016c)</ref>. <ref type="bibr">1</ref> The effect is due to the use of conditional likelihood as a de- coding objective -maximizing conditional like- lihood is a suitable choice for text-to-text gen- eration tasks such as machine translation, where the source and target are semantically equivalent, however, in conversation there are many accept- able ways to respond. Simply choosing most pre- dictable reply often leads to very dull conversa- tion. <ref type="figure">Figure 1</ref> illustrates the problem with conditional likelihood using an example. After encoding the source message using a bidirectional LSTM with attention, and fixing the first two words of the re- sponse, we show the highest ranked words (ac- cording to log-likelihood scores) taken from a list of stop words 2 in contrast to those selected from a list of topic words. <ref type="bibr">3</ref> As illustrated in the figure, re- sponse generation that is based on maximum like- lihood is biased towards stop-words and therefore results in responses that are safe (likely to be plau- sible in the context of the input), but also bland (don't contribute any new information to the con- versation). This motivates the need for augment- ing the decoding objective to encourage the use of more content words.</p><p>To address the dull-response problem in neu- ral conversation, in this paper, we propose a new decoding objective that flexibly incorporates side- information in the form of distributional con- straints. We explore two constraints, one which encourages the distribution over topics and syntax in the response to match that found in the user's input. To estimate these distributions, we leverage the unsupervised model of topics and syntax pro- posed by <ref type="bibr">Griffiths and Steyvers (2005)</ref>. The sec- ond constraint encourages generated responses to be semantically similar to the user's input; seman- tic similarity is measured using fixed-dimensional sentence embeddings ( <ref type="bibr" target="#b0">Arora et al., 2016)</ref>.</p><p>After introducing distributional constraints into the decoding objective, we empirically demon- strate, in an evaluation that is based on hu- man judgments, that our approach generates more content-rich responses when compared with two competitive baselines: Maximum Mutual Infor- mation (MMI) ( <ref type="bibr" target="#b12">Li et al., 2016a)</ref>, in addition to an approach that conditions on topic models as addi- tional context in neural conversation ( <ref type="bibr" target="#b38">Xing et al., 2017)</ref>. While encouraging the model to generate less bland responses can be risky, we find that our approach achieves comparable plausibility while introducing significantly more content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Conversation Generation</head><p>As a starting point for our approach we leverage the Seq2Seq model ( <ref type="bibr" target="#b29">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) which has been used as a basis for a broad range of recent work on neural con- versation ( <ref type="bibr">Kannan et al., 2016;</ref><ref type="bibr" target="#b12">Li et al., 2016a;</ref><ref type="bibr" target="#b26">Shao et al., 2017)</ref>. This model consists of two parts, an encoder and a decoder both of which are typically stacked LSTM layers. The encoder reads the input sequence and creates a hidden representation. The decoder conditions on this representation, using attention, and gener- ates the response using a neural network language model ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b28">Sutskever et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distributional Topic and Semantic Similarity Constraints</head><p>Neural generation models select a response, Ë† Y by maximizing over a decoding objective, typically using greedy beam search from left to right over partially completed responses, which are scored using the decoder RNN language model. A com- monly used decoding objective is the conditional likelihood of the target given the source, P (Y |X):</p><formula xml:id="formula_0">Ë† Y = arg max Y {log P (Y |X)}<label>(1)</label></formula><p>= arg max</p><formula xml:id="formula_1">w 1 ,...,wn { n i=1 log P (w i |w 1 , . . . w iâˆ’1 , X)}</formula><p>As discussed in Section 1, models trained to max- imize conditional likelihood tend to assign low probability to content words as compared to (more frequent) function words, leading to bland, generic responses most of the time. To ameliorate this, we introduce distributional constraints in the form of additional terms in the decoding objective that favor hypotheses containing more content words that are similar to the source in the Topical and Semantic sense. For the constraint in the topic domain, we are interested in the topic probability distributions of the source, X, and target Y , P (T |X) and P (T |Y ), where T is a random variable defined over k topics. Then we can modify the decoding objective from Eq 1:</p><formula xml:id="formula_2">Ë† Y T = arg max Y { log P (Y |X)+ Î± Ã— âˆ†(P (T |X), P (T |Y ))} (2)</formula><p>Here, âˆ† is a similarity function between the two probability distributions and Î± is a tunable hyper- parameter to adjust impact of this constraint.</p><p>Much recent work has investigated how to en- code the semantic meaning of a sentence into a fixed high dimensional embedding space ( <ref type="bibr" target="#b9">Kiros et al., 2015;</ref><ref type="bibr" target="#b35">Wieting and Gimpel, 2017)</ref>. Given such an embedding representation of X and Y , one can find the semantic similarity between the two and similar to Eq 2 we can add a semantic similarity constraint to the likelihood objective as follows:</p><formula xml:id="formula_3">Ë† Y Emb = arg max Y { log(P (Y |X))+ Î² Ã— (Emb(X), Emb(Y ))} (3)</formula><p>where, Emb() is a function that maps an utterance to a semantic vector representation, is a func- tion that computes similarity of the two embed- dings and Î² is a tunable parameter.</p><p>Both of the constraint terms from Eq 2 and Eq 3 are additive in nature and thus can be com- bined in a straightforward fashion. This formula- tion allows us to systematically combine informa- tion from three different models to produce bet- ter responses in terms of topic and semantic rele- vance. Conceptually, the likelihood term governs the grammatical structure of the response while the topic and semantic constraints drive content selection ( <ref type="bibr" target="#b20">Nenkova and Passonneau, 2004;</ref><ref type="bibr" target="#b2">Barzilay and Lapata, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding with Distributional Constraints</head><p>In Section 3, we defined two constraints (one topic constraint and one semantic) for use in the decod- ing objective. Incorporating these constraints dur- ing decoding requires that they factorize in a way that is compatible with left-to-right beam search over words in the response. The standard approach to computing posterior distributions in topic mod- els requires a probabilistic inference procedure over the entire source and target. Furthermore, computing semantic representations can involve the use of complex neural architectures. Both of these proceedures are difficult to integrate into de- coding, because they are computationally expen- sive and would need to be called repeatedly within the inner loop of the decoder. Furthermore, when performing left-to-right beam search, as is com- mon practice in neural generation, the complete response is generally not available. To address these challenges, we propose using simple addi- tive variants of these methods that factorize over words and which we found to enable efficient de- coding without sacrificing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topic Similarity</head><p>Estimating the topic distribution of the source, P (T |X), and response, P (T |Y ), is a key step in implementing the topic-similarity constraint. HMM-LDA is a generative model that is able to separate topic and syntax words, by inferring topic distributions in a corpus while flexibly modeling function words. We briefly summarize this model before describing our implementation. <ref type="bibr">Griffiths et. al. (2005)</ref> suggested an unsupervised generative model that simultaneously labels each word in a document with a syntax (c) and topic (z) state. They modify the Latent Dirichlet Alloca- tion model to include a syntactic component akin to a Hidden Markov Model (HMM). In LDA, each topic (z) is associated with a probability distribu- tion over the vocabulary Ï† (z) . HMM-LDA adds additional distributions over words for each syn- tactic class (c) as Ï† (c) . A special class, c = 0, is reserved for topics. The transition model be- tween classes c iâˆ’1 to c i follows a multinomial dis- tribution distribution Ï€ (c iâˆ’1 ) . Each document has an associated distribution over topics Î¸ (d) ; each word, w j , in the document has an associated la- tent topic variable, z j , that is drawn from Î¸ (d) and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Syntax-Topics model</head><formula xml:id="formula_4">c j is drawn from Ï€ (c jâˆ’1 ) . If c j = 0, then w j is drawn from Ï† (z j ) , otherwise it is drawn from Ï† (c j ) .</formula><p>Markov Chain Monte Carlo inference (MCMC) is used to infer values for the hidden topic and syntax variables associated with a given document collec- tion. To estimate topic and syntax distributions, we performed collapsed Gibbs sampling over our training corpus of conversations, where each con- versation is treated as a document. One sample of the hidden variables was used to estimate model parameters after 2,500 iterations of burn in. Our code for training the HMM-LDA model is avail- able online 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Estimating Topic Distributions with</head><p>HMM-LDA To compute distributional topic constraints in neu- ral response generation, we first need an efficient method for estimating topic distributions that fac- torizes over words, given a point estimate of an HMM-LDA model's parameters. We would like to estimate topic distributions based on content words contained in a sentence and ignore func- tion words. HMM-LDA provides us with topic, Ï† (z) , and syntax, Ï† (c) , distributions over the vo- cabulary of words, w âˆˆ V . Treating a sentence as a bag-of-words we can estimate its distribution over topics as a sum of topic distributions over all words normalized by sentence length. However, we found this approach does not to work well in practice because it gives equal weight to topic and syntax words. To address this issue, we weighted each word's topic distribution P (T |w) by its prob- ability of being generated by the topic component of the HMM-LDA model (i.e. P (C = 0|w)). The topic distribution of a sentence, S, is estimated as:</p><formula xml:id="formula_5">P (T |S) = 1 Z Î£ wâˆˆS P (T |w)P (C = 0|w) (4)</formula><p>where Z = Î£ wâˆˆS P (C = 0|w) is a normalizing constant that corresponds to the expected number of content words in the sentence. As mentioned earlier, a more accurate estimate of the topic distri- bution could be obtained using MCMC inference or by applying the forward-backward algorithm. However, these methods are computationally ex- pensive and not well-suited to the decoding frame- work used in neural generation. The method described above allows us to ef- ficiently compute the topic distribution of a sen- tence for use in the topic constraint in Eq 2. For a similarity function, âˆ†, we simply use the vec- tor dot product, which is closely related to cosine similarity. This formulation has the advantage that it enables memoization during decoding. Another advantage is that it captures the ratio of topic to syntax words due to the weights P (C = 0|w). 5 Therefore, the overall constraint has the effect of keeping the syntax-topics ratio in generated hy- pothesis similar to the source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Similarity</head><p>To define the semantic similarity constraint we first encode a semantic representation of the source and target into a fixed dimensional embed- ding space. There are many sentence embedding methods that could be used, however we want this encoding to be relatively efficient as it will be used many times during beam search. <ref type="bibr" target="#b0">Arora et. al. (2016)</ref> recently proposed a simple sentence embedding method, which was shown to have competitive performance across a variety of tasks. Their approach uses a weighted average of word embeddings where each word is weighted by a a+P (w) ; here, P (w) is the unigram probabil- ity and a is a hyperparameter. Such a weighting scheme reduces the impact of frequent words (typ-ically function words) in the overall sentence em- bedding. Next the first principal component of all the sentence embeddings in the corpus is removed. ( <ref type="bibr" target="#b0">Arora et al., 2016)</ref> points that the first principal component has high cosine similarity with com- mon function words. Removing this component gives sentence embeddings that encapsulate the semantic meaning of the sentence. We use this technique in our implementation of Emb() in Eq 3. For the similarity function, âˆ†, we use the dot product. Analogous to the topic constraint described above, this approach to measuring se- mantic similarity also decomposes over words and works well in the decoding framework.  Column 2 shows the total number of dialogues that we got after all pre-processing and Column 3 shows the number of sampled dialogues in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>For training purposes we use OpenSubtitles ( <ref type="bibr" target="#b30">Tiedemann, 2009)</ref>, a large corpus of movie subti- tles (roughly 60M-70M lines) that is freely avail- able and has been used in a broad range of recent work on data-driven conversation. OpenSubtitles does not contain speaker annotations on the di- alogue turns, so as previously noted when used for learning data-driven conversation models the data is somewhat noisy. Nonetheless, it is possi- ble to create a useful corpus of conversations from this data by assuming each line corresponds to a full speaker turn. Although this assumption is of- ten violated, prior work has successfully trained and evaluated neural conversation models using this corpus. In our experiments we used a prepro- cessed version of this dataset distributed by <ref type="bibr" target="#b12">Li et. al. (2016a)</ref>. <ref type="bibr">6</ref> The dataset contains large number of two turn dialogues out of which we sampled 23M to use as our training set and 10k as a validation set. <ref type="table" target="#tab_2">Table 2</ref>. From each bucket we randomly sampled â‰ˆ333 dialogues for a total of 1000 dialogues in our test set. We evaluate all models on this test set. Since automatic metrics do not correlate with human judgment, we manually tuned the hyperpa- rameters (Î± and Î²) on a small development set (4 dialogues from each bucket to create a small 12 sentence development set; disjoint from test set). We manually inspected the responses generated by the model on the development set for different val- ues of Î± and Î² and choose those that performed best.</p><note type="other">Due to the noisy nature of the OpenSubtitles conversations we do not use them for evalua- tion. Instead, we leverage the Cornell Movie Dia- logue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) which is much smaller but contains accu- rate speaker annotations. We extracted all two turn conversations (source target pair) from this cor- pus and removed those with less than three and more than 25 words. After this, we divided the remaining conversations into three buckets based on source length. The numbers can be found in</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Conditions and Baselines</head><p>During learning we use the same hyperparame- ters for all models; these are displayed in <ref type="table">Table  1</ref>, and are based on those reported by <ref type="bibr" target="#b12">Li et. al. (2016a)</ref>. <ref type="bibr">7</ref> We compare our approach with the fol- lowing baselines: MMI: We re-implemented the MMI-bidi method proposed by <ref type="bibr" target="#b12">Li et. al. (2016a)</ref>. MMI is a par- ticularly appropriate baseline for comparison, as it encourages responses that have higher relevance to the input in contrast to conditional likelihood, which tends to favor responses with higher uncon- ditional probability. MMI-bidi generates B can- didates using Beam search on a Seq2Seq model trained to maximize conditional likelihood of the target given the source, P (Y |X), then re-ranks them using a separately trained source given tar-get model, P (X|Y ). Combining both directions in this way has the effect of maximizing mutual information ( <ref type="bibr" target="#b12">Li et al., 2016a</ref>). TA-Seq2Seq: Another relevant baseline is the TA- Seq2Seq model of <ref type="bibr" target="#b38">Xing et. al. (2017)</ref> that inte- grates information from a pre-trained topic model into neural response generation using an attention mechanism to condition on relevant topic words. They evaluate their model on a dataset of Chi- nese forum posts. Unfortunately we could not use the code provided by the authors due to data- mismatch (their model makes use of user iden- tities which are not available in the OpenSubti- tles corpus). We therefore compare with a re- implementation of their approach in which we modify each source sentence to include a list of the 20 most relevant topic words from HMM-LDA and then train using the same Seq2Seq framework with attention. This enables the model to condi- tion on the relevant topic words. In addition to in- corporating attention over topics, Xing et. al. also introduced an approach to biased generation -to replicate this we add a constant factor to all topic words during the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Analysis</head><p>Our proposed decoding objective constraints (topic and semantic) are complementary to the MMI objective, which encourages diversity and relevance to the source input. Therefore, in addi- tion to comparing against the baselines described above, we evaluated three variants of our model: (1) maximum conditional likelihood combined with semantic and topic distributional constraints with a beam size of 10 (DC-10) (2) The same configuration with MMI-bidi re-ranking using a beam size of 10 DC-MMI10 and (3) MMI-bidi re- ranking with a beam size of 200 (DC-MMI200). We test all configurations on the 1000 conversa- tions test set described in Section 5 and compare them on automatic metrics and also in a crowd- sourced human evaluation. We do not consider TA-200 (TA-Seq2Seq, Beam=200), DC-200 and MMI-10 for human evaluation as they appear to perform worse than other model variants in auto- matic metrics and also on our set of development sentences. Sample responses for all the remaining models are presented in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Automatic Metrics</head><p>Following <ref type="bibr" target="#b12">Li et. al. (2016a)</ref>, we report distinct-1 and distinct-2, which measure the diversity of re-  <ref type="table">Table 3</ref>: Sample responses of all the models on the dev set sponses. These are the ratios of types to tokens for unigrams and bigrams, respectively. We also report BLEU-1 scores following previous work, however it should be noted that BLEU-1 is not generally accepted to correlate with human judg- ments in conversation generation tasks ( <ref type="bibr" target="#b17">Liu et al., 2016)</ref> as there are many acceptable ways to re- ply to an input which may not match a reference response. Lastly, we compare the percentage of stop-words 8 of the responses generated by each model (smaller values, that are closer to the distri- bution of human conversations are preferred). The automatic evaluation is presented in <ref type="table" target="#tab_4">Table 4.</ref> 8 Long Stopword List from https://www.ranks. nl/stopwords. We appended punctuations to this list.</p><p>For brevity we define aliases for each system in the 2 nd column of <ref type="table" target="#tab_4">Table 4</ref> which are used in subsequent discussion. The human responses are diverse and also generally longer than au- tomatically generated responses. MMI200 has higher diversity than TA-Seq2Seq in terms of distinct-1 and distinct-2. This illustrates the im- portance of re-ranking using MMI. Our approach produces almost twice as many distinct unigrams and bigrams. We also observe MMI200 and TA- Seq2Seq achieve higher BLEU scores than our models, however this is not surprising since our models are designed to generate more interest- ing responses containing rarer content words that are less likely to appear in reference responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Alias distinct-1 distinct-2 BLEU -1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg. length</head><p>Stop- word% Human responses human 2381/0.176 7532/0.602 - 13.5 70.66 MMI (Beam=200) MMI200 351/0.058 990/0.197 12.8 6.0 84.91 TA-Seq2Seq <ref type="table">(Beam=10)</ref> TA-10 237/0.036 524/0.095 12.9 6.5 79.40 Dist. Const. <ref type="table">(Beam=10)</ref> DC-10 710/0.097 2014/0.320 11.0  As expected we observe that MMI200 and TA-10 have a higher percentage of stop-words than hu- man responses. According to the human evalua- tion discussed in Section 7.2, these models were also found to have lower content richness.</p><note type="other">7.3 72.04 Dist. Const. + MMI (Beam=10) DC-MMI10 732/0.099 2098/0.327 11.4 7.4 73.87 Dist. Const. + MMI (Beam=200) DC-MMI200 850/0.116 2946/0.465 11.6 7.3 72.25</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Human Evaluation</head><p>We conducted a survey on the crowd-sourcing platform, Amazon Mechanical Turk. Every model response is scored on 2 categories: 1) Plausibility -is the response plausible for the given source? and 2) Content Richness -does the response add new information to the conversation? We asked the evaluators to respond on a 5-point scale to the questions above (Strongly Agree, Agree, Unsure, Disagree, Strongly Disagree). These were later collapsed to 3 categories (Agree, Unsure, Dis- agree). The results for plausibility and content richness of our model in addition to the MMI and TA-Seq2Seq baselines and human responses are presented in <ref type="table" target="#tab_5">Table 5</ref>. We observe that MMI200 and TA-10 models  achieve slightly better plausibility scores since they tend to generate safe, dull responses. How- ever, we find that when using a beam size of 200 and MMI re-ranking, our approach which incor- porates distributional constraints, DC-MMI200, achieves competitive plausibility, while achieving significantly higher content richness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Statistical Significance of Results</head><p>To verify the statistical significance of our find- ings, we conducted a pairwise bootstrap test <ref type="bibr">(Efron and Tibshirani, 1994;</ref><ref type="bibr" target="#b4">Berg-Kirkpatrick et al., 2012)</ref> comparing the difference between percentage of Agree annotations (Yes column in the <ref type="table" target="#tab_5">Table 5</ref>). We computed p-values for each pair of models: MMI200 vs DC-MMI200 and TA vs DC-MMI200. For plausibility, we did not find a significant difference in either comparison (p- value â‰ˆ 0.25) while for content richness, both differences were found to be significant (p-value &lt;10 âˆ’4 ). To summarize: our model significantly beats both baselines in terms of content richness while the difference in plausibility was not found to be statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Pairwise Evaluation of Interestingness</head><p>To further validate our claims we also did a side by side comparison study between MMI200 and DC- MMI200. For every test case, we showed Mechan- ical Turk workers the source sentence along with responses generated by both systems and asked them select which is more interesting. We observe that in 56% out of 1000 cases, DC-MMI200 was rated as the more interesting response. The result is statistically significant with p-value &lt;4 Ã— 10 âˆ’4 (using an exact binomial test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Model Variations</head><p>To see the effectiveness of our decoding con- straints separately, we compare the best perform- ing DC-MMI200 model with DC-10 and DC- MMI10, both of which use a beam size of 10 -DC-10 does not include MMI reranking. The results of Mechanical Turk evaluation, following the approach described in Section 7.2, are pre- sented in <ref type="table" target="#tab_7">Table 6</ref>. We observe that with a beam size of 10 our model is able to generate content rich responses, but suffers in terms of plausibil- ity. The values in the table suggests the decoding constraints defined in this work successfully inject content words into candidate hypotheses and that MMI is able to effectively choose plausible candi- dates. In the case of DC-10 and DC-MMI10, both models generate the same candidates, but MMI is able to re-rank the results and thus improves plau- sibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Conversational agents primarily fall into two cate- gories: task oriented dialogue systems ( <ref type="bibr" target="#b36">Williams et al., 2013;</ref><ref type="bibr" target="#b34">Wen et al., 2015</ref>) and chatbots <ref type="bibr" target="#b33">(Weizenbaum, 1966)</ref>, although there have been some efforts to integrate the two ( <ref type="bibr">Dodge et al., 2015;</ref>). Some of the earliest work on data-driven chatbots <ref type="bibr" target="#b23">(Ritter et al., 2011</ref>) explored the use of phrase-based Statistical Ma- chine Translation (SMT) on large numbers of con- versations gathered from Twitter ( <ref type="bibr" target="#b22">Ritter et al., 2010)</ref>. Subsequent progress on the use of neu- ral networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation ( <ref type="bibr" target="#b25">Shang et al., 2015;</ref><ref type="bibr" target="#b27">Sordoni et al., 2015;</ref><ref type="bibr" target="#b12">Li et al., 2016a)</ref>. Our approach, which incorporates distributional constraints into the decoding objective, is related to prior work on posterior regularization <ref type="bibr" target="#b19">(Mann and McCallum, 2008;</ref><ref type="bibr">Ganchev et al., 2010;</ref><ref type="bibr" target="#b41">Zhu et al., 2014</ref>). Posterior regularization introduces similar distributional constraints on expectations computed over unlabeled data using a model's pa- rameters. These are typically added to the learn- ing objective for semi-supervised scenarios where available labeled data is limited. In contrast, our approach introduces distributional constraints into the decoding objective as a way to combine neural conversation models trained on large quantities of conversational data with separately trained mod- els of topics and semantic similarity that can drive content selection.</p><p>There are numerous examples of related work on improving neural conversation models. <ref type="bibr" target="#b26">Shao et. al. (2017)</ref> introduced a stochastic approach to beam search that does segment-by-segment reranking to promote diversity. <ref type="bibr" target="#b40">Zhang et. al. (2018)</ref> develop models which converse while as- suming a persona defined by a short description of attributes. <ref type="bibr" target="#b32">Wang et. al. (2017)</ref> suggested de- coding methods that influence the style and topic of the generated response. <ref type="bibr">Bosselutet al. (2018)</ref> develop discourse-aware rewards with reinforce- ment learning (RL) to generate long and coherent texts. <ref type="bibr" target="#b15">Li et. al. (2016c)</ref> applied deep reinforce- ment learning to dialogue generation to maximize long-term reward of the conversation, as opposed to directly maximizing likelihood of the response. This line of work was further extended with adver- sarial learning ( <ref type="bibr" target="#b16">Li et al., 2017</ref>) that rewards gener- ated conversations that are indistinguishable from real conversations in the data. <ref type="bibr" target="#b11">Lewis et. al. (2017)</ref> applied reinforcement learning with dialogue roll- outs to generate replies that maximize expected re- ward, while learning to generate responses from a crowdsourced dataset of negotiation dialogues.  used crowd-workers to gather a corpus of 100K information-seeking QA dia- logues that are answerable using text spans from Wikipedia. <ref type="bibr" target="#b21">Niu and Bansal (2018)</ref> designed a number of weakly-supervised models that gener- ate polite, neutral or rude responses. Their fusion model combines a language model trained on po- lite utterances with the decoder. In the second method they prepend the utterance with a polite- ness label and scale its embedding to vary polite- ness. The third model is Polite-RL which assigns a reward based on a politeness classifier. <ref type="bibr">Gimpel et. al. (2013)</ref> explored methods for increasing the di- versity of N-best lists in machine translation by in-troducing a pairwise dissimilarity function. Simi- lar ideas have been explored in the context of neu- ral generation models. ( <ref type="bibr" target="#b31">Vijayakumar et al., 2016;</ref><ref type="bibr" target="#b13">Li and Jurafsky, 2016;</ref><ref type="bibr" target="#b14">Li et al., 2016b)</ref> Following previous work we evaluated our ap- proach using a combination of automatic metrics and human judgments. Some recent work has ex- plored the possibility of adversarial evaluation of neural conversation models ( <ref type="bibr" target="#b18">Lowe et al., 2017;</ref><ref type="bibr" target="#b16">Li et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>We presented an approach to generate more in- teresting responses in neural conversation models by incorporating side information in the form of distributional constraints. When using maximum likelihood decoding objectives, neural conversa- tion models tend to generate safe responses, such as "I don't know" for most inputs. Our proposed approach provides a flexible method of incorporat- ing a broad range of distributional constraints into the decoding objective. We proposed and empiri- cally evaluated two constraints that factorize over words, and therefore naturally fit into the com- monly used left-to-right beam search decoding framework. The first encourages the use of more relevant topic words in the response the second en- courages semantic similarity between the source and target. We empirically demonstrated, through human evaluation, that when taken together these constraints lead to responses that contribute sig- nificantly more information to the conversation, while maintaining plausibility in the context of the input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Test set from Cornell Movie Dialogue Corpus.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Automatic metrics evaluation. The 3 rd and 4 th columns show the ratio of types to tokens for unigrams 
and bigrams respectively. 7 th Column shows the % of stop-words generated by the models in their responses. 

Model Alias No(%) Unsure(%) Yes(%) 
Plausible? 
human 
19.807 
23.448 
56.745 
MMI200 
27.623 
26.445 
45.931 
TA-10 
26.981 
26.874 
46.146 
DC-MMI200 30.835 
24.41 
44.754 
Content Richness? 
human 
16.488 
19.914 
63.597 
MMI200 
23.662 
32.976 
43.362 
TA-10 
31.799 
30.086 
38.116 
DC-MMI200 20.021 
26.660 
53.319 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Human judgments for Plausibility of the dif-
ferent models. Each numerical cell contains a percent-
age value corresponding to its row truncated to 2 deci-
mal precision. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparing the model variation by reducing 
beam size to 10 and also comparing decoder constraints 
without MMI reranking 

</table></figure>

			<note place="foot" n="1"> https://research.googleblog.com/2015/ 11/computer-respond-to-this-email.html</note>

			<note place="foot" n="2"> https://www.ranks.nl/stopwords 3 The top 10 topic words were taken from each of the 50 topics inferred by an HMM-LDA model (after removing stop words).</note>

			<note place="foot" n="4"> https://github.com/abaheti95/HMM-LDA</note>

			<note place="foot" n="5"> Assuming topic distribution of syntax words to be uniform, a sentence with more syntax words will dampen modes in the distribution. Alternately, with less syntax words the overall distribution will be more peaked.</note>

			<note place="foot" n="6"> http://nlp.stanford.edu/data/OpenSubData.tar 7 OpenNMT is used for training our models (Klein et al., 2017).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able feedback. This material is based upon work supported by the National Science Foundation un-der Grant No. IIS-1464128.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collective content selection for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical investigation of statistical significance in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="995" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03766</idno>
		<title level="m">Discourse-aware neural rewards for coherent text generation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quac : Question answering in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="955" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deal or no deal? end-to-end learning of negotiation dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mutual information and diverse decoding improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00372</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>SË™ Ebastien Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2157" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards an automatic turing test: Learning to evaluate dialogue responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Angelard-Gontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1116" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized expectation criteria for semi-supervised learning of conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gideon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="870" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference of the north american chapter of the association for computational linguistics: Hlt-naacl</title>
		<meeting>the human language technology conference of the north american chapter of the association for computational linguistics: Hlt-naacl</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Polite dialogue generation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised modeling of twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating high-quality and informative conversation responses with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2210" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">News from opus-a collection of multilingual parallel corpora with tools and interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¶rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent advances in natural language processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Steering output style and topic in neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2140" to="2150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Elizaa computer program for the study of natural language communication between man and machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>MrkÅ¡iÂ´mrkÅ¡iÂ´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting recurrent networks for paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2078" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepak Ramachandran, and Alan Black</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2013</title>
		<meeting>the SIGDIAL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>The dialog state tracking challenge</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conference</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning conversational systems that interleave task and non-task content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentySixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the TwentySixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07243</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>do you have pets too? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian inference with posterior regularization and applications to infinite latent svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1799" to="1847" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
