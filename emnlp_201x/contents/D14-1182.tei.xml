<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sometimes Average is Best: The Importance of Averaging for Prediction using MCMC Inference in Topic Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-An</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Resnik</forename><surname>Linguistics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umiacs</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sometimes Average is Best: The Importance of Averaging for Prediction using MCMC Inference in Topic Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1752" to="1757"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Markov chain Monte Carlo (MCMC) approximates the posterior distribution of latent variable models by generating many samples and averaging over them. In practice, however, it is often more convenient to cut corners, using only a single sample or following a suboptimal averaging strategy. We systematically study different strategies for averaging MCMC samples and show empirically that averaging properly leads to significant improvements in prediction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic topic models are powerful methods to un- cover hidden thematic structures in text by projecting each document into a low dimensional space spanned by a set of topics, each of which is a distribution over words. Topic models such as latent Dirichlet alloca- tion ( <ref type="bibr">Blei et al., 2003, LDA)</ref> and its extensions discover these topics from text, which allows for effective ex- ploration, analysis, and summarization of the otherwise unstructured corpora <ref type="bibr" target="#b4">(Blei, 2012;</ref><ref type="bibr" target="#b5">Blei, 2014)</ref>.</p><p>In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unanno- tated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable <ref type="bibr" target="#b2">(Blei and McAuliffe, 2007;</ref><ref type="bibr" target="#b27">Zhu et al., 2009;</ref><ref type="bibr" target="#b15">Nguyen et al., 2013)</ref>, single label <ref type="bibr" target="#b20">(Rosen-Zvi et al., 2004;</ref><ref type="bibr" target="#b11">Lacoste-Julien et al., 2008;</ref><ref type="bibr" target="#b25">Wang et al., 2009)</ref> or multiple labels ( <ref type="bibr" target="#b17">Ramage et al., 2009;</ref><ref type="bibr" target="#b18">Ramage et al., 2011</ref>) to predict metadata from text.</p><p>Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the poste- rior is often intractable, which motivates approximate inference techniques <ref type="bibr" target="#b1">(Asuncion et al., 2009)</ref>. One popu- lar approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC al- gorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate samples to estimate document-level latent variables for test data. The underlying theory requires averaging on both training and test samples, but in practice it is often convenient to cut corners: either skip averaging entirely by using just the values of the last sample or use a single training sample and average over test samples.</p><p>We systematically study non-averaging and averaging strategies when performing predictions using MCMC in topic modeling (Section 2). Using popular unsupervised (LDA in Section 3) and supervised (SLDA in Section 4) topic models via thorough experimentation, we show empirically that cutting corners on averaging leads to consistently poorer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning and Predicting with MCMC</head><p>While reviewing all of MCMC is beyond the scope of this paper, we need to briefly review key concepts. <ref type="bibr">1</ref> To estimate a target density p(x) in a high-dimensional space X , MCMC generates samples {x t } T t=1 while ex- ploring X using the Markov assumption. Under this assumption, sample x t+1 depends on sample x t only, forming a Markov chain, which allows the sampler to spend more time in the most important regions of the density. Two concepts control sample collection:</p><p>Burn-in B: Depending on the initial value of the Markov chain, MCMC algorithms take time to reach the target distribution. Thus, in practice, samples before a burn-in period B are often discarded.</p><p>Sample-lag L: Averaging over samples to estimate the target distribution requires i.i.d. samples. However, future samples depend on the current samples (i.e., the Markov assumption). To avoid autocorrelation, we dis- card all but every L samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MCMC in Topic Modeling</head><p>As generative probabilistic models, topic models define a joint distribution over latent variables and observable evidence. In our setting, the latent variables consist of corpus-level global variables g and document-level lo- cal variables l; while the evidence consists of words w and additional metadata y-the latter omitted in unsu- pervised models.</p><p>During  </p><formula xml:id="formula_0">(l TE | w TE , ˆ g(i)</formula><p>) of test data. Each sample j of test chain i provides a fully estimated local latent variablesîvariablesˆvariablesî TE (i, j) to make a prediction. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview. To reduce the ef- fects of unconverged and autocorrelated samples, dur- ing training we use a burn-in period of B TR and a sample-lag of L TR iterations. We use</p><formula xml:id="formula_1">T TR = {i | i ∈ (B TR , T TR ] ∧ (i − B TR ) mod L TR = 0}</formula><p>to denote the set of indices of the selected models. Similarly, B TE and L TE are the test burn-in and sample-lag. The set of indices of selected samples in test chains is</p><formula xml:id="formula_2">T TE = {j | j ∈ (B TE , T TE ] ∧ (j − B TE ) mod L TE = 0}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Averaging Strategies</head><p>We use S(i, j) to denote the prediction obtained from sample j of the test chain i. We now discuss different strategies to obtain the final prediction:</p><p>• Single Final (SF) uses the last sample of last test chain to obtain the predicted value, SSF = S(TTR, TTE).</p><p>(1)</p><p>• Single Average (SA) averages over multiple sam- ples in the last test chain</p><formula xml:id="formula_3">SSA = 1 |TTE| j∈TTE S(TTR, j).<label>(2)</label></formula><p>This is a common averaging strategy in which we obtain a point estimate of the global latent variables at the end of the training chain. Then, a single test chain is generated on the test data and multiple sam- ples of this test chain are averaged to obtain the final prediction <ref type="bibr" target="#b7">(Chang, 2012;</ref><ref type="bibr" target="#b21">Singh et al., 2012;</ref><ref type="bibr" target="#b8">Jiang et al., 2012;</ref><ref type="bibr" target="#b28">Zhu et al., 2014</ref>).</p><p>• Multiple Final (MF) averages over the last sam- ples of multiple test chains from multiple models</p><formula xml:id="formula_4">SMF = 1 |TTR| i∈TTR S(i, TTE).<label>(3)</label></formula><p>• Multiple Average (MA) averages over all samples of multiple test chains for distinct models,</p><formula xml:id="formula_5">SMA = 1 |TTR| 1 |TTE| i∈TTR j∈TTE S(i, j),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Topic Models</head><p>We evaluate the predictive performance of the unsu- pervised topic model LDA using different averaging strategies in Section 2.</p><p>LDA: Proposed by Blei et al. in 2003, LDA posits that each document d is a multinomial distribution θ d over K topics, each of which is a multinomial distribution φ k over the vocabulary. LDA's generative process is:</p><formula xml:id="formula_6">1. For each topic k ∈ [1, K] (a) Draw word distribution φ k ∼ Dir(β) 2. For each document d ∈ [1, D] (a) Draw topic distribution θ d ∼ Dir(α) (b) For each word n ∈ [1, N d ] i. Draw topic z d,n ∼ Mult(θ d ) ii. Draw word w d,n ∼ Mult(φ z d,n )</formula><p>In LDA, the global latent variables are topics {φ k } K k=1 and the local latent variables for each document d are topic proportions θ d .</p><p>Train: During training, we use collapsed Gibbs sam- pling to assign each token in the training data with a topic ( <ref type="bibr" target="#b22">Steyvers and Griffiths, 2006</ref>). The probability of assigning token n of training document d to topic k is</p><formula xml:id="formula_7">p(z TR d,n = k | z TR −d,n , w TR −d,n , w TR d,n = v) ∝ N −d,n TR,d,k + α N −d,n TR,d,· + Kα · N −d,n TR,k,v + β N −d,n TR,k,· + V β ,<label>(5)</label></formula><p>where N TR,d,k is the number of tokens in the training document d assigned to topic k, and N TR,k,v is the num- ber of times word type v assigned to topic k. Marginal counts are denoted by ·, and −d,n denotes the count excluding the assignment of token n in document d.</p><p>At each training iteration i, we estimate the distribu- tion over wordsˆφwordsˆ wordsˆφ k (i) of topic k asˆφ</p><formula xml:id="formula_8">asˆ asˆφ k,v (i) = N TR,k,v (i) + β N TR,k,· (i) + V β<label>(6)</label></formula><p>where the counts N TR,k,v (i) and N TR,k,· (i) are taken at training iteration i. </p><formula xml:id="formula_9">n in w TE1 d is p(z TE1 d,n = k | z TE1 −d,n , w TE1 , ˆ φ(i)) ∝ N −d,n TE1,d,k + α N −d,n TE1,d,· + Kα · ˆ φ k,w TE 1 d,n (i)<label>(7)</label></formula><p>where N TE1,d,k is the number of tokens in w TE1 d assigned to topic k. At each iteration j in test chain i, we can estimate the topic proportion vectorˆθvectorˆ vectorˆθ TE d (i, j) for test document d asˆθ</p><formula xml:id="formula_10">asˆ asˆθ TE d,k (i, j) = N TE1,d,k (i, j) + α N TE1,d,· (i, j) + Kα<label>(8)</label></formula><p>where both the counts N TE1,d,k (i, j) and N TE1,d,· (i, j) are taken using sample j of test chain i.</p><formula xml:id="formula_11">Prediction: GivenˆθGivenˆ Givenˆθ TE d (i, j) andˆφandˆ andˆφ(i) at</formula><note type="other">sample j of test chain i, we compute the predicted likeli- hood for each unseen token w</note><formula xml:id="formula_12">TE2 d,n as S(i, j) ≡ p(w TE2 d,n | ˆ θ TE d (i, j), ˆ φ(i)) = K k=1ˆθ k=1ˆ k=1ˆθ TE d,k (i, j) · ˆ φ k,w TE 2 d,n<label>(i)</label></formula><p>. Using different strategies described in Section 2, we obtain the final predicted likelihood for each un- seen token p(w TE2 d,n | ˆ θ TE d , ˆ φ) and compute the perplex-</p><formula xml:id="formula_13">ity as exp −( d n log(p(w TE2 d,n | ˆ θ TE d , ˆ φ)))/N TE2</formula><p>where N TE2 is the number of tokens in w TE2 .</p><p>Setup: We use three Internet review datasets in our experiment. For all datasets, we preprocess by tokeniz- ing, removing stopwords, stemming, adding bigrams to the vocabulary, and we filter using TF-IDF to obtain a vocabulary of 10,000 words. <ref type="bibr">3</ref> The three datasets are:</p><p>• HOTEL: 240,060 reviews of hotels from TripAdvi- sor ( <ref type="bibr" target="#b26">Wang et al., 2010</ref>).</p><p>• RESTAURANT: 25,459 reviews of restaurants from Yelp (Jo and Oh, 2011).</p><p>• MOVIE: 5,006 reviews of movies from Rotten Tomatoes (Pang and <ref type="bibr" target="#b16">Lee, 2005)</ref> We report cross-validated average performance over five folds, and use K = 50 topics for all datasets. To update the hyperparameters, we use slice sampling <ref type="bibr" target="#b24">(Wallach, 2008</ref>, p. 62). 4</p><p>Results: <ref type="figure" target="#fig_2">Figure 2 shows</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supervised Topic Models</head><p>We evaluate the performance of different prediction methods using supervised latent Dirichlet allocation (SLDA) <ref type="bibr" target="#b2">(Blei and McAuliffe, 2007</ref>) for sentiment anal- ysis: predicting review ratings given review text. Each review text is the document w d and the metadata y d is the associated rating.</p><p>SLDA: Going beyond LDA, SLDA captures the rela- tionship between latent topics and metadata by mod- eling each document's continuous response variable using a normal linear model, whose covariates are the document's empirical distribution of topics:</p><formula xml:id="formula_14">y d ∼ N (η T ¯ z d , ρ)</formula><p>where η is the regression parameter vec- tor and ¯ z d is the empirical distribution over topics of document d. The generative process of SLDA is:</p><formula xml:id="formula_15">1. For each topic k ∈ [1, K] (a) Draw word distribution φ k ∼ Dir(β) (b) Draw parameter η k ∼ N (µ, σ) 2. For each document d ∈ [1, D] (a) Draw topic distribution θ d ∼ Dir(α) (b) For each word n ∈ [1, N d ] i. Draw topic z d,n ∼ Mult(θ d ) ii. Draw word w d,n ∼ Mult(φ z d,n ) (c) Draw response y d ∼ N (η T ¯ z d , ρ) where ¯ z d,k = 1 N d N d n=1 I [z d,n = k]</formula><p>where I [x] = 1 if x is true, and 0 otherwise.</p><p>In SLDA, in addition to the K multinomials {φ k } K k=1 , the global latent variables also contain the regression parameter η k for each topic k. The local latent variables of SLDA resembles LDA's: the topic proportion vector θ d for each document d.</p><p>Train: For posterior inference during training, follow- ing Boyd- <ref type="bibr" target="#b6">Graber and Resnik (2010)</ref>, we use stochastic EM, which alternates between (1) a Gibbs sampling step to assign a topic to each token, and (2) optimizing the regression parameters. The probability of assigning topic k to token n in the training document d is</p><formula xml:id="formula_16">p(z TR d,n = k | z TR −d,n , w TR −d,n , w TR d,n = v) ∝ N (y d ; µ d,n , ρ) · N −d,n TR,d,k + α N −d,n TR,d,· + Kα · N −d,n TR,k,v + β N −d,n TR,k,· + V β<label>(9)</label></formula><p>where</p><formula xml:id="formula_17">µ d,n = ( K k =1 η k N −d,n TR,d,k + η k )/N TR,d</formula><p>is the mean of the Gaussian generating y d if z TR d,n = k. Here, N TR,d,k is the number of times topic k is assigned to tokens in the training document d; N TR,k,v is the number of times word type v is assigned to topic k; · represents marginal counts and −d,n indicates counts excluding the assignment of token n in document d.</p><p>We optimize the regression parameters η using L- BFGS ( <ref type="bibr" target="#b12">Liu and Nocedal, 1989</ref>) via the likelihood</p><formula xml:id="formula_18">L(η) = − 1 2ρ D d=1 (y TR d −η T ¯ z TR d ) 2 − 1 2σ K k=1 (η k −µ) 2 (10)</formula><p>At each iteration i in the training chain, the estimated global latent variables include the a multinomiaî φ k (i) and a regression parameterˆηparameterˆ parameterˆη k (i) for each topic k.</p><p>Test: Like LDA, at test time we sample the topic as- signments for all tokens in the test data</p><formula xml:id="formula_19">p(z TE d,n = k | z TE −d,n , w TE ) ∝ N −d,n TE,d,k + α N −d,n TE,d,· + Kα · ˆ φ k,w TE d,n (11)</formula><p>Prediction: The predicted value S(i, j) in this case is the estimated value of the metadata review rating</p><formula xml:id="formula_20">S(i, j) ≡ ˆ y TE d (i, j) = ˆ η(i) T ¯ z TE d (i, j),<label>(12)</label></formula><p>where the empirical topic distribution of test document    Methods using multiple test chains (MF and MA) perform as well as or better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse.</p><formula xml:id="formula_21">d is ¯ z TE d,k (i, j) ≡ 1 N TE,d N TE,d n=1 I z TE d,n (i, j) = k .</formula><p>Experimental setup: We use the same data as in Sec- tion 3. For all datasets, the metadata are the review rating, ranging from 1 to 5 stars, which is standard- ized using z-normalization. We use two evaluation metrics: mean squared error (MSE) and predictive R- squared ( <ref type="bibr" target="#b2">Blei and McAuliffe, 2007)</ref>. For comparison, we consider two baselines: (1) multi- ple linear regression (MLR), which models the metadata as a linear function of the features, and (2) support vec- tor regression <ref type="bibr">(Joachims, 1999, SVR)</ref>. Both baselines use the normalized frequencies of unigrams and bigrams as features. As in the unsupervised case, we report av- erage performance over five cross-validated folds. For all models, we use a development set to tune their pa- rameter(s) and use the set of parameters that gives best results on the development data at test. 5</p><p>Results: <ref type="figure" target="#fig_3">Figure 3</ref> shows SLDA prediction results with different averaging strategies, computed at different training iterations. <ref type="bibr">6</ref> Consistent with the unsupervised results in Section 3, SA outperforms SF, but both are outperformed significantly by the two methods using multiple test chains (MF and MA).</p><p>We also compare the performance of the four pre- diction methods obtained at the final iteration T TR of the training chain with the two baselines. The results in <ref type="figure" target="#fig_7">Figure 4</ref> show that the two baselines (MLR and SVR) out- perform significantly the SLDA using only a single test <ref type="bibr">5</ref> For MLR we use a Gaussian prior N (0, 1/λ) with λ = a · 10 b where a ∈ <ref type="bibr">[1,</ref><ref type="bibr">9]</ref> and b ∈ <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>; for SVR, we use SVM light <ref type="bibr" target="#b10">(Joachims, 1999</ref>) and vary C ∈ <ref type="bibr">[1,</ref><ref type="bibr">50]</ref>, which trades off between training error and margin; for SLDA, we fix σ = 10 and vary ρ ∈ {0.1, 0.5, 1.0, 1.5, 2.0}, which trades off between the likelihood of words and response variable. <ref type="bibr">6</ref> MCMC setup: TTR = 5, 000 for RESTAURANT and MOVIE and 1, 000 for HOTEL; for all datasets BTR = 500, LTR = 50, TTE = 100, BTE = 20 and LTE = 5. chains (SF and SA). Methods using multiple test chains (MF and MA), on the other hand, match the baseline 7 (HOTEL) or do better (RESTAURANT and MOVIE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>MCMC relies on averaging multiple samples to approxi- mate target densities. When used for prediction, MCMC needs to generate and average over both training sam- ples to learn from training data and test samples to make prediction. We have shown that simple averaging-not more aggressive, ad hoc approximations like taking the final sample (either training or test)-is not just a ques- tion of theoretical aesthetics, but an important factor in obtaining good prediction performance.</p><p>Compared with SVR and MLR baselines, SLDA using multiple test chains (MF and MA) performs as well as or better, while SLDA using a single test chain (SF and SA) falters. This simple experimental setup choice can determine whether a model improves over reasonable baselines. In addition, better prediction with shorter training is possible with multiple test chains. Thus, we conclude that averaging using multiple chains produces above-average results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of training and test chains in MCMC, showing samples used in four prediction strategies studied in this paper: Single Final (SF), Single Average (SA), Multiple Final (MF), and Multiple Average (MA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Test:</head><label></label><figDesc>Because we lack explicit topic annotations for these data (c.f. Nguyen et al. (2012)), we use perplexity- a widely-used metric to measure the predictive power of topic models on held-old documents. To compute perplexity, we follow the estimating θ method (Wal- lach et al., 2009, Section 5.1) and evenly split each test document d into w TE1 d and w TE2 d . We first run Gibbs sampling on w TE1 d to estimate the topic proportionˆθproportionˆ proportionˆθ TE d of test document d. The probability of assigning topic k to token</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Perplexity of LDA using different averaging strategies with different number of training iterations T TR. Perplexity generally decreases with additional training iterations, but the drop is more pronounced with multiple test chains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of SLDA using different averaging strategies computed at each training iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of SLDA using different averaging strategies computed at the final training iteration T TR , compared with two baselines MLR and SVR. Methods using multiple test chains (MF and MA) perform as well as or better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>the perplexity of the four averaging methods, computed with different number of training iterations T TR . SA outperforms SF, showing the benefits of averaging over multiple test samples from a single test chain. However, both multiple chain methods (MF and MA) significantly outperform these two methods. This result is consistent with Asuncion et al. (2009), who run multiple training chains but a single test chain for each training chain and average over them. This is more costly since training chains are usually signif- icantly longer than test chains. In addition, multiple training chains are sensitive to their initialization.</figDesc><table>MSE 

pR.squared 

0.60 

0.65 

0.70 

0.75 

0.25 

0.30 

0.35 

0.40 

1000 
2000 
3000 
4000 
5000 

1000 
2000 
3000 
4000 
5000 

Number of iterations 

(a) Restaurant reviews 

MSE 

pR.squared 

9000 

10000 

11000 

12000 

13000 

30000 

31000 

32000 

33000 

34000 

1000 
2000 
3000 
4000 
5000 

1000 
2000 
3000 
4000 
5000 

Number of iterations 

(b) Movie reviews 

MSE 

pR.squared 

0.400 

0.425 

0.450 

0.475 

0.500 

0.500 

0.525 

0.550 

0.575 

0.600 

600 
700 
800 
900 
1000 

600 
700 
800 
900 
1000 

Number of iterations 

</table></figure>

			<note place="foot" n="1"> For more details please refer to Neal (1993), Andrieu et al. (2003), Resnik and Hardisty (2010). 2 We omit hyperparameters for clarity. We split data into training (TR) and testing (TE) folds, and denote the training iteration i and the testing iteration j within the corresponding Markov chains.</note>

			<note place="foot" n="3"> To find bigrams, we begin with bigram candidates that occur at least 10 times in the corpus and use a χ 2 test to filter out those having a χ 2 value less than 5. We then treat selected bigrams as single word types and add them to the vocabulary. 4 MCMC setup: TTR = 1, 000, BTR = 500, LTR = 50, TTE = 100, BTE = 50 and LTE = 5.</note>

			<note place="foot" n="7"> This gap is because SLDA has not converged after 1,000 training iterations (Figure 3).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jonathan Chang, Ke Zhai and Mohit Iyyer for helpful discussions, and thank the anonymous reviewers for insightful comments. This research was supported in part by NSF under grant #1211153 (Resnik) and #1018625 (Boyd-Graber and Resnik). Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An introduction to MCMC for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="5" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On smoothing and inference for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Latent Dirichlet allocation. JMLR, 3</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Build, compute, critique, repeat: Data analysis with latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Graber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">lda: Collapsed Gibbs sampling methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://cran.r-project.org/web/packages/lda/index.html" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monte Carlo methods for maximum margin supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixia</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aspect and sentiment unification model for online review analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making large-scale SVM learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning, chapter 11</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DiscLDA: Discriminative learning for dimensionality reduction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Math. Prog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probabilistic inference using Markov chain Monte Carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<idno>CRG-TR- 93-1</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SITS: A hierarchical nonparametric model using speaker identity for topic segmentation in multiparty conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lexical and hierarchical topic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Partially labeled topic models for interpretable text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="457" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gibbs sampling for the uninitiated</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Hardisty</surname></persName>
		</author>
		<idno>UMIACS-TR-2010-04</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monte Carlo MCMC: Efficient inference by approximate sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1104" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum</title>
		<editor>T. Landauer, D. Mcnamara, S. Dennis, and W. Kintsch</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimno</surname></persName>
		</author>
		<editor>Leon Bottou and Michael Littman</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Structured Topic Models for Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanna M Wallach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis on review text data: A rating regression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MedLDA: maximum margin supervised topic models for regression and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gibbs max-margin topic models with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1073" to="1110" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
