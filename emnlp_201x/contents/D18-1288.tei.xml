<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deconvolutional Time Series Regression: A Technique for Modeling Temporally Diffuse Effects</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Shain</surname></persName>
							<email>shain.3@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
							<email>schuler.77@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deconvolutional Time Series Regression: A Technique for Modeling Temporally Diffuse Effects</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2679" to="2689"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2679</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Researchers in computational psycholinguis-tics frequently use linear models to study time series data generated by human subjects. However, time series may violate the assumptions of these models through temporal diffusion , where stimulus presentation has a lingering influence on the response as the rest of the experiment unfolds. This paper proposes a new statistical model that borrows from digital signal processing by recasting the predictors and response as convolutionally-related signals , using recent advances in machine learning to fit latent impulse response functions (IRFs) of arbitrary shape. A synthetic experiment shows successful recovery of true latent IRFs, and psycholinguistic experiments reveal plausible, replicable, and fine-grained estimates of latent temporal dynamics, with comparable or improved prediction quality to widely-used alternatives.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series are abundant in many naturally- occurring phenomena of interest to science, and they frequently violate the assumptions of linear modeling and its generalizations. One confound that may be widespread in psycholinguistic data is temporal diffusion: the dependent variable may evolve slowly in response to its inputs, with the result that a particular predictor observed at a par- ticular time may continue to exert an influence on the response as the rest of the process unfolds. If not properly controlled for, such a confound could have a detrimental impact on parameter estima- tion, model interpretation, and hypothesis testing.</p><p>The problem of temporal diffusion remains largely unsolved in the general case. 1 A stan-dard approach for handling the possibility of tem- porally diffuse relationships between the predic- tors and the response is to use spillover or lag regressors, where the observed predictor value is used to predict subsequent observations of the re- sponse ( <ref type="bibr" target="#b6">Erlich and Rayner, 1983)</ref>. But this strat- egy has several undesirable properties. First, the choice of spillover position(s) for a given predic- tor is difficult to motivate empirically. Second, in experiments with variably long trials the use of relative event indices obscures potentially impor- tant details about the actual amount of time that passed between events. And third, including mul- tiple spillover positions per predictor quickly leads to parametric explosion on realistically complex models over realistically sized data sets, especially if random effects structures are included.</p><p>As a solution to the problem of temporal diffu- sion, this paper proposes deconvolutional time se- ries regression (DTSR), a technique that directly models diffusion by learning parametric impulse response functions (IRFs) of the predictors that mediate their relationship to the response variable over time. Parametric deconvolution is difficult in the general case because the likelihood surface depends on the choice IRF kernel, requiring the user to re-derive estimators for each unique model structure. Furthermore, arbitrary IRF kernels are not guaranteed to afford analytical estimator func- tions or unique real-valued solutions. However, recent advances in machine learning have led to libraries like Tensorflow ( <ref type="bibr">Abadi et al., 2015</ref>) - which uses auto-differentiation to support opti- mization of arbitrary computation graphs -and Edward ( <ref type="bibr" target="#b24">Tran et al., 2016</ref>) -which enables black box variational inference (BBVI) on Tensorflow graphs. While these libraries are typically used to build and train deep networks, DTSR uses them using generalized additive models (GAM) with a particular structure ( <ref type="bibr" target="#b1">Baayen et al., 2017</ref><ref type="bibr" target="#b2">Baayen et al., , 2018</ref>.  to overcome the aforementioned difficulties with general-purpose temporal deconvolution by elim- inating the need for hand-derivation of estimators and sampling distributions for each model. The IRFs learned by DTSR are interpretable as estimates of the temporal shape of predictors' influence on the response variable. By convolv- ing predictors with their IRFs, DTSR is able to consider arbitrarily long histories of independent variable observations in generating a given predic- tion, and (in contrast to spillover) model complex- ity is constant on the length of the history win- dow. DTSR is thus a parsimonious technique for directly measuring temporal diffusion.</p><p>Figures 1-3 illustrate the present proposal and how it differs from linear time series models. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a standard linear model as- sumes conditional independence of the response from all preceding observations of the predictor. This independence assumption can be weakened by including additional spillover predictors <ref type="figure" target="#fig_1">(Fig- ure 2)</ref>, at a cost of requiring additional parameters. In both cases, only the relative order of events is considered, not their actual distance in time. By contrast, DTSR recasts the predictor and response vectors as streams of impulses and responses (re- spectively) localized in time. It then fits latent IRFs that govern the influence of each predictor value on the response as a function of time ( This paper presents evidence that DTSR can (1) recover known underlying IRFs from synthetic data, (2) discover previously unknown temporal structure in human data (psycholinguistic reading time experiments), (3) provide support for the ab- sence of temporal diffusion in settings where it might exist in principle, and (4) provide compara- ble (or in some cases improved) prediction quality to standard linear mixed-effects (LME) and gener- alized additive (GAM) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 Non-deconvolutional time series modeling</head><p>The two most widely used tools for analyzing psy- cholinguistic time series are linear mixed effects regression (LME) ( <ref type="bibr" target="#b3">Bates et al., 2015)</ref> and gener- alized additive models (GAM) <ref type="bibr" target="#b13">(Hastie and Tibshirani, 1986;</ref><ref type="bibr" target="#b27">Wood, 2006</ref>). LME learns a lin- ear combination of the predictors that generates a given response variable. GAM generalizes lin- ear models by allowing the response variable to be computed as the sum of smooth functions of one or more predictors.</p><p>In both approaches, responses are modeled as conditionally independent of preceding obser- vations of predictors unless spillover terms are added, with the attendant drawbacks discussed in Section 1. To make this point more forcefully, take for example <ref type="bibr" target="#b23">Shain et al. (2016)</ref>, who find signif- icant effects of constituent wrap-up (p = 2.33e- 14) and dependency locality (p = 4.87e-10) in the Natural Stories self-paced reading corpus ( <ref type="bibr" target="#b9">Futrell et al., 2018)</ref>. They argue that this constitutes the first strong evidence of memory effects in broad- coverage sentence processing. However, it turns out that when one baseline predictor -probabilis- tic context free grammar (PCFG) surprisal -is spilled over one position, the reported effects dis- appear: p = 0.816 for constituent wrap-up and p = 0.370 for dependency locality. Thus, a reasonable but ultimately inaccurate assumption about base- line effect timecourses can have a dramatic im- pact on the conclusions supported by the statistical model. DTSR offers a way forward by bringing temporal diffusion under direct statistical control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deconvolutional time series modeling</head><p>Deconvolutional modeling has long been used in a variety of scientific fields, including economics (Ramey, 2016), epidemiology <ref type="bibr" target="#b10">(Goldstein et al., 2011)</ref>, and neuroimaging ( <ref type="bibr" target="#b8">Friston et al., 1998</ref>). Non-parametric deconvolutional models quantize the time series and fit estimates for each time point within some window, similarly to the spillover approach discussed above. These estimates can be unconstrained, as in finite impulse response models (FIR) (H. <ref type="bibr" target="#b12">Glover, 1999;</ref><ref type="bibr" target="#b26">Ward, 2006</ref>), or smoothed with some form of regularization ( <ref type="bibr" target="#b11">Goutte et al., 2000;</ref><ref type="bibr" target="#b21">Pedregosa et al., 2014</ref>). Ad- ditional post-hoc interpolation is necessary in or- der to obtain a closed-form continuous IRF. These non-parametric approaches are prone to paramet- ric explosion as well as sparsity problems when trials are variably spaced in time.</p><p>Parametric deconvolutional approaches (i.e. specific instantiations of DTSR) have evolved in certain fields (e.g. fMRI modeling) to solve particular problems, generally with some independently-motivated IRF kernel like the hemodynamic response function (HRF) ( <ref type="bibr" target="#b8">Friston et al., 1998;</ref><ref type="bibr" target="#b17">Lindquist and Wager, 2007;</ref><ref type="bibr" target="#b18">Lindquist et al., 2009)</ref>. However, to our knowledge DTSR constitutes the first mathematical formulation and software implementation of general-purpose mixed effects parametric deconvolutional re- gression for arbitrary impulse response kernels. DTSR also supports Bayesian inference, enabling quantification of uncertainty in the absence of analytic formulae for standard errors. With these properties, DTSR expands the range of possible applications of parametric deconvolution beyond those fields for which appropriate formulations have already been developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model definition</head><p>This section presents the mathematical definition of DTSR. For readability, only a fixed effects model is presented below, since mixed modeling substantially complicates the equations. The full model definition is provided in Appendix A. Note that the full definition is used to construct all read- ing time models reported in subsequent sections, since they contain random effects.</p><p>Let X ∈ R M ×K be a design matrix of M ob- servations for K predictor variables and y ∈ R N be a vector of N responses, both of which contain contiguous temporally-sorted time series. DTSR models the relationship between X and y using parameters consisting of:</p><formula xml:id="formula_0">• a scalar intercept µ ∈ R • a vector u ∈ R K of K coefficients • a matrix A ∈ R R×K of R IRF kernel param- eters for K fixed impulse vectors • a scalar variance σ 2 ∈ R of the response</formula><p>To define the convolution step, let g k for k ∈ {1, 2, . . . , K} be a set of parametric IRF kernels, one for each predictor; let a ∈ R M and b ∈ R N be vectors of timestamps associated with each obser- vation in X and y, respectively; and let c ∈ N M and d ∈ N N be vectors of series ID's associated with each observation in X and y, respectively. A filter F ∈ R N ×M admits only those observations in X that precede y <ref type="bibr">[n]</ref> in the same time series:</p><formula xml:id="formula_1">F [n,m] def = 1 c [m] = d [n] ∧ a [m] ≤ b [n] 0 otherwise (1)</formula><p>The inputs X can be convolved with each IRF g k by premultiplication with sparse matrix G k ∈ R N ×M for k ∈ {1, 2, ..., K} as defined below:</p><formula xml:id="formula_2">G k = g k b1 − 1a ; A [ * ,k] F (2)</formula><p>The convolution that yields the design matrix of convolved predictors X ∈ R N ×K is then defined using products of the G matrices and the design matrix X: 2</p><formula xml:id="formula_3">X [ * ,k] def = G k X [ * ,k]<label>(3)</label></formula><p>Following convolution, DTSR is simply a linear model. The full model mean is the sum of (1) the intercept µ and (2) the product of the convolved predictor matrix X and the coefficient vector u:</p><formula xml:id="formula_4">y ∼ N µ + X u, σ 2 (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>The present implementation defines the afore- mentioned equations 3 as a Bayesian computation graph in Tensorflow and Edward and trains it with black box variation inference (BBVI) using the Nadam optimizer (Dozat, 2016) 4 with a constant learning rate of 0.01 and minibatches of size 1024. For computational efficiency, histories are trun- cated at 128 timesteps. Prediction from the net- work uses an exponential moving average of pa- rameter iterates with a decay rate of 0.998. Con- vergence was visually diagnosed.</p><p>The present experiments use a ShiftedGamma IRF kernel:</p><formula xml:id="formula_5">f (x; α, β, δ) = β α (x − δ) α−1 e −β(x−δ) Γ(α)<label>(5)</label></formula><p>This is simply the PDF of the Gamma distribution augmented with a shift parameter δ allowing the lower bound of the support of the distribution to deviate from 0. We constrain δ to be strictly neg- ative, thereby allowing the model to find a non- zero instantaneous response. We also constrain k to be strictly greater than 1, which deconfounds the shape and shift parameters. All bounded vari- ables are constrained using the softplus bijection:</p><formula xml:id="formula_6">softplus(x) = log(e x + 1)<label>(6)</label></formula><p>The ShiftedGamma kernel is used here because it can fit a wide range of response shapes and has precedent in the fMRI literature, where HRF kernels are often assumed to be Gamma-shaped ( <ref type="bibr" target="#b18">Lindquist et al., 2009)</ref>. <ref type="bibr">5</ref> All parameters are given normal priors with unit variance. Prior means for the fixed IRF kernel <ref type="bibr">3</ref> As noted above, for expository purposes the definition in Section 3 only supports fixed-effects models. The full def- inition for mixed-effects DTSR models is provided in Ap- pendix A. Mixed models are used throughout the experiments reported below. <ref type="bibr">4</ref> The Adam optimizer ( <ref type="bibr" target="#b16">Kingma and Ba, 2014</ref>) with Nes- terov momentum <ref type="bibr" target="#b19">(Nesterov, 1983)</ref> 5 Other IRF kernels, including spline functions and com- position of convolutions, are supported by the current imple- mentation of DTSR but are not explored in these experiments. More details are provided in the software documentation. parameters are domain-specific and discussed in the experiments sections below. To center the prior at an intercept-only model, <ref type="bibr">6</ref> prior means for the intercept µ and variance σ 2 are set (respec- tively) to the empirical mean and variance of the response, and prior means for both fixed coeffi- cients and random effects 7 are set to 0. Although the Bayesian implementation of DTSR is used for this study because it provides quantification of un- certainty, placing priors on the IRF kernel param- eters is not crucial to the success of the system. In all experiments reported below, the MLE imple- mentation arrives at similar solutions and achieves slightly better error.</p><p>In the interests of enabling the use of DTSR by the scientific community, the implementation of DTSR used here is offered as a documented open-source Python package with support for (1) Bayesian, variational Bayesian, and MLE infer- ences and (2) a variety model structures and im- pulse response kernels. The Tensorflow back- end also enables GPU acceleration where avail- able. Source code and links to documenta- tion are available at https://github.com/ coryshain/dtsr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment 1: Synthetic data</head><p>An initial experiment fits DTSR estimates to syn- thetic data to determine whether the model can re- cover known ground truth IRFs. Synthetic data were synthesized using the following procedure. First, 20 input vectors of size 10,000 were drawn from a standard normal distribution. These values synthesize an impulse stream containing 20 co- variates, each with 10,000 observations. A Shift- edGamma IRF was then drawn for each of the 20 covariates. Coefficients were drawn from a uni- form distribution U(−50, 50), and IRF parame- ters were drawn from the following distributions: <ref type="figure" target="#fig_0">(−1, 0)</ref>. The prior means for the corresponding IRF kernel pa- rameters are placed at the centers of these ranges. The stream of responses was generated by con- volving the covariates with their corresponding IRFs. Gaussian noise with standard deviation 20 was injected into the response following genera- tion. The 10,000 trials were spaced 100ms apart. As shown in <ref type="figure" target="#fig_3">Figure 4</ref> synthetic data are very similar to the ground truth, confirming that when the data-generating model matches the assumptions of DTSR, DTSR can re- cover its latent structure with high fidelity.</p><formula xml:id="formula_7">α ∼ U(1, 6), β ∼ U(0, 5), δ ∼ U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment 2: Human reading times</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Background and experimental design</head><p>The main interest of DTSR is the potential to bet- ter understand real-world dynamical systems like the human sentence processing response. There- fore, Experiment 2 applies DTSR to three exist- ing datasets of naturalistic reading: Natural Sto- ries ( <ref type="bibr" target="#b9">Futrell et al., 2018</ref>), Dundee ( <ref type="bibr" target="#b15">Kennedy et al., 2003)</ref>, and UCL ( <ref type="bibr" target="#b7">Frank et al., 2013)</ref>.</p><p>Natural Stories is a self-paced reading (SPR) corpus consisting of narratives designed to provide context-rich, fluent-sounding stimuli that nonethe- less contain many grammatical constructions that rarely occur naturally in texts. The public release of the corpus contains data collected from 181 subjects. The stimulus set contains 10 stories with a total of 485 sentences and 10,245 tokens, for a total 848,768 fixation events.</p><p>Dundee is an eye-tracking (ET) corpus con- taining newspaper editorials read by 10 subjects, with incremental eye fixation data recorded during reading. The stimulus set contains 20 editorials with a total of 2,368 sentences and 51,502 tokens, for a total of 260,065 fixation events.</p><p>UCL is a reading corpus containing individual sentences that were extracted from novels written by amateur authors. The sentences were shuffled and presented in isolation to 42 subjects. The eye- tracking portion of the UCL corpus used in these experiments contains 205 sentences with a total of 1,931 tokens, for a total of 53,070 fixation events.</p><p>In all experiments, the response variable is log . In addition, DTSR enables fitting of a Rate predictor, which is simply a vec- tor of ones, one for each observation, that is con- volved using a latent IRF. Rate thus measures the response to density of stimulus presentation in the recent past. Since without deconvolution Rate is identical to the intercept, it is excluded from non- deconvolutional baseline models. Following stan- dard practice in psycholinguistics, by-subject ran- dom coefficients for each of these predictors are included in all models (baseline and DTSR). <ref type="bibr">9</ref> ShiftedGamma IRFs are fitted to all predic- tors except Sentence Position, which is assigned a Dirac delta IRF (i.e. a linear coefficient) since it increases linearly within the sentence and is not expected to have a diffuse response. In plots, the Sentence Position estimate is shown as a stick function at time 0s. Prior means used for the IRF kernel parameters are α = 2, β = 5, and δ = −0.5. Together, these priors define an ex- pected exponential-like IRF which decays to near- zero in about 1s, which seems plausible for human reading times. In practice they do not appear to be very constraining, since posterior means of fitted Existing work provides some expectations about the relationships of these variables to read- ing time. Processing difficulty is expected to in- crease with Saccade Length, Word Length, and 5- gram Surprisal, and positive linear relationships have been shown experimentally <ref type="bibr" target="#b4">(Demberg and Keller, 2008)</ref>. Unigram Logprob is expected to be negatively correlated with reading times, since more frequent words are expected to be easier to process. Sentence Position, Trial, and Rate index different kinds of change in the response over time and their relationship has not been carefully stud- ied, in part for lack of deconvolutional regression tools. Although reading times tend to decrease over the course of the experiment ( <ref type="bibr" target="#b2">Baayen et al., 2018)</ref>, suggesting an expected negative effect of Trial, this may be partially explained by temporal diffusion. For the present study, all predictors are rescaled by their standard deviations. <ref type="bibr">10</ref> In all reading experiments, data are partitioned into training (50%), development (25%) and test (25%) sets. Outlier filtering is also performed. For Natural Stories, following <ref type="bibr" target="#b23">Shain et al. (2016)</ref>, items are excluded if they have fixations shorter than 100ms or longer than 3000ms, if they start or end a sentence, or if subjects missed 4 or more subsequent comprehension questions. For Dundee, following van <ref type="bibr" target="#b25">Schijndel and Schuler (2015)</ref>, unfixated items are excluded as well as (1) items following saccades longer than 4 words and (2) starts and ends of sentences, screens, doc- uments, and lines. For UCL, unfixated items are excluded as well as <ref type="formula">(1)</ref> items following saccades longer than 4 words and (2) sentence starts and ends. Partitioning and filtering are applied only to the response series. The entire predictor history remains visible to the model. From a modeling perspective, the primary re- sults of interest in Experiment 2 are the IRFs them- selves and the insights they provide into human sentence processing. However, to check the re- liability of the DTSR estimates, prediction qual- ity on unseen data is compared to that of non- deconvolutional baseline models fitted with LME and GAM. 11 Both baselines are fitted with and without three preceding spillover positions for each predictor (baselines with spillover are desig- nated throughout this paper with the suffix -S). <ref type="bibr">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>The fitted IRFs for Natural Stories, Dundee, and UCL are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. Effect sizes by corpus -computed here as the integral of each IRF over the first 10s -are shown in <ref type="table">Table 1</ref>, along with Natural Stories Dundee UCL Predictor Mean 2.5% 97.5% Mean 2.5% 97.5% Mean 2.5% 97.5% Trial -0.0053 -0.0057 -0.0049 -0.0085 -0.0010 -0.0071 - - - Sent pos 0.0154 0.0148 0.0160 0.0004 -0.0013 0.0022 0.0340 0.0301 0.0379 Rate -0.1853 -0.1858 -0.1848 -0.0649 -0.0659 -0.0640 -0.0806 -0.0832 -0.0781 Sac len - - - 0.0249 0.0216 0.0207 0.0217 0.0209 0.0225 Word len 0.0020 0.0019 0.0021 0.0107 0.0105 0.0109 -8e-07 -1.7e-5 1.4e-5 Unigram 2.6e-6 -5e-6 2.2e-5 -2.0e-6 -3.9e-5 2.8e-5 1e-06 -4e-6 1.2e-5 5-gram 0.0057 0.0056 0.0059 0.0139 0.0134 0.0145 0.0159 0.0148 0.0171 <ref type="table">Table 1</ref>: Effect sizes by corpus with 95% credible intervals based on 1024 posterior samples 95% credible intervals (CI). The IRFs (curves) in these plots represent the expected change in the response over time from observing a unit impulse of the predictor. For example, the Dundee model estimates that observing a standard deviation of 5-gram surprisal engenders a slowdown of about 0.05 log ms instantaneously and a slowdown of about 0.03 log ms 250 ms after stimulus presen- tation. Because the response is reading time, pos- itive IRFs represent inhibition and negative IRFs represent facilitation. Detailed interpretation of these curves is provided below in Section 6.3. <ref type="table" target="#tab_1">Table 2</ref> shows prediction error from DTSR vs. baselines fitted to the same feature set. As shown, DTSR provides comparable or improved predic- tion performance to the baselines, even against the -S models which are more heavily parameterized. DTSR outperforms LME models on unseen data across all corpora and generally improves upon or closely matches the performance of GAM (with no spillover). Compared to GAM-S (with three additional spillover positions), there is a clear ad- vantage of DTSR for Natural Stories but not for the eye-tracking (ET) datasets. This is likely due to more pronounced temporal confounds in Natu- ral Stories (especially of Rate, which the baseline models cannot estimate) compared to the other corpora. <ref type="bibr">13</ref> However, even in the absence of suf- ficiently diffuse effects to afford prediction im- provements, the ability to measure diffusion di- rectly is a major advantage of the DTSR model, since it can be used to detect the absence of dif- fusion in settings where it might in principle ex- ist. Further discussion of the DTSR IRF estimates themselves is provided in Section 6.3.</p><p>As shown in <ref type="table" target="#tab_2">Table 3</ref>, pooling across corpora, permutation testing reveals a significant improve- <ref type="bibr">13</ref> Note that GAM-S is more heavily parameterized than DTSR in that it fits multidimensional spline functions of each spillover position of each predictor. This makes it difficult to generalize information about effect timecourses from GAM fits, motivating the use of DTSR for studies in which time- courses are a quantity of interest. ment in MSE on test data of DTSR over each base- line system (p = 0.0001 for all comparisons). <ref type="bibr">14</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Some key generalizations emerge from the DTSR estimates shown in <ref type="figure" target="#fig_5">Figure 5</ref>. The first is the pro- nounced facilitative role of Rate in all three mod- els, but especially in Natural Stories. This means that fast reading in the recent past engenders fast reading in the present, because (1) observ- ing a stimulus exerts a large-magnitude, diffuse, and negative (facilitative) influence on the sub- sequent response, and (2) the Rate contributions of the stimuli are additive. This result demon- strates an important pre-linguistic influence of in- ertia -a tendency toward slow overall change in base response rate. This effect is especially large-magnitude and diffuse in Natural Stories, which is self-paced reading and therefore differs in modality from the other datasets (which are eye-tracking). This suggests that SPR participants strongly habituate to repeated button pressing and stresses the importance of deconvolutional regres- sion for bringing this low-level confound under control in analyzing SPR data, since it appears to have a large influence on the response and might otherwise confound model interpretation.</p><p>Second, effects are generally consistent with ex- pectations: positive effects for Saccade Length, Word Length, and 5-gram Surprisal, and a nega- tive effect of Trial. The null influence of Unigram Logprob is likely due to the presence in the model of both 5-gram Surprisal (which interpolates uni- gram probabilities) and Word Length (which is in- versely correlated with Unigram Logprob).   Third, the response estimates for Dundee and UCL (both of which are eye-tracking) are very similar, which suggests that DTSR is discovering replicable population-level features of the tempo- ral profile for eye-tracking data.</p><p>Fourth, there is a general asymmetry in degree of diffusion between low-level perceptual-motor variables like Saccade Length and Word Length, whose responses tend to decay quickly, and the high-level 5-gram Surprisal variable, whose re- sponse tends to decay more slowly. This is consis- tent with expectations from the sentence process- ing literature. Perceptual-motor variables involve rapid bottom-up computation (e.g. visual process- ing or motor planning/execution) and are there- fore not expected to have a diffuse response, while surprisal involves top-down computation of future words given context, which might be more com- putationally expensive and therefore engender a slower response. While this outcome is suggested e.g. by the aforementioned finding that spillover 1 winds up being a stronger position for a surprisal predictor in the <ref type="bibr" target="#b23">Shain et al. (2016)</ref> models, DTSR permits direct investigation of these dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">A note on hypothesis testing</head><p>As a Bayesian model, DTSR supports hypothesis testing by querying the variational posterior. For example, as shown in <ref type="table">Table 1</ref>, the credible interval (CI) for 5-gram Surprisal in Natural Stories does not include zero (rejecting the null hypothesis of no effect), while the CI for Unigram logprob does (failing to reject). To control for effects of mul- ticolinearity, one could perform ablative tests of fitted null and alternative models using (1) likeli- hood comparison or (2) predictive performance on unseen data.</p><p>However, DTSR estimates are obtained through non-convex stochastic optimization, which com- plicates hypothesis testing because of possible es- timation noise due to (1) convergence to a lo- cal but not global optimum, (2) imperfect con- vergence to the local optimum, and/or (3) Monte Carlo estimation of the test statistic via posterior sampling. It cannot therefore be guaranteed that hypothesis testing results are due to differences in model structure rather than differences in relative amounts of estimation noise introduced by the fit- ting procedure. Thus, p-values (and, consequently, hypothesis tests) based on direct comparison of DTSR models should be considered approximate.</p><p>However, even in situations where such un- certainty in hypothesis testing is not acceptable, DTSR is appropriate for certain important use cases. First, DTSR can be used for exploratory data analysis in order to empirically motivate the spillover structure of the linear model. Spillover variables can be excluded or included based on the degree of temporal diffusion revealed by DTSR, permitting construction of linear models that are both parsimonious and effective for controlling temporal diffusion. Second, DTSR can be used to fit a data transform which is then applied to the data prior to statistical analysis. This approach is identical in spirit to e.g. the use of the canon- ical HRF to convolve predictors in fMRI models prior to linear regression. However, since DTSR is domain-general, it can be a valuable component in any analysis toolchain for time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper presented a variational Bayesian de- convolutional time series regression method as a solution to the problem of temporal diffusion in psycholinguistic time series data and applied it to both synthetic and human responses in order to better understand and control for latent temporal dynamics. Results showed that DTSR can yield a plausible, replicable, parsimonious, insightful, and predictive model of a complex dynamical sys- tem like the human sentence processing response and therefore support the use of DTSR for psy- cholinguistic time series modeling. While the present study explored the use of DTSR to under- stand human reading times, DTSR can in princi- ple also be used to deconvolve other kinds of re- sponse variables, such as the HRF in fMRI model- ing or the power/coherence response in oscillatory measures like electroencephalography, suggesting a rich array of potential applications of DTSR in computational psycholinguistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Definition of mixed effects DTSR</head><p>For expository purposes, in Section 3 the DTSR model was defined only for fixed effects. How- ever, DTSR is compatible with mixed modeling and the implementation used here supports ran- dom effects in the model intercepts, coefficients, and IRF parameters. The full mixed-effects DTSR equations are presented below.</p><p>The definitions of X, y, µ, σ 2 , a, b, c, d, F, M , N , K, and R presented in Section 3 are retained for the mixed model definition. The remaining variables and equations must be redefined to some extent. Mixed-effects DTSR models additionally contain the following parameters:</p><p>• a vector o ∈ R O of O random intercepts</p><p>• a vector u ∈ R U of U fixed coefficients</p><p>• a vector v ∈ R V of V random coefficients</p><p>• a matrix A ∈ R R×L of R fixed IRF kernel parameters for L fixed impulse vectors</p><p>• a matrix B ∈ R R×W of R random IRF kernel parameters for W random impulse vectors Random parameters o, v, and B are constrained to be zero-centered within each random grouping factor.</p><p>To support mixed modeling, the fixed and ran- dom effects must first be combined using addi- tional utility matrices. Let O ∈ {0, 1} N ×O be a mask matrix for random intercepts. A vector q ∈ R N of intercepts is:</p><formula xml:id="formula_8">q def = µ + O o<label>(7)</label></formula><p>Let U ∈ {0, 1} L×U be an indicator matrix for fixed coefficients, V ∈ {0, 1} L×V be an indi- cator matrix for random coefficients, and V ∈ {0, 1} N ×V be a mask matrix for random coeffi- cients. A matrix Q ∈ R N ×L of coefficients is:</p><formula xml:id="formula_9">Q def = 1 (U u) + V diag(v) V<label>(8)</label></formula><p>Let W ∈ {0, 1} L×W be an indicator matrix for random IRF parameters and W 1 , . . . , W n ∈ {0, 1} R×W be mask matrices for random IRF pa- rameters. Then matrices P n ∈ R R×L for n ∈ {1, 2, . . . , N } are:</p><formula xml:id="formula_10">P n def = A + (W n B) W<label>(9)</label></formula><p>In each equation above, the random effects param- eters are masked using the random effects filter associated with each data point. Q and P n are then transformed into the impulse vector space us- ing the indicator matrices V and W, respectively. This procedure sums the random effects associ- ated with each data point and adds them to the population-level parameters.</p><p>To define the convolution step, let g l for l ∈ {1, 2, . . . , L} be parametric IRF kernels, one for each impulse. Convolution of X with each IRF kernel is performed by premultiplying the inputs X with sparse matrix G l ∈ R N ×M for l ∈ {1, 2, ..., L}:</p><formula xml:id="formula_11">(G l ) [n, * ] def = g l b [n] − a ; (P n ) [ * ,l] F [n, * ]<label>(10</label></formula><p>) Finally, let L ∈ {0, 1} K×L be an indicator ma- trix mapping the K predictors of X to the corre- sponding L impulse vectors of the model. <ref type="bibr">15</ref> The convolution that yields the design matrix of con- volved predictors X ∈ R N ×L is then defined us- ing a product of the convolution matrices G, the design matrix X, and the impulse indicator L:</p><formula xml:id="formula_12">X [ * ,l] def = G l X L [ * ,l]<label>(11)</label></formula><p>The full model mean is the sum of (1) the in- tercepts and (2) the sum-product of the convolved predictors X with the coefficient parameters Q:</p><formula xml:id="formula_13">y ∼ N q + (X Q) 1, σ 2<label>(12)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Effects in a linear time series model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Linear time series model with spillover</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effects of predictors in DTSR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Synthetic data. True IRFs (left) and estimated IRFs with 95% credible intervals (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>fixation duration (go-past duration for ET). Mod- els use the following set of predictor variables in common use in psycholinguistics: Sentence posi- tion (index of word in sentence), Trial (index of trial in series), 8 Saccade Length (in words, ET only), Word Length (in characters), Unigram Log- prob, and 5-gram Surprisal. Unigram Logprob and 5-gram Surprisal are computed by the KenLM toolkit (Heafield et al., 2013) trained on Gigaword 4 (Parker et al., 2009)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FixationFigure 5 :</head><label>5</label><figDesc>Figure 5: Human data. Estimated IRFs with 95% credible intervals for Natural Stories (left), Dundee (center) and UCL (right). Intervals are too tight to be seen.</figDesc><graphic url="image-3.png" coords="6,80.41,62.81,145.14,145.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The biggest departure from prior expectations is the null estimate for Word Length in UCL. It appears</figDesc><table>Natural Stories 
Dundee 
UCL 
System 
Train 
Dev 
Test 
Train 
Dev 
Test 
Train 
Dev 
Test 
LME 
0.0803 
0.0818 
0.0815 
0.2135 
0.2133 
0.2128 
0.2613 
0.2776 
0.2561 
LME-S 0.0789  † 0.0807  † 0.0804  † 0.2099  † 0.2103  † 0.2095  † 0.2509  † 0.2754  † 0.2557  † 
GAM 
0.0798 
0.0814 
0.081 
0.212 
0.2116 
0.2111 
0.2576 
0.2741 
0.2538 
GAM-S 
0.0784 
0.0802 
0.0799 
0.2083 
0.2085 
0.2078 
0.2440 
0.2661 
0.2457 
DTSR 
0.0648 
0.0655 
0.0650 
0.2100 
0.2094 
0.2088 
0.2590 
0.2752 
0.2543 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Mean squared prediction error by system (daggers indicate convergence warnings) 

Baseline DTSR improvement (z-units) 
p-value 
LME 
0.059 
0.0001  *  *  *  
LME-S 
0.054 
0.0001  *  *  *  
GAM 
0.057 
0.0001  *  *  *  
GAM-S 0.051 
0.0001  *  *  *  

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Overall pairwise significance of prediction 
improvement from DTSR vs. baselines 

that the contribution of Word Length in this corpus 
can be effectively explained by other variables. 
</table></figure>

			<note place="foot" n="1"> Although recent work in computational psycholinguistics has begun to address separate but related problems in time series modeling (auto-correlation and non-stationarity)</note>

			<note place="foot" n="2"> This implementation of convolution is only exact when the predictors fully describe a discrete impulse signal. Exact convolution of samples from continuous signals is generally not possible because the signal is generally not analytically integrable. For continuous signals, DTSR can approximate the convolution as long as the predictor is interpolated between sample points at a fixed frequency prior to fitting.</note>

			<note place="foot" n="6"> A model in which the response is insensitive to the model structure. 7 See Appendix A for the definition of the mixed-effects DTSR model, which includes random effects.</note>

			<note place="foot" n="8"> Except UCL, which contains isolated sentences, in which case Trial is identical to Sentence Position. 9 By-subject IRF parameters were not used for this study because they substantially complicate the model and initial experiments using them showed little benefit on training data.</note>

			<note place="foot" n="10"> Except Rate, which has no variance and therefore cannot be scaled by its standard deviation of 0.</note>

			<note place="foot" n="11"> Formulae used to construct each model reported in this study are available in the associated code repository. 12 This number of spillover positions is among the largest attested in the psycholinguistic literature because model complexity in LME and GAM increases substantially with each spillover position added, especially when by-subject random slopes are included for each spillover position for each variable. Indeed, many of the baseline models run for these experiments are already at the limits of tractability, as shown by the non-convergence reported in certain cells of Table 2. An advantage of the DTSR approach is that it can consider arbitrarily long histories at no cost to model complexity. While this permits DTSR to consider longer histories than its competitors (in these experiments, 128 timepoints vs. 4), DTSR is more constrained in its use of history since it must apply the same set of IRFs to all datapoints, while the baselines essentially fit separate models for each spillover position.</note>

			<note place="foot" n="14"> To ensure comparability across corpora with different error variances, per-datum errors were first scaled by their standard deviations within each corpus. Standard deviations were computed over the joint set of error values in each pair of DTSR and baseline models.</note>

			<note place="foot" n="15"> Predictors and impulse vectors are distinguished because in principle multiple IRFs can be applied to the same predictor. In the usual case where this distinction is not needed, L is identity and K = L.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for their helpful comments. This work was supported by National Science Foundation grants #1551313 and #1816891. All views ex-pressed are those of the authors and do not nec-essarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martn</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vigas</editor>
		<imprint/>
	</monogr>
	<note>and Xiaoqiang Zheng. 2015. Tensorflow: Large-scale machine learning on heterogeneous distributed systems</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cave of shadows: Addressing the human factor with generalized additive mixed models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="206" to="234" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Supplement C</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autocorrelated errors in experimental data in the language sciences: Some solutions offered by Generalized Additive Mixed Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacolien</forename><surname>Van Rij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile</forename><surname>De Cat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mixed Effects Regression Models in Linguistics</title>
		<editor>Dirk Speelman, Kris Heylen, and Dirk Geeraerts</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fitting linear mixed-effects models using lme4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data from eyetracking corpora as evidence for theories of syntactic processing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="210" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating Nesterov momentum into Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pronoun assignment and semantic integration during reading: Eye movements and immediacy of processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Erlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning &amp; Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="75" to="87" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading time data for evaluating broad-coverage models of English sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><forename type="middle">Fernandez</forename><surname>Monsalve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">L</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1182" to="1190" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonlinear event-related responses in fMRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Josephs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geraint</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anastasia Vishnevetsky, Steve Piantadosi, and Evelina Fedorenko. 2018. The Natural Stories corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Tily</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating incidence curves of several infections using symptom surveillance data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">J</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><forename type="middle">E</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saki</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lipsitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling the hemodynamic response in fMRI using smooth FIR filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1188" to="1201" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deconvolution of impulse response in event-related BOLD fMRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="416" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized additive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="310" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Dundee corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pynte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European conference on eye movement</title>
		<meeting>the 12th European conference on eye movement</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Validity and power in hemodynamic response modeling: A comparison study and a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tor</forename><surname>Wager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="764" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling the hemodynamic response function in fmri: Efficiency, bias and mismodeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Meng</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><forename type="middle">Y</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tor</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S187-S198. Mathematics in Brain Imaging</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">45</biblScope>
		</imprint>
	</monogr>
	<note>Supplement 1</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yurii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>English Gigaword LDC2009T13</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Data-driven hrf estimation for encoding and decoding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eickenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Ciuciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Macroeconomic shocks and their propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Ramey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="71" to="162" />
		</imprint>
	</monogr>
	<note>of Handbook of Macroeconomics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Memory access during incremental sentence processing causes reading time latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Shain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computational Linguistics for Linguistic Complexity Workshop</title>
		<meeting>the Computational Linguistics for Linguistic Complexity Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adji</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09787</idno>
		<title level="m">A library for probabilistic modeling, inference, and criticism</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchic syntax improves reading time prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2015</title>
		<meeting>NAACL-HLT 2015</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deconvolution analysis of fmri time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalized Additive Models: An Introduction with R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Chapman and Hall/CRC</publisher>
			<pubPlace>Boca Raton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
