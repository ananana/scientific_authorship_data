<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Factorization of Latent Variables in Distributional Semantic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvid¨osterlund</forename><surname>Arvid¨</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David¨odling</forename><surname>David¨</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David¨odling</forename></persName>
							<email>arvidos|dodling@kth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><forename type="middle">Sahlgren</forename><surname>Gavagai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sweden</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Factorization of Latent Variables in Distributional Semantic Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper discusses the use of factoriza-tion techniques in distributional semantic models. We focus on a method for redistributing the weight of latent variables, which has previously been shown to improve the performance of distributional semantic models. However, this result has not been replicated and remains poorly understood. We refine the method, and provide additional theoretical justification, as well as empirical results that demonstrate the viability of the proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional Semantic Models (DSMs) have be- come standard paraphernalia in the natural lan- guage processing toolbox, and even though there is a wide variety of models available, the basic parameters of DSMs (context type and size, fre- quency weighting, and dimension reduction) are now well understood. This is demonstrated by the recent convergence of state-of-the-art results <ref type="bibr" target="#b2">(Baroni et al., 2014;</ref><ref type="bibr" target="#b8">Levy and Goldberg, 2014)</ref>.</p><p>However, there are a few notable exceptions. One is the performance improvements demon- strated in two different papers using a method for redistributing the weight of principal components (PCs) in factorized DSMs <ref type="bibr" target="#b4">(Caron, 2001;</ref><ref type="bibr" target="#b3">Bullinaria and Levy, 2012</ref>). In the latter of these pa- pers, the factorization of latent variables in DSMs is used to reach a perfect score of 100% correct answers on the TOEFL synonym test. This re- sult is somewhat surprising, since the factorization method is the inverse of what is normally used.</p><p>Neither the result nor the method has been repli- cated, and therefore remains poorly understood. The goal of this paper is to replicate and explain the result. In the following sections, we first pro- vide a brief review of DSMs and factorization, and review the method for redistributing the weight of latent variables. We then replicate the 100% score on the TOEFL test and provide additional state-of- the-art scores for the BLESS test. We also provide a more principled reformulation of the factoriza- tion method that is better suited for practical ap- plications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Distributional Semantics</head><p>Consider a set of words W = {w 1 , . . . , w n } and a set of context words C = {c 1 , . . . , c m }. The DSM representation is created by registering an occur- rence of a word w i with a set of context words c j , . . . , c k with a corresponding increment of the projection of w i on the c j , . . . , c k bases. In other words, each cell f ij in the matrix representation F represents a co-occurrence count between a word w i and a context c j . In the following, we use W = C, making the co-occurrence matrix sym- metric F n×n . We also adhere to standard practice of weighting the co-occurrence counts with Posi- tive Pointwise Mutual Information (PPMI) <ref type="bibr" target="#b11">(Niwa and Nitta, 1994)</ref>, which is a variation of the stan- dard PMI weighting, 1 which simply discards non- positive PMI values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Singular Value Decomposition</head><p>The high dimensionality of the co-occurrence ma- trix makes it necessary in most practical applica- tions to apply some form of dimensionality re- duction to F , with the goal of finding a ba- sis { ˆ x j , ..., ˆ x k } that restates the original basis {x k , ...} in a lower-dimensional spacê F , wherêwherê F denotes the rank-k approximation of F :</p><formula xml:id="formula_0">minˆF minˆ minˆF ∈R n ×R k |F − ˆ F | (1) 1 PMI(fij) = log f ij ( ij f ij ) 2 i f ij j f ij ij f ij .</formula><p>Assuming Gaussian-like distributions, 2 a canoni- cal way of achieving this is to maximize the vari- ance of the data in the new basis. This enables or- dering of the new basis according to how much of the variance in the original data each component describes.</p><p>A standard co-occurrence matrix is positive and symmetric and thus has, by the spectral theorem, a spectral decomposition of an ordered set of posi- tive eigenvalues and an orthogonal set of eigenval- ues:</p><formula xml:id="formula_1">F = U ΣV T (2)</formula><p>where U holds the eigenvectors of F , Σ holds the eigenvalues, and V ∈ U (w) is a unitary matrix mapping the original basis of F into its eigenba- sis. Hence, by simply choosing the first k eigen- values and their respective eigenvectors we have the central result:</p><formula xml:id="formula_2">min k |F − ˆ F | → ˆ F ≈ U k Σ k V T k (3)</formula><p>wherê F is the best rank-k approximation in the Frobenius-norm. This is commonly referred to as truncated Singular Value Decomposition (SVD).</p><p>Finally, using cosine similarity, 3 V is redundant due to invariance under unitary transformations, which means we can represent the principal com- ponents ofˆFofˆ ofˆF in its most compact formˆFformˆ formˆF ≡ U Σ without any further comment.</p><p>This projection onto the eigenbasis does not only provide an efficient compression of the sparse co-occurrence data, but has also been shown to improve the performance and noise tolerance of DSMs <ref type="bibr" target="#b12">(Schütze, 1992;</ref><ref type="bibr" target="#b7">Landauer and Dumais, 1997;</ref><ref type="bibr" target="#b3">Bullinaria and Levy, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Caron p-transform</head><p>Caron <ref type="formula">(2001)</ref> introduce a method for renormaliza- tion of the latent variables through an exponent factor p ∈ R:</p><formula xml:id="formula_3">U Σ → U Σ p (4)</formula><p>which is shown to improve the results of factorized models using both information retrieval and ques- tion answering test collections. We refer to this renormalization as the Caron p-transform. Bulli- naria and Levy (2012) further corroborate Caron's 2 It is well known that the Gaussian assumption does not hold in reality, and consequently there are also other ap- proaches to dimensionality reduction based on multinomial distributions, which we will not consider in this paper.</p><p>3 cos(wi, wj) =</p><formula xml:id="formula_4">w i ·w j |w i ||w j |</formula><p>result, and show that the optimum exponent pa- rameter p for DSMs is with strong statistical sig- nificance p &lt; 1. Moreover, due to the redistri- bution of weight to the lower variance PCs, Bul- linaria and <ref type="bibr" target="#b3">Levy (2012)</ref> show that similar perfor- mance improvements can be achieved by simply removing the first PCs. We refer to this as PC- removal. A highlight of their results is a perfect score of 100% on the TOEFL synonym test. Apart form the perfect score on the TOEFL test, it is noteworthy that the PC-removal scheme is the inverse of how SVD is normally used in DSMs; in- stead of retaining only the first PCs -which is the standard way of using the SVD in DSMs -the PC- removal scheme deletes them, and instead retains all the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We replicate the experiment setup of Bullinaria and Levy (2012) by removing punctuation and decapitalizing the ukWaC corpus ( <ref type="bibr" target="#b1">Baroni et al., 2009</ref>). The DSM includes the 50,000 most fre- quent words along with the remaining 23 TOEFL words and is populated using a ±2-sized context window. Co-occurrence counts are weighted with PPMI, and SVD is applied to the resulting ma- trix, reducing the dimensionality to 5,000. The results of removing the first PCs versus apply- ing the Caron p-transform are shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which replicates the results from Bullinaria and <ref type="bibr" target="#b3">Levy (2012)</ref>.</p><p>In order to better understand what influence the transform has on the representations, we also pro- vide results on the BLESS test ( <ref type="bibr" target="#b0">Baroni and Lenci, 2011)</ref>, which lists a number of related terms to 200 target terms. The related terms represent 8 dif- ferent kinds of semantic relations (co-hyponymy, hypernymy, meronymy, attribute, event, and three random classes corresponding to randomly se- lected nouns, adjectives and verbs), and it is thus possible to use the BLESS test to determine what type of semantic relation a model favors. Since our primary interest here is in paradigmatic relations, we focus on the hypernymy and co-hyponymy re- lations, and require that the model scores one of the related terms from these classes higher than the related terms from the other classes. The cor- pus was split into different sizes to test the statisti- cal significance of the weight redistribution effect. Furthermore, it shows that the optimal weight dis- tribution depends on the size of the data.  <ref type="figure" target="#fig_1">Figure 2</ref> shows the BLESS results for both the PC removal scheme and the Caron p-transform for different sizes of the corpus. The best score is 92.96% for the PC removal, and 92.46% for the Caron p-transform, both using the full data set. Similarly to the TOEFL results, we see better re- sults for a larger number of removed PCs. Inter- estingly, there is clearly a larger improvement in performance of the Caron p-transform than for the PC removal scheme. <ref type="figure" target="#fig_2">Figure 3</ref> shows how the redistribution affects the different relations in the BLESS test. The vi- olin plots are based on the maximum values of each relation, and the width of the violin repre- sents the normalized probability density of cosine measures. The cosine distributions, Θ i , are based on the best matches for each category i, and nor- malized by the total mean and variance amongst all categoriesˆΘcategoriesˆ categoriesˆΘ i = Θ i −µ σ . Thus, the figure illus- trates how well each category is separated from each other, the larger separation the better.</p><p>The results in <ref type="figure" target="#fig_2">Figure 3</ref> indicate that the top 120 PCs contain a higher level of co-hyponymy rela-  tions than the lower; removing the top 120 PCs gives a violin shape that resembles the inverse of the plot for the top 120 PCs. Although neither part of the PC span is significantly better in separating the categories, it is clear that removing the first 120 PCs increases the variance within the categories and especially amongst the coord-category. This is an interesting result, since it seems to contra- dict the hypothesis that removing the first PCs im- proves the semantic quality of the representations -there is obviously valuable semantic information in the first PCs. <ref type="table">Table 1</ref> summarizes our top results on the TOEFL, BLESS, and also the SimLex-999 simi- larity test <ref type="bibr" target="#b6">(Hill et al., 2014)</ref>, and compares them to a baseline score from the Skipgram model ( <ref type="bibr" target="#b9">Mikolov et al., 2013a</ref>), trained on the same data using a window size of 2, negative samples, and 400-dimensional vectors. Unfortunately, the optimal redistribution of weight on the PCs for the respective top scores differ between the experiments. For the PC re- moval the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 128 for SimLex- 999, while the optimal number for the Caron p- transform is -1.4 for TOEFL, 0.5 for BLESS and -0.40 for SimLex-999. Hence, there is likely no easy way to find a general expression of the opti- mal redistribution of weight on the PCs for a given application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Pareto Principle</head><p>It is common practice to reduce the dimensional- ity of an n-dimensional space to as many PCs as it takes to cover 80% of the total eigenvalue mass. This convention is known as the Pareto principle (or 80/20-rule), and generally gives a good trade- off between compression and precision. The re- sults presented in the previous section suggest a type of inversion of this principle in the case of DSMs.</p><p>Given a computational and practical limit of the number of PCs m with weights Σ = {σ 1 , ..., σ m }, the optimal redistribution of weight on these com- ponents is such that the first l − m components σ 1 , ..., σ m−l is transformed such that they consti- tute 20% of the new total mass. Where l −m is the number of components representing the last 20 % of the original mass. In other words, the function f : Σ → ˆ Σ performing this redistribution is such that:</p><formula xml:id="formula_5">m−l i=1ˆσi=1ˆ i=1ˆσ i m i=1ˆσi=1ˆ i=1ˆσ i ≈ 20% (5)</formula><p>In this formulation, we can consider the Caron p- transform and the PC-removal scheme as special cases, where the Caron p-transform is given by:</p><formula xml:id="formula_6">f (σ i ) = σ p i ∀i, p ∈ R (6)</formula><p>and the PC-removal scheme by:</p><formula xml:id="formula_7">f (σ i ) = (1 − δ(F ))σ i ∀i, F = {1....l} (7)</formula><p>where δ(F ) denotes the generalized Kronecker delta function.</p><p>To test this claim, we form this quotient for the distributions of weights at the optimal parame- ters for the Caron p-transform and the PC-removal scheme for both the BLESS and TOEFL tests for each of the 40 sub-corpora.</p><p>Even though the results are not as optimal for the BLESS test as for the TOEFL, the results in This result does not only apply for 1,400 PCs, but has also been verified on a smaller set of matri- ces with sizes of 2,500 PCs, 4,000 PCs and 5,000 PCs. The results for 1,400 PCs and 5,000 PCs are shown in <ref type="table">Table 2</ref>. As can be seen in this table, the rule of thumb yields reasonable good guesses for both Caron p and PC removal, over the different tests and for various number of PCs.   <ref type="table">Table 2</ref>: Results for the PC removal and Caron p using the 80/20 rule</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5,000 PC representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and future work</head><p>This paper has discussed the method of redistribut- ing the weight of the first PCs in factorized DSMs. We have replicated previously published results, and provided additional empirical justification for the method. The method significantly outperforms the baseline Skipgram model on all tests used in the experiments. Our results also suggest a slight refinement of the method, for which we have pro- vided both theoretical and empirical justification. The resulting rule of thumb method leads to stable results that may be useful in practice.</p><p>Although the experiments in this paper has pro- vided further evidence for the usefulness of re- distributing the weight in factorized models, it also raises additional interesting research ques- tions. For example, does the method also im- prove models that have been trained on smaller data sets? Does it also hold for non-Gaussian factorization like Non-negative Matrix Factoriza- tion? How does the method affect the (local) struc- tural properties of the representations; do factor- ized models display the same type of structural regularities as has been observed in word embed- dings ( <ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>), and would it be pos- sible to use methods such as relative neighborhood graphs ( <ref type="bibr" target="#b5">Gyllensten and Sahlgren, 2015)</ref> to explore the local effects of the transformation?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TOEFL score for the PC-removal scheme and the Caron p-transform for the span of PCs.</figDesc><graphic url="image-2.png" coords="3,72.00,222.20,230.40,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLESS score for the PC-removal scheme and the Caron p-transform for the span of PCs.</figDesc><graphic url="image-4.png" coords="3,307.28,222.20,230.40,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: BLESS targets versus categories from 1,400 PCs representation of the entire corpus.</figDesc><graphic url="image-5.png" coords="3,307.28,444.64,222.24,181.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 pointFigure 4 :</head><label>44</label><figDesc>Figure 4: The mass redistribution ratio for the best results on the 1,400 PC models.</figDesc><graphic url="image-8.png" coords="4,309.46,512.02,106.95,73.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>PC</head><label></label><figDesc></figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GEMS</title>
		<meeting>GEMS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="890" to="907" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Experiments with LSA scoring: Optimal rank and basis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Information Retrieval</title>
		<editor>Michael Berry</editor>
		<imprint>
			<biblScope unit="page" from="157" to="169" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Navigating the semantic horizon using relative neighborhood graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Amaru Cuba Gyllensten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>abs/1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cooccurrence vectors from corpora vs. distance vectors from dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Niwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="304" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensions of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing</title>
		<meeting>Supercomputing</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
