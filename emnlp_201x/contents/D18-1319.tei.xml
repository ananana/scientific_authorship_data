<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Compression for Natural Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2910</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadezhda</forename><surname>Chirkova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Samsung-HSE Laboratory</orgName>
								<orgName type="institution" key="instit2">National Research University Higher School of Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Lobacheva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Samsung-HSE Laboratory</orgName>
								<orgName type="institution" key="instit2">National Research University Higher School of Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Samsung-HSE Laboratory</orgName>
								<orgName type="institution" key="instit2">National Research University Higher School of Economics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Center Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Compression for Natural Language Processing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2910" to="2915"/>
							<date type="published">October 31-November 4, 2018. 2018. 2910</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In natural language processing, a lot of the tasks are successfully solved with recurrent neural networks, but such models have a huge number of parameters. The majority of these parameters are often concentrated in the embedding layer, which size grows proportionally to the vocabulary length. We propose a Bayesian sparsification technique for RNNs which allows compressing the RNN dozens or hundreds of times without time-consuming hyperparameters tuning. We also generalize the model for vocabulary sparsification to filter out unnecessary words and compress the RNN even further. We show that the choice of the kept words is interpretable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNNs) are among the most powerful models for natural language pro- cessing, speech recognition, question-answering systems <ref type="bibr" target="#b0">(Chan et al., 2016;</ref><ref type="bibr" target="#b4">Ha et al., 2017;</ref><ref type="bibr" target="#b22">Wu et al., 2016;</ref><ref type="bibr" target="#b19">Ren et al., 2015)</ref>. For complex tasks such as machine translation ( <ref type="bibr" target="#b22">Wu et al., 2016</ref>) mod- ern RNN architectures incorporate a huge number of parameters. To use these models on portable de- vices with limited memory the model compression is desired.</p><p>There are a lot of RNNs compression meth- ods based on specific weight matrix representa- tions ( <ref type="bibr" target="#b20">Tjandra et al., 2017;</ref><ref type="bibr" target="#b9">Le et al., 2015</ref>) or sparsification ( <ref type="bibr" target="#b16">Narang et al., 2017;</ref><ref type="bibr" target="#b21">Wen et al., 2018)</ref>. In this paper we focus on RNNs compres- sion via sparsification. One way to sparsify RNN is pruning where the weights with a small abso- lute value are eliminated from the model. Such methods are heuristic and require time-consuming hyperparameters tuning. There is another group of sparsification techniques based on Bayesian ap- proach.  describe a model * Equal contribution.</p><p>called SparseVD in which parameters controlling sparsity are tuned automatically during neural net- work training. However, this technique was not previously investigated for RNNs. In this pa- per, we apply Sparse VD to RNNs taking into account the specifics of recurrent network struc- ture <ref type="bibr">(Section 3.2)</ref>. More precisely, we use the in- sight about using the same sample of weights for all timesteps in the sequence ( <ref type="bibr" target="#b2">Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b1">Fortunato et al., 2017)</ref>. This modifica- tion makes local reparametrization trick ( ) not applicable and changes SparseVD training procedure.</p><p>In natural language processing tasks the ma- jority of weights in RNNs are often concentrated in the first layer that is connected to the vocabu- lary, for example in embedding layer. However, for some tasks the most of the words are unnec- essary for accurate predictions. In our model we introduce multiplicative weights for the words to perform vocabulary sparsification (Section 3.3). These multiplicative weights are zeroing out dur- ing training causing filtering corresponding unnec- essary words out of the model. It allows to boost RNN sparsification level even further.</p><p>To sum up, our contributions are as follows: (i) we adapt SparseVD to RNNs explaining the specifics of the resulting model and (ii) we gen- eralize this model by introducing multiplicative weights for words to purposefully sparsify the vo- cabulary. Our results show that Sparse Variational Dropout leads to a very high level of sparsity in re- current models without a significant quality drop. Models with additional vocabulary sparsification boost compression rate on text classification tasks but do not help that much on language model- ing tasks. In classification tasks the vocabulary is compressed dozens of times, and the choice of words is interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Reducing RNN size is an important and rapidly developing area of research. There are three re- search directions: approximation of weight ma- tries ( <ref type="bibr" target="#b20">Tjandra et al., 2017;</ref><ref type="bibr" target="#b9">Le et al., 2015)</ref>, re- ducing the precision of the weights ( <ref type="bibr" target="#b5">Hubara et al., 2016</ref>) and sparsification of the weight matri- ces ( <ref type="bibr" target="#b16">Narang et al., 2017;</ref><ref type="bibr" target="#b21">Wen et al., 2018)</ref>. We focus on the last one. The most popular approach here is pruning: the weights of the RNN are cut off on some threshold. <ref type="bibr" target="#b16">Narang et al. (2017)</ref> choose threshold using several hyperparameters that con- trol the frequency, the rate and the duration of the weights eliminating. <ref type="bibr" target="#b21">Wen et al. (2018)</ref> propose to prune the weights in LSTM by groups corre- sponding to each neuron, this allows to accelerate forward pass through the network.</p><p>Another group of sparsification methods relies on Bayesian neural networks ( <ref type="bibr" target="#b17">Neklyudov et al., 2017;</ref><ref type="bibr" target="#b10">Louizos et al., 2017)</ref>. In Bayesian NNs the weights are treated as random variables, and our desire about sparse weights is expressed in a prior distribution over them. During training, the prior distribution is transformed into the posterior distribution over the weights, used to make predictions on test- ing phase. <ref type="bibr" target="#b17">Neklyudov et al. (2017)</ref> and <ref type="bibr" target="#b10">Louizos et al. (2017)</ref> also introduce group Bayesian sparsi- fication techniques that allow to eliminate neurons from the model.</p><p>The main advantage of the Bayesian sparsifica- tion techniques is that they have a small number of hyperparameters compared to pruning-based methods. Also, they lead to a higher sparsity level ( <ref type="bibr" target="#b17">Neklyudov et al., 2017;</ref><ref type="bibr" target="#b10">Louizos et al., 2017)</ref>.</p><p>There are several works on Bayesian recurrent neural networks ( <ref type="bibr" target="#b2">Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b1">Fortunato et al., 2017)</ref>, but these methods are hard to extend to achieve sparsification. We apply sparse variational dropout to RNNs taking into ac- count its recurrent specifics, including some in- sights highlighted by <ref type="bibr" target="#b2">Gal and Ghahramani (2016)</ref>, <ref type="bibr" target="#b1">Fortunato et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method 3.1 Notations</head><p>In the rest of the paper x = [x 0 , . . . , x T ] is an in- put sequence, y is a true output andˆyandˆ andˆy is an out- put predicted by the RNN (y andˆyandˆ andˆy may be single vectors, sequences, etc.), X, Y denotes a training set {(x 1 , y 1 ), . . . , (x N , y N )}. All weights of the RNN except biases are denoted by ω, while a sin- gle weight (an element of any weight matrix) is denoted by w ij . Note that we detach biases and de- note them by B because we do not sparsify them.</p><p>For definiteness, we will illustrate our model on an example architecture for the language modeling task, where y = [x 1 , . . . , x T ]:</p><formula xml:id="formula_0">embedding : ˜ x t = w e xt ; recurrent : h t+1 = σ(W h h t + W x ˜ x t+1 + b r ); fully-connected : ˆ y t = softmax(W d h t + b d ).</formula><p>In this example</p><formula xml:id="formula_1">ω = {W e , W x , W h , W d } , B = {b r , b d }.</formula><p>However, the model may be di- rectly applied to any recurrent architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse variational dropout for RNNs</head><p>Following , , we put a fully-factorized log-uniform prior over the weights:</p><formula xml:id="formula_2">p(ω) = w ij ∈ω p(w ij ), p(w ij ) ∝ 1 |w ij |</formula><p>and approximate the posterior with a fully factor- ized normal distribution:</p><formula xml:id="formula_3">q(w|θ, σ) = w ij ∈ω N w ij |θ ij , σ 2 ij .</formula><p>The task of posterior approximation min θ,σ,B KL(q(ω|θ, σ)||p(ω|X, Y, B)) is equivalent to variational lower bound opti- mization ( :</p><formula xml:id="formula_4">− N i=1 q(ω|θ, σ) log p(y i |x i 0 , . . . , x i T , ω, B)dω+ + w ij ∈ω KL(q(w ij |θ ij , σ ij )||p(w ij )) → min θ,σ,B .<label>(1)</label></formula><p>Here the first term, a task-specific loss, is ap- proximated with one sample from q(ω|θ, σ). The second term is a regularizer that moves posterior closer to prior and induces sparsity. This regu- larizer can be very closely approximated analyti- cally ( :</p><formula xml:id="formula_5">KL(q(w ij |θ ij , σ ij )||p(w ij )) ≈ k σ 2 ij θ 2 ij ,<label>(2)</label></formula><note type="other">k(α) ≈ 0.64σ(1.87 + 1.49 log α) − 1 2 log 1 + 1 α .</note><p>To make integral estimation unbiased, sampling from the posterior is performed with the use of reparametrization trick <ref type="bibr" target="#b8">(Kingma and Welling, 2014)</ref>:</p><formula xml:id="formula_6">w ij = θ ij + σ ij ij , ij ∼ N ( ij |0, 1)<label>(3)</label></formula><p>The important difference of RNNs compared to feed-forward networks consists in sharing the same weight variable between different timesteps. Thus, we should use the same sample of weights for each timestep t while computing the like- </p><formula xml:id="formula_7">(W x x t ) i = j θ x ij x tj + i j (σ x ij ) 2 x 2 tj .</formula><p>Tied weight sampling makes LRT not applicable to weight matrices that are used in more than one timestep in the RNN. For the hidden-to-hidden matrix W h the linear combination (W h h t ) is not normally distributed because h t depends on W h from the previous timestep. As a result, the rule about the sum of in- dependent normal distributions with constant co- efficients is not applicable. In practice, network with LRT on hidden-to-hidden weights cannot be trained properly.</p><p>For the input-to-hidden matrix W x the lin- ear combination (W x x t ) is normally distributed. However, sampling the same W x for all timesteps and sampling the same noise i for preactivations for all timesteps are not equivalent. The same sample of W x corresponds to different samples of noise i at different timesteps because of the differ- ent x t . Hence theoretically LRT is not applicable here. In practice, networks with LRT on input-to- hidden weights may give the same results and in some experiments, they even converge a little bit faster.</p><p>Since the training procedure is effective only with 2D noise tensor, we propose to sample the noise on the weights per mini-batch, not per indi- vidual object.</p><p>To sum up, the training procedure is as fol- lows. To perform forward pass for a mini-batch, we firstly sample all weights ω following (3) and then apply RNN as usual. Then the gradients of <ref type="formula" target="#formula_4">(1)</ref> are computed w.r.t θ, log σ, B.</p><p>During the testing stage, we use the mean weights θ ( . Regular- izer (2) causes the majority of θ components ap- proach 0, and the weights are sparsified. More precisely, we eliminate weights with low signal- to-noise ratio </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiplicative weights for vocabulary sparsification</head><p>One of the advantages of Bayesian sparsification is an easy generalization for the sparsification of any groups of the weights that doesn't complicate the training procedure ( <ref type="bibr" target="#b10">Louizos et al., 2017)</ref>. To do so, one should introduce shared multiplicative weight per each group, and elimination of this multiplica- tive weight will mean the elimination of the cor- responding group. In our work we utilize this ap- proach to achieve vocabulary sparsification. Precisely, we introduce multiplicative proba- bilistic weights z ∈ R V for words in the vocab- ulary (here V is the size of the vocabulary). The forward pass with z looks as follows:</p><p>1. sample vector z i from the current approxima- tion of the posterior for each input sequence x i from the mini-batch;</p><p>2. multiply each one-hot encoded token x i t from the sequence x i by z i (here both x i t and z i are V -dimensional);</p><p>3. continue the forward pass as usual.</p><p>We work with z in the same way as with other weights W : we use a log-uniform prior and ap- proximate the posterior with a fully-factorized normal distribution with trainable mean and vari- ance. However, since z is a one-dimensional vec- tor, we can sample it individually for each object in a mini-batch to reduce the variance of the gradi- ents. After training, we prune elements of z with a low signal-to-noise ratio and subsequently, we do not use the corresponding words from the vocab- ulary and drop columns of weights from the em- bedding or input-to-hidden weight matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform experiments with LSTM architecture on two types of problems: text classification and language modeling. Three models are compared here: baseline model without any regularization, SparseVD model and SparseVD model with mul- tiplicative weights for vocabulary sparsification (SparseVD-Voc).</p><p>To measure the sparsity level of our models we calculate the compression rate of individual weights as follows: |w|/|w = 0|. The sparsifica- tion of weights may lead not only to the compres- sion but also to the acceleration of RNNs through group sparsity. Hence, we report the number of re- maining neurons in all layers: input (vocabulary), embedding and recurrent. To compute this number for vocabulary layer in SparseVD-Voc we use in- troduced variables z v . For all other layers in Spar- seVD and SparseVD-Voc, we drop a neuron if all weights connected to this neuron are eliminated.</p><p>We optimize our networks using Adam ( <ref type="bibr" target="#b6">Kingma and Ba, 2015)</ref>. Baseline networks overfit for all our tasks, therefore, we present results for them with early stopping. For all weights that we sparsify, we initialize log σ with -3. We eliminate weights with signal-to-noise ratio less then τ = 0.05. More details about experiment setup are presented in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Classification</head><p>We evaluated our approach on two stan- dard datasets for text classification: IMDb dataset ( <ref type="bibr" target="#b11">Maas et al., 2011</ref>) for binary classifica- tion and AGNews dataset ( <ref type="bibr" target="#b23">Zhang et al., 2015)</ref> for four-class classification. We set aside 15% and 5% of training data for validation purposes respectively. For both datasets, we use the vocabulary of 20,000 most frequent words.</p><p>We use networks with one embedding layer of 300 units, one LSTM layer of 128 / 512 hidden units for IMDb / AGNews, and finally, a fully connected layer applied to the last out- put of the LSTM. Embedding layer is initial- ized with word2vec ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>) / GloVe ( <ref type="bibr" target="#b18">Pennington et al., 2014</ref>) and SparseVD and SparseVD-Voc models are trained for 800 / 150 epochs on IMDb / AGNews.</p><p>The results are shown in <ref type="table">Table 1</ref>. SparseVD leads to a very high compression rate without a significant quality drop. SparseVD-Voc boosts the compression rate even further while still preserv- ing the accuracy. Such high compression rates are achieved mostly because of the sparsification of the vocabulary: to classify texts we need to read only some important words from them. The re- maining words in our models are mostly inter- pretable for the task (see Appendix B for the list of remaining words for IMBb). <ref type="figure" target="#fig_2">Figure 1</ref> shows the only kept embedding component for remain- ing words on IMDb. This component reflects the sentiment score of the words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Modeling</head><p>We evaluate our models on the task of character- level and word-level language modeling on the Penn Treebank corpus <ref type="bibr" target="#b12">(Marcus et al., 1993)</ref> ac- cording to the train/valid/test partition of <ref type="bibr" target="#b13">Mikolov et al. (2011)</ref>. The dataset has a vocabulary of 50 characters or 10,000 words.</p><p>To solve character / word-level tasks we use net- works with one LSTM layer of 1000 / 256 hidden units and fully-connected layer with softmax acti- vation to predict next character or word. We train SparseVD and SparseVD-Voc models for 250 / 150 epochs on character-level / word-level tasks.</p><p>The results are shown in <ref type="table">Table 2</ref>. To obtain these results we employ LRT on the last fully- connected layer. In our experiments with lan- guage modeling LRT on the last layer acceler- ate the training without harming the final result. Here we do not get such extreme compression rates as in the previous experiment but still, we are able to compress the models several times while achieving better quality w.r.t. the baseline because of the regularization effect of SparseVD. Vocab- ulary is not sparsified in the character-level task because there are only 50 characters and all of them matter. In the word-level task more than a half of the words are dropped. However, since in language modeling almost all words are impor- tant, the sparsification of the vocabulary makes the task more difficult to the network and leads to the drop in quality and the overall compression (net- work needs more difficult dynamic in the recurrent layer).  <ref type="table">Table 2</ref>: Results on language modeling tasks. Compression is equal to |w|/|w = 0|. In last two columns number of remaining neurons in input and recurrent layers are reported.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>lihood p(y i |x i 0 , . . . , x i T , ω, B) (Gal and Ghahra- mani, 2016; Fortunato et al., 2017). Kingma et al. (2015), Molchanov et al. (2017) also use local reparametrization trick (LRT) that is sampling preactivation instead of individual weights. For example,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: IMDB: remained embedding component vs sentiment score ((#pos.-#neg.) / #all texts with the word).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Results on SparseVD for RNNs shown in Section 3.2 have been supported by Russian Science Foundation (grant <ref type="bibr">17-71-20072)</ref>. Results on mul-tiplicative weights for vocabulary sparsification shown in Section 3.3 have been supported by Samsung Research, Samsung Electronics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02798</idno>
		<title level="m">Bayesian recurrent neural networks. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ran El-Yaniv, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07061</idno>
	</analytic>
	<monogr>
		<title level="m">Quantized neural networks: Training neural networks with low precision weights and activations. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference for Learning Representations</title>
		<meeting>the 3rd International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for Learning Representations</title>
		<meeting>the International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian compression for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3288" to="3298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring sparsity in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for Learning Representations</title>
		<meeting>the International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured bayesian pruning via log-normal multiplicative noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6778" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-712" />
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compressing recurrent neural network with tensor train</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08052</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning intrinsic sparse structures within long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Characterlevel convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
