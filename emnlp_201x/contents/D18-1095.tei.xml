<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Labeled Anchors and a Scalable, Transparent, and Interactive Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lund</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brigham Young University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cowley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brigham Young University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Fearn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brigham Young University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Hales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brigham Young University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brigham Young University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Labeled Anchors and a Scalable, Transparent, and Interactive Classifier</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="824" to="829"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>824</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose Labeled Anchors, an interactive and supervised topic model based on the anchor words algorithm (Arora et al., 2013). Labeled Anchors is similar to Supervised Anchors (Nguyen et al., 2014) in that it extends the vector-space representation of words to include document labels. However, our formulation also admits a classifier which requires no training beyond inferring topics, which means our approach is also fast enough to be interactive. We run a small user study that demonstrates that untrained users can interactively update topics in order to improve classification accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Labeled Anchors</head><p>In this section we describe our approach, com- bining interactive and supervised topic model- ing, which we call Labeled Anchors. We ex- tend the Anchor Words algorithm ( <ref type="bibr" target="#b0">Arora et al., 2013</ref>) which takes as input a V × D matrix M of document-word counts and recovers a V × K ma- trix A of word probabilities conditioned by topic, where there are V word types, D documents, and K topics. Our approach extends this algorithm to incorporate L possible document labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Vanilla Anchor Words</head><p>In order to compute the topic-word matrix A, the Anchor Words algorithm uses a V × V cooccur- rence matrix ¯ Q. Each entry ¯ Q i,j gives the condi- tional probability of word j occurring after observ- ing word i in a document. Following Appendix D.1 of <ref type="bibr" target="#b0">Arora et al. (2013)</ref>, ¯ Q is obtained by row- normalizing Q, which in turn is constructed using</p><formula xml:id="formula_0">Q = ¯ M ¯ M T − ˆ M (1)</formula><p>where ¯ M is a normalized version of the document- word matrix M giving equal weight to each doc- ument regardless of document length, andˆMandˆ andˆM ac- counts for words not cooccurring with themselves.</p><p>¯ Q is a V -dimensional vector-space representa- tion of each word and is used to compute a set of anchor words S. Each anchor word uniquely identifies a topic by having non-zero probability in one topic only. These anchors are computed using an adaptation of the Gram-Schmidt process from <ref type="bibr" target="#b0">Arora et al. (2013)</ref>. Once the set of anchor words S has been computed, we reconstruct the non-anchor words as a convex combination of the anchor word vectors. The coefficients of these combinations C are computed using exponenti- ated gradient descent to optimize</p><formula xml:id="formula_1">C i = argmin C i D KL ( ¯ Q i || k∈S C i,k ¯ Q k ) (2)</formula><p>where i is the ith word of the vocabulary, ¯ Q i is the vector-space representation of word i, and</p><formula xml:id="formula_2">D KL (·||·) is Kullback-Leibler divergence. 1</formula><p>Because the occurrence pattern of each anchor word throughout the documents must mirror that of the topic it anchors, each coefficient C i,j gives the conditional probability of topic j occurring given word i. This is the inverse conditioning we desire in the topic-word matrix A. We can there- fore compute A using Bayes' Rule by multiplying the coefficient matrix C with the empirical proba- bility of each word to get the probability of a word given a particular topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Vector-Space Representations</head><p>Supervised Anchors ( <ref type="bibr" target="#b9">Nguyen et al., 2014</ref>) aug- ments ¯ Q by appending L additional columns to ¯ Q corresponding to the probability of words cooc- curring with the L possible document labels. Be- cause this augmented vector-space representation includes dimensions corresponding to document labels, both the anchor words and the resulting topics will reflect the document labels.</p><p>Our algorithm, called Labeled Anchors, also augments the vector-space representation to in- clude the L document labels. However, we do not directly modify ¯ Q. Instead, we treat the L possible document labels as words and pretend that we ob- serve these label pseudowords directly in each la- beled document; a graphical representation of this is shown below in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="bibr">2</ref> Consequently, our document-word matrix M is a (V + L) × D matrix. The first V entries of each column of M give the word counts for a particular document. The last L entries are zero, except for the entry corresponding to the label of that docu- ment.</p><p>We then construct ¯ Q using Equation 1, obtain- ing an order V + L square matrix. As with Su- pervised Anchors, these additional L dimensions guide anchor selection to include anchors which reflect the underlying document labels. When we use Equation 2 to compute C, we also obtain an additional L rows of coefficients which each cor- respond to the conditional probability of a topic given a label. Finally, the first V rows of A are computed using Bayes' Rule to give us the proba- bility of words given topics.</p><p>Labeled Anchors inherits the run time charac- teristics of the original Anchor Words algorithm. As shown in <ref type="bibr" target="#b0">Arora et al. (2013)</ref>, topic recov- ery requires O(KV 2 + K 2 V T ), where V is the size of the vocabulary, K is the number of an- chors/topics, and T is the average number of iter- ations (typically around 100 in our experiments). Since V K, adding any modest number of top- ics (less than 200) does not noticeably increase the runtime. Furthermore, since vocabulary size tends to grow logarithmically with respect to the size of the data <ref type="bibr" target="#b4">(Heaps, 1978)</ref>, this approach is scalable even for very large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Free Classifier</head><p>Note that once the cooccurrence matrix ¯ Q has been computed, the recovery of the topic-word matrix A scales with the size of the vocabulary, not the size of the data. However, Supervised An- chors requires topic assignments for each train- ing document 3 for use as features for some down- stream classifier. Therefore, the process of build- ing a classifier scales linearly with the number of documents and can be time consuming compared to topic recovery.</p><p>In contrast, the formulation of Labeled Anchors allows us to construct a classifier with no addi- tional training. To do so, rather than using LDA with fixed topics, we employ a simple model simi- lar to Labeled LDA ( <ref type="bibr" target="#b10">Ramage et al., 2009</ref>) with the following generative story for an individual docu- ment containing N words:</p><formula xml:id="formula_3">1. Draw label ∼ Cat(λ) 2. For each i ∈ [1, ..., N ] : (a) Draw topic assignment z i | ∼ Cat(ψ l ) (b) Draw word w i |z i ∼ Cat(φ z i )</formula><p>The prior over document labels λ is simply the proportion of each label in the training data. We can estimate topic-label probabilities ψ using the last L rows of the coefficient matrix C, while the word-topic probabilities φ are the first V rows of A. Using these hyperparameters, we make predic- tions using the following: * = argmax</p><formula xml:id="formula_4">p(|w) = argmax p(, w) (3) = argmax K z 1 =1 ... K z N =1 p(, z, w)<label>(4)</label></formula><p>= argmax p()</p><formula xml:id="formula_5">N i=1 K z i =1 p(z i |)p(w i |z i )<label>(5)</label></formula><p>= argmax</p><formula xml:id="formula_6">λ N i=1 K z i =1 ψ ,z i φ z i ,w i (6) = argmax logλ + N i=1 log K z i =1 C ,z i A z i ,w i<label>(7)</label></formula><p>where Equation 4 unmarginalizes the probabili- ties across the word-topic assignments, Equation 5 uses the model's conditional independencies to ex- pand and simplify the probabilities, Equation 6 ex- plicitly uses the parameters from the generative model, and Equation 7 transitions to the matrix representations for these probabilities as found in Section 1.2. In Equation 7 we also switch to log space to mitigate numeric precision issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">User Interaction</head><p>Assuming that ¯ Q is precomputed and fixed, La- beled Anchors is fast enough to allow interactive modification of the topics as well as interactive display of classification accuracy, even on large datasets. The final step to solving the problem of creating an interactive and transparent classifier is to allow users to inject domain specific knowledge #Docs #Vocab <ref type="table" target="#tab_0">Labeled Supervised  39388  3406  .532s  17.1s  99955  4829  .886s  28.6s  990820  6648</ref> 1.10s 282s into the topic model. To do so, we use the idea of Tandem Anchors ( <ref type="bibr" target="#b6">Lund et al., 2017)</ref>, which al- lows users to manually select sets of words to form anchors.</p><p>Ordinarily, anchor words can be somewhat in- scrutable to human users. Because anchor words must uniquely identify topics, good anchors are typically esoteric low-to-mid frequency words. In- tuitive, high frequency words usually appear in multiple topics. However, if we examine Equa- tion 2, we can see that the anchor words are just points in V -dimension space; they do not actually have to correspond to any particular word so long as that point in space uniquely identifies a topic.</p><p>Tandem Anchors allows multiple words to form a single anchor pseudoword by computing the element-wise harmonic mean of a set of words. Since the harmonic mean tends towards the lowest values, the resulting pseudoword anchor largely ignores superfluous cooccurrence patterns in the constituent words. Consequently, while individ- ual words forming the anchor may be ambiguous, users can combine multiple ambiguous words to intuitively express a single coherent idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Results</head><p>Before running a user study to validate that La- beled Anchors works as an interactive and trans- parent classifier, we first run a synthetic experi- ment to determine the runtime characteristics of our algorithm. We take subsets of a large collec- tion of Amazon reviews 4 to produce datasets of various sizes. Using this data, we compare the run- time of Labeled Anchors with that of Supervised Anchors. All results are obtained using a single core of an Intel Core i7-4770K. <ref type="bibr">5</ref> As shown in <ref type="table" target="#tab_0">Table 1</ref>, Labeled Anchors is or- ders of magnitude faster than Supervised Anchors, even for moderately sized datasets. Both Labeled Anchors and Supervised Anchors require us to re- cover topic-word distributions, an operation which scales with the size of the vocabulary. How- ever, Supervised Anchors also requires us to infer document-topic distributions in order to train an external classifier, an operation which scales lin- early with the number of documents. Since vocab- ulary size typically grows logarithmically with re- spect to the number of documents <ref type="bibr" target="#b4">(Heaps, 1978)</ref>, Labeled Anchors scales much better than Super- vised Anchors.</p><p>When a user updates the anchors, the system must reinfer the topics, create the classifier, and evaluate the development dataset, all within a few seconds. If the update is too slow, the interac- tion will suffer due to increased cognitive load on users <ref type="bibr" target="#b3">(Cook and Thomas, 2005)</ref>. Results from an exploratory user study confirm this: when partic- ipants are faced with update times around 10 sec- onds, they are not successful in their topic-based tasks. <ref type="bibr">6</ref> Having established that Labeled Anchors is fast enough to be interactive, we now demonstrate that participants can use our system to improve topics for classification. In order demonstrate the role of human knowledge in interactive topic model- ing, we ask the users to identify sentiment (i.e. product rating) rather than product category, since the natural topics which arise from the Anchor Words algorithm tend to reflect product category instead of rating. We preprocess a set of Ama- zon product reviews with standard tokenization, stopword removal, and by removing words which appear in fewer than 100 documents. After pre- processing, empty documents are discarded, re- sulting in 39,388 documents. We use an 80/20 train/test split, with 1,500 training documents re- served as development data. We recruit five partic- ipants drawn from a university student body. The median age is 21. Three are male and two are fe- male. None of the students have any prior famil- iarity with topic modeling.</p><p>We present the participants with a user inter- face similar to that of <ref type="bibr" target="#b6">Lund et al. (2017)</ref>. Users can view and edit a list of anchor words (or rather, sets of words which form each anchor), and they can view the top ten most probable words for each topic. We display the classification accuracy on the development data to give users an indication of how they are doing. After a brief training on the interface, users are asked to modify the anchors to produce topics which reflect the underlying prod- uct ratings and improve the classification accuracy on the development dataset as much as possible. Participants are given forty minutes to perform this task. <ref type="figure" target="#fig_1">Figure 2</ref> summarizes the results of our user study. With just baseline anchors from Gram- Schmidt, the classification accuracy of Labeled Anchors is on par with that of Supervised Anchors using logistic regression as the downstream clas- sifier. However, because Labeled Anchors is fast enough to allow interaction, participants are able to improve classification accuracy on the devel- opment set by an average of 5.31%. This corre- sponds to a 2.31% increase in accuracy on the test set.</p><p>We record each step of the user interactions and find a Pearson correlation coefficient of .88 between development accuracy and test accuracy. Thus, Labeled Anchors allows participants to in- teractively see updated classification accuracy and have confidence that held-out test accuracy will also improve.</p><p>With regard to the interaction that users had with the dataset, we observe several common strategies. Firstly, we notice that users who made more edits tend to have more success in terms of accuracy; this validates our assertion that slower update times hurt performance. Secondly, users end with a median of 21 topics, which is close to the 20 topics they start with, suggesting that either the users felt like this was an appropriate number of topics, or that they felt uneasy dras- tically changing the total number of topics from what they started with. Lastly, we find that users chose more single word anchors than we expected, with about 88% of anchors being single word an- chors. Most of the multiword anchors users used were short 2-3 word phrases which did not have an obvious single word counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>Our results demonstrate that Labeled Anchors yields a classifier that is both human-interpretable and fast. Our approach not only combines the strengths of Supervised Anchors and Tandem An- chors, but introduces a mathematical construct for producing a classifier as a by-product of topic in- ference. Compared to Supervised Anchors, which requires costly training of a downstream classi- fier in addition to topic inference, our approach is much more scalable. Using Labeled Anchors, our participants are able to adjust the classifier so as to obtain superior classification results than those produced by Supervised Anchors alone.</p><p>Returning to our original motivating problem of quickly annotating a large collection of unla- beled emails, we assert that our approach could aid in quickly labeling the entire collection. With a modest investment of manual annotation, the ini- tial training set could be labeled, and then with the help of our system the remaining documents could be automatically labeled in a transparent and ex- plainable fashion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Labeled Anchors treats labels as observed words in each labeled documents and updates ¯ Q under this assumption, creating the additional rows and columns highlighted here.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,204.07,103.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: User study accuracy results comparing accuracy on the development set to the accuracy on the test set. The black horizontal line indicates the baseline accuracy from Supervised Anchors. The black star indicates the initial accuracy using Gram-Schmidt anchors with Labeled Anchors. The blue dots indicate various intermediate steps while editing the anchors. The red pluses are the final states after each user completes the task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Runtime for Labeled Anchors and Super</head><label>1</label><figDesc></figDesc><table>-
</table></figure>

			<note place="foot" n="1"> Alternatively, we can use l 2-norm in place of KLdivergence. 2 We could add multiple such words per label, but our preliminary experiments indicate that one per label is sufficient.</note>

			<note place="foot" n="3"> This is typically done using LDA with fixed topics.</note>

			<note place="foot" n="4"> http://jmcauley.ucsd.edu/data/amazon 5 Our Python implementation is available at https://github.com/byu-aml-lab/ankura.</note>

			<note place="foot" n="6"> For this reason, we do not report interactive results with Supervised Anchors.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the NSF Grant IIS-1409739.</p><p>Thanks to Piper Armstrong, Naomi Johnson, Connor Cook and Nozomu Okuda for their invalu-able help with this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing a nonnegative matrix factorization-provably</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-fourth annual ACM symposium on Theory of computing</title>
		<meeting>the forty-fourth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Illuminating the path: The research and development agend for visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Information retrieval: Computational and theoretical aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Stanley Heaps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Academic Press, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tandem anchoring: A multiword anchor approach for interactive topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Is your anchor going up or down? Fast and accurate supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Labeled lda: A supervised topic model for credit attribution in multilabeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical topic models for multi-label document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">America</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="157" to="208" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
