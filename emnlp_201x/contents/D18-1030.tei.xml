<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast, Compact, Accurate Model for Language Identification of Codemixed Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Riesa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakalov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Language</surname></persName>
						</author>
						<title level="a" type="main">A Fast, Compact, Accurate Model for Language Identification of Codemixed Text</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">328</biblScope>
							<biblScope unit="page" from="328" to="337"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We address fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages. Such text is prevalent online, in documents , social media, and message boards. We show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.5% averaged absolute gain on three codemixed datasets. It furthermore outperforms several benchmark systems on monolingual language identification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Codemixed text is common in user-generated con- tent, such as web articles, tweets, and message boards, but most current language ID models ig- nore it. Codemixing involves language switches within and across constituents, as seen in these English-Spanish and English-Hindi examples.</p><p>(1) dame [ N P ese book that you told me about] Give me this book that you told me about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) [ N P aapki profile photo] [ V P pyari hai]</head><p>Your profile photo is lovely.</p><p>Codemixing is the norm in many communities, e.g. speakers of both Hindi and English. As much as 17% of Indian Facebook posts ( <ref type="bibr">Bali et al., 2014</ref>) and 3.5% of all tweets ( <ref type="bibr" target="#b21">Rijhwani et al., 2017)</ref> are codemixed. This paper addresses fine-grained (token-level) language ID, which is needed for many multilingual downstream tasks, including syntactic analysis <ref type="bibr" target="#b4">(Bhat et al., 2018)</ref>, machine translation and dialog systems. Consider this ex- ample, which seeks a Spanish translation for the English word squirrel:</p><p>(3) como se llama un squirrel en español What do you call a squirrel in Spanish?</p><p>Per-token language labels are needed; a system cannot handle the whole input while assuming it is entirely English or Spanish. Fine-grained language ID presents new chal- lenges beyond sentence-or document-level lan- guage ID. Document-level labels are often avail- able in metadata, but token-level labels are not. Obtaining token-level labels for hundreds of lan- guages is infeasible: candidate codemixed exam- ples must be identified and multilingual speak- ers are required to annotate them. Furthermore, language ID models typically use character-and word-level statistics as signals, but shorter inputs have greater ambiguity and less context for pre- dictions. Moreover, codemixing is common in informal contexts that often have non-standard words, misspellings, transliteration, and abbre- viations ( <ref type="bibr" target="#b0">Baldwin et al., 2013)</ref>. Consider (4), a French-Arabic utterance that has undergone transliteration, abbreviation and diacritic removal. Language ID models must be fine-grained and ro- bust to surface variations to handle such cases. We introduce CMX, a fast, accurate language ID model for CodeMiXed text that tackles these challenges. CMX first outputs a language distri- bution for every token independently with efficient feed-forward classifiers. Then, a decoder chooses labels using both the token predictions and global constraints over the entire sentence. This decoder produces high-quality predictions on monolingual texts as well as codemixed inputs. We furthermore show how selective, grouped dropout enables a blend of character and word-level features in a sin- gle model without the latter overwhelming the for- mer. This dropout method is especially important for CMX's robustness on informal texts.</p><p>We also create synthetic training data to com- pensate for the lack of token-level annotations. Based on linguistic patterns observed in real- world codemixed texts, we generate two mil- lion codemixed examples in 100 languages. In addition, we construct and evaluate on a new codemixed corpus of token-level language ID la- bels for 25k codemixed sentences (330k tokens). This corpus contains examples derived from user- generated posts that contain English mixed with Spanish, Hindi or Indonesian.</p><p>Language ID of monolingual text has been ex- tensively studied <ref type="bibr" target="#b10">(Hughes et al., 2006;</ref><ref type="bibr" target="#b1">Baldwin and Lui, 2010;</ref><ref type="bibr">Lui and Baldwin, 2012;</ref><ref type="bibr" target="#b14">King and Abney, 2013</ref>), but language ID for codemixed text has received much less attention. Some prior work has focused on identifying larger language spans in longer documents ( <ref type="bibr" target="#b17">Lui et al., 2014;</ref><ref type="bibr" target="#b13">Jurgens et al., 2017)</ref> or estimating proportions of multiple languages in a text ( <ref type="bibr" target="#b17">Lui et al., 2014;</ref><ref type="bibr" target="#b15">Kocmi and Bojar, 2017)</ref>. Others have focused on token-level language ID; some work is constrained to pre- dicting word-level labels from a single language pair <ref type="bibr" target="#b20">(Nguyen and Do˘ gruöz, 2013;</ref><ref type="bibr" target="#b22">Solorio et al., 2014;</ref><ref type="bibr" target="#b18">Molina et al., 2016a;</ref><ref type="bibr" target="#b23">Sristy et al., 2017)</ref>, while others permit a handful of languages ( <ref type="bibr" target="#b7">Das and Gambäck, 2014;</ref><ref type="bibr" target="#b23">Sristy et al., 2017;</ref><ref type="bibr" target="#b21">Rijhwani et al., 2017)</ref>. In contrast, CMX supports 100 lan- guages. Unlike most previous work-with Rijh- wani et al. 2017 a notable exception-we do not as- sume a particular language pair at inference time. Instead, we only assume a large fixed set of lan- guage pairs as a general constraint for all inputs.</p><p>We define and evaluate CMX and show that it strongly outperforms state-of-the-art language ID models on three codemixed test sets cover- ing ten languages, and a monolingual test set in- cluding 56 languages. It obtains a 19.5% abso- lute gain on codemixed data and a 1.1% abso- lute gain (24% error reduction) on the monolin- gual corpus. Our analysis reveals that the gains are even more pronounced on shorter text, where the language ID task naturally becomes more dif- ficult. In terms of runtime speed, CMX is roughly 800x faster than existing token-level models when tested on the same machine. Finally, we demon- strate a resource-constrained but competitive vari- ant of CMX that reduces memory usage from 30M to 0.9M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>We create synthetic codemixed training examples to address the expense and consequent paucity of token-level language ID labels. We also anno- tate real-world codemixed texts to measure per- formance of our models, understand code-mixing patterns and measure the impact of having such examples as training data.</p><p>Synthetic data generation from monolingual text. For training models that support hun- dreds of languages, it is simply infeasible to ob- tain manual token-level annotations to cover ev- ery codemixing scenario ( <ref type="bibr" target="#b21">Rijhwani et al., 2017)</ref>. However, it is often easy to obtain sentence-level language labels for monolingual texts. This allows projection of sentence-level labels to each token, but a model trained only on such examples will lack codemixed contexts and thus rarely switch within a sentence. To address this, we create syn- thetic training examples that mix languages within the same sequence.</p><p>To this end, we first collect a monolingual cor- pus of 100 languages from two public resources: the W2C corpus 1 and the Corpus Crawler project. <ref type="bibr">2</ref> Then we generate a total of two million synthetic codemixed examples for all languages.</p><p>In generating each training example, we first sample a pair of languages uniformly. <ref type="bibr">3</ref> We sample from a set of 100 language pairs, mainly includ- ing the combination of English and a non-English language. The full set is listed in the supplemental material. Then we choose uniformly between gen- erating an intra-mix or inter-mix example, which are two of the most prominent types of codemix- ing in the real world ( <ref type="bibr" target="#b3">Barman et al., 2014;</ref><ref type="bibr" target="#b7">Das and Gambäck, 2014</ref>). <ref type="bibr">4</ref> An intra-mix sentence like (1) starts with one language and switches to another language, while an inter-mix sentence like (2) has en/es en/hi en/id Number of tokens 98k 140k 94k Number of sentences 9.5k 9.9k 5.3k  an overall single language with words from a sec- ond language in the middle. To generate an exam- ple, we uniformly draw phrases from our monolin- gual corpus for the chosen target languages, and then concatenate or mix phrases randomly. The shorter phrase in inter-mix examples contains one or two tokens, and the maximum length of each example is eight tokens.</p><p>Manual annotations on real-world codemixed text. We obtain candidates by sampling codemixed public posts from Google+ 5 and video comments from YouTube, <ref type="bibr">6</ref> limited to three language pairs with frequent code switch- ing:</p><p>English-Spanish, English-Hindi 7 and English-Indonesian. All texts are tokenized and lowercased by a simple rule-based model before annotation. Both the candidate selection and the annotation procedures are done by linguists proficient in both languages. The final annotated corpus contains 24.7k sentences with 334k tokens; 30% are monolingual, 67% are bilingual and 3% have more than two languages. Finally, we create an 80/10/10 split (based on tokens) for training, development and testing, respectively. <ref type="table" target="#tab_0">Table 1</ref> gives the token and sentence counts per language. In the rest of the paper, we refer to this dataset as GY-Mix.</p><p>Evaluation datasets. We evaluate on four datasets, three codemixed and one monolingual. For a fair comparison, we report accuracies on </p><formula xml:id="formula_0">P (y) / exp{w &gt; y h 0 + b y } · · · Unigram Script Context</formula><p>This is an [ example ] sentence <ref type="figure">Figure 1</ref>: Basic feed-forward network unit for scor- ing each token in the input and predicting possible languages. Multiple features are embedded, con- catenated, and fed into a hidden layer with ReLU activation.</p><formula xml:id="formula_1">X G E G X 1 E 1 X 0 E 0 h 0 = max{0, W &gt; [X g E g |8g] + b 0 } h 0 =  h 0 = h h 1 = max{0, W &gt; h 0 + b 0 } P (y) / exp{w &gt; y h 1 + b y }</formula><p>subsets of these test sets that include languages supported by all tested models. Examples with Hindi words written in Latin script are also re- moved because the benchmark systems we com- pare to do not support it.</p><p>• Twitter-Mix: Codemixed data from the EMNLP 2016 shared task ( <ref type="bibr" target="#b19">Molina et al., 2016b</ref>).</p><p>• Web-Mix6: Codemixed data crawled from multilingual web pages <ref type="bibr" target="#b14">(King and Abney, 2013)</ref>, using a subset of six languages.</p><p>• GY-Mix: The test set of our token-level codemixed data (en-es, en-hi, and en-id).</p><p>• KB-Mono56: Monolingual test set of <ref type="bibr" target="#b15">Kocmi and Bojar (2017)</ref>, using a subset of 56 lan- guages. <ref type="table" target="#tab_1">Table 2</ref> summarizes the final language setting of each test set used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Identifying Language Spans in Codemixed Text</head><p>CMX uses two stages to assign language codes to every token in a sentence. First, it predicts a dis- tribution over labels for each token independently with a feed-forward network that uses character and token features from a local context window. Then, it finds the best assignment of token labels for an entire sentence using greedy search, sub- ject to a set of global constraints. Compared to sequence models like CRFs or RNNs, this two- stage strategy has several major advantages for fine-grained language ID: (1) it does not require annotated codemixed text over hundreds of lan- guages and their mixed pairings, (2) learning in- dependent classifiers followed by greedy decoding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Window D V</head><p>Character n-gram <ref type="table" target="#tab_0">+/-1  16 1000-5000  Script  0  8  27  Lexicon  +/-1  16  100   Table 3</ref>: Feature spaces of CMX. The window col- umn indicates that CMX uses character n-gram and lexicon features extracted from the previous and following tokens as well as the current one.</p><p>is significantly faster than structured training (es- pecially considering the large label set inherent in language ID), and (3) it is far easier to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Token Model</head><p>Simple feed-forward networks have achieved near state-of-the-art performance in a wide range of NLP tasks ( <ref type="bibr" target="#b5">Botha et al., 2017;</ref><ref type="bibr" target="#b24">Weiss et al., 2015)</ref>. CMX follows this strategy, with embedding, hid- den, and softmax layers as shown in <ref type="figure">Figure 1</ref>. The inputs to the network are grouped feature matrices, e.g. character, script and lexicon features. Each group g's features are represented by a sparse ma- trix X g ∈ R Fg×Vg , where F g is the number of feature templates and V g is the vocabulary size of the feature group. The network maps sparse features to dense embedding vectors and concate- nates them to form the embedding layer:</p><formula xml:id="formula_2">h 0 = vec[X g E g |∀g]<label>(1)</label></formula><p>where E g ∈ R Vg×Dg is a learned embedding ma- trix per group. The final size of the embedding layer |h 0 | = g F g D g is the sum of all embedded feature sizes. CMX uses both discrete and contin- uous features. We use a single hidden layer with size 256 and apply a rectified linear unit (ReLU) over hidden layer outputs. A final softmax layer outputs probabilities for each language. The net- work is trained per-token with cross-entropy loss.</p><p>We explain the extraction process of each fea- ture type below. <ref type="table">Table 3</ref> summarizes the three types of features and their sizes used in CMX. Character and lexicon features are extracted for the previous and following tokens as well as the current token to provide additional context.</p><p>Character n-gram features We apply character n-gram features with n = <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>. RNNs or CNNs would provide more flexible character feature rep- resentations, but our initial experiments did not show significant gains over simpler n-gram fea- tures. We use a distinct feature group for each n. The model averages the embeddings accord- ing to the fractions of each n-gram string in the input token. For example, if the token is banana, then one of the extracted trigrams is ana and the corresponding fraction is 2/6. Note that there are six trigrams in total due to an additional boundary symbol at both ends of the token.</p><p>Following • Language distribution. The language distri- bution itself is included as the feature vector.</p><note type="other">Botha et al. (2017), we use feature hashing to control the size V and avoid storing a big string-to-id map in memory during runtime. The feature id of an n-gram string x is given by H(x)mod V g (Ganchev and Dredze, 2008), where H is a well-behaved hash function. We set V = 1000, 1000, 5000, 5000 for n = 1, 2, 3, 4 respec- tively; these values yield good performance and are far smaller than the number of n-gram types. Script features Some text scripts are strongly correlated with specific languages. For example, Hiragana is only used in Japanese and Hangul is only used in Korean. Each character</note><p>• Active languages. As above, but feature val- ues are set to 1 for all non-zero probabili- ties. For example, the word mango has fea- ture value 1 on both English and Spanish.</p><p>• Singletons. If the token is associated with only one language, return a one-hot vector whose only non-zero value is the position in- dicating that language.</p><p>The size of all lexicon feature vectors is equal to the number of supported languages.  The model randomly sets the lexicon feature vec- tors to zero with 50% probability while n-gram features are always used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Selective Feature Dropout</head><p>Preliminary experiments showed poor perfor- mance, especially on informal texts, when all three types of features are simply merged. Consider the following example outputs on misspelled word Ennnglish, for which no lexicon features fire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Ennnglish</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With Lexicon Features W/o Lexicon Features</head><p>• p(sv) = 0.27</p><formula xml:id="formula_3">• p(en) = 0.74 • p(da) = 0.24 • p(nl) = 0.10 • p(nl) = 0.18</formula><p>• p(f y) = 0.06 • . . .</p><p>• . . .</p><p>Without dropout, the model with lexicon features does not make effective use of the token's charac- ter n-grams and makes a catastrophically wrong prediction. The core problem is that lexicon fea- tures are both prevalent and highly predictive for language ID; during training, this dampens the up- dating of weights of n-gram features and thus di- minishes their overall utility. To address this issue and make CMX more robust to noisy inputs, we selectively apply a grouped feature dropout strategy that stochasti- cally down-weights lexicon features during train- ing. <ref type="figure">Figure 3</ref> illustrates the idea: for each input, after feature extraction, the vector of lexicon fea- tures is randomly set to zero. This way, the model must rely entirely on n-gram features for this par- ticular input. Note that our feature dropout is dif- ferent from standard dropout in at least two ways: (1) dropout happens to entire feature groups rather than on individual neurons, (2) we selectively ap- ply dropout only on a subset of features. After tuning the dropout rate on development data <ref type="figure" target="#fig_5">(Fig- ure 5</ref>) we choose a dropout rate of 50%. Section 4.3 explains the tuning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding with Global Constraints</head><p>Given a trained model, the goal of decoding is to find the sequence of per-token languages that max- imizes the overall score. The simple, greedy strat- egy of picking the top prediction for each token over-predicts too many languages in a single sen- tence. For example, on average the greedy method predicts more than 1.7 languages per sentence on monolingual inputs. Because the token classier uses a window including only the previous, cur- rent, and next token, it has a quite limited view on the entire sequence.</p><p>Motivated by this observation, we add the following global constraint in decoding: only monolingual outputs or codemixed outputs from a fixed set of language pairs are permitted. We choose a set of 100 language pairs, primarily in- cluding the combination of English and a non- English language. The full set is listed in the sup- plemental material.</p><p>Finally, we introduce a straightforward variant of greedy decoding that finds the optimal language assignment in the presence of these global con- straints. We independently find the best assign- ment under each allowed language combination (monolingual or language pair) and return the one with the highest score. <ref type="figure">Figure 2</ref> shows paths for example <ref type="formula">(4)</ref>   fr/ar-Latn. <ref type="bibr">8</ref> The two paths in dashed and solid lines indicate the best assignment for each lan- guage pair respectively. Because scoring is in- dependent across tokens, each subtask is com- puted in O(N ) time. The total decoding time is</p><formula xml:id="formula_4">O(N |L|)</formula><p>where L is the constraint set, and the global optimality of this algorithm is guaranteed because the assignment found in each subtask is optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Setup</head><p>We train CMX on the concatenation of three datasets: (a) GY-Mix's training portion, (b) syn- thetic codemixed data and (c) a monolingual cor- pus that covers 100 languages. Every token in the training set spawns a training instance. Our train- ing set consists of 38M tokens in total, which is on the same magnitude as the sizes of training data reported in previous work <ref type="bibr" target="#b13">(Jurgens et al., 2017;</ref><ref type="bibr">Joulin et al., 2016)</ref>. We use mini-batched averaged stochastic gra- dient descent (ASGD) (Bottou, 2010) with mo- mentum (Hinton, 2012) and exponentially decay- ing learning rates to learn the parameters of the network. We fix the mini-batch size to 256 and the momentum rate to 0.9. We tune the initial learning rate and the decay step using development data. <ref type="table" target="#tab_5">Table 4</ref> lists our main results on the codemixed datasets. We primarily compare our approach against two benchmark systems: EquiLID (Jurgens et al., 2017) and LanideNN ( <ref type="bibr" target="#b15">Kocmi and Bojar, 2017)</ref>. Both achieved state- of-the-art performance on several monolingual and codemixed language ID datasets. LanideNN makes a prediction for every character, so we con- vert its outputs to per-token predictions by a vot- ing method over characters in each word. For both benchmarks, we use the public pre-trained model provided by the authors. The EquiLID model uses 53M parameters, LanideNN uses 3M, and CMX only uses 0.28M parameters. <ref type="bibr">9</ref> Across all datasets, CMX consistently outper- forms both benchmark systems by a large mar- gin. On average, our full model (CMX) is 19.5% more accurate than EquiLID (93.4% vs. 73.9%); the gain is even larger compared to LanideNN. Note that none of the models are trained on the Twitter-Mix or the Web-Mix6 dataset, so these two datasets provide an evaluation on the out- domain performance of each approach. In this set- ting CMX also yields significant improvement in accuracy, e.g. a 4.5% (absolute) gain over Equi- LID on the Twitter-Mix dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Codemixed Texts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Even Smaller Model</head><p>We further compare between CMX and a variant we call CMX-small, which has no access to lexicon resources or lex- icon features. This smaller variant has only 237k parameters and reduces the memory foot- print from 30M to 0.9M during runtime, while the (average) loss on accuracy is only 2.5%. This comparison demonstrates that our approach is also an excellent fit for resource-constrained environ- ments, such as on mobile phones.  <ref type="table">Table 5</ref>: Monolingual Texts: Sentence-level ac- curacy (%) on KB-Mono56. Monolingual models make per-sentence predictions only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Texts</head><p>Langid.py (Lui and Baldwin, 2012), CLD2 10 and fastText ( <ref type="bibr">Joulin et al., 2016</ref><ref type="bibr" target="#b12">Joulin et al., , 2017</ref>)-all are pop- ular off-the-shelf tools for monolingual language ID. Sentence-level predictions for EquiLID and LanideNN models are obtained by simple vot- ing. <ref type="table">Table 5</ref> summarizes sentence-level accuracy of different approaches on the KB-Mono56 test set. CMX achieves the best sentence-level accu- racy over all monolingual and codemixing bench- mark systems. The resource-constrained CMX- small also performs strongly, obtaining 94.6% ac- curacy on this test set. Our approach also maintains high performance on very short texts, which is especially important for many language identification contexts such as user-generated content. This is demonstrated in <ref type="figure" target="#fig_4">Figure 4</ref>, which plots the cumulative accuracy curve on KB-Mono56 over sentence length (as measured by the number of non-whitespace char- acters). For example, points at x=50 show the averaged accuracies over sentences with no more than 50 characters. We compare CMX against the best performing monolingual and codemixing benchmark systems. The relative gain is more prominent on shorter sentences than on longer ones. For example, the improvement is 4.6% on short sentences (≤30 characters), while the gain on segments ≤150 characters is 1.9%. Similar pat- terns are seen with respect to other systems.</p><p>Inference Speed <ref type="table">Table 5</ref> also shows the infer- ence speed of each method in characters per sec- ond, tested on a machine with a 3.5GHz Intel   Xeon processor and 32GB RAM. CMX (written in C++) is far faster than other fine-grained sys- tems, e.g. it has an 800x speed-up over Equi- LID. It is not surprising that monolingual mod- els are faster than CMX, which makes a predic- tion for every token rather than once for the entire sequence. Of course, monolingual models do not support language ID on codemixed inputs, and fur- thermore CMX performs the best even on mono- lingual texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Feature Dropout Rate To analyze how the fea- ture dropout rate impacts the model performance, we create a set of synthetically misspelled tokens by random duplication or replacement of one or two characters. In addition, we ensure that every token has at least one language-unique character, so a model with character n-gram features should be able to easily identify the language of this to- ken. <ref type="figure" target="#fig_5">Figure 5</ref> shows the tuning results for dropout values on misspelled tokens and the GY-Mix de- velopment set. Without feature dropout (p=0.0), our model only gets 72.1% on misspelled tokens,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRAINING DATA Twitter-Mix Web-Mix6 GY-Mix (Test) KB-Mono56</head><p>All   indicating that n-gram features are not properly trained. The proposed feature dropout method ef- fectively addresses this issue, improving the accu- racy to 95.3% with p ≥ 0.5. We choose p = 0.5 ( <ref type="figure">Figure 3</ref>) because it gives the best trade-off on the two tuning sets. The curves in <ref type="figure" target="#fig_5">Figure 5</ref> also show that model performance is robust across a wide range of dropout rates between the two extremes, so the strategy is effective, but is not highly sensi- tive and does not require careful tuning. <ref type="table" target="#tab_9">Table 7</ref> shows a comparison over different decoding strategies, including (a) independent greedy prediction for each token, (b) adding a switching penalty and de- coding with Viterbi, (c) and our bilingually con- strained decoding. For the second method, we add a fixed transition matrix that gives a penalty score log p for every code switch in a sentence. We choose p = 0.5, which gives the best over- all results on the development set. Our approach outperforms switching penalty by more than 2% on both GY-Mix and KB-Mono56. To analyze the reason behind this difference we show the average number of languages in each sentence in <ref type="table" target="#tab_9">Table 7</ref>. Both baseline approaches on average predict more than 1.5 languages per sentence while the oracle number based on gold labels is only 1.15. Our global bilingual constraints effectively address this over-prediction issue, reducing the average num- ber of predicted languages to 1.27. We also mea- sure the running time of all methods. The decod- ing speed of our method is 206k char/sec <ref type="table">(Table  5)</ref>, while the independent method is 220k char/sec. Our decoding with global constraints thus only in- creases the running time by a factor of 1.07.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Decoding Algorithm</head><p>Codemixed Training Datasets Our training data consists of two codemixed corpora: manual annotations on real-world data (GY-Mix) and a synthetic corpus. To analyze their contribution, we remove each corpus in turn from the training set and report the results in Both examples are likely potential queries "Translate apple to English" with apple replaced by its translation in German(de) or Portuguese(pt). The underlying language pairings never appear in GY-Mix. CMX with synthetic training data is able to correctly identify the single token inter-mixed in a sentence, while the model trained without syn- thetic data fails on both cases.</p><p>Contribution of Features CMX has three types of features: character n-gram, script, and lexicon features. n-gram features play a crucial role as back-off from lexicon features. Consider infor- mal Latin script inputs, like hellooooo, for which no lexicon features fire. Foregoing n-gram fea- tures results in abysmal performance (&lt;20%) on this type of input because script features alone are inadequate. The main impact of script features is to avoid embarrassing mistakes on inputs that can be easily identified from their scripts. Finally, note that removing lexicon features corresponds to the CMX-small model. On monolingual inputs <ref type="table">(Table  5)</ref>, the lexicon features in CMX provide a 2.0% absolute improvement in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>CMX is a fast and compact model for fine- grained language identification. It outperforms re- lated models on codemixed and monolingual texts, which we show on several datasets covering text in a variety of languages and gathered from diverse sources. Furthermore, it is particularly robust to the idiosyncrasies of short informal text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>4) cv bien hmd w enti c ¸a va bien alhamdullilah wa enti c ¸a va bien It's going well, thank God, and you?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Example of our decoding algorithm with global constraints for example (4) for two allowed language pairs, en/ar-Latn and fr/ar-Latn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>10 https://github.com/CLD2Owners/cld2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sentence-level accuracy (y-axis) on KBMono56 as a function of the maximum number of non-whitespace characters in a sentence (x-axis). For example, the point at x = 50 denotes the accuracy on all the sentences with ≤ 50 characters.</figDesc><graphic url="image-2.png" coords="7,314.36,266.29,204.09,117.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy on development sets with various feature dropout rate values p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Statistics of our YouTube and Google+ dataset, GY-Mix.</head><label>1</label><figDesc></figDesc><table>Test Set Languages 
Twitter-Mix en, es 
Web-Mix6 cs, en, eu, hu, hr, sk 
GY-Mix en, es, hi, id 
KB-Mono56 56 languages 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The languages of each testing corpora 
in our experiments. The first three sets primarily 
include codemixed texts while the last one (KB-
Mono56) is monolingual. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>is assigned one of the 27 types of scripts based on its unicode value. The final feature vector contains the nor- malized counts of all character scripts observed in the input token. Lexicon features This feature group is backed by a large lexicon table which holds a language distribution for each token observed in the mono- lingual training data. For example, the word mango occurs 48% of the time in English docu- ments and 18% in Spanish ones. The table con- tains about 6.2 million entries. We also construct an additional prefix table of language distributions for 6-gram character prefixes. If the input token matches an entry in the lexicon table (or failing that, the prefix table), our model extracts the fol- lowing three groups of features.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Codemixed Texts: Token-level accuracy (%) of different approaches on codemixed texts. 
"CMX-small" corresponds to our small model without lexicon features and vocabulary tables. The hi/hi-
Latn/en column shows the accuracy on texts in English, Latin Hindi and Devanagari Hindi; the baseline 
models do not support identification of text in Hindi in Latin script. Average shows averaged accuracy 
on all sets except hi/hi-Latn/en. Boldface numbers indicate the best accuracy for each testing set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Token-level accuracy of our full model with different training sets, removing either GY-Mix 
annotations or synthetic codemixed corpus at a time. 

METHOD 
GY-Mix KB-Mono56 #Lang/Sent 

Independent 
87.8 
91.9 
1.78 
Switching Penalty 
89.4 
93.1 
1.58 
Bilingually Constrained 93.6 
95.1 
1.27 

Gold 
-
-
1.15 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Token-level accuracy of different decod-
ing methods on GY-Mix and KB-Mono56, as well 
as the averaged number of predicted languages in 
each sentence. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Adding the GY-
</table></figure>

			<note place="foot" n="1"> http://ufal.mff.cuni.cz/w2c 2 https://github.com/googlei18n/corpuscrawler 3 Both our collected codemixed data and Barman et al. (2014) indicate that more than 95% of codemixed instances are bilingual. 4 The two types of codemixing have roughly equal proportions in our labeled corpus.</note>

			<note place="foot" n="5"> https://plus.google.com/ 6 https://www.youtube.com/ 7 Hindi texts found in both Devanagari and Latin scripts.</note>

			<note place="foot" n="8"> Scores are sorted. Some languages omitted for illustration purposes.</note>

			<note place="foot" n="9"> We explain how we compute the number of parameters of our model in the supplemental material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Emily Pitler, Slav Petrov, John Alex, Daniel Andor, Kellie Webster, Vera Axelrod, Kuz-man Ganchev, Jan Botha, and Manaal Faruqui for helpful discussions during this work, and our anonymous reviewers for their thoughtful com-ments and suggestions. We also thank Elixabete Gomez, Héctor Alcalde, Knot Pipatsrisawat, and their stellar team of linguists who helped us to an-notate and curate much of our data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How noisy social media text, how diffrnt social media sources?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing</title>
		<meeting>the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language identification: The long and the short of the matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the ACL</title>
		<meeting>Human Language Technologies: The Annual Conference of the North American Chapter of the ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2014. I am borrowing ya mixing, an analysis of english-hindi code mixing in facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computational Approaches to Code Switching</title>
		<meeting>the First Workshop on Computational Approaches to Code Switching<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="116" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Code mixing: A challenge for language identification in the language of social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsab</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitava</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computational Approaches to Code Switching</title>
		<meeting>the First Workshop on Computational Approaches to Code Switching</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal dependency parsing for hindi-english code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riyaz</forename><forename type="middle">A</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing with small feed-forward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Jan A Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bakalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salcianu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2879" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying languages at the word level in code-mixed indian social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitava</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Gambäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Processing</title>
		<meeting>the 11th International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="378" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Small statistical models by random feature mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing</title>
		<meeting>the ACL-08: HLT Workshop on Mobile Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="19" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reconsidering language identification for written language resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baden</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="485" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016. Fasttext.zip: Compressing text classification models</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating dialectal variability for socially equitable language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Labeling the languages of words in mixed-language documents using weakly supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>cedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lanidenn: Multilingual language identification on character window</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2012. langid.py: An off-the-shelf language identification tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012: System Demonstrations</title>
		<meeting>the ACL 2012: System Demonstrations</meeting>
		<imprint>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic detection and language identification of multilingual documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview for the second shared task on language identification in code-switched data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Ghoneim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelati</forename><surname>Hawwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Reyvillamizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Approaches to Code Switching</title>
		<meeting>the Second Workshop on Computational Approaches to Code Switching<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview for the second shared task on language identification in code-switched data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Ghoneim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelati</forename><surname>Hawwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Reyvillamizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Approaches to Code Switching</title>
		<meeting>the Second Workshop on Computational Approaches to Code Switching</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word level language identification in online multilingual communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seza Do˘</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gruöz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="857" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating code-switching on twitter with a novel generalized word-level language detection technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Royal</forename><surname>Sequiera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><forename type="middle">Shekhar</forename><surname>Maddila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overview for the first shared task on language identification in code-switched data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Ghoneim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelati</forename><surname>Hawwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computational Approaches to Code Switching</title>
		<meeting>the First Workshop on Computational Approaches to Code Switching<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="62" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language identification in mixed script</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagesh</forename><surname>Bhattu Sristy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Satya Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shiva Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadlamani</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FIRE &apos;17: Forum for Information Retrieval Evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
