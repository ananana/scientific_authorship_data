<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Debate Automation: a Recurrent Model for Predicting Debate Winners</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Potash</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Debate Automation: a Recurrent Model for Predicting Debate Winners</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2465" to="2475"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we introduce a practical first step towards the creation of an automated debate agent: a state-of-the-art recurrent predictive model for predicting debate winners. By having an accurate pre-dictive model, we are able to objectively rate the quality of a statement made at a specific turn in a debate. The model is based on a recurrent neural network architecture with attention, which allows the model to effectively account for the entire debate when making its prediction. Our model achieves state-of-the-art accuracy on a dataset of debate transcripts annotated with audience favorability of the debate teams. Finally, we discuss how future work can leverage our proposed model for the creation of an automated debate agent. We accomplish this by determining the model input that will maximize audience favorability toward a given side of a debate at an arbitrary turn.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversational agents are a well-researched area of natural language generation ( <ref type="bibr" target="#b22">Pilato et al., 2007;</ref><ref type="bibr" target="#b7">Bigham et al., 2008;</ref><ref type="bibr" target="#b3">Augello et al., 2008;</ref><ref type="bibr" target="#b1">Agostaro et al., 2005;</ref><ref type="bibr" target="#b6">Bessho et al., 2012)</ref>. Else- where in the field of natural language generation, there is work that seeks to generate persuasive text <ref type="bibr" target="#b8">(Carenini and Moore, 2006;</ref><ref type="bibr" target="#b23">Reiter et al., 2003;</ref><ref type="bibr" target="#b24">Rosenfeld and Kraus, 2016)</ref>, which is a logical first step towards creating an automated debate agent. One major deficiency of existing work in this area is its assessment of how convincing (or compelling) a piece of text is; the approaches use theory-driven models of persuasion, rather than being empirically motivated. Furthermore, none of these works provide a model that can optimize persuasiveness at an arbitrary point in a conversa- tion.</p><p>One of the main reasons for a lack of empirically-driven persuasive generation systems is the absence of labeled data. In order to allevi- ate this problem (though not directly for the sake of producing an automated debate agent), <ref type="bibr" target="#b33">Zhang et al. (2016)</ref> have introduced a dataset of debate transcripts from the "Intelligence Squared" (IQ2) 1 debates. In these debates, two teams are present, arguing either for or against a given topic. For each debate, an audience poll is conducted both prior to and after the debate. Whichever team has the largest gain in audience support between the pre/post debate polls is the winner. This is a natu- ral way to account for the fact that some sides of a debate may be harder to argue than others, and that audience members may be initially biased given a debate topic.</p><p>Because of the sequential nature of debating, a Recurrent Neural Network (RNN) is an attractive choice for modeling the problem. Rather than just using the final hidded state for prediction, which likely has lost information from early in the de- bate, we propose to use an attention mechanism ( ) that creates a weighted sum over all hidden states, and is subsequently used for the final prediction. We motivate the use of an RNN, as opposed to a temporally flat clas- sifier, for several reasons. First, using an RNN allows us to naturally incorporate predicting au- dience favorability at each turn while explicitly modeling the turn sequence. Logistic regression, on the other hand, would not allow us to model the sequence explicitly. Secondly, our model allows us to take raw features as input, without having to compute summary statistics necessary for the fea-tures used in the model of <ref type="bibr" target="#b33">Zhang et al. (2016)</ref>. Fi- nally, since our end goal is debate automation, an RNN is a natural choice for debate turn generation.</p><p>There are two major difficulties dealing with the IQ2 dataset: first, since the construction of the dataset is non-trivial, there are only 108 data points, resulting in Zhang et al.'s proposal for leave-one-out (LOO) evaluation. Second, con- sidering the use of an RNN, the sequences are long, with an average length of 246 (and a stan- dard deviation of 67). In order to overcome this, we incorporate signals based on implicit audience feedback during the debate into the model's loss function. Instead of just training the model based on error from the audience's final verdict, pro- pogated through a substantial amount of timesteps, there are intermittent errors propogated backward through the network based on audience reactions, such as applauding or laughing. These internal signals also help regularize the network. In a way, they help generalize the hidden representation of the RNN, allowing it to better contain a distributed representation of the audience's favorability to- wards a given team.</p><p>In our proposed model, the audience's opinion is directly a function of the weighted hidden repre- sentations. Since the previous hidden representa- tions are all fixed at a given timestep, and the cur- rent hidden representation is directly a function of these previous hidden representations as well as the current input, the audience's current poll de- pends directly on the timestep's input. Therefore, at a given timestep, our framework allows us to determine the input that would maximize the audi- ence's favorability toward the orating team. This is due to the fact that the inputs are themselves rep- resentations of a given team's statement at a par- ticular turn in the debate. We evaluate our model on the dataset from Zhang et al., posting state-of-the-art accuracy. Our results show that our proposed regularization tech- nique is imperative for the RNN-based model to perform competitively with the models previously proposed by Zhang et al.. The attention mecha- nism also contributes to the best performing sys- tem. Afterward, we show how our model can be used to track audience favorability throughout the debate, as well as the aforementioned input opti- mization, using it in a case study to instruct a de- bate team about optimal debate strategy at a given turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous work that focuses on conversational language seeks to predict such qualities as disagreements <ref type="bibr" target="#b2">(Allen et al., 2014;</ref><ref type="bibr" target="#b32">Wang and Cardie, 2016)</ref>, divergence (Niculae and Danescu- Niculescu-Mizil, 2016), and participant stance ( <ref type="bibr" target="#b29">Sridhar et al., 2015;</ref><ref type="bibr" target="#b28">Somasundaran and Wiebe, 2010;</ref><ref type="bibr" target="#b31">Thomas et al., 2006;</ref><ref type="bibr">Rosenthal and McKeown, 2015)</ref>. What is most relevant for our pur- poses are the methods these models use for dealing with conversational data. <ref type="bibr" target="#b2">Allen et al. (2014)</ref> apply discourse parsing ( <ref type="bibr" target="#b16">Joty et al., 2013</ref>) and fragment quotation graph <ref type="bibr" target="#b9">(Carenini et al., 2007</ref>) tools to detect disagreement in online discussion threads. <ref type="bibr" target="#b32">Wang and Cardie (2016)</ref> believe that disagreement can be predicted by the presence of substantially long sequences of negative sentiment, motivating them to build a sequential sentiment prediction model using a particular kind of Conditional Ran- dom <ref type="bibr">Field (Mao and Lebanon, 2007)</ref>. Niculae and Danescu-Niculescu-Mizil (2016) use several novel features that capture the flow of ideas in the data, as well as team dynamics. Ultimately, how- ever, all these models apply manually derived, pre- processed features and use a basic classifier, like Random Forest or Logistic Regression. In con- trast, an RNN model is able to learn which inter- actions and overall sequences of rhetoric are im- portant for predictive power.</p><p>There is much less work that approaches the problem of predicting persuasiveness of text. This is due primarily to the lack applicable datasets. However, <ref type="bibr" target="#b13">Habernal and Gurevych (2016b)</ref> have recently presented a dataset where argument pairs are annotated for argument convincingness, as well as finer-grained annotations related to the ef- fectiveness of arguments ( <ref type="bibr" target="#b12">Habernal and Gurevych, 2016a</ref>). The authors experimented with feature- based classifiers, as well as various RNN architec- tures to construct predictive models for the dataset.</p><p>The most relevant work for this paper is of course <ref type="bibr" target="#b33">Zhang et al. (2016)</ref>. The authors use a set of features derived from the notion of idea flow in the debate. More specifically, they follow the method of <ref type="bibr" target="#b19">Monroe et al. (2008)</ref> to identify talk- ing points used by the sides present in a debate. The authors then create features based on the cov- erage of talking points during the debate. Finally, a Logistic Regression model uses these features to predict which team wins the debate. We also note the work of <ref type="bibr" target="#b26">Santos et al. (2016)</ref>, which also makes predictions on a dataset derived from the IQ2 de- bates. In contrast, their work analyses speech sig- nals, as opposed to textual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Predictive Model</head><p>In this section we explain how we apply an RNN to the task of predicting debate winners. We start by addressing the fact that for IQ2 dataset, each timestep involves a text span, as opposed to single tokens, and explaining how we convert this text span into a vector representation for RNN input. Secondly, we explain our RNN model architec- ture, including our use of an attention mechanism to create a weighted sum over all hidden states, as well as a regularization technique based on im- plicit audience reaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representing Debate Turns</head><p>Our work follows that of <ref type="bibr" target="#b33">Zhang et al. (2016)</ref>, and uses talking point-based features, specifically a 'bag of talking points'. Talking points for each de- bate are identified using a term frequency inverse document frequency (tfidf) metric applied to text tokens. Token counts, whether at a document or corpus level, occur only for the introduction text, as done by Zhang et al. This is based on the belief that the introductory arguments best showcase po- tential talking points. We take the 10 tokens with highest tfidf scores for each debate, and, across all debates, each token ranking maps to a fixed index in the turn representation. This representation is binary.</p><p>Zhang et al.'s results suggest that the interaction of talking points between debate teams can pos- sess strong predictive power. Therefore, we also calculate talking points at a team level within de- bates. We accomplish this by simply taking term frequency counts for tokens spoken by a given team. Like with the overall debate talking points, we chose the 10 highest ranked talking points from each side and include them in the input represen- tation. Moreover, we believe we can use a simpler talking point metric than that proposed by <ref type="bibr" target="#b19">Monroe et al. (2008)</ref> (and used by Zhang et al.) because the recurrent nature of the model will naturally cap- ture the interaction, coverage, and ignorance of the two team's (and overall) talking points.</p><p>Aside from talking point-based features, we in- clude the following linguistic features: 1) bag-of- words for tokens that have been used in at least 50 debates; 2) GloVe embeddings of tokens <ref type="bibr" target="#b21">(Pennington et al., 2014</ref>). We use max pooling over all the tokens' embeddings to create the embed- ding features. We also use the following non- linguistic features: 1) whether the turn occurs dur- ing the opening, discussion, or conclusion phase of the debate; 2) whether the turn is from the 'for' or 'against' team, as well as moderator or other speakers, such as show host etc; 3) the initial audi- ence poll is provided at each timestep. This is sim- ilar in spirit to 's decoder model that accesses the final encoder hidden state at each timestep.</p><p>We acknowledge that it would be possible to model individual turns (sequences of tokens) with a separate RNN. We choose to use hand- engineered features for two reasons: First, the current representation, mainly the talking points and BOW features, are easily interpretable given the goal of providing rhetorical strategy for de- baters. Using an RNN for this purpose would re- quire training a decoder in order to interpret the optimal rhetoric at a given turn (see Section 7). Secondly, it follows that having a trainable rep- resentation would introduce additional parameters into the model, which is a concern, given the lim- ited amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent Architecture</head><p>Our RNN model uses a long short-term mem- ory (LSTM) <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997</ref>) component. At each timestep, the model re- ceives as input a turn representation defined in Section 3.1. After consuming all turn represen- tations, a simple model without attention woud pass the final hidden state, h f , through two fully- connected layers (with an intermediate represen- tation h a to which we apply sigmoid activation), whose weights have subscripts post to identify that this transformation happens after the debate:</p><formula xml:id="formula_0">h a = σ(W 1 post h f + b 1 post ) (1) a = W 2 post h a + b 2 post (2)</formula><p>where σ is the sigmoid function. This transforma- tion outputs a vector with three dimensions, which corresponds to the fact that the audience poll has three possibilities: for, against, and undecided.</p><p>Since the polling is given as a percentage break- down, we apply sof tmax to create a valid prob- ability probability distribution for the audience, </p><formula xml:id="formula_1">p(A): p(A|Θ) = softmax(a)<label>(3)</label></formula><p>which is for a given set of model paramters, Θ. We train the model to minimize the Kullback- Leibler (KL) divergence between the target and predicted audience poll percentages. Given a training corpus of debates D with target post- debate audience polls A target i , the optimization objective is:</p><formula xml:id="formula_2">arg min Θ d∈D D KL (p(A target d )||p(A d |Θ)) (4)</formula><p>which simply sums the KL-Divergence of the tar- get and predicted audience poll percentages (prob- abilities) across all training examples. At test time, the model uses the percentages from p(A|Θ) to calculate which team increased their support from the audience the most, using the pre-debate audi- ence poll, which is given. For notation purposes, we refer to this KL-divergence for post debate audience polling D post KL . The optimization objec- tive from Equation 4 describes our base model. Shortly, we will describe how we regularize this base model using implicit audience feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Attention Mechanism</head><p>The model we have described to this point uses the final hidden state to predict the final audience poll. A concern with this approach is that the final hid- den state has a difficult time encoding the activity from the earlier parts of the debate. We propose to rectify this issue by creating a weighted sum over all hidden states, following the the attention mech- anism from . Given hidden states from all RNN timesteps, (h 1 , ..., h f ), we de- termine the weight for h i as follows. First, we compute a raw attention score:</p><formula xml:id="formula_3">r i = v T tanh(W a h i + b a )<label>(5)</label></formula><p>where v, W a , b a are model parameters. h i 's weight is computed from applying softmax to r:</p><formula xml:id="formula_4">α i = softmax(r) i<label>(6)</label></formula><p>which we use to compute the weighted sum across all hidden states:</p><formula xml:id="formula_5">h s = f i=1 α i h i<label>(7)</label></formula><p>Therefore, the attention version of our model uses h s in Equation 1 to predict the final audience poll.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Initializing RNN Hidden State</head><p>As we have mentioned, audience polls occur both before and after the debate. Thus, we continue the theme of using the RNN hidden state to ex- press audience polling by exploiting the initial au- dience poll to initialize the RNN hidden state, h 0 . The model uses the initial audience poll, a pre , and applies a fully-connected layer with parameters W pre and b pre :</p><formula xml:id="formula_6">h 0 = tanh(W pre a pre + b pre )<label>(8)</label></formula><p>We choose tanh for the activation function be- cause it is the same activation function used by the LSTM cell. The RNN now is initialized with a hidden state that reflects the audience's initial atti- tude towards a given debate topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Regularization via Implicit Audience Feedback</head><p>The IQ2 dataset offers two challenges for imple- menting an RNN-based approach. First, which is a difficulty for any type of supervised model, is the small dataset size. There are a total of 108 data points, which, even with LOO evaluation, leaves only 107 examples for training a model. For neural networks in particular, there is worry that overfitting easily occurs when the amount of model parameters is much greater than the dataset size ( <ref type="bibr" target="#b17">Lawrence et al., 1998;</ref><ref type="bibr" target="#b15">Ingrassia and Morlini, 2005</ref>). Aside from the dataset size, the se- quences of debate turns are long, averaging 246. This means that, on average, our model will run for 246 timesteps, making it difficult to train the network ( <ref type="bibr" target="#b5">Bengio et al., 1994</ref>) (the structure of the LSTM memory cell was designed to solve this is- sue, which motivates our use of it in our model).</p><p>In order to overcome these difficulties, we pro- pose to regularize our network based on implicit audience feedback that occurs during the debate, and is provided as metadata with the debate tran- script. Specifically, provided along side each de- bate turn, there is a 'non-text' field that indicates if any sounds occurred during the turn such as ap- plause or laughter from the audience. We view the presence of applause or laughter from the au- dience as a sign of endorsement during that par- ticular turn. Therefore, at that particular timestep, the hidden state should be able to directly predict this occurrence. Considering applause as a sign of endorsement is not controversial, but laughter could be viewed as more ambiguous. However, consider the audience of the debates: the debates air on the Bloomberg network and National Pub- lic Radio, suggesting a higher level of maturity of the audience, which is less likely to laugh at the participants, rather than at their jokes. For exam- ple, here is a turn in the debate 'Men are Finished' wherein laughter occurs: "Wait. What was that phrase you used, surviving off the fumes of sex- ism? I think we are our finest example there." This is an intentional joke by the speaker, who is part of of the winning team in the debate.</p><p>This signal can be integrated in a supervised manner into the loss function by converting the audience reaction at a given timestep into a three- dimensional vector, representing the current, im- plied audience favorability. We create such a vec- tor at a debate turn if either applause or laughter occurs at that timestep, and the speaker is one of the debate teams. On possibility is to create have a one-hot vector implying the audience favorability at the turn, with the mapping of side to index dic- tated by the target vector, A target i , and is set for the corpus. There is a major problem with using a one- hot vector: the probability distributions learned by the model will become too skewed, since the ulti- mate goal is to better generalize the prediction of debate polls, rarely are the polls so unbalanced to- ward one side. Moreover, the one-hot vector will only ever have mass in the indices for the 'for' and 'against' teams, and neglecting the 'undecided' in- dex, which is an important sector in the polling. Therefore, we create a soft vector as follows: a random number, n, is chosen in the interval ( 1 3 , 1). The index corresponding to the speaking team at timestep i has value n. The remaining two indices have value 1−n 2 . This vector is notated A target it , specifying that the reaction occurred at timestep t for debate i. On average, such reactions occur 21 times during a debate, with a standard deviation of 10. Consequently, this approach adds 2,268 more supervised signals to the dataset.</p><p>As we did with the post-debate poll, we can compute a lost based on the kl-divergence between A target it and the prediction probability at timestep t, which is a function of h t using the same trans- formations described in Equations 1, 2, and 3, but replacing h f with h t . The attention model can been used as well. In this case, we compute h st by slicing r (from equation 5) to only include in- dices up to t. We denote the KL-divergence be- tween target and prediction distributions across all timesteps of a training example is D react KL , since these signals are based on audience reaction.</p><p>The same strategy can be applied to h i using the pre-debate polls. Although this signal does not propagate through the RNN, it can still train the weights of the fully-connected layers used in our model. We refer to this KL-divergence as D pre KL , since it uses the pre-debate poll. Bringing together these separate error signals, we arrive at the train- ing objective of our full model:</p><formula xml:id="formula_7">arg min Θ d∈D D pre KL + D react KL + D post KL (9)</formula><p>where Θ is the model parameters used to produce the prediction probabilities. <ref type="figure">Figure 1</ref> provides an illustration of our training objective, unrolled over time.</p><p>With this new optimization objective, each ex- ample now trains our model based on (on average) 23 supervised signals. As a result, each training example allows the model to become more gen- eralizable, particularly because the hidden states are now better-tuned to encode audience favora- bility. This methodology allows the model to bet- ter leverage the small dataset size. Moreover, the intermittent error signals from audience reaction, D react KL , combined with the pre-debate error signal, D pre KL , help assuage the difficulties of training our model based on a final error signal propagated for many timesteps. We would like to reiterate that this regularization technique is only used to train the model, and not used for prediction, and there- fore will not be an issue when making predictions for new debates, nor will it create an unrealistic circumstance for using the model for creating a de- bate agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Design</head><p>Our experiments are conducted on the IQ2 dataset ( <ref type="bibr" target="#b33">Zhang et al., 2016</ref>). We use LOO evaluation, re- sulting in a training set of 107 examples. The eval- uation metric is simply prediction accuracy for de- bate winners. The winning team is based on audi- ence polling. Polls are conducted before and af- ter the debate, and audience members can vote as being either for or against a given debate topic, as well as being undecided. The team that has the highest increase in audience support from the pre to post debate poll is the winning team. The model trains for 100 epochs. Once training is com- plete, we test on the held-out data point. As Zhang et al. note, there are three debates in the dataset that have a tie between the debate teams. Follow- ing their procedure, we do not test on these data points. However, we still include these examples in the training sets, because our training objective is to predict polls, not debate winners. The final test accuracy is averaged across the remaining 105 LOO runs. Furthermore, we note that the dataset is effectively balanced, as there are 53 and 52 ex- amples with the two possible labels.</p><p>We implement all our models in TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2016)</ref>. We use the LSTM cell equipped with peephole connections <ref type="bibr" target="#b11">(Gers et al., 2002</ref>). This architecture allows the gates to see the current cell state, along with the hidden state. We believe that because of the long sequences present in the dataset, it is important to have all the gates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The results of our experiment are presented in <ref type="table" target="#tab_1">Table 1</ref>. Att means the model has the attention mechanism from Section 3.2.1; Reg means the model uses the optimization objective from Equa- tion 9 (all other models use the optimization ob- jective from Equation 4); Drpt means the model uses dropout (a popular regularization technique for neural networks <ref type="bibr" target="#b30">(Srivastava et al., 2014)</ref>) of 0.5. We compare our results against the best mod- els from <ref type="bibr" target="#b33">Zhang et al. (2016)</ref>. Each model uses a Logistic Regression (LR) classifier, and distin- guishes itself by the features it uses. The main features developed by the authors relate to the in- teraction (flow) of talking points between the de- bate teams. There are two types of models that use the flow features: LR Flow and LR Flow*. Whereas the former uses all developed flow fea- tures, the latter uses feature selection to keep the most powerful flow features. LR React uses fea- tures based on audience reaction metadata, and LR BOW uses bag-of-words features.</p><p>The results show that the LSTM attention model regularized by audience reaction achieves the highest accuracy. Furthermore, the results high- light the importance of this regularization tech- nique, since the simple LSTM model records the second lowest accuracy of any of the models pre- sented. This leads us to believe that the regular LSTM model falls victim to the lack of training data, preventing the larger amount of model pa- rameters (compared to a logistic regression model) from generalizing. The results also show that the attention model has higher performance than the regular LSTM model, and the difference in per- formance is heightened when the regularization technique is applied. We believe this is because the attention mechanism adds additional param- eters to the model, so it seems reasonable that adding additional training signals helps the model to generalize better. Lastly, our proposed regular- ization technique is far superior for generalization than the popular dropout method. We believe the strong performance of the proposed regularization technique is because it causes the LSTM's hidden states to better generalize the notion of encoding audience favorability. Furthermore, our model's goal is to predict distributions, as opposed to la- bels. Whereas dropout can be effective at aiding in collapsing representations of the same class into neighboring points of a latent space, our model needs to be able to predict polls that it may have not encountered in training. Our regularization technique aids in this as well by providing more training data, more polls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Tracking Audience Favorability</head><p>One of the advantages of mapping a recurrent model's hidden states to audience favorability is that we can produce a favorability poll at any turn (timestep) during the debate. In contrast, a tem- porally flat model, such as the logistic regression models from Zhang et al., produce a prediction of audience favorability based on features extracted from the entire debate. Using our mapping of hid- den states to audience favorability, we can deter- mine, at each turn, the current audience favorabil- ity, and track it throughout the entire debate. <ref type="figure">Fig- ure 2</ref> shows this applied to the "men are finished" debate, wherein the lines on the graph, cut verti- cally, represent predicted audience polls at a given debate turn. This debate saw the greatest increase in audience support from the pre to post debate poll: the 'for' increased their favorability by 46% (46 points). The three lines correspond to the three <ref type="figure">Figure 2</ref>: A visualization of audience favorabil- ity for the debate "men are finished". At each turn in the debate, our model predicts the audi- ence favorability. The y-axis shows the percent- age of the audience that supports a given side, and the x-axis show the turn number for a given poll. Even though these are purely predictions from the model, it is able to show the rise in audience fa- vorability for the 'for' team, as well as the decline in favorability for the 'against' team. From the graph, we can see that the 'for' team had a large spike in audience support roughly between turns 20 and 40, which corresponds to the beginning of the debate's discussion section.</p><p>possible positions an audience member can take regarding the debate topic. This visualization can be particularly useful for rhetorical analysis of de- bate performance, because the resulting graph al- lows us to see inflection points in audience favora- bility. These inflection points suggest that a debate team used very effective (or ineffective) rhetoric at that particular turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Optimizing Input for Audience Favorability</head><p>Aside from achieving a new state-of-the-art result on the IQ2 debate corpus, the main appeal of the model we have introduced is that it creates a map- ping between the hidden states and audience fa- vorability of the debate teams. This mapping is given in Equations 1 and 2, where a weighted sum over all over all hidden states (the actual notation in these equation apply the fully-connected trans- formation to a final hidden state, h f , unlike the attention model which uses h s from Equation 7) is transformed into a real-valued 3-dimensional vec- tor a. The values of the vector indicate 'raw' fa-vorability, which is realized as a probability dis- tribution (or alternatively, a poll of the audience) after applying the softmax activation. Further- more, given fixed model parameters Θ, the current hidden state is a function of the previous hidden states, previous cell state (if, like our model, an LSTM cell is used), as well as the current input. At a given timestep, the previous hidden and cell states are known. Therefore, a is directly a func- tion of the current input x at a given timestep. This notion of optimizing input for a target 'class' is akin to the work of <ref type="bibr" target="#b27">Simonyan et al. (2013)</ref>, who use a trained convolutional neural network to find the optimal input image for a desired object class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Input Optimization Objective</head><p>Similar to our approach in Section 3.2.3 to en- code implicit audience feedback, we can construct a three-dimensional one-hot vector with the index switched on that corresponds to the debate team whose favorability we seek to optimize. We will call this vector A f av . Given input x i at timestep i, we seek to to optimize the probability of A f av given x i :</p><p>arg max</p><formula xml:id="formula_8">x i p(A f av |x i , h 1 , ..., h i−1 , c i−1 ; Θ) (10)</formula><p>Where i ∈ (1, ..., T ) and T is the maximum num- ber of timesteps (turns) for a debate. In practice, we achieve this optimization by minimizing the cross-entropy between the the target one-hot vec- tor and the output of applying the softmax function to a, as in Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Applying Optimized Input for Persuasive Strategy</head><p>In the debate 'men are finished', the 'for' team won the debate, increasing their favorability by an astonishing 46% (conversely, the 'against' team saw a 25% decrease in favorability). According to our model's sequential predictions (and visible in <ref type="figure">Figure 2</ref>), a major turning point occurred at turn 27. Quantitatively, we can examine the turn-by-turn change in audience favorability: from this we see that one of the largest increases in audience favorability occurred at turn 27. It is not a surprise to find out that the team that spoke during turn 27 was the 'for' team. When asked by the moderator if there can be equality between the sexes without deeming men as being finished, the 'for' team said the following (the text is annotated for the presence of talking points, marked by a subscript that specifies whose talking point it is: A (against), F (for), or G, a general talking point based on overall token frequency (see Section 3.1)):</p><p>It is possible, but it just doesn't work that way. I mean, if we can all agree that there was male dominance for a long time and that male dominance is over, then I think we agree that men G,A are finished. So the resolution is about male dominance which we've taken for granted for so many tens of thousands of years. And so, even if you have parity, you have the end of male dominance. I mean, if you have women F rising and catching up to men G,A , then you no longer have male dominance. And so that's what I meant when I, early on, tried to define the resolution as men G,A are finished, the era of male dominance, it's finished, which we've taken for granted for all this time.</p><p>Note that the term 'women' is only a talking point for the 'for' team. In their response the 'against' team says:</p><p>They are not finished. That's absurd. You agreed to it in your opening that you didn't want to say men G,A are finished. You thought there might be inklings of a suggestion that it may be happening. But what you're defending now is that men G,A are finished. I'm saying it's absurd. I'm saying that some men G,A are in trouble. But rather than declare their extinction, we should be doing what we can to help them.</p><p>To determine our model's strategy immediately af- ter the 27th turn, we apply the previous hidden and cell states to the optimization objective in Equa- tion 10, taking the place of h 1 , ..., hi − 1 and c i−1 , respectively. We fit the training objective to the current states, as well as the weights of the pre- viously trained predictive model, and examine the resulting optimized input vector. We train the opti- mized input model for 15,000 epochs, which goes very fast because there is a 'single' training data point, and the model is not recurrent. As we can see in the actual 'against' team's response, the only talking point brought up is 'men', which can hardly be viewed as an enlightening notion in the context of the debate. Alternatively, the highest rated talking point from the optimized input is in fact the exact talking point brought up by the 'for' team: 'women'. This suggestion by our model is in line with the hypothesis of <ref type="bibr" target="#b33">Zhang et al. (2016)</ref>, that winning teams are effective in adopting their opponent's talking points. In terms of bag-of-word features: the optimized input ranks the following tokens as the ten highest (in descending order of score, and note the tokens have been stemmed):</p><p>'sound', 'present', 'recent', 'line', 'decid', 'veri', 'spent', 'save', 'moder', and 'found'. Most of these tokens remain somewhat vague with respect to their relevance to the debate. The token 're- cent' seems relevant, given that the debate topic has an inherent temporal nature. 'Save' is rel- evant in that some of the debate discussion ap- proaches the question of whether men need saving.</p><p>In the top 20 tokens we also find 'done', 'com- par', 'grow', and 'without', which are all rele- vant: 'done' is synonymous with 'finished', 'com- par' given that the debate is often comparing men to women, 'grow' could refer to the growth of women in society, and 'without' is a token specif- ically in the question the moderator asked prior to turn 27 (equality between the sexes without deem- ing men as being finished).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented an RNN model for predicting debate winners, with the specific goal of predict- ing the final (or intermediate) audience poll. The model takes at each timestep a representation of a given debate turn. The model uses an attention mechanism that creates a weighted sum over all hidden states. In order to achieve state-of-the-art results on a corpus of debate transcripts ( <ref type="bibr" target="#b33">Zhang et al., 2016)</ref>, we regularize the RNN model by propagating errors based on implicit audience re- action. Our results show that this regularization technique is critical for obtaining a state-of-the-art result. We have also shown the practical appli- cation of our model in two scenarios. First, the model can be used to make a prediction of au- dience polling at every debate turn. This allows for an analysis of the key turning points during the debate, based on inflections in audience fa- vorability. Second, the model can be used to de- termine the optimal input at a given debate turn. Knowledge of this input can inform debaters as to the best current persuasive strategy. Future work can leverage optimal inputs to create a language model that can become an automated debate agent. However, since the input is partially based on the knowledge of talking points, there is a potential for an information retrieval-based task to provide the talking points for the debate agent (if one desires a fully-automated system than can work without the presence of introductory remarks, from which talking points are currently extracted). Finally, fu- ture work can also examine the trained model itself in further detail, seeking to understand the debate strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>The results of LOO evaluation on the IQ2 dataset. See the beginning of Section 5 for an ex- planation of the models. take into account the cell state when producing a hidden layer. This adds a stronger notion of mem- ory to the model. While we expect the hidden state to represent audience favorability, we believe the cell state can capture the further latent notion of debate strategy, observable through the interaction of talking points between the debate teams. The models have cell and hidden size of 128, and the intermediate layer from Equation 2 has size 16. Lastly, we use a batch size of 8.</figDesc><table>Model 
Accuracy 
LR BOW 
0.50 
LR React 
0.60 
LR Flow 
0.63 
LR Flow* 
0.65 
LSTM 
0.55 
LSTM + Att 
0.57 
LSTM + Reg 
0.64 
LSTM + Att, Reg 0.71 
LSTM + Att, Drpt 0.60 

</table></figure>

			<note place="foot" n="1"> http://www.intelligencesquaredus.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the U.S. Army Research Office under Grant No. W911NF-16-1-0174. We would like to thank Alexey Romanov for his help with the figures.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martın</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A conversational agent based on a conceptual interpretation of a data driven semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Agostaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnese</forename><surname>Augello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pilato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Vassallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Gaglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress of the Italian Association for Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting disagreement in conversations using pseudo-monologic rhetorical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelsey</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1169" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Humorist bot: Bringing computational humour in a chat-bot system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnese</forename><surname>Augello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Saccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Gaglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pilato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex, Intelligent and Software Intensive Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="703" to="708" />
		</imprint>
	</monogr>
	<note>CISIS 2008. International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dialog system using real-time crowdsourcing and twitter large-scale corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumihiro</forename><surname>Bessho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuo</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="227" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inspiring blind high school students to pursue computer science with instant messaging chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">T</forename><surname>Maxwell B Aller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">O</forename><surname>Brudvik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Lindsay A Yazzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCSE Bulletin</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="449" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating and evaluating evaluative arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna D</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="925" to="952" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Summarizing email conversations with clue words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What makes a convincing argument? empirical analysis and detecting attributes of convincingness in web argumentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Which argument is more convincing? analyzing and predicting convincingness of web arguments using bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural network modeling for small datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Ingrassia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabella</forename><surname>Morlini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="297" to="311" />
		</imprint>
	</monogr>
<note type="report_type">Technometrics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Shafiq R Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">What size neural network gives optimal generalization? convergence properties of backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah Chung</forename><surname>Tsoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Isotonic conditional random fields and local sentiment flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">961</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fightin&apos;words: Lexical feature selection and evaluation for identifying the content of political conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burt L Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin M</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="372" to="403" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07407</idno>
		<title level="m">Conversational markers of constructive discussions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sub-symbolic semantic layer in cyc for intuitive chat-bots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pilato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnese</forename><surname>Augello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Vassallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Gaglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Computing, 2007. ICSC 2007. International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lessons from a failure: Generating tailored smoking cessation letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="41" to="58" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Strategical argumentative agent for human persuasion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Netherlands-Including Prestigious Applications of Artificial Intelligence (PAIS 2016)</title>
		<meeting><address><addrLine>The Hague</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2016-08-29" />
			<biblScope unit="volume">285</biblScope>
			<biblScope unit="page">320</biblScope>
		</imprint>
	</monogr>
	<note>ECAI 2016: 22nd European Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2015. I couldnt agree more: The role of conversational structure in agreement and disagreement detection in online discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<biblScope unit="page">168</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A domain-agnostic approach for opinion prediction on speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Bispo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Beinborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">163</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognizing stances in ideological on-line debates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
		<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint models of disagreement and stance in online debate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>James R Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn A</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Get out the vote: Determining support or opposition from congressional floor-debate transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 conference on empirical methods in natural language processing</title>
		<meeting>the 2006 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="327" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A piece of my mind: A sentiment analysis approach for online dispute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03114</idno>
		<title level="m">Conversational flow in oxford-style debates</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
