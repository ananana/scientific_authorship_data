<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Youssef</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4415" to="4424"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4415</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Activation functions play a crucial role in neural networks because they are the non-linearities which have been attributed to the success story of deep learning. One of the currently most popular activation functions is ReLU, but several competitors have recently been proposed or &apos;discovered&apos;, including LReLU functions and swish. While most works compare newly proposed activation functions on few tasks (usually from image classification) and against few competitors (usually ReLU), we perform the first large-scale comparison of 21 activation functions across eight different NLP tasks. We find that a largely unknown activation function performs most stably across all tasks, the so-called penalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the standard choices on a challenging NLP task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Activation functions are a crucial component of neural networks because they turn an otherwise linear classifier into a non-linear one, which has proven key to the high performances witnessed across a wide range of tasks in recent years. While different activation functions such as sigmoid or tanh are often equivalent on a theoretical level, in the sense that they can all approximate arbitrary continuous functions <ref type="bibr" target="#b8">(Hornik, 1991)</ref>, different ac- tivation functions often show very diverse behav- ior in practice.</p><p>For example, sigmoid, one of the activation functions dominating in neural network practice for several decades eventually turned out less suit- able for learning because (according to accepted wisdom) of its small derivative which may lead to vanishing gradients. In this respect, the so-called ReLU function <ref type="bibr" target="#b5">(Glorot et al., 2011</ref>) has proven much more suitable. It has an identity deriva- tive in the positive region and is thus claimed to be less susceptible to vanishing gradients. It has therefore (arguably) become the most popular ac- tivation function. The recognition of ReLU's suc- cess has led to various extensions proposed ( <ref type="bibr" target="#b16">Maas et al., 2013;</ref><ref type="bibr" target="#b7">He et al., 2015;</ref><ref type="bibr" target="#b12">Klambauer et al., 2017)</ref>, but none has reached the same popular- ity, most likely because of ReLU's simplicity and because the gains reported tended to be inconsis- tent or marginal across datasets and models <ref type="bibr" target="#b20">(Ramachandran et al., 2017)</ref>.</p><p>Activation functions have been characterized by a variety of properties deemed important for suc- cessful learning, such as ones relating to their derivatives, monotonicity, and whether their range is finite or not. However, in recent work, <ref type="bibr" target="#b20">Ramachandran et al. (2017)</ref> employed automatic search to find high-performing novel activation functions, where their search space contained compositions of elementary unary and binary functions such as max, min, sin, tanh, or exp. They found many functions violating properties deemed as useful, such as non-monotonic activa- tion functions or functions violating the gradient- preserving property of ReLU. Indeed, their most successful function, which they call swish, vio- lates both of these conditions. However, as with previous works, they also only evaluated their newly discovered as well as their (rectifier) base- line activation functions on few different datasets, usually taken from the image classification com- munity such as CIFAR ( <ref type="bibr" target="#b13">Krizhevsky, 2009)</ref> and ImageNet ( <ref type="bibr" target="#b22">Russakovsky et al., 2015)</ref>, and using few types of different networks, such as the deep convolutional networks abounding in the image classification community ( <ref type="bibr" target="#b25">Szegedy et al., 2016)</ref>.</p><p>To our best knowledge, there exists no large- scale empirical comparison of different activations across a variety of tasks and network architec- tures, and even less so within natural language processing (NLP). <ref type="bibr">1</ref> Thus, the question which acti- vation function really performs best and most sta- bly across different NLP tasks and popular NLP models remains unanswered to this date.</p><p>In this work, we fill this gap. We compare (i) 21 different activation functions, including the 6 top performers found from automatic search in <ref type="bibr" target="#b20">Ramachandran et al. (2017)</ref>, across (ii) three popu- lar NLP task types (sentence classification, docu- ment classification, sequence tagging) comprising 8 individual tasks, (iii) using three different popu- lar NLP architectures, namely, MLPs, CNNs, and RNNs. We also (iv) compare all functions across two different dimensions, namely: top vs. average performance.</p><p>We find that a largely unknown activation func- tion, penalized tanh ( <ref type="bibr" target="#b26">Xu et al., 2016)</ref>, performs most stably across our different tasks. We also find that it can successfully replace tanh and sigmoid activations in LSTM cells. We further find that the majority of top performing functions found in <ref type="bibr" target="#b20">Ramachandran et al. (2017)</ref> do not perform well for our tasks. An exception is swish, which per- formed well across several tasks, but less stably than penalized tanh and other functions. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theory</head><p>Activation functions We consider 21 activation functions, 6 of which are "novel" and proposed in <ref type="bibr" target="#b20">Ramachandran et al. (2017)</ref>. The functional form of these 6 is given in <ref type="table" target="#tab_0">Table 1</ref>, together with the sigmoid function.</p><p>The remaining 14 are: tanh, sin, relu, lrelu- 0.01, lrelu-0.30, maxout-2, maxout-3, maxout- 4, prelu, linear, elu, cube, penalized tanh, selu. We briefly describe them: lrelu-0.01 and lrelu- 0.30 are the so-called leaky relu (LReLU) func- tions ( <ref type="bibr" target="#b16">Maas et al., 2013)</ref>; the idea behind them is to avoid zero activations/derivatives in the nega- tive region of relu. Their functional form is given in <ref type="table" target="#tab_0">Table 1</ref>. prelu ( <ref type="bibr" target="#b7">He et al., 2015</ref>) generalizes the LReLU functions by allowing the slope in the neg- ative region to be a learnable parameter. The max- out functions ( <ref type="bibr" target="#b6">Goodfellow et al., 2013</ref>) are dif- ferent in that they introduce additional parameters and do not operate on a single scalar input. For example, maxout-2 is the operation that takes the maximum of two inputs: max{xW +b, xV +c}, so the number of learnable parameters is doubled. maxout-3 is the analogous function that takes the maximum of three inputs. As shown in <ref type="bibr" target="#b6">Goodfellow et al. (2013)</ref>, maxout can approximate any convex function. sin is the standard sine func- tion, proposed in neural network learning, e.g., in <ref type="bibr" target="#b18">Parascandolo et al. (2016)</ref>, where it was shown to enable faster learning on certain tasks than more established functions. penalized tanh ( <ref type="bibr" target="#b26">Xu et al., 2016</ref>) has been defined in analogy to the LReLU functions, which can be thought of as "penalizing" the identity function in the negative region. The reported good performance of penalized tanh on CIFAR-100 <ref type="bibr" target="#b13">(Krizhevsky, 2009</ref>) lets the authors speculate that the slope of activation functions near the origin may be crucial for learning. lin- ear is the identity function, f (x) = x. cube is the function f (x) = x 3 , proposed in <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> for an MLP used in dependency parsing. elu <ref type="bibr" target="#b2">(Clevert et al., 2015</ref>) has been pro- posed as (yet another) variant of relu that assumes negative values, making the mean activations more zero-centered. selu is a scaled variant of elu used in <ref type="bibr" target="#b12">Klambauer et al. (2017)</ref> in the context of so- called self-normalizing neural nets.</p><formula xml:id="formula_0">sigmoid f (x) = σ(x) = 1/(1 + exp(−x)) swish f (x) = x · σ(x) maxsig f (x) = max{x, σ(x)} cosid f (x) = cos(x) − x minsin f (x) = min{x, sin(x)} arctid f (x) = arctan(x) 2 − x maxtanh f (x) = max{x, tanh(x)} lrelu-0.01 f (x) = max{x, 0.01x} lrelu-0.30 f (x) = max{x, 0.3x} penalized tanh f (x) = tanh(x) x &gt; 0, 0.25 tanh(x) x ≤ 0</formula><p>Properties of activation functions Many prop- erties of activation functions have been speculated to be crucial for successful learning. Some of these are listed in <ref type="table" target="#tab_1">Table 2</ref>, together with brief de-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property</head><p>Description Problems Examples derivative f &gt; 1 exploding gradient (e) sigmoid (v), tanh (v), cube (e) &lt; 1 vanishing (v) zero-centered range centered around zero? if not, slower learning tanh (+), relu (−) saturating finite limits vanishing gradient in the limit tanh, penalized tanh, sigmoid monotonicity x &gt; y =⇒ f (x) ≥ f (y) unclear exceptions: sin, swish, minsin </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct experiments using three neural net- work types and three types of NLP tasks, de- scribed in §3.1, §3.2, and §3.3 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MLP &amp; Sentence Classification</head><p>Model We experiment with a multi-layer per- ceptron (MLP) applied to sentence-level classifi- cation tasks. That is, input to the MLP is a sen- tence or short text, represented as a fixed-size vec- tor embedding. The output of the MLP is a label which classifies the sentence or short text. We use two sentence representation techniques, namely, Sent2Vec ( <ref type="bibr" target="#b17">Pagliardini et al., 2018)</ref>, of dimension- ality 600, and InferSent ( <ref type="bibr" target="#b3">Conneau et al., 2017)</ref>, of dimensionality 4096. Our MLP has the form:</p><formula xml:id="formula_1">x i = f (x i−1 · W i + b i ) y = softmax(x N W N +1 + b N +1 )</formula><p>where x 0 is the input representation, x 1 , . . . , x N are hidden layer representations, and y is the out- put, a probability distribution over the classes in the classification task. Vectors b and matrices W are the learnable parameters of our network. The activation function is given by f and ranges over the choices described in §2.</p><p>Data We use four sentence classification tasks, namely: movie review classification (MR), sub- jectivitiy classification (SUBJ), question type clas- sification (TREC), and classifying whether a sen- tence contains an argumentation structure of a cer- tain type (claim, premise, major claim) or else is non-argumentative (AM). The first three datasets are standard sentence classification datasets and contained in the SentEval framework. <ref type="bibr">3</ref> We choose the AM dataset for task diversity, and derive it by projecting token-level annotations in the dataset from <ref type="bibr" target="#b24">Stab and Gurevych (2017)</ref> to the sentence level. In the rare case (&lt;5% of the cases) when a sentence contains multiple argument types, we choose one based on the ordering Major Claim (MC) &gt; Claim (C) &gt; Premise (P). Datasets and examples are listed in <ref type="table">Table 3</ref>.</p><p>Approach We consider 7 "mini-experiments":</p><p>• <ref type="formula">(1)</ref>: MR dataset with Sent2Vec-unigram em- beddings as input and 1% of the full data as training data; (2): the same mini-experiment with 50% of the full data as training data. In both cases, the dev set comprises 10% of the full data and the rest is for testing.</p><p>• (3,4): SUBJ with InferSent embeddings and likewise 1% and 50% of the full data as train data, respectively. • (5): The TREC dataset with original split in train and test; 50% of the train split is used as dev data.</p><p>• (6): The AM dataset with original split in train, dev, and test <ref type="bibr" target="#b4">(Eger et al., 2017)</ref>, and with InferSent input embeddings. <ref type="formula">(7)</ref>: the same mini-experiment with Sent2Vec-unigram em- beddings.</p><p>We report accuracy for mini-experiments (1-5) and macro-F1 for (6-7). We report macro-F1 for (6-7) because the AM dataset is imbalanced. The motivation behind choosing different input embeddings for different tasks was to investigate a wider variety of conditions. Choosing subsets of the full data had the same intention.</p><p>For all 7 mini-experiments, we draw the same 200 randomly chosen hyperparameters from the ranges indicated in <ref type="table" target="#tab_4">Table 4</ref>. All experiments are conducted in keras. <ref type="bibr">4</ref> For each of the 21 different activation functions detailed in §2, we conduct each mini-experiment with the 200 randomly chosen hyperparameters.  <ref type="table">Table 3</ref>: Evaluation tasks used in our experiments, grouped by task type (sentence classification, document classi- fication, sequence tagging), with statistics and examples. C is the number of classes to predict.</p><p>All activation functions use the same hyperparam- eters and the same train, dev, and test splits. We store two results for each mini-experiment, namely: (i) the test result corresponding to the best (best) dev performance; (ii) the average (mean) test result across all hyperparameters. The best result scenario mirrors standard optimiza- tion in machine learning: it indicates the score one can obtain with an activation function when the MLP is well-optimized. The mean result scenario is an indicator for what one can expect when hy- perparameter optimization is 'shallow' (e.g., be- cause computing times are prohibitive): it gives the average performance for randomly chosen hy- perparameters. We note that we run each hyper- parameter combination with 5 different random weight initializations and all the reported scores (best dev score, best best, best mean) are aver- ages over these 5 random initializations.</p><p>Finally, we set the following hyperparameters for all MLP experiments: patience of 10 for early stopping, batch size 16, 100 epochs for training.</p><p>Results <ref type="figure" target="#fig_0">Figure 1</ref> shows best and mean results, averaged over all 7 mini-experiments, for each ac- tivation function. To make individual scores com- parable across mini-experiments, we perform max normalization and divide each score by the maxi- mum score achieved in any given mini-experiment (for best and mean, respectively) before averag- ing. <ref type="bibr">5</ref> For best, the top performers are the rectifier functions (relu, lrelu-0.01, prelu) as well as max- out and penalized tanh. The newly discovered activation functions lag behind, with the best of them being minsin and swish. linear is worst, together with elu and cube. Overall, the differ- ence between the best activation function, relu, and the worst, linear, is only roughly 2pp, how- ever. This means that if hyperparameter search is done carefully, the choice of activation func- tion is less important for these sentence classifica- tion tasks. Particularly the (binary) tasks MR and SUBJ appear robust against the choice of activa- tion function, with the difference between the best and worst function being always less than 1pp, in all settings. For TREC and AM, the situation is slightly different: for TREC, the difference is 2pp (swish vs. maxsig) and for AM, it is 3pp using InferSent embeddings (swish vs. cube) and 12pp using Sent2Vec embeddings (relu vs. linear). It is noteworthy that swish wins 2 out of 3 cases in which the choice of activation function really mat- ters.  mean results are very different from best re- sults. Here, somewhat surprisingly, the oscillating</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Hyperparameter Range (a) MLP (1) optimizer {Adam,RMSprop,Adagrad,Adadelta,Adamax,Nadam,sgd} (2) #hidden layers N {1, 2, 3, 4} (3) dropout value [0.1, 0.75] (4) hidden units [30, 500] (5) learning rate N (m, m/5) (6) weight initializer {random-n, random-u, varscaling, orthogonal, lecun-u, glorot-n, glorot-u, he-n, he-u}   <ref type="figure">N (µ, s)</ref> is the normal distri- bution with mean µ and std s; µ = m is the default value from keras for the specific optimizer (if drawn learning rate is &lt; 0, we choose it to be m).</p><note type="other">(b) CNN (a) (1,3,5,6) same as MLP embedding dimension [40, 200] number of filters n k [30, 500] #hidden layers N {1, 2, 3} filter size h {1, 2, 2, 3</note><p>sin function wins, followed by penalized tanh, maxout and swish. The difference between the best mean function, sin, and the worst, cube, is more than 30pp. This means that using cube is much riskier and requires more careful hyperpa- rameter search compared to sin and the other top performers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CNN &amp; Document Classification</head><p>Model Our second paradigm is document clas- sification using a CNN. This approach has been popularized in NLP by the ground-breaking work of <ref type="bibr" target="#b11">Kim (2014)</ref>. Even though shallow CNNs do not reach state-of-the-art results on large datasets anymore <ref type="bibr" target="#b9">(Johnson and Zhang, 2017)</ref>, simple ap- proaches like (shallow) CNNs are still very com- petitive for smaller datasets <ref type="bibr" target="#b10">(Joulin et al., 2016)</ref>. Our model operates on token-level and first embeds a sequence of tokens x 1 , . . . , x n , repre- sented as 1-hot vectors, into learnable embed- dings x 1 , . . . , x n . The model then applies 1D- convolution on top of these embeddings. That is, a filter w of size h takes h successive embeddings x i:i+h−1 , performs a scalar product and obtains a feature c i :</p><formula xml:id="formula_2">c i = f (w · x i:i+h−1 + b).</formula><p>Here, f is the activation function and b is a bias term. We take the number n k of different filters as a hyperparameter. When our network has multiple layers, we stack another convolutional layer on top of the first (in total we have n k outputs at each time step), and so on. Our penultimate layer is a global max pooling layers that selects the maximum from each feature map. A final softmax layer terminates the network.</p><p>Data We use two document classification tasks, namely: 20 Newsgroup (NG) and Reuters-21578 R8 (R8). Both datasets are standard document classification datasets. In NG, the goal is to classify each document into one of 20 newsgroup classes (alt.atheism, sci.med, sci.space, etc.). In R8, the goal is to classify Reuters news text into one of eight classes (crude, earn, grain, inter- est, etc.). We used the preprocessed files from https://www.cs.umb.edu/ ˜ smimarog/ textmining/datasets/ (in particular, stopwords are removed and the text is stemmed).</p><p>Approach We consider 4 mini-experiments:</p><p>• (1,2) NG dataset with 5% and 50%, respec- tively of the full data as train data. In both cases, 10% of the full data is used as dev data, and the rest as test data.</p><p>• (3,4) Same as (1,2) for R8.</p><p>We report accuracy for all experiments. We use a batch size of 64, 50 epochs for training, and a pa- tience of 10. For all mini-experiments, we again draw 200 randomly chosen hyperparameters from the ranges indicated in <ref type="table" target="#tab_4">Table 4</ref>. The hyperparam- eters and train/dev/test splits are the same for all activation functions.</p><p>Results <ref type="figure" target="#fig_2">Figure 2</ref> shows best and mean results, averaged over all mini-experiments. This time, the winners for best are elu, selu (again two members from the rectifier family), and maxout- 3, but the difference between maxout-3 and sev- eral lower ranked functions is minimal. The cube function is again worst and sigmoid and cosid have similarly bad performance. Except for minsin, the newly proposed activation functions from <ref type="bibr" target="#b20">Ramachandran et al. (2017)</ref> again consider- ably lag behind. The most stable activation func- tions are the maxout functions as well as penal- ized tanh, tanh and sin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RNN &amp; Sequence Tagging</head><p>Model Our third paradigm is sequence tagging, a ubiquitous model type in NLP. In sequence tag- ging, a sequence of input tokens w 1 , . . . , w K is mapped to a sequence of labels y 1 , . . . , y K . Clas- sical sequence tagging tasks include POS tagging, chunking, NER, discourse parsing ( <ref type="bibr" target="#b0">Braud et al., 2017)</ref>, and argumentation mining ( <ref type="bibr" target="#b4">Eger et al., 2017;</ref><ref type="bibr" target="#b23">Schulz et al., 2018</ref>). We use a standard re- current net for sequence tagging, whose form is:</p><formula xml:id="formula_3">h i = f (h i−1 W + w i · U + b) y i = softmax(h i V + c)</formula><p>Here, w i are (pre-trained) word embeddings of words w i . Vectors b, c and matrices U, V, W are parameters to be learned during training. The above describes an RNN with only one hidden layer, h i , at each time step, but we consider the generalized form with N ≥ 1 hidden layers; we also choose a bidirectional RNN in which the hid- den outputs of a forward RNN and a backward RNN are combined. RNNs are particularly deep networks-indeed, the depth of the network corre- sponds to the length of the input sequence-which makes them particularly susceptible to the vanish- ing gradient problem ( <ref type="bibr" target="#b19">Pascanu et al., 2012</ref>). Initially, we do not consider the more popular LSTMs here for reasons indicated below. How- ever, we include a comparison after discussing the RNN performance.</p><p>Data We use two sequence tagging tasks, namely: English POS tagging (POS), and token- level argumentation mining (TL-AM) using the same dataset (consisting of student essays) as for the sentence level experiments. In token-level AM, we tag each token with a BIO-label plus the component type, i.e., the label space is Y = {B, I} × {MC, C, P} ∪ {O}, where 'O" is a la- bel for non-argumentative tokens. The motivation for using TL-AM is that, putatively, AM has more long-range dependencies than POS or similar se- quence tagging tasks such as NER, because argu- ment components are much longer than named en- tities and component labels also depend less on the current token.</p><p>Approach We consider 6 mini-experiments:</p><p>• (1): TL-AM with Glove-100d word embed- dings and 5% of the original training data as train data; (2) the same with 30% of the origi- nal training data as train data. In both cases, dev and test follow the original train splits (Eger et al., 2017).</p><p>• (3,4) Same as (1) and (2) but with 300d Levy word embeddings ( <ref type="bibr" target="#b14">Levy and Goldberg, 2014</ref>).</p><p>• (5,6): POS with Glove-100d word embed- dings and 5% and 30%, respectively, of the train data of a pre-determined train/dev/test split (13k/13k/178k tokens). Dev and test are fixed in both cases.</p><p>We report macro-F1 for mini-experiments (1-4) and accuracy for (5-6). For our RNN implemen- tations, we use the accompanying code of (the state-of-the-art model of) <ref type="bibr" target="#b21">Reimers and Gurevych (2017)</ref>, which is implemented in keras. The net- work uses a CRF layer as an output layer. We use a batch size of 32, train for 50 epochs and use a patience of 5 for early stopping.</p><p>Results <ref type="figure" target="#fig_3">Figure 3</ref> shows best and mean results, averaged over all 6 mini-experiments, for each ac- tivation function. We exclude prelu and the max- out functions because the keras implementation does not natively support these activation func- tions for RNNs. We also exclude the cube func- tion because it performed very badly. Unlike for sentence classification, there are much larger differences between the activation functions. For example, there is almost 20pp dif- ference between the best best activation func- tions: relu, lrelu-0.01, swish, penalized tanh, and the worst ones: linear, cosid, and sig- moid (the differences were larger had we included cube). Interestingly, this difference is mostly due to the TL-AM task: for POS, there is only 3pp dif- ference between the best function (sigmoid (sic!), though with almost zero margin to the next best ones) and the worst one (linear), while this differ- ence is almost 40pp for TL-AM. This appears to confirm our concerns regarding the POS tagging task as not being challenging enough due to lack of, e.g., long-range dependencies.</p><p>The four best best activation functions in <ref type="figure" target="#fig_3">Fig- ure 3</ref> are also the functions with the best mean results, i.e., they are most stable over different hy- perparameters. The clear winner in this category is penalized tanh with 100% mean score, fol- lowed by swish with 91%. Worst is cosid with 30%. It is remarkable how large the difference be- tween tanh and penalized tanh is both for best and mean-7pp and 20pp, respectively, which is much larger than the differences between the anal- ogous pair of LReLU and relu. This appears to make a strong case for the importance of the slope around the origin, as suggested in <ref type="bibr" target="#b26">Xu et al. (2016)</ref>.</p><p>LSTM vs. RNN Besides an RNN, we also im- plemented a more popular RNN model with (bidi- rectional) LSTM blocks in place of standard hid- den layers. Standard LSTM units follow the equa- tions (simplified):</p><formula xml:id="formula_4">f t = σ([h t−1 ; x t ] · W f ), i t = σ([h t−1 ; x t ] · W i ), o t = σ([h t−1 ; x t ] · W o ) c t = f t c t−1 + i t τ ([h t−1 ; x t ] · W c ) h t = o t τ (c t ),</formula><p>where f t and i t are perceived of as gates that con- trol information flow, x t is the input at time t and h t is the hidden layer activation. In keras (and most standard references), σ is the (hard) sigmoid function, and τ is the tanh function.</p><p>We ran an LSTM on the TL-AM dataset with Levy word embeddings and 5% and 30% data size setup. We varied σ and τ independently, keeping the respective other function at its default.</p><p>We find that the top two choices for τ are penal- ized tanh and tanh (margin of 10pp), given that σ is sigmoid. For τ = tanh, the best choices are σ = penalized tanh, sigmoid, and tanh. All other functions perform considerably worse. Thus, the top-performers are all saturating functions, indi- cating the different roles activation functions play in LSTMs-those of gates-compared to standard layers. It is worth mentioning that choosing σ or τ as penalized tanh is on average better than the standard choices for σ and τ . Indeed, choosing τ = σ = penalized tanh is on average 2pp better than the default choices of τ, σ.</p><p>It is further worth mentioning that the best best results for the LSTM are roughly 5pp better (absolute) than the best corresponding choices for the simple RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis &amp; Discussion</head><p>Winner statistics Each of the three meta-tasks sentence classification, document classification, and sequence tagging was won, on average, by a member from the rectifier family, namely, relu (2) and elu, for best. Also, in each case, cube and cosid were among the worst performing ac- tivation functions. The majority of newly pro- posed functions from <ref type="bibr" target="#b20">Ramachandran et al. (2017)</ref> ranked somewhere in the mid-field, with swish and minsin performing best in the best cate- gory. For the mean category, we particulary had the maxout functions as well as penalized tanh and sin regularly as top performers.</p><p>To get further insights, we computed a winner statistic across all 17 mini-experiments, counting how often each activation function was among the top 3. <ref type="table" target="#tab_6">Table 5</ref> shows the results, excluding prelu and the maxout functions because they were not considered in all mini-experiments.</p><p>best penalized tanh (6), swish (6), elu (4), relu (4), lrelu-0.01 (4) mean penalized tanh (16), tanh <ref type="formula">(13)</ref> sin (10) We see that penalized tanh and swish win here for best, followed by further rectifier functions. The mean category is clearly won by saturating activation functions with finite range. If this com- parison were restricted to sentence and document classification, where we also included the maxout functions, then penalized tanh would have been outperformed by maxout for mean.</p><p>This appears to yield the conclusion that functions with limited range behave more sta- bly across hyperparameter settings while non- saturating functions tend to yield better top- performances. The noteworthy exception to this rule is penalized tanh which excels in both cat- egories (the more expensive maxout functions would be further exceptions). If the slope around the origin of penalized tanh is responsible for its good performance, then this could also explain why cube is so bad, since it is very flat close to the origin.</p><p>Influence of hyperparameters To get some in- tuition about how hyperparameters affect our dif- ferent activation functions, we regressed the score of the functions on the test set on all the employed hyperparameters. For example, we estimated:</p><formula xml:id="formula_5">y = α l · log(n l ) + α d · d + · · · (1)</formula><p>where y is the score on the test set, n l is the num- ber of layers in the network, d is the dropout value, etc. The coefficients α k for each regressor k is what we want to estimate (in particular, their size and their sign). We logarithmized certain variables whose scale was substantially larger than those of others (e.g., number of units, number of fil- ters). For discrete regressors such as the optimizer we used binary dummy variables. We estimated Eq. (1) independently for each activation function and for each mini-experiment. Overall, there was a very diverse pattern of outcomes, preventing us from making too strong conclusions. Still, we ob- served that while all models performed on average better with fewer hidden layers, particularly swish was robust to more hidden layers (small negative coefficient α l ), but also, to a lesser degree, penal- ized tanh. In the sentence classification tasks, sin and the maxout functions were particulary robust to an increase of hidden layers. Since penalized tanh is a saturating function and sin even an oscil- lating one, we therefore conclude that preserving the gradient (derivative close to one) is not a nec- essary prerequisite to successful learning in deeper neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding remarks</head><p>We have conducted the first large scale compar- ison of activation functions across several differ- ent NLP tasks (and task types) and using differ- ent popular neural network types. Our main focus was on so-called scalar activation functions, but we also partly included the more costly 'many-to- one' maxout functions. Our findings suggest that the rectifier functions (and the similarly shaped swish) can be top per- formers for each task, but their performance is un- stable and cannot be predicted a priori. One of our major findings is that, in contrast, the saturat- ing penalized tanh function performs much more stably in this respect and can with high probabil- ity be expected to perform well across tasks as well as different choices of hyperparameters. This appears to make it the method of choice particu- larly when hyperparameter optimization is costly. When hyperparameter optimization is cheap, we recommend to consider the activation function as another hyperparameter and choose it, e.g., from the range of functions listed in <ref type="table" target="#tab_6">Table 5</ref> along with maxout.</p><p>Another major advantage of the penalized tanh function is that it may also take the role of a gate (because of its finite range) and thus be employed in more sophisticated neural network units such as LSTMs, where the rectifiers fail com- pletely. In this context, we noticed that replacing sigmoid and tanh in an LSTM cell with penal- ized tanh leads to a 2pp increase on a challenging NLP sequence tagging task. Exploring whether this holds across more NLP tasks should be scope for future work. Additionally, our research sug-gests it is worthwhile to further explore penal- ized tanh, an arguably marginally known activa- tion function. For instance, other scaling factors than 0.25 (default value from <ref type="bibr" target="#b26">Xu et al. (2016)</ref>) should be explored. Similarly as for prelu, the scaling factor can also be made part of the opti- mization problem.</p><p>Finally, we found that except for swish none of the newly discovered activation functions found in <ref type="bibr" target="#b20">Ramachandran et al. (2017)</ref> made it in our top cat- egories, suggesting that automatic search of acti- vation functions should be made across multiple tasks in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sentence Classification. Left y-axis: best. Right y-axis: mean. Score on y-axes is the average over all mini-experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Doc classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sequence tagging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Top: sigmoid activation function as well as 
6 top performing activation functions from Ramachan-
dran et al. (2017). Bottom: the LReLU functions with 
different parametrizations as well as penalized tanh. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Frequently cited properties of activation functions 

. 

scriptions and illustrations. 
Graphs of all activation functions can be found 
in the appendix. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Hyperparameter ranges for each network type. Hyperparameters are drawn using a discrete or continuous 
uniform distribution from the indicated ranges. Repeated values indicate multi-sets. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Top-3 winner statistics. In brackets: number 
of times within top-3, keeping only functions with four 
or more top-3 rankings. 

</table></figure>

			<note place="foot" n="1"> An exception may be considered Xu et al. (2015), who, however, only contrast the rectifier functions on image classification datasets. 2 Accompanying code to reproduce our experiments is available from https://github.com/UKPLab/ emnlp2018-activation-functions.</note>

			<note place="foot" n="3"> https://github.com/facebookresearch/ SentEval</note>

			<note place="foot" n="4"> https://keras.io/</note>

			<note place="foot" n="5"> We chose max normalization so that certain tasks/miniexperiments would not unduly dominate our averaged scores. Overall, our averaged scores are not (much) affected by this decision, however: the Spearman correlation of rankings of activation functions under max normalization and under no max normalization are above 0.98 in all our three classification scenarios considered in §3.1, §3.2, §3.3.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Teresa Bötschen, Nils Reimers and the anonymous reviewers for helpful comments. This work has been supported by the German Federal Ministry of Education and Research (BMBF) un-der the promotional reference 01UG1816B (CED-IFOR).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual and cross-domain discourse segmentation of entire documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophélie</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="237" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural end-to-end learning for computational argumentation mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>III- 1319-III-1327. JMLR.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;13</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks. In I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter ; Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2018-Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Taming the waves: sine as activation function in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heikki</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multitask learning for argumentation mining in lowresource settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Kahse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>page to appear. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parsing argumentation structures in persuasive essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="659" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Revise saturated activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruitong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1602.05980</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
