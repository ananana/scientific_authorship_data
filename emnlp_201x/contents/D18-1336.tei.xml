<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><forename type="middle">Libovick´y</forename><surname>Libovick´y</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostranské náměstí 25</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<postCode>118 00</postCode>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostranské náměstí 25</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<postCode>118 00</postCode>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3016" to="3021"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connec-tionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parallelization is the key ingredient for making deep learning models computationally tractable. While the advantages of parallelization are ex- ploited on many levels during training and infer- ence, autoregressive decoders require sequential execution.</p><p>Training and inference algorithms in sequence- to-sequence tasks with recurrent neural networks (RNNs) such as neural machine translation (NMT) have linear time complexity w.r.t. the target se- quence length, even when parallelized ( <ref type="bibr" target="#b12">Sutskever et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2014</ref>).</p><p>Recent approaches such as convolutional sequence-to-sequence learning <ref type="bibr" target="#b2">(Gehring et al., 2017)</ref> or self-attentive networks a.k.a. the Trans- former ( <ref type="bibr">Vaswani et al., 2017)</ref> replace RNNs with parallelizable components in order to reduce the time complexity of the training. In these models, the decoding is still sequential, because the probability of emitting a symbol is conditioned on the previously decoded symbols.</p><p>In non-autoregressive decoders, the inference algorithm can be parallelized because the decoder does not depend on its previous outputs. The apparent advantage of this approach is the near- constant time complexity achieved by the paral- lelization. On the other hand, the drawback is that the model needs to explicitly determine the target sentence length and reorder the state sequence be- fore it starts generating the output. In the current research contributions on this topic, these parts are trained separately and the inference is done in sev- eral steps.</p><p>In this paper, we propose an end-to-end non- autoregressive model for NMT using Connec- tionist Temporal Classification (CTC; <ref type="bibr" target="#b3">Graves et al. 2006</ref>). The proposed technique achieves promising results on translation between English- Romanian and English-German on the WMT News task datasets.</p><p>The paper is organized as follows. In Sec- tion 2, we summarize the related work on non- autoregressive NMT. Section 3 describes the ar- chitecture of our proposed model. Section 4 presents details of the conducted experiments. The results are discussed in Section 5. We conclude and present ideas for future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Non-Autoregressive NMT</head><p>In this section, we describe two methods for non- autoregressive decoding in NMT. Both of them are based on the Transformer architecture ( <ref type="bibr">Vaswani et al., 2017)</ref>, with the encoder part unchanged. <ref type="bibr" target="#b5">Gu et al. (2017)</ref> use a latent fertility model to copy the sequence of source embeddings which is then used for the target sentence generation. The fertility (i.e. the number of target words for each source word) is estimated using a softmax on the encoder states. In the decoder, the input embeddings are repeated based on their fertility.</p><p>The decoder has the same architecture as the en- coder plus the encoder attention. The best re- sults were achieved by sampling fertilities from the model and then rescoring the output sentences using an autoregressive model. The reported infer- ence speed of this method is 2-15 times faster than of a comparable autoregressive model, depending on the number of fertility samples. <ref type="bibr" target="#b9">Lee et al. (2018)</ref> propose an architecture with two decoders. The first decoder generates a can- didate translation from a source sentence padded to an estimated target length. The explicit length estimate is done with a softmax over possible sen- tence lengths (up to a fixed maximum). The output of the first decoder is then fed as an input to the second decoder. The second decoder is used as a denoising auto-encoder and can be applied itera- tively. Both decoders have the same architecture as in <ref type="bibr" target="#b5">Gu et al. (2017)</ref>. They achieved a speedup of 16 times over the autoregressive model with a sin- gle denoising iteration. They report the best result in terms of BLEU ( <ref type="bibr" target="#b10">Papineni et al., 2002</ref>) after 20 iterations with almost no inference speedup com- pared to their autoregressive baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Architecture</head><p>Similar to the previous work ( <ref type="bibr" target="#b5">Gu et al., 2017;</ref><ref type="bibr" target="#b9">Lee et al., 2018</ref>), our models are based on the Trans- former architecture as described by <ref type="bibr">Vaswani et al. (2017)</ref>, keeping the encoder part unchanged. Fig- ure 1 illustrates our method and highlights the dif- ferences from the Transformer model.</p><p>In order to generate output words in parallel, we formulate the translation as a sequence labeling problem. Neural architectures used for encoding input in NLP tasks usually generate sequences of hidden states of the same or shorter length as the input sequence. For this reason, we cannot apply the sequence labeling directly over the states be- cause the target sentence might be longer than the source sentence.</p><p>To enable the labeler to generate sentences that are longer than the source sentence, we project the encoder output states h into a k-times longer se- quence s, such that: words, after a linear projection, each state is sliced to k vectors, creating a sequence of length kT x .</p><formula xml:id="formula_0">s ci+b = W spl h c + b spl bd:(b+1)d<label>(1)</label></formula><p>In the next step, we process the sequence s with a decoder. Unlike the Transformer architecture, our decoder does not use the temporal mask in the self-attention step.</p><p>Finally, the decoder states are labeled either with an output token or a null symbol. The num- ber of combinations of the possible positions of the null symbols in the output sequence given ref- erence sequence length T y is kTx</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ty</head><p>. Because there is no prior alignment between the input and out- put symbols, we consider all output sequences that yield the correct output in the loss function. Be- cause summing the exponential number of com- binations directly is not tractable, we we use the CTC loss ( <ref type="bibr" target="#b3">Graves et al., 2006</ref>) which employs dy- namic programming to compute the negative log- likelihood of the output sequence, summed over all the combinations.</p><p>The loss can be computed using a linear al- gorithm similar to training Hidden Markov Mod- els <ref type="bibr" target="#b11">(Rabiner, 1989)</ref>. The algorithm computes and stores partial log-probabilities sums for all pre- fixes and suffixes of the output symbol sequence using dynamic programming. The table of pre- computed log-probablities allows us to compute the probability of being a part of a correct output sequence by combining the log-probabilities of its prefix and suffix.</p><p>An appealing property of training using the CTC loss is that the models support left-to-right beam search decoding by recombining prefixes that yield the same output. Unlike the greedy decoding this can no longer be done in parallel. However, the linear computation is in theory still faster than autoregressive decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experiment with three variants of this archi- tecture. All of them have the same total number of layers. First, the deep encoder uses a stack of self- attentive layers only. We apply the state splitting and the labeler on the output of the last encoder layer. In contrast to <ref type="figure">Figure 1</ref>, this variant omits the decoder part. Second, the encoder-decoder con- sists of two stacks of self-attentive layers -en- coder and decoder. The outputs of the encoder are transformed using Equation 1 and processed by the decoder. In each layer, the decoder part at- tends to the encoder output. Third, we extend the encoder-decoder variant with positional encoding ( <ref type="bibr">Vaswani et al., 2017</ref>). The positional encoding vectors are added to the decoder input s.</p><p>In all the experiments, we used the same hyper- parameters. We set the model dimension to 512 and the feed-forward layer dimension to 4096. We use multi-head attention with 16 heads. In the deep encoder setup, we use 12 layers in the en- coder, in the encoder-decoder setup, we use 6 lay- ers for the encoder and 6 layers for the decoder. We set the split factor k to 3, so the encoder states are projected to vectors of 1536 units.</p><p>We conduct our experiments on English- Romanian and English-German translation. These language pairs were selected by the authors of the previous work because the training datasets for these language pairs are of considerably different sizes. We follow these choices in order to present comparable results.</p><p>For English-Romanian experiments, we used the WMT16 ( <ref type="bibr">Bojar et al., 2016</ref>) news dataset. The training data consists of 613k sentence pairs, vali- dation 2k and test 2k. We used a shared vocabulary of 38k wordpieces ( <ref type="bibr">Wu et al., 2016;</ref><ref type="bibr" target="#b8">Johnson et al., 2017)</ref>.</p><p>The English-German dataset consists of 4.6M training sentence pairs from WMT competitions. As a validation set, we used the test set from WMT13 ( <ref type="bibr">Bojar et al., 2013)</ref>, which contains 3k sentence pairs. To enable comparison to other non-autoregressive approaches, we evaluate our models on the test sets from WMT14 ( <ref type="bibr">Bojar et al., 2014</ref>) with 3k sentence pairs and WMT15 <ref type="bibr">(Bojar et al., 2015</ref>) with 2.1k sentence pairs. As in the previous case, we used shared vocabulary for both languages which contained 41k wordpieces.</p><p>The experiments were conducted using Neural Monkey 1 <ref type="bibr" target="#b6">(Helcl and Libovick´yLibovick´y, 2017)</ref>. We evalu- ate the models using BLEU score <ref type="bibr" target="#b10">(Papineni et al., 2002</ref>) as implemented in SacreBLEU, 2 originally a part of the Sockeye toolkit ( <ref type="bibr" target="#b7">Hieber et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Quantitative results are tabulated in <ref type="table">Table 1</ref>. In general, our models achieve a similar performance to other non-autoregressive models. In case of English-German, our results in both directions are comparable on the WMT 14 test set and slightly better on the WMT 15 test set. This might be given by the fact that our autoregressive baseline performs better for this language pair than for English-Romanian.</p><p>The encoder-decoder setup outperforms the deep encoder setup. Including positional encod- ing seems beneficial when translating into Ger- man. Weight averaging from the 5 models with the highest validation score during the training im- proves the performance consistently.</p><p>We performed a manual evaluation on 100 randomly sampled sentences from the English- German test sets in both directions. The results of the analysis are summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Non-autoregressive translations of sentences that had errors in the autoregressive translation were often incomprehensible. In general, less than a quarter of the sentences was completely cor- rect and over two thirds (one half in the de→en direction) were comprehensible. The most fre- quent errors include omitting verbs at the end of German sentences and corruption of named enti- ties and infrequent words that are represented by more wordpieces. Most of these errors can be attributed to insufficient language-modeling capa- bilities of the model. The results suggest that in- tegrating an external language model into an effi- cient beam search implementation could boost the translation quality while preserving the speedup over the auto-regressive models.</p><p>We also evaluated the translations using sentence-level BLEU score <ref type="bibr">(Chen and Cherry, WMT 16</ref> WMT 14 WMT 15 en-ro ro-en en-de de-en en-de de-en autoregressive b = 1 31.93 31. <ref type="bibr">55</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2014</head><p>) and measure the Pearson correlation with the length of the source sentence and the number of null symbols generated in the output. With a growing sentence length, the scores degrade more in the non-autoregressive model (r = −0.42) than in its autoregressive counterpart (r = −0.39).</p><p>The relation between sentence-level BLEU and the source length is plotted in <ref type="figure">Figure 2</ref>. The sentence-level score is mildly correlated with the number of null symbols in the non-autoregressive output (r = 0.15). This suggests that increasing the splitting factor k in Equation 1 might improve the model performance. However, it also reduces the efficiency in terms of GPU memory usage. ing time by autoregressive and non-autoregressive models. The average times of decoding a single sentence are shown in <ref type="table" target="#tab_2">Table 3</ref>. We suspect that the small difference between CPU and GPU times in the non-autoregressive setup is caused by the CPU-only implementation of the CTC decoder in TensorFlow ( <ref type="bibr">Abadi et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we presented a novel method for training a non-autoregressive model end-to-end using connectionist temporal classification. We evaluated the proposed method on neural machine  translation in two language pairs and compared the results to the previous work.</p><p>In general, the results match the translation quality of equivalent variants of the models pre- sented in the previous work. The BLEU score is usually around 80-90% of the score of the au- toregressive baselines. We measured a 4-times speedup compared to our autoregressive baseline, which is a smaller gain than reported by the au- thors of the previous work. We suspect this might be due to a larger overhead with data loading and processing in Neural Monkey compared to Ten- sor2Tensor ( <ref type="bibr">Vaswani et al., 2018</ref>) used by others.</p><p>As a future work, we can try to improve the per- formance of the model by iterative denoising as done by <ref type="bibr" target="#b9">Lee et al. (2018)</ref> while keeping the non- autoregressive nature of the decoder.</p><p>Another direction of improving the model might be efficient implementation of beam search which can contain rescoring using an external lan- guage model as often done in speech recogni- tion ( <ref type="bibr" target="#b4">Graves et al., 2013</ref>). The non-autoregressive model would play a role a of the translation model in the traditional statistical MT problem decompo- sition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>for b = 0 .</head><label>0</label><figDesc>Figure 1: Scheme of the proposed architecture. The part between the encoder and the decoder is expressed by Equation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 shows the comparison of theFigure 2 :</head><label>32</label><figDesc>Figure 3 shows the comparison of the decod</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of CPU decoding time by our autoregressive (AR) and non-autoregressive (NAR) models based on the source sentence length. CPU GPU AR, b = 1 2247 ms 1200 ms NAR 386 ms 350 ms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of manual evaluation of the autore-
gressive (AR) and non-autoregressive (NAR) models 
(in percents). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average per sentence decoding time for en-de 
translation. 

</table></figure>

			<note place="foot" n="1"> https://github.com/ufal/neuralmonkey 2 https://github.com/mjpost/sacreBLEU</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been funded by the Czech Sci-ence Foundation grant no. P103/12/G084, Charles University grant no. 976518 and SVV project no. 260 453.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Findings of the 2015 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic comparison of smoothing techniques for sentencelevel bleu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="362" to="367" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno>abs/1711.02281</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Monkey: An open-source tool for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ylibovick´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="issue">107</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05690</idno>
		<title level="m">Sockeye: A Toolkit for Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1802.06901</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence R Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
