<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rationale-Augmented Convolutional Neural Networks for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
							<email>yezhang@cs.utexas.edu, iain.marshall@kcl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Marshall</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Primary Care and Public Health Sciences</orgName>
								<orgName type="institution">Kings College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rationale-Augmented Convolutional Neural Networks for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="795" to="804"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new Convolutional Neural Network (CNN) model for text classification that jointly exploits labels on documents and their constituent sentences. Specifically, we consider scenarios in which annotators explicitly mark sentences (or snippets) that support their overall document categorization, i.e., they provide rationales. Our model exploits such supervision via a hierarchical approach in which each document is represented by a linear combination of the vector representations of its component sentences. We propose a sentence-level convolutional model that estimates the probability that a given sentence is a rationale, and we then scale the contribution of each sentence to the aggregate document representation in proportion to these estimates. Experiments on five classification datasets that have document labels and associated rationales demonstrate that our approach consistently outperforms strong base-lines. Moreover, our model naturally provides explanations for its predictions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural models that exploit word embeddings have recently achieved impressive results on text classifi- cation tasks <ref type="bibr" target="#b3">(Goldberg, 2015)</ref>. Feed-forward Con- volutional Neural Networks (CNNs), in particular, have emerged as a relatively simple yet powerful class of models for text classification <ref type="bibr" target="#b9">(Kim, 2014)</ref>.</p><p>These neural text classification models have tended to assume a standard supervised learning set- ting in which instance labels are provided. Here we consider an alternative scenario in which we assume that we are provided a set of rationales ( <ref type="bibr" target="#b27">Zaidan et al., 2007;</ref><ref type="bibr" target="#b26">Zaidan and Eisner, 2008;</ref><ref type="bibr" target="#b16">McDonnell et al., 2016</ref>) in addition to instance labels, i.e., sen- tences or snippets that support the corresponding document categorizations. Providing such rationales during manual classification is a natural interaction for annotators, and requires little additional effort <ref type="bibr" target="#b20">(Settles, 2011;</ref><ref type="bibr" target="#b16">McDonnell et al., 2016)</ref>. Therefore, when training new classification systems, it is natu- ral to acquire supervision at both the document and sentence level, with the aim of inducing a better pre- dictive model, potentially with less effort.</p><p>Learning algorithms must be designed to capital- ize on these two types of supervision. Past work (Section 2) has introduced such methods, but these have relied on linear models such as Support Vector Machines (SVMs) <ref type="bibr" target="#b5">(Joachims, 1998)</ref>, operating over sparse representations of text. We propose a novel CNN model for text classification that exploits both document labels and associated rationales.</p><p>Specific contributions of this work as follows. <ref type="formula" target="#formula_1">(1)</ref> This is the first work to incorporate rationales into neural models for text classification. (2) Empiri- cally, we show that the proposed model uniformly outperforms relevant baseline approaches across five datasets, including previously proposed models that capitalize on rationales ( <ref type="bibr" target="#b27">Zaidan et al., 2007;</ref><ref type="bibr" target="#b15">Marshall et al., 2016)</ref> and multiple baseline CNN vari- ants, including a CNN equipped with an attention mechanism. We also report state-of-the-art results on the important task of automatically assessing the risks of bias in the studies described in full-text biomedical articles ( <ref type="bibr" target="#b15">Marshall et al., 2016)</ref>. (3) Our model naturally provides explanations for its predic-tions, providing interpretability.</p><p>We have made available online both a Theano 1 and a Keras implementation 2 of our model. <ref type="bibr" target="#b9">Kim (2014)</ref> proposed the basic CNN model we de- scribe below and then build upon in this work. Prop- erties of this model were explored empirically in ( <ref type="bibr" target="#b29">Zhang and Wallace, 2015</ref>). We also note that <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> extended this model to jointly accom- modate multiple sets of pre-trained word embed- dings. Roughly concurrently to <ref type="bibr">Kim, Johnson and Zhang (2014)</ref> proposed a similar CNN architecture, although they swapped in one-hot vectors in place of (pre-trained) word embeddings. They later de- veloped a semi-supervised variant of this approach <ref type="bibr" target="#b7">(Johnson and Zhang, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural models for text classification</head><p>In related recent work on Recurrent Neural Net- work (RNN) models for text, <ref type="bibr" target="#b23">Tang et al. (2015)</ref> pro- posed using a Long Short Term Memory (LSTM) layer to represent each sentence and then passing another RNN variant over these. And <ref type="bibr" target="#b24">Yang et al. (2016)</ref> proposed a hierarchical network with two levels of attention mechanisms for document clas- sification. We discuss this model specifically as well as attention more generally and its relationship to our proposed approach in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Exploiting rationales</head><p>In long documents the importance of sentences varies; some are more central than others. Prior work has investigated methods to measure the rel- ative importance sentences ( <ref type="bibr" target="#b10">Ko et al., 2002;</ref><ref type="bibr" target="#b17">Murata et al., 2000</ref>). In this work we adopt a particular view of sentence importance in the context of document classification. In particular, we assume that docu- ments comprise sentences that directly support their categorization. We call such sentences rationales.</p><p>The notion of rationales was first introduced by <ref type="bibr" target="#b27">Zaidan et al. (2007)</ref>. To harness these for classifi- cation, they proposed modifying the Support Vec- tor Machine (SVM) objective function to encode a preference for parameter values that result in in- stances containing manually annotated rationales being more confidently classified than 'pseudo'- instances from which these rationales had been stripped. This approach dramatically outperformed baseline SVM variants that do not exploit such ra- tionales. <ref type="bibr" target="#b25">Yessenalina et al. (2010)</ref> later developed an approach to generate rationales.</p><p>Another line of related work concerns models that capitalize on dual supervision, i.e., labels on indi- vidual features. This work has largely involved in- serting constraints into the learning process that fa- vor parameter values that align with a priori feature- label affinities or rankings <ref type="bibr" target="#b2">(Druck et al., 2008;</ref><ref type="bibr" target="#b13">Mann and McCallum, 2010;</ref><ref type="bibr" target="#b21">Small et al., 2011;</ref><ref type="bibr" target="#b20">Settles, 2011</ref>). We do not discuss this line of work further here, as our focus is on exploiting provided ratio- nales, rather than individual labeled features.  We first review the simple one-layer CNN for sentence modeling proposed by <ref type="bibr" target="#b9">Kim (2014)</ref>. Given a sentence or document comprising n words w 1 , w 2 ,...,w n , we replace each word with its d- dimensional pretrained embedding, and stack them row-wise, generating an instance matrix A ∈ R n×d .</p><p>We then apply convolution operations on this ma- trix using multiple linear filters, these will have the same width d but may vary in height. Each filter thus effectively considers distinct n-gram features, where n corresponds to the filter height. In practice, we introduce multiple, redundant features of each height; thus each filter height might have hundreds of corresponding instantiated filters. Applying filter i parameterized by W i ∈ R h i ·d to the instance ma- trix induces a feature map f i ∈ R n−h i +1 . This pro- cess is performed by sliding the filter from the top of the matrix (the start of the document or sentence) to the bottom. At each location, we apply element- wise multiplication between filter i and sub-matrix A[j : j + h i − 1], and then sum up the resultant matrix elements. In this way, we induce a vector (feature map) for each filter.</p><p>We next run the feature map through an element- wise non-linear transformation. Specifically, we use the Rectified Linear Unit, or ReLU ( <ref type="bibr" target="#b11">Krizhevsky et al., 2012</ref>). We extract the maximum value o i from each feature map i (1-max pooling).</p><p>Finally, we concatenate all of the features o i to form a vector representation o ∈ R |F | for this in- stance, where |F | denotes the total number of filters. Classification is then performed on top of o, via a softmax function. Dropout ( <ref type="bibr" target="#b22">Srivastava et al., 2014</ref>) is often applied at this layer as a means of regular- ization. We provide an illustrative schematic of the basic CNN architecture just described in <ref type="figure" target="#fig_0">Figure 1</ref>. For more details, see <ref type="bibr" target="#b29">(Zhang and Wallace, 2015)</ref>. This model was originally proposed for sentence classification <ref type="bibr" target="#b9">(Kim, 2014</ref>), but we can adapt it for document classification by simply treating the doc- ument as one long sentence. We will refer to this basic CNN variant as CNN in the rest of the paper. Below we consider extensions that account for doc- ument structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rationale-Augmented CNN for Document Classification</head><p>We now move to the main contribution of this work: a rationale-augmented CNN for text classi- fication. We first introduce a simple variant of the above CNN that models document structure (Section 4.1) and then introduce a means of incorporating rationale-level supervision into this model (Section 4.2). In Section 4.3 we discuss connections to atten- tion mechanisms and describe a baseline equipped with one, inspired by <ref type="bibr" target="#b24">Yang et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modeling Document Structure</head><p>Recall that rationales are snippets of text marked as having supported document-level categorizations.</p><p>We aim to develop a model that can exploit these an- notations during training to improve classification. Here we achieve this by developing a hierarchical model that estimates the probabilities of individual sentences being rationales and uses these estimates to inform the document level classification.</p><p>As a first step, we extend the CNN model above to explicitly account for document structure. Specif- ically, we apply a CNN to each individual sentence in a document to obtain sentence vectors indepen- dently. We then sum the respective sentence vectors to create a document vector. 3 As before, we add a softmax layer on top of the document-level vector to perform classification. We perform regularization by applying dropout both on the individual sentence vectors and the final document vector. We will re- fer to this model as Doc-CNN. Doc-CNN forms the basis for our novel approach, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RA-CNN</head><p>In this section we present the Rationale-Augmented CNN (RA-CNN). Briefly, RA-CNN induces a document-level vector representation by taking a weighted sum of its constituent sentence vectors. Each sentence weight is set to reflect the estimated probability that it is a rationale in support of the most likely class. We provide a schematic of this model in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>RA-CNN capitalizes on both sentence-and document-level supervision. There are thus two steps in the training phase: sentence level training and document level training. For the former, we ap- ply a CNN to each sentence j in document i to obtain sentence vectors x ij sen . We then add a softmax layer parametrized by W sen ; this takes as input sentence vectors. We fit this model to maximize the probabil- ities of the observed rationales:   The sentences comprising a text are passed through a sentence model that outputs probabilities encoding the likelihood that sentences are neutral or a (positive or negative) rationale. Sentences likely to be rationales are given higher weights in the global document vector, which is the input to the document model.</p><formula xml:id="formula_0">sen p( ) W sen x sen i0 x sen i0 exp( . ) . x sen ij = k k x sen il x sen i y il sen = k N i j=1 N i p( ) y i sen = k N i x doc i W doc x doc i exp( ) k p( ) = k document i</formula><formula xml:id="formula_1">p(y ij sen = k; E, C, W sen ) = exp(W (k)T sen x ij sen ) Ksen k=1 exp(W (k)T sen x ij sen )<label>(1)</label></formula><p>Where y ij sen denotes the rationale label for sentence j in document i, K sen denotes the number of possible classes for sentences, E denotes the word embed- ding matrix, C denotes the convolution layer param- eters, and W sen is a matrix of weights (comprising one weight vector per sentence class).</p><p>In our setting, each sentence has three possible labels (K sen = 3). When a rationale sentence ap- pears in a positive document, 4 it is a positive ratio- nale; when a rationale sentence appears in a negative document, it is a negative rationale. All other sen-tences belong to a third, neutral class: these are non- rationales. We also experimented with having only two sentence classes: rationales and non-rationales, but this did not perform as well as explicitly main- taining separate classes for rationales of different polarities.</p><p>We train an estimator using the provided ratio- nale annotations, optimizing over {E, C, W sen } to minimize the categorical cross-entropy of sentence labels. Once trained, this sub-model can provide conditional probability estimates regarding whether a given sentence is a positive or a negative rationale, which we will denote by p pos and p neg , respectively.</p><p>We next train the document-level classification model. The inputs to this are vector representations of documents, induced by summing over constituent sentence vectors, as in Doc-CNN. However, in the RA-CNN model this is a weighted sum. Specifi- cally, weights are set to the estimated probabilities that corresponding sentences are rationales in the most likely direction. More precisely:</p><formula xml:id="formula_2">x i doc = N i j=1 x ij sen · max{p ij pos , p ij neg }<label>(2)</label></formula><p>Where N i is the number of sentences in the ith doc- ument. The intuition is that sentences likely to be ra- tionales will have greater influence on the resultant document vector representation, while the contribu- tion of neutral sentences (which are less relevant to the classification task) will be minimized. The final classification is performed by a softmax layer parameterized by W doc ; the inputs to this layer are the document vectors. The W doc parameters are trained using the document-level labels, y i doc :</p><formula xml:id="formula_3">p(y i doc = k; E, C, W doc ) = exp(W (k)T doc x i doc ) K doc k=1 exp(W (k)T doc x i doc )<label>(3)</label></formula><p>where K doc is the cardinality of the document label set. We optimize over parameters to minimize cross- entropy loss (w.r.t. the document labels).</p><p>We note that the sentence-and document-level models share word embeddings E and convolution layer parameters C, but the document-level model has its own softmax parameters W doc . When train- ing the document-level model, E, C and W doc are fit, but we hold W sen fixed.</p><p>The above two-step strategy can be equivalently described as follows. We first estimate E, C and W sen , which parameterize our model for identifying rationales in documents. We then move to fitting our document classification model. For this we initialize the word embedding and convolution parameters to the E and C estimates from the preceding step. We then directly minimize the document level classifica- tion objective, tuning E and C and simultaneously fitting W doc .</p><p>Note that this sequential training strategy differs from the alternating training approach commonly used in multi-task learning <ref type="bibr" target="#b1">(Collobert and Weston, 2008)</ref>. We found that the latter approach does not work well here, leading us to instead adopt the cascade-like feature learning approach (Collobert and Weston, 2008) just described.</p><p>One nice property of our model is that it naturally provides explanations for its predictions: the model identifies rationales and then categorizes documents informed by these. Thus if the model classifies a test instance as positive, then by construction the sen- tences associated with the highest p ij pos estimates are those that the model relied on most in coming to this disposition. These sentences can of course be out- put in conjunction with the prediction. We provide concrete examples of this in Section 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Rationales as 'Supervised Attention'</head><p>One may view RA-CNN as a supervised variant of a model equipped with an attention mechanism (Bah- danau et al., 2014). On this view, it is apparent that rather than capitalizing on rationales directly, we could attempt to let the model learn which sentences are important, using only the document labels. We therefore construct an additional baseline that does just this, thereby allowing us to assess the impact of learning directly from rationale-level supervision.</p><p>Following the recent work of <ref type="bibr" target="#b24">Yang et al. (2016)</ref>, we first posit for each sentence vector a hidden rep- resentation u ij sen . We then define a sentence-level context vector u s , which we multiply with each u ij sen to induce a weight α ij . Finally, the document vec- tor is taken as a weighted sum over sentence vectors, where weights reflect α's. We have:</p><formula xml:id="formula_4">u ij sen = tanh(W s x ij sen + b s )<label>(4)</label></formula><formula xml:id="formula_5">α ij = exp(u T s u ij sen ) N i j exp(u T s u ij sen )<label>(5)</label></formula><formula xml:id="formula_6">x i doc = N i j α ij x ij sen (6)</formula><p>where x i doc again denotes the document vector fed into a softmax layer, and W s , u s and b s are learned during training. We will refer to this attention-based method as AT-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>We used five text classification datasets to evaluate our approach in total. Four of these are biomedical text classification datasets (5.1) and the last is a col- lection of movie reviews (5.2). These datasets share the property of having recorded rationales associated with each document categorization. We summarize attributes of all datasets used in this work in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Risk of Bias (RoB) Datasets</head><p>We used a collection Risk of Bias (RoB) text classifi- cation datasets, described at length elsewhere <ref type="bibr" target="#b15">(Marshall et al., 2016</ref>). Briefly, the task concerns as- sessing the reliability of the evidence presented in full-text biomedical journal articles that describe the conduct and results of randomized controlled trials (RCTs). This involves, e.g., assessing whether or not patients were properly blinded as to whether they were receiving an active treatment or a comparator (such as a placebo). If such blinding is not done correctly, it compromises the study by introducing statistical bias into the treatment efficacy estimate(s) derived from the trial.</p><p>A formal system for making bias assessments is codified by the Cochrane Risk of Bias Tool <ref type="bibr" target="#b4">(Higgins et al., 2011</ref>). This tool defines multiple do- mains; the risk of bias may be assessed in each of these. We consider four domains here. (1) Random sequence generation (RSG): were patients were as- signed to treatments in a truly random fashion? (2) Allocation concealment (AC): were group assign- ments revealed to the person assigning patients to groups (so that she may have knowingly or unknow- ingly) influenced these assignments? (3) Blinding of Participants and Personnel (BPP): were all trial participants and individuals involved in running the trial blinded as to who was receiving which treat- ment? (4) Blinding of outcome assessment (BOA): were the parties who measured the outcome(s) of in- terest blinded to the intervention group assignments? These assessments are somewhat subjective. To in- crease transparency, researchers performing RoB as- sessment therefore record rationales (sentences from articles) supporting their assessments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Movie Review Dataset</head><p>We also ran experiments on a movie review (MR) dataset with accompanying rationales. <ref type="bibr" target="#b18">Pang and Lee (2004)</ref> developed and published the original ver- sion of this dataset, which comprises 1000 positive and 1000 negative movie reviews from the Internet Movie Database (IMDB). <ref type="bibr">5</ref>  augmented this dataset by adding rationales corre- sponding to the binary classifications for 1800 doc- uments, leaving the remaining 200 for testing. Be- cause 200 documents is a modest test sample size, we ran 9-fold cross validation on the 1800 annotated documents (each fold comprising 200 documents).</p><p>The rationales, as originally marked in this dataset, were sub-sentential snippets; for the purposes of our model, we considered the entire sentences contain- ing the marked snippets as rationales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines</head><p>We compare against several baselines to assess the advantages of directly incorporating rationale-level supervision into the proposed CNN architecture. We describe these below.</p><p>SVMs. We evaluated a few variants of linear Sup- port Vector Machines (SVMs). These rely on sparse representations of text. We consider variants that ex- ploit uni-and bi-grams; we refer to these as uni-SVM and bi-SVM, respectively. We also re-implemented the rationale augmented SVM (RA-SVM) proposed by <ref type="bibr" target="#b27">Zaidan et al. (2007)</ref>, described in Section 2.</p><p>For the RoB dataset, we also compare to a re- cently proposed multi-task SVM (MT-SVM) model developed specifically for these RoB datasets <ref type="bibr" target="#b14">(Marshall et al., 2015;</ref><ref type="bibr" target="#b15">Marshall et al., 2016)</ref>. This model exploits the intuition that the risks of bias across the domains codified in the aforementioned Cochrane RoB tool will likely be correlated. That is, if we know that a study exhibits a high risk of bias for one domain, then it seems reasonable to assume it is at an elevated risk for the remaining domains. Further- more, <ref type="bibr" target="#b15">Marshall et al. (2016)</ref> include rationale-level supervision by first training a (multi-task) sentence- level model to identify sentences likely to support RoB assessments in the respective domains. Special features extracted from these predicted rationales are then activated in the document-level model, inform- ing the final classification. This model is the state- of-the-art on this task. CNNs. We compare against several baseline CNN variants to demonstrate the advantages of our ap- proach. We emphasize that our focus in this work is not to explore how to induce generally 'better' document vector representations -this question has been addressed at length elsewhere, e.g., <ref type="bibr" target="#b12">(Le and Mikolov, 2014;</ref><ref type="bibr" target="#b8">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b23">Tang et al., 2015;</ref><ref type="bibr" target="#b24">Yang et al., 2016)</ref>.</p><p>Rather, the main contribution here is an augmen- tation of CNNs for text classification to capitalize on rationale-level supervision, thus improving perfor- mance and enhancing interpretability. This informed our choice of baseline CNN variants: standard CNN <ref type="bibr" target="#b9">(Kim, 2014)</ref>, Doc-CNN (described above) and AT- CNN (also described above) that capitalizes on an (unsupervised) attention mechanism at the sentence level, described in Section 4.3. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation/Hyper-Parameter Details</head><p>Sentence splitting. To split the documents from all datasets into sentences for consumption by our Doc- CNN and RA-CNN models, we used the Natural Language Toolkit (NLTK) 7 sentence splitter. SVM-based models. We kept the 50,000 most frequently occurring features in each dataset. For estimation we used SGD. We tuned the C hyper- parameter using nested development sets. For the RA-SVM, we additionally tuned the µ and C contrast parameters, as per <ref type="bibr" target="#b27">Zaidan et al. (2007)</ref>. CNN-based models. For all models and datasets we initialized word embeddings to pre-trained vec- tors fit via Word2Vec. For the movie reviews dataset these were 300-dimensional and trained on Google News. 8 For the RoB datasets, these were 200-dimensional and trained on biomedical texts in PubMed/PubMed Central ( <ref type="bibr" target="#b19">Pyysalo et al., 2013</ref>). 9</p><p>Training proceeded as follows. We first extracted all sentences from all documents in the training data. The distribution of sentence types is highly imbalanced (nearly all are neutral). Therefore, we downsampled sentences before each epoch, so that sentence classes were equally represented. After training on sentence-level supervision, we moved to document-level model fitting. For this we initialized embedding and convolution layer parameters to the estimates from the preceding sentence-level training step (though these were further tuned to optimize the document-level objective).</p><p>For RA-CNN, we tuned the dropout rate (range: 0-.9) applied at the sentence vector level on each training fold (using a subset of the training data as a validation set) during the document level training phase. Anecdotally, we found this has a greater ef- fect than the other model hyperparameters, which we thus set after a small informal process of exper- imentation on a subset of the data. Specifically, we fixed the dropout rate at the document level to 0.5, and we used 3 different filter heights: 3, 4 and 5, following <ref type="bibr" target="#b29">(Zhang and Wallace, 2015)</ref>. For each fil- ter height, we used 100 feature maps for the baseline CNN, and 20 for all the other CNN variants.</p><p>For parameter estimation we used ADADELTA <ref type="bibr" target="#b28">(Zeiler, 2012)</ref>, mini-batches of size 50, and an early stopping strategy (using a validation set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Quantitative Results</head><p>For all CNN models, we replicated experiments 5 times, where each replication constituted 5-fold and 9-fold CV respectively the RoB and the movies datasets, respectively. We report the mean and ob- served ranges in accuracy across these 5 replications for these models, because attributes of the model (notably, dropout) and the estimation procedure ren- der model fitting stochastic ( <ref type="bibr" target="#b29">Zhang and Wallace, 2015</ref>). We do not report ranges for SVM-based models because the variance inherent in the estima- tion procedure is much lower for these simpler, lin- ear models.</p><p>Results on the RoB datasets and the movies dataset are shown in <ref type="table">Tables 2 and Table 3</ref>  <ref type="table">Table 2</ref>: Accuracies on the four RoB datasets. Uni-SVM: unigram SVM, Bi-SVM: Bigram SVM, RA-SVM: Rationale-augmented SVM ( <ref type="bibr" target="#b27">Zaidan et al., 2007)</ref>, MT-SVM: a multi-task SVM model specifically designed for the RoB task, which also exploits the available sentence supervision ( <ref type="bibr" target="#b15">Marshall et al., 2016</ref>). We also report an estimate of human-level performance, as calculated using subsets of the data for each domain that were assessed by two experts (one was arbitrarily assumed to be correct). We report these numbers for reference; they are not directly comparable to the cross-fold estimates reported for the models.</p><p>observe that CNN/Doc-CNN do not necessarily im- prove over the results achieved by SVM-based mod- els, which prove to be strong baselines for longer document classification. This differs from previ- ous comparisons in the context of classifying shorter texts. In particular, in previous work (Zhang and Wallace, 2015) we observed that CNN outperforms SVM uniformly on sentence classification tasks (the average sentence-length in these datasets was about 10). In contrast, in the datasets we consider in the present paper, documents often comprise hundreds of sentences, each in turn containing multiple words. We believe that it is in these cases that explicitly modeling which sentences are most important will result in the greatest performance gains, and this aligns with our empirical results.</p><p>Another observation is that AT-CNN does of- ten improve performance over vanilla variants of CNN (i.e., without attention), especially on the RoB datasets, probably because these comprise longer documents. However, as one might expect, RA- CNN clearly outperforms AT-CNN by exploiting rationale-level supervision directly. And by exploit- ing rationale information directly, RA-CNN is able to consistently perform better than baseline CNN and SVM model variants. Indeed, we find that RA- CNN outperformed MT-SVM on all of the RoB datasets, and this was accomplished without exploit- ing cross-domain correlations (i.e., without multi- task learning).  <ref type="table">Table 3</ref>: Accuracies on the movie review dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Qualitative Results: Illustrative Rationales</head><p>In addition to realizing superior classification perfor- mance, RA-CNN also provides explainable catego- rizations. The model can provide the highest scoring rationales (ranked by max{p pos , p neg }) for any given target instance, which in turn -by construction -are those that most influenced the final document classi- fication.</p><p>For example, a sample positive rationale support- ing a correct designation of a study as being at low risk of bias with respect to blinding of outcomes assessment reads simply The study was performed double blind. An example rationale extracted for a study (correctly) deemed at high risk of bias, mean- while, reads as the present study is retrospective, there is a risk that the woman did not properly re- call how and what they experienced ....</p><p>Turning to the movie reviews dataset, an exam- ple rationale extracted from a glowing review of 'Goodfellas' (correctly classified as positive) reads this cinematic gem deserves its rightful place among the best films of 1990s. While a rationale extracted from an unfavorable review of 'The English Patient' asserts that the only redeeming qualities about this film are the fine acting of Fiennes and Dafoe and the beautiful desert cinematography.</p><p>In each of these cases, the extracted rationales directly support the respective classifications. This provides direct, meaningful insight into the auto- mated classifications, an important benefit for neural models, which are often seen as opaque.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We developed a new model (RA-CNN) for text clas- sification that extends the CNN architecture to di- rectly exploit rationales when available. We showed that this model outperforms several strong, rele- vant baselines across five datasets, including vanilla and hierarchical CNN variants, and a CNN model equipped with an attention mechanism. Moreover, RA-CNN automatically provides explanations for classifications made at test time, thus providing in- terpretability.</p><p>Moving forward, we plan to explore additional mechanisms for exploiting supervision at lower lev- els in neural architectures. Furthermore, we believe an alternative approach may be a hybrid of the AT- CNN and RA-CNN models, wherein an auxiliary loss might be incurred when the attention mecha- nism output disagrees with the available direct su- pervision on sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A toy example of a CNN for sentence classification. Here there are four filters, two with heights 2 and two with heights 3, resulting in feature maps with lengths 6 and 5 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The film, however, is all good.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A schematic of our proposed Rationale-Augmented Convolution Neural Network (RA-CNN). The sentences comprising a text are passed through a sentence model that outputs probabilities encoding the likelihood that sentences are neutral or a (positive or negative) rationale. Sentences likely to be rationales are given higher weights in the global document vector, which is the input to the document model.</figDesc><graphic url="image-8.png" coords="4,105.07,261.94,85.21,97.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>p( ) sentence vectors</head><label></label><figDesc></figDesc><table>Films adapted from 
comic books... 

... 
The film, however, 
is all good. 

Now onto from hell's 
appearance: it's... 
... 

... 
... 

sentence model 

Σ 

... 
... 

y i 

doc 

y i0 

</table></figure>

			<note place="foot" n="1"> https://github.com/yezhang-xiaofan/Rationale-CNN 2 https://github.com/bwallace/rationale-CNN</note>

			<note place="foot" n="3"> We also experimented with taking the average of sentence vectors, but summing performed better in informal testing.</note>

			<note place="foot" n="4"> All of the document classification tasks we consider here are binary, although extension of our model to multi-class scenarios is straight-forward.</note>

			<note place="foot" n="6"> We also experimented briefly with LSTM and GRU (Gated Recurrent Unit) models, but found that simple CNN performed better than these. Moreover, CNNs are relatively robust and less sensitive to hyper-parameter selection. 7 http://www.nltk.org/api/nltk.tokenize.html 8 https://code.google.com/archive/p/word2vec/ 9 http://bio.nlplab.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Research reported in this article was supported by the National Library of Medicine (NLM) of the Na-tional Institutes of Health (NIH) under award num-ber R01LM012086. The content is solely the re-sponsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. This work was also made possible by the support of the Texas Advanced Computer Center (TACC) at UT Austin.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from labeled features using generalized expectation criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00726</idno>
		<title level="m">A primer on neural network models for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cochrane collaborations tool for assessing risk of bias in randomised trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas G Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gøtzsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jüni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Andrew D Oxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savovi´csavovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan Ac</forename><surname>Weeks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sterne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bmj</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page">5928</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional neural networks for text categorization via region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPs)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="919" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic text categorization using the importance of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized expectation criteria for semi-supervised learning with weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gideon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="955" to="984" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automating risk of bias assessment for clinical trials. Biomedical and Health Informatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Kuiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1406" to="1412" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robotreviewer: evaluation of a system for automatically assessing bias in clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Kuiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why Is That Relevant? Collecting Annotator Rationales for Relevance Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Elsayad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mucahid</forename><surname>Kutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP). 10 pages</title>
		<meeting>the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP). 10 pages</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Japanese probabilistic information retrieval using location and category information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiromi</forename><surname>Ozaku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth international workshop on on Information retrieval with Asian languages</title>
		<meeting>the fifth international workshop on on Information retrieval with Asian languages</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Languages in Biology and Medicine</title>
		<meeting>Languages in Biology and Medicine</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1467" to="1478" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The constrained weight space svm: learning with ranked features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Trikalinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatically generating annotator rationales to improve sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling annotators: A generative approach to learning from annotator rationales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using&quot; annotator rationales&quot; to improve machine learning for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1522" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
