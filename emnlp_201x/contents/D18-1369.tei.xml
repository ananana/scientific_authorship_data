<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeon</forename><surname>Seonwoo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3316" to="3325"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3316</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In news and discussions, many articles and posts are provided without their related previous articles or posts. Hence, it is difficult to understand the context from which the articles and posts have occurred. In this paper, we propose the Hierarchical Dirichlet Gaus-sian Marked Hawkes process (HD-GMHP) for reconstructing the narratives and thread structures of news articles and discussion posts. HD-GMHP unifies three modeling strategies in previous research: temporal characteristics , triggering event relations, and meta information of text in news articles and discussion threads. To show the effectiveness of the model, we perform experiments in narrative reconstruction and thread reconstruction with real world datasets: articles from the New York Times and a corpus of Wikipedia conversations. The experimental results show that HD-GMHP outperforms the baselines of LDA, HDP, and HDHP for both tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online news sites and discussion forums generate large volumes of articles and discussions, which we can call "events". To fully understand the dis- cussions and the news stories, one often needs a larger context for that text, such as what related posts and relevant articles have been posted be- fore. For instance, to understand a news article about the presidential elections, we would need to know the history of the candidates' political actions through relevant previous articles. While there are some news articles with a curated set of related articles and discussion threads with a well- organized structure, there are many more articles and discussion threads for which the structure is absent or incomplete. In this context, automat- ically reconstructing the narrative of articles and thread structure is an important problem.</p><p>Generally, textual information and various meta information such as location and keywords are used as features to solve this problem of narra- tive reconstruction. With these features, previous research mainly focus on three modeling strate- gies. First, they model the triggering relationship of events to identify which preceding events led to the occurrence of the current event. Second, they use meta information such as location and keywords. Third, they consider the temporal char- acteristics in the event stream, such that events in close temporal proximity are more likely to be re- lated. However, there is no method that effectively considers all three of these. In narrative recon- struction, there are several approaches that focus on using meta information and temporal character- istics with clustering methods ( <ref type="bibr" target="#b31">Zhou et al., 2016;</ref><ref type="bibr" target="#b23">Tang et al., 2015;</ref><ref type="bibr" target="#b0">Ahmed et al., 2011)</ref>, and there are several approaches using the Hawkes process to model the temporal characteristics ( <ref type="bibr" target="#b11">Du et al., 2015;</ref><ref type="bibr" target="#b20">Mavroforakis et al., 2017;</ref><ref type="bibr" target="#b16">Jankowiak and Gomez-Rodriguez, 2017)</ref>. In thread reconstruc- tion, there are approaches that focus on modeling triggering relationships of events and using meta information ( <ref type="bibr" target="#b17">Kim et al., 2010;</ref><ref type="bibr" target="#b18">Louis and Cohen, 2015;</ref><ref type="bibr" target="#b27">Wang et al., 2011b)</ref>.</p><p>In this paper, we propose a novel Gaussian Marked Hawkes Process (GMHP) that effectively reconstructs the narrative structure of articles and the thread structure of discussions considering all three modeling strategies. GMHP uses the Hawkes process to model events in continuous time, a Gaussian distribution for modeling the meta information of text, and the mixture of Gaus- sian for modeling the triggering relationships of events. The detailed modeling strategies are de- scribed as follows. We use the Hawkes process to model time in the continuous domain, as the Hawkes process is a stochastic process used to un- derstand a sequence of events in continuous time <ref type="bibr" target="#b15">(Iwata et al., 2013;</ref><ref type="bibr" target="#b22">Rong et al., 2015)</ref>. To use meta information, we represent text and meta informa- tion in a general vector form and use the Hawkes process to handle the vector of event information with a Gaussian distribution. To model the trig- gering relationships, we assume a model structure parameterized by each preceding event so that an event can be directly generated from a probability distribution parameterized by preceding events.</p><p>The GMHP models a single narrative or thread in event streams. To find the narratives or threads from a mixture of event streams, we combine our GMHP model with the Hierarchical Dirichlet Pro- cess to build HD-GMHP.</p><p>We evaluate the effectiveness of our model with two real world datasets: articles from the New York Times, and discussion threads from Wikipedia. In the New York Times dataset, we perform a narrative reconstruction experiment and compare the results with the human annotated nar- rative labels. In the Wikipedia discussion cor- pus, we perform two kinds of thread reconstruc- tion experiment. One is grouping posts in the same thread. The other is reconstructing the post-reply structure of the posts. From these experiments, we see that our model outperforms the state-of-the-art model, the hierarchical Dirichlet Hawkes process (HDHP) ( <ref type="bibr" target="#b20">Mavroforakis et al., 2017)</ref>.</p><p>The contributions of our research are threefold. First, we propose the Gaussian Marked Hawkes Process that effectively models a single narrative (event stream) with all three modeling strategies used in previous research. Second, we propose HD-GMHP, a combination of the GMHP model with the HDP to reconstruct the narratives of ar- ticles and the thread structure of discussions from a mixture of event streams. Finally, we propose a novel inference algorithm of the HD-GMHP with the Sequential Monte Carlo method ( <ref type="bibr" target="#b9">Doucet et al., 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Narrative Reconstruction: One major approach to reconstructing narratives from news articles is clustering articles by using a variant of the Chi- nese Restaurant Process (CRP). Related work such as ( <ref type="bibr" target="#b31">Zhou et al., 2016;</ref><ref type="bibr" target="#b23">Tang et al., 2015;</ref><ref type="bibr" target="#b0">Ahmed et al., 2011</ref>) models chronologically ordered news articles with text and various meta information in- cluding author, organization, keywords, and loca- tion. They use the CRP, distant-dependent CRP <ref type="bibr" target="#b4">(Blei and Frazier, 2011)</ref>. There is research that uses recurrent CRP <ref type="bibr" target="#b1">(Ahmed and Xing, 2008)</ref> and exponential time decaying kernel to model prob- ability of time difference between two relevant events. But they use discrete time information in- stead of continuous form and handcrafted param- eters of the kernel <ref type="bibr" target="#b0">(Ahmed et al., 2011</ref>).</p><p>There is another approach that reconstructs nar- ratives by directly extracting important sentences from articles. ( <ref type="bibr" target="#b30">Xu et al., 2013)</ref> proposes a model that considers the sentence and image level narra- tive reconstruction as an optimization problem and solves it by maximizing the divergence of narra- tives with some constraints. ( <ref type="bibr" target="#b28">Wang et al., 2016)</ref> solves the narrative reconstruction problem as a sentence recommendation problem and uses ma- trix factorization. But these existing models focus on how to handle text and meta information of ar- ticles, while our model uses the Hawkes process to effectively model continuous time information of events. Discussion Thread Reconstruction: There are several approaches to reconstruct threads from a corpus of unstructured discussions. ( <ref type="bibr" target="#b26">Wang et al., 2011a</ref>) uses Conditional Random Field to recon- struct reply structure in discussion corpus. ( <ref type="bibr" target="#b3">Balali et al., 2014</ref>) uses content, time and author infor- mation as features of a single post with rank SVM to reconstruct thread structure. ( <ref type="bibr" target="#b8">Dehghani et al., 2013;</ref><ref type="bibr" target="#b2">Aumayr et al., 2011</ref>) uses SVM and a deci- sion tree with meta information of posts.</p><p>However, a major limitation in these previous research is that they are assuming that for each post, the main thread where it belongs is given. That is, the problem they solve is finding the post for which a post is immediately replying, rather than treating the corpus as a single set of posts with no known information about the threads, the initial post of each thread, and the posts that be- long to each thread. This limitation of the previous research means those approaches are not applica- ble in more general online conversation data, such as IRC or a Facebook group chat which is a mas- sive unstructured online discussion for which the initial post of a thread is not labeled. Unlike this strong assumption in previous research, we use a more general assumption that the initial posts are unknown, so our approach would be applicable to a wider, more general discussion data. Also, as in the narrative reconstruction research area, pre- vious research focuses on how to handle text and meta information in posts. Again, unlike previous research, our research uses the Hawkes process to model continuous time information. Continuous Time Modeling: The Hawkes pro- cess, a stochastic process that models continuous time information of events with event occurrence history, is an effective solution to model events in continuous time. One of the main research themes in the Hawkes process literature is find- ing which events trigger which other events. ( <ref type="bibr" target="#b13">He et al., 2015</ref>) models the topic diffusion patterns in a social network by inferring the triggering node with the Hawkes process. The Hawkes process is also used to model social event streams ( <ref type="bibr" target="#b22">Rong et al., 2015)</ref> and to classify rumors ( <ref type="bibr" target="#b19">Lukasik et al., 2016)</ref>, and a combination of the Hawkes process and the Dirichlet mixture model is used to cluster event streams ( <ref type="bibr" target="#b29">Xu and Zha, 2017)</ref>.</p><p>Recent research clusters text streams with the Hawkes process and the Chinese Restaurant Pro- cess or the Chinese Restaurant Franchise <ref type="bibr" target="#b20">(Mavroforakis et al., 2017;</ref><ref type="bibr" target="#b11">Du et al., 2015)</ref>. They use the bag-of-words representation of text in their model, while <ref type="bibr" target="#b16">(Jankowiak and Gomez-Rodriguez, 2017)</ref> proposes a Hawkes process model that can han- dle a more general vector representation of events. The main difference of our model compared to this research is that we add the triggering relationship of two events. With this addition, our model can reconstruct narratives with an explicit relation of two documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hawkes Processes</head><p>Before we describe our proposed model, we briefly explain the Hawkes process, one of two main stochastic processes used in our model. We leave out the explanation of the HDP due to space.</p><p>The Hawkes process <ref type="bibr" target="#b12">(Hawkes, 1971</ref>) is a sub- class of temporal point processes, whose func- tional form for intensity with exponential decay- ing kernel is represented as</p><formula xml:id="formula_0">λ * (t) = λ 0 (t) + t 0 αβe −β(t−s) dN (s),</formula><p>where the intensity, λ * (t) represents the condi- tional probability of an event occurrence within time window [t, t + dt). The Hawkes process is used to model the number of occurrences of events where one event can trigger other events. In the equation above, the base intensity λ 0 (t) models the intensity of events that occur on their own ini- tiative whereas αβe −β(t−s) models the intensity of events that are triggered by the previous event that occurred at time s. Here, multiplication of α and β represents influence of the previous event and β represents decaying rate of the influence. Thus, the effect of the previous event exponentially de- cays with respect to the time difference. From the definition of intensity λ * (t), the derived likelihood form of the Hawkes process is as follows,</p><formula xml:id="formula_1">f (D|Θ) = e −Λ(T ) n i=1 λ * (t i ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">Λ(T ) = T 0 λ * (t)dt.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem Setting</head><p>In this section, we define the event stream and the narrative and the thread reconstruction problems. Definition of Event Stream: If a text appears at time t i , we define the event s i as</p><formula xml:id="formula_3">(t i , e i , z i , x i ).</formula><p>Here, e i is the feature vector of the text, x i is the latent global cluster indicator of event s i which represents the cluster for events with similar text information, and z i is the latent local cluster in- dicator for events that are temporally related in the same cluster. We define event stream S as [s 1 , .., s n ].</p><p>Assumptions: 1) We assume that two events in same local cluster occur in near time and have sim- ilar feature vectors e. These properties are called temporal and spatial locality. 2) We assume hierar- chy structure of a global cluster and a local cluster. That is, one global cluster can consist of multiple local clusters.</p><p>Problem Formulation: We formulate the spa- tial locality of two events in the same local cluster with a Gaussian distribution. If two events s i and s j are in the same local cluster and t i &gt; t j , then we assume the later event e i is generated from one of two relations,</p><formula xml:id="formula_4">e i ∼ N ( e j , Σ v ), e i , ∼ N ( e 0 , Σ 0 ).</formula><p>Here, e 0 is the base event vector and Σ 0 is the co- variance matrix of the cluster. Σ v is the covariance matrix of the Gaussian distribution generated by a past event in the cluster. We use the Hawkes process to formulate the temporal locality of two events in the same local cluster. If event s i and s j are in the same local cluster and t i &gt; t j , then t i is generated from in either following relations,</p><formula xml:id="formula_5">t i ∼ Poisson Process(µ),</formula><p>Here, if t i is generated from Hawkes process of parameter α and β with time t j and e i is generated from e j , then we say that event s j is the parent event of event s i . We formulate the hierarchy structure of the global cluster and the local cluster with Hierarchi- cal Dirichlet Process ( <ref type="bibr" target="#b24">Teh et al., 2006</ref>). If the pa- rameters θ z i of local clusters z 1 , z 2 , .., z n are equal to the parameters of the global cluster Θ x , then we say that there is a hierarchy between all the local clusters and the global cluster. And this hierarchy structure can be written as follows,</p><formula xml:id="formula_6">Θ x = θ z 1 = θ z 2 = ... = θ zn .</formula><p>Now, we define the narrative reconstruction and the thread reconstruction problem as a problem of inferring the latent variables in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model</head><p>We now describe clustering a mixture of event stream S with the Gaussian Marked Hawkes Pro- cess and the hierarchical Dirichlet process. We first propose Gaussian Marked Hawkes Process (GMHP) that models temporal and spatial local- ity assumptions that described in section 4. And after defining the GMHP, we propose Hierarchical Dirichlet Gaussian Marked Hawkes Process (HD- GMHP), a combination of the GMHP with the Hi- erarchical Dirichlet Process. The GMHP models event streams with the same local cluster z and HDP groups the local clusters to one global cluster x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Gaussian Marked Hawkes Processes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Model Description</head><p>In GMHP, we assume events are generated by a past event or by their own initiative. If event s i is generated by event s j , then we say that event s j is the parent event of event s i . If event s i occurs on their own initiative, the index of the parent event is 0. We define the intensity function with the given parent event c i as follows,</p><formula xml:id="formula_7">λ(t i |c i ) = µ if c i = 0 αβe −β(t i −tc i ) otherwise (2)</formula><p>To model the spatial locality of two D- dimensional event vectors e i , e c i , we define prob- ability distribution for e i as follows,</p><formula xml:id="formula_8">p c i ( e i ) = N ( e i | e 0 , Σ 0 ) if c i = 0 N ( e i | e c i , Σ v ) otherwise<label>(3)</label></formula><p>Here, e 0 is the base event vector for when c i = 0. Σ 0 and Σ v are covariance matrix for when an event occurs by their own initiative or occurs by past event. From the above definitions, we can calcu- late the intensity of the event vector e at time t as follows,</p><formula xml:id="formula_9">λ e (t) = µN ( e| e 0 , Σ 0 ) + t j &lt;t λ(t|j)N ( e| e j , Σ v ).<label>(4)</label></formula><p>The total intensity of GMHP can be obtained by integrating the above intensity with the event vec- tor e.</p><formula xml:id="formula_10">λ(t) = R D λ e (t) d e = µ + t j &lt;t λ(t|j). (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Parameter estimation</head><p>From equation 1, the likelihood of the observed event stream can be computed as follows,</p><formula xml:id="formula_11">f (D|θ) = e −Λ(T ) n i=1 0≤j&lt;i p j ( e i )λ(t i |j), (6)</formula><p>where</p><formula xml:id="formula_12">Λ(T ) = µT + n i=1 α(1 − e −β(T −t i ) ).</formula><p>Since the likelihood of GMHP is hard to maxi- mize, instead of using the likelihood, we define a likelihood with the given parent events as follows,</p><formula xml:id="formula_13">f (D|C, θ) =e −Λ(T ) × n i=1 {(µN ( e i | e 0 , Σ 0 )) C i0 × i−1 j=1 (αβe β(t i −t j ) N ( e i | e j , Σ v )) C ij },<label>(7)</label></formula><p>where C ij becomes 1 when c i = j and 0 other- wise. By maximizing equation 7, we can estimate the parameter θ = {µ, α, e 0 , Σ 0 , Σ v }. The infer- ence step of the parent events is described in sec- tion 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modeling a Mixture of GMHP with the HDP</head><p>When clustering a mixture of streams using the Hawkes process, the exponential triggering func- tion prevents two events with a large time differ- ence from being assigned to the same global clus- ter. To solve this problem, ( <ref type="bibr" target="#b20">Mavroforakis et al., 2017</ref>) uses the HDP instead of using the Dirichlet process used in ( <ref type="bibr" target="#b11">Du et al., 2015</ref>). The hierarchy structure of the HDP assigns a cluster label with a probability proportional to the size of the clus- ter. This allows assignment of two events with a large time difference to the same cluster. For the same reason, we use the HDP to model mixture of the GMHP. We consider each GMHP in mix- ture as a table in the Chinese Restaurant Franchise metaphor. Since the intensity of k'th GMHP, λ k (t) represents how likely an event occurs in table k at time t, we use the intensity as the number of cus- tomers in the CRF metaphor. The whole genera- tive process of HD-GMHP is as follows.</p><p>1. Initialize the number of local clusters K = 0, the number of global clusters M = 0. 2. For n ∈ 1, 2, ..., N (a) Draw t n from Hawkes(</p><formula xml:id="formula_14">λ 0 + K k=0 λ k ) (b) Draw z n as follows. z n ∼ λ 0 δ(K + 1) + K k=1 λ k δ(k) (8) (c) If z n = K + 1, assign global cluster x n ,</formula><p>which is interpreted as parameter(θ xn ) for local cluster z n , and Increment K. Here, N m is number of local cluster in global cluster m.</p><formula xml:id="formula_15">x n ∼ γδ(M + 1) + M m=1 N m δ(m) (9) (d) If x n = M + 1, increment M and draw new parameter as follows. α xn ∼ Γ(α a , β a ), µ xn ∼ Γ(α µ , β µ ) 1 Σ xn 0 ∼ Γ(α 0 , β 0 ), 1 Σ xn v ∼ Γ(α v , β v ) e 0,xn ∼ N ( e 0 , Σ xn 0 / λ e 0 )</formula><p>Note that we assume that the covariance matrix Σ xn v , and Σ xn 0 are diagonal. (e) Draw c n and e n . Here, g xn (t) = α xn βe −β(tn−t) .</p><formula xml:id="formula_16">c n ∼ µ xn δ(N zn + 1) + Nz n j=1 g xn (t j )δ(j)<label>(10)</label></formula><p>if c n = N zn + 1, then replace c n with 0 and sample event vector.</p><formula xml:id="formula_17">e n ∼ N ( e 0,xn , Σ xn 0 ) if c n = 0 N ( e cn , Σ xn v ) otherwise (11) λ 0 , γ, e 0 , λ e 0 , (α a , β a ), (α µ , β µ ), (α 0 , β 0 )</formula><p>, and (α v , β v ) are the hyperparameters used in HD- GMHP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Inference</head><p>To infer the latent variables z and x for each event from an observed event stream s 1:n o where s i o = (t i , e i ) with observation time T , we propose an</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Inference</head><p>Input: Stream data S o Initialize w i 1 = 1 P , i ∈ {1, 2, ..., P }. for n = 1 to N do for i = 1 to P do Update Θ as described in section 6.2. Sample (x, z, c) i n with equation 16, 10 Update w i n with equation 17 end for Normalize w 1:P n if ||w n || −2 2 &lt; thresh then Resample particles end if end for online inference algorithm with Sequential Monte Carlo (SMC) ( <ref type="bibr" target="#b9">Doucet et al., 2001</ref>). To calculate the posterior of the latent variables z and x for each timestamp t i in the inference, we need the es- timated parameter to calculate the intensity at each time t i , λ(t i ). As described in section 5.1.2, the parameter estimation step needs the parent event information. In our proposed inference, the par- ent events are inferred from SMC. The inference algorithm is summarized in algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sequential Monte Carlo with parent event inference</head><p>To approximate the posterior of the latent vari- ables, SMC samples the latent variables from the proposal distribution and calculates the weight of each sampled variables which is called the parti- cle weight. To infer the parent event in SMC, we define the particle weight of our modified SMC as follows:</p><formula xml:id="formula_18">w i n = p(ψ i 1:n |s 1:n o ) q(ψ i 1:n |s 1:n o )</formula><p>p(c i 1:n |ψ i 1:n , s 1:n o ) q(c i 1:n |ψ i 1:n , s 1:n o )</p><p>Here, ψ i n is (x i n , z i n ). .</p><p>and η c is p( e n |t n , s 1:n−1</p><formula xml:id="formula_21">o , δ i 1:n ) p(c i n |t n , δ i 1:n−1 , s 1:n−1 o , ψ i n ) q(c i n |δ i 1:n−1 , s 1:n o , ψ i n ) . (14) Here, δ i n is (x i n , z i n , c i n ).</formula><p>We use p(ψ n |ψ 1:n−1 , s 1:n o ) as the proposal dis- tribution of ψ i n in the equation 13 to minimize the variance of w i n ( <ref type="bibr" target="#b10">Doucet et al., 2000</ref>) and p(c n |δ 1:n−1 , ψ n , t n , s </p><p>From the proposal distribution of ψ i n , we can sample ψ i n as follows: p(ψ n |rest) ∝p( e n |ψ 1:n , t n , s 1:n−1</p><formula xml:id="formula_23">o ) × p(ψ n |ψ 1:n−1 , t n , s 1:n−1 o ) × p(t n |ψ 1:n−1 , s 1:n−1 o ) (16)</formula><p>Here, the term p( e n |ψ 1:n , t n , s 1:n−1 o ) × p(ψ n |ψ 1:n−1 , t n , s 1:n o ) can be simply calcu- lated by the student's t-distribution derived from the conjugate relation between the parameter { e k0 , Σ 0,k , Σ v,k } and the normal-inverse-gamma and inverse-gamma prior in the generative process of HD-GMHP.</p><p>From η c i n = p( e n |t n , s 1:n−1 o , c i 1:n , ψ 1:n ) and 15, the particle weight can be updated by the follow- ing, w i n ∝w i n−1 × p(t n |s 1:n−1 o , ψ i 1:n )p( e n |c n , t n , s 1:n−1</p><formula xml:id="formula_24">o , ψ 1:n ) × zn (p( e n |z n , ψ 1:n−1 , t n , s 1:n−1 o ) × p(z n |t n , ψ 1:n−1 , s 1:n−1 o )).<label>(17)</label></formula><p>When calculating the probability of t n in 17, we assume the parameters µ 1:K , α 1:K are given <ref type="bibr" target="#b6">(Carvalho et al., 2010)</ref>. From the likelihood of GMHP, the probability term p(t n |ψ 1:n , s 1:n−1 o ) in equa- tion 17 can be calculated by λ zn (t n )e −Λ(tn,t n−1 ) . Where Λ(t n , t n − 1) is λ 0 (t n − t n−1 ) + (t n − t n−1 )</p><formula xml:id="formula_25">K k=1 µ k + 1 β (1 − e −β(tn−t n−1 ) ) K k=1 λ k (t n−1 ).<label>(18)</label></formula><p>In the case of the probability term p( e n |c n , rest) and p( e n |z n , rest) in 17, as explained in the sam- pling process of ψ n , we can calculate the terms by student's t-distribution. With the particle weight update rule 17 and the parameter update rule de- scribed in section 6.2, we infer latent variables with algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Updating Parameter</head><p>From the equation 7 and the prior of the parame- ters used in GMHP, we can estimate the parame- ters by following form.</p><formula xml:id="formula_26">α m = α a − 1 + x i =m 0&lt;j&lt;i C ij β a + x i =m (1 − e −β(T −t i ) )<label>(19)</label></formula><formula xml:id="formula_27">µ m = α µ − 1 + x i =m C i0 β µ + θ k =Θm (T − t 0,k )<label>(20)</label></formula><formula xml:id="formula_28">e 0,m = e 0 • λ e 0 + x i =m C i0 e i λ e 0 + x i =m C i0 (21) diag(Σ m 0 ) ={ λ e 0 • ( e 0,m − e 0 ) 2 + 2 β 0 + x i =m C i0 ( e i − e 0,m ) 2 } × {2 α 0 + 3 + x i =m C i0 } −1 (22) diag(Σ m v ) = 2 β v + x i =m 0&lt;j&lt;i C ij ( e i − e j ) 2 2 + 2 α v + x i =m 0&lt;j&lt;i C ij<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Approximation</head><p>To reduce the computation time in the inference al- gorithm, we use several approximation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Marginal distribution Approximation</head><p>To calculate p( e n |z n , ψ 1:n−1 , t n , s o 1:n−1 ) in the equation 17, we need marginalization of p( e n , c n |z n , ψ 1:n−1 , t n , s 1:n−1 o )</p><p>which takes time complexity of O(n of events in z n ) and cause the time complexity of the equation 17 to be O(n).</p><p>To reduce the time complex- ity, we note that event vector e n is sampled from a Gaussian mixture that the influence of each Gaussian distribution is exponentially decreases. We assume the marginal distribution p( e n |z n , ψ 1:n−1 , t n , s 1:n−1 o ) can be approximated to p( e n |c 1:n = 0, z n , ψ 1:n−1 , t n , s 1:n−1 o ). From the approximation, we can calculate the posterior predictive with student's t-distribution. The result of approximation is as follows, p( e n |z n , ψ 1:n−1 , t n , s 1:n−1</p><formula xml:id="formula_29">o ) = t νn ( e n | m n , κ n + 1 κ n ν n S n ),<label>(24)</label></formula><p>where</p><formula xml:id="formula_30">ν n = 2α 0 + N zn , κ n = λ e 0 + N zn , m n = λ e 0 • e 0 + z i =zn e i κ n , S n = 2β 0 + z i =zn e 2 i + λ e 0 • e 2 0 − κ n m 2 n .</formula><p>To calculate p( e n |c n , t n , s 1:n−1 o , ψ 1:n ) in the equation 17, we need to calculate posterior pre- dictive for each past event.</p><p>To reduce the computation time in the process of calcula- tion, we approximate the probability distribution p( e n |c n , t n , s 1:n−1 o , ψ 1:n ) as follows.</p><p>p( e n |c n , t n , s 1:n−1</p><formula xml:id="formula_31">o , ψ 1:n ) ≈ N ( e 0,xn , Σ xn 0 ) if c n = 0 N ( e cn , Σ xn v ) otherwise<label>(25)</label></formula><p>6.3.2 Sampling c n from recent W events Sampling c n has time complexity of O(N zn ). To reduce the time complexity to O(1), we sample c n from recent W events in the local cluster z n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiment</head><p>In this section, we demonstrate the narrative reconstruction and thread reconstruction perfor- mance of our model on a corpus of the New York Times articles and the Wikipedia conversa- tion dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Dataset</head><p>New York Times Dataset: We collected 112,538 New York Times news articles from January 2016 to July 2017. The dataset contains the text, times- tamp, the news section, and the keywords. These keywords are semantic tags specified by the news- room to indicate the main topics of the articles. We select news articles in sections "U.S.", "World", "Opinion", and "Sports" that contain at least one  <ref type="bibr">, 2012</ref>). The dataset contains the timestamp, the initial post of the conversation, "reply to" link information, and the text information of each post in conversation threads in Wikipedia talk pages. We select threads that have ten or more posts from September 2010 to December 2010. The final number of posts used in our experiment is 2,004 and the final number of threads is 154.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Preprocessing</head><p>To apply our model to the real world datasets, we represent each event with time information and an event vector. For the time information, we take the first article or post and set the time as zero, the last article or post as one, and scale the timestamps of all other articles and posts accordingly. To extract the event vectors, we use different vectorization methods for the two datasets. For the NYT dataset, we use the document topic vector from LDA ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>). For the Wikipedia dataset, because there are only a few words in each post, we cannot use the LDA topic vector, so we use the averaged word embedding vector ( <ref type="bibr" target="#b21">Mikolov et al., 2013)</ref> of the words used in each post. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Task</head><p>Narrative reconstruction: To demonstrate the narrative reconstruction performance of our model, we apply the inference method to our cor- pus of NYT articles. We use a set of multiple keywords of each article as the ground truth la- bel. Then we run our model and consider the set of articles with the same global cluster information as one narrative. We compare the results with the ground truth labels using the common clustering metrics AMI and ARI <ref type="bibr" target="#b14">(Hubert and Arabie, 1985;</ref><ref type="bibr" target="#b25">Vinh et al., 2010)</ref> to evaluate the narrative recon- struction performance of our model. We compare HD-GMHP with the following baselines: LDA and HDP with DBSCAN, and the Hierarchical Dirichlet Hawkes Process (HDHP) ( <ref type="bibr" target="#b20">Mavroforakis et al., 2017)</ref> which is a state-of-the-art model for text and continuous timestamps of an event. Also, to measure the similarity of each recovered narra- tive and the ground truth narrative, we use the F1 score of the top ten narratives. Thread reconstruction: In this experiment, we use two evaluation criteria. One is post grouping and the other is reply structure recovery, which is simply the recovery of the child nodes. Here, we use a different child node recovery task com- pared to the child node recovery used in previous research. In our task, we do not give the initial post of each thread, while previous research does. This makes thread reconstruction problem more general and more difficult.</p><p>In post grouping, we use the initial post of each of the posts as the ground truth label and measure the clustering metrics used in the NYT dataset. In the child node recovery experiment, we use the parent event information inferred from our method as the recovered tree structure of the threads. We measure the performance with node precision and node recall metrics ( <ref type="bibr" target="#b26">Wang et al., 2011a;</ref><ref type="bibr" target="#b8">Dehghani et al., 2013)</ref>. We compare our model with the fol- lowing baselines: HDHP, and a naive baseline that reconstructs threads in the form of a single linked list of posts in chronological order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Metrics</head><p>AMI, ARI are commonly used to measure cluster- ing performance <ref type="bibr" target="#b14">(Hubert and Arabie, 1985;</ref><ref type="bibr" target="#b25">Vinh et al., 2010)</ref>. P node , R node measure local simi- larity between two thread structures ( <ref type="bibr" target="#b26">Wang et al., 2011a</ref>).</p><formula xml:id="formula_32">P node = 1 N i=1:N |child GT (i) ∩ child E (i)| |child E (i)| R node = 1 N i=1:N |child GT (i) ∩ child E (i)| |child GT (i)|</formula><p>where, child GT (i) and child E (i) are the sets of children of node i in the ground truth thread struc- ture and the recovered thread structure, respec- tively. The author ( <ref type="bibr" target="#b26">Wang et al., 2011a</ref>) also pro- posed P path , R path to measure the similarity of the global structure of two threads. The path metrics are sensitive to the recovered initial post of each thread, but since we do not give the initial post of each thread in our experiment, the path metrics are no longer proper in our experiment. So we mea- sure the node metrics only. <ref type="table" target="#tab_1">Table 2</ref> shows the clustering accuracy of our method and the baseline methods in real world datasets. We average the results with five runs for each model. The highest value for each metric is indicated with boldface. From the results, we establish that our model outperforms the baseline methods in both the NYT narrative reconstruction task and the Wikipedia thread reconstruction task. For the NYT, to see the accuracy of our model in more detail, we compute and show the F-scores for the top ten most frequent labels and the micro and macro averages in table 3. To compute the F-score between the true labels and the recovered cluster labels, we select the cluster with the high- est F-score as the corresponding cluster. From the results, we establish that our model performs bet- ter than the baseline model, HDHP. <ref type="table" target="#tab_3">Table 4</ref> shows the thread reconstruction re- sults of our model and the baseline models in the Wikipedia conversation dataset. Since the HDHP model does not infer the parent event, we recon- struct threads in the form of chronologically or- dered linked list of posts in each local cluster that inferred from HDHP. From the F1 node score of the results, we establish our model performs better than other baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Results</head><p>To demonstrate the robustness of HD-GMHP on dimensional change of the input vector, we mea- sure the performance of each task in using 50, 100, and 150 dimensional vectors. The results are de- scribed in table 5 and 6. From the results, we ver- ify there are no drastic changes in performance in both the NYT dataset and the Wikipedia dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we defined the narrative and thread reconstruction problems as clustering problems. To cluster the event streams with continuous time information and triggering event information, we  proposed the Gaussian Marked Hawkes process that models event streams with additional event in- formation represented in a vector form. Further- more, we combined our model GMHP with the HDP to cluster event streams (HD-GMHP). We showed that our model performs better than sev- eral baseline methods in both narrative reconstruc- tion in a dataset of NYT articles and thread recon- struction in a dataset of Wikipedia conversations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Let the left part on the right hand term and right part on the right hand term of the equation 12 are w ψ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><label></label><figDesc>as the proposal distri- bution of c i n in the equation 14. From the above proposal distribution, η c i n can be calculated as η c i n = p( e n |t n , s 1:n−1 o , c i 1:n , ψ 1:n ) and η ψ i n can be calculated by the following form . η ψ i n =p(t n |ψ 1:n−1 , z n , s 1:n−1 o ) × zn (p( e n |ψ 1:n−1 , z n , t n , s 1:n−1 o ) × p(z n |t n , ψ 1:n−1 , s 1:n−1 o ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Statistics of keywords. "N" column lists the number of articles with the corresponding keyword.</head><label>1</label><figDesc></figDesc><table>Keyword 
N 

Trump, Donald J 
7940 
Presidential Election of 2016 
5737 
United States Politics and Government 4986 
Republican Party 
2371 
Clinton, Hillary Rodham 
2330 
Baseball 
2058 
United States International Relations 
1817 
Terrorism 
1618 
Obama, Barack 
1551 
Russia 
1400 

of the top ten most frequently used keywords. The 
statistics of these keywords are described in table 
1. Further, we select articles with more than ten 
words in its body. The final number of articles 
used in our experiment is 16,858. The dataset is 
publicly available 1 . 
Wikipedia Conversation Dataset is released by 
(Danescu-Niculescu-Mizil et al.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Narrative reconstruction results in NYT dataset and post grouping results in Wikipedia conver- sation dataset.</head><label>2</label><figDesc></figDesc><table>AMI 
ARI 

LDA + DBSCAN 
0.0627 0.0117 
HDP + DBSCAN 
0.0260 0.0203 
HDHP 
0.1768 0.0746 
NYT 

HD-GMHP (100D) 0.2479 0.1416 

W2V + DBSCAN 
0.0055 0.0001 
HDHP 
0.4240 0.3512 
Wiki 
HD-GMHP (100D) 0.5848 0.3834 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : F1-score of each label</head><label>3</label><figDesc></figDesc><table>Label 
N 
HD-
GMHP 
HDHP 

Baseball 
2011 0.8114 0.8899 
Trump, Donald 
&amp;Politics and 
Government 

1664 0.1833 0.2157 

Terrorism 
1260 0.6052 0.4059 
Trump, Donald 
&amp;Election 
1110 0.2537 0.1939 

Trump, Donald 
994 0.0975 0.1227 
Politics and 
Government 
822 0.1677 0.1215 

Clinton, Hillary 
&amp;Election 
&amp;Trump, Donald 

755 0.1754 0.1402 

Election 
714 0.1378 0.1280 
Clinton, Hillary 
&amp;Election 
665 0.1669 0.1157 

Russia 
637 0.3177 0.2223 

Micro F-score 
N/A 0.2874 0.2189 
Macro F-score 
N/A 0.3637 0.3165 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Reply structure recovery results in Wikipedia conversation dataset.</head><label>4</label><figDesc></figDesc><table>P node 
R node 
F1 node 

Naive Baseline 0.3223 0.6501 0.4310 
HDHP 
0.5598 0.5834 0.5714 
HD-GMHP 
0.6433 0.5468 0.5911 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : Model Robustness on dimensional change of input vectors in NYT dataset.</head><label>5</label><figDesc></figDesc><table>AMI 
ARI 

HD-GMHP (50D) 
0.2310 0.1518 
HD-GMHP (100D) 0.2479 0.1416 
HD-GMHP (150D) 0.2421 0.1191 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>HD-GMHP model robustness on dimen-
sional change of input vector in Wikipedia conversation 
dataset. 

AMI 
ARI 
P node 
R node 

50D 
0.5836 0.3782 0.6466 0.5554 
100D 0.5848 0.3834 0.6433 0.5468 
150D 0.5948 0.3670 0.6450 0.5473 

</table></figure>

			<note place="foot">t i − t j ∼ Hawkes(α, β).</note>

			<note place="foot" n="1"> https://github.com/yeonsw/NYT-dataset</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yeon Seonwoo is supported by NCSoft Corpo-ration. Sungjoon Park and Alice Oh are sup-ported by the Engineering Research Center Pro-gram through the National Research Foundation of Korea (NRF) funded by the Korean Govern-ment MSIT (NRF-2018R1A5A1059921).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified analysis of streaming news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choon Hui</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic nonparametric mixture models and the recurrent chinese restaurant process: with applications to evolutionary clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reconstruction of threaded conversations in online discussion forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Aumayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A supervised approach to predict the hierarchical structure of conversation threads for comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asadpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Scientific World Journal</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distance dependent chinese restaurant processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2488" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet Allocation. Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Particle learning and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hedibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="106" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Echoes of power: Language effects and power differences in social interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A learning approach for email conversation thread reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Shakery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="846" to="863" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Masoud Asadpour, and Arash Koushkestani</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An introduction to sequential monte carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sequential Monte Carlo methods in practice</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rao-blackwellised particle filtering for dynamic bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dirichlethawkes processes with applications to clustering continuous-time document streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spectra of some self-exciting and mutually exciting point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hawkestopic: A joint model for network inference and topic modeling from text-based cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Rekatsinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discovering latent influence in online social activities via shared cascade poisson processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uncovering the spatiotemporal patterns of collective social activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jankowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tagging and linking web forum posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conversation trees: A grammar model for topic structure in forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Annie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shay B Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hawkes processes for continuous time sequence classification: an application to rumour stance classification in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Srijith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modeling the dynamics of learning activity on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charalampos</forename><surname>Mavroforakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Why it happened: Identifying and modeling the reasons of the happening of social events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sketch the storyline with charcoal: A non-parametric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning online discussion structures by conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting thread discourse structure over technical web forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A low-rank approximation approach to learning joint embeddings of news stories and images for timeline summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A dirichlet mixture model of hawkes processes for event sequence clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarizing complex events: a cross-modal solution of storylines extraction and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shize</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised storyline extraction from news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
