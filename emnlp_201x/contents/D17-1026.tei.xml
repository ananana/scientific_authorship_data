<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="274" to="285"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embed-dings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and data sources, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from human-written sentences, finding clear differences in length, the amount of repetition, and the use of rare words. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained word embeddings have received a great deal of attention from the research community, but there is much less work on developing pretrained embeddings for sentences. Here we target sen- tence embeddings that are "paraphrastic" in the sense that two sentences with similar meanings are close in the embedding space. <ref type="bibr" target="#b39">Wieting et al. (2016b)</ref> developed paraphrastic sentence embed- dings that are useful for semantic textual similar- ity tasks and can also be used as initialization for supervised semantic tasks. R: We understand that has already commenced, but there is a long way to go. T: This situation has already commenced, but much still needs to be done. R: The restaurant is closed on Sundays. No breakfast is available on Sunday mornings. T: The restaurant stays closed Sundays so no breakfast is served these days. R: Improved central bank policy is another huge factor. T: Another crucial factor is the improved policy of the central banks. <ref type="table">Table 1</ref>: Illustrative examples of references (R) paired with back-translations (T).</p><p>To learn their sentence embeddings, Wieting et al. used the Paraphrase Database (PPDB) ( <ref type="bibr" target="#b16">Ganitkevitch et al., 2013)</ref>. PPDB contains a large set of paraphrastic textual fragments extracted auto- matically from bilingual text ("bitext"), which is readily available for languages and domains. Ver- sions of PPDB have been released for several lan- guages ( <ref type="bibr" target="#b15">Ganitkevitch and Callison-Burch, 2014</ref>).</p><p>However, more recent work has shown that the fragmental nature of PPDB's pairs can be problematic, especially for recurrent net- works ( <ref type="bibr" target="#b41">Wieting and Gimpel, 2017)</ref>. Better per- formance can be achieved with a smaller set of sentence pairs derived from aligning Simple En- glish and standard English Wikipedia ( <ref type="bibr" target="#b10">Coster and Kauchak, 2011)</ref>. While effective, this type of data is inherently limited in size and scope, and not available for languages other than English.</p><p>PPDB is appealing in that it only requires bi- text. We would like to retain this property but develop a data resource with sentence pairs rather than phrase pairs. We turn to neural machine trans- lation (NMT) ( <ref type="bibr" target="#b36">Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b33">Sennrich et al., 2016a</ref>), which has ma- tured recently to yield strong performance espe- cially in terms of producing grammatical outputs.</p><p>In this paper, we build NMT systems for three language pairs, then use them to back-translate the non-English side of the training bitext. The re- sulting data consists of sentence pairs containing an English reference and the output of an X-to- English NMT system. <ref type="table">Table 1</ref> shows examples. We use this data for training paraphrastic sen- tence embeddings, yielding results that are much stronger than when using PPDB and competitive with the Simple English Wikipedia data.</p><p>Since bitext is abundant and available for many language pairs and domains, <ref type="bibr">2</ref> we also develop sev- eral methods of filtering the data, including based on sentence length, quality measures, and mea- sures of difference between the reference and its back-translation. We find length to be an effective filtering method, showing that very short length ranges-where the translation is 1 to 10 words- are best for learning.</p><p>In studying quality measures for filtering, we train a classifier to predict if a sentence is a ref- erence or a back-translation, then score sentences by the classifier score. This investigation allows us to examine the kinds of phenomena that best distinguish NMT output from references in this controlled setting of translating the bitext training data. NMT output has more repetitions of both words and longer n-grams, and uses fewer rare words than the references.</p><p>We release our generated sentence pairs to the research community with the hope that the data can inspire others to develop additional filtering methods, to experiment with richer architectures for sentence embeddings, and to further analyze the differences between neural machine transla- tions and references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We describe related work in learning general- purpose sentence embeddings, work in automat- ically generating or discovering paraphrases, and finally prior work in leveraging neural machine translation for embedding learning.</p><p>Paraphrastic sentence embeddings. Our learn- ing and evaluation setting is the same as that con- sidered by <ref type="bibr" target="#b39">Wieting et al. (2016b)</ref> and <ref type="bibr" target="#b38">Wieting et al. (2016a)</ref>, in which the goal is to learn para- phrastic sentence embeddings that can be used for downstream tasks. They trained models on PPDB and evaluated them using a suite of semantic tex- tual similarity (STS) tasks and supervised seman- tic tasks. Others have begun to consider this set- ting as well ( <ref type="bibr" target="#b5">Arora et al., 2017)</ref>.</p><p>Other work in learning general purpose sen- tence embeddings has used autoencoders <ref type="bibr" target="#b35">(Socher et al., 2011;</ref><ref type="bibr" target="#b20">Hill et al., 2016)</ref>, encoder-decoder ar- chitectures ( <ref type="bibr">Kiros et al., 2015)</ref>, or other learning frameworks ( <ref type="bibr" target="#b27">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b31">Pham et al., 2015)</ref>. <ref type="bibr" target="#b39">Wieting et al. (2016b)</ref> and <ref type="bibr" target="#b20">Hill et al. (2016)</ref> provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest configurations from their results.</p><p>Paraphrase generation and discovery. There is a rich history of research in generating or finding naturally-occurring sentential paraphrases <ref type="bibr" target="#b8">(Barzilay and McKeown, 2001;</ref><ref type="bibr" target="#b12">Dolan and Brockett, 2005;</ref><ref type="bibr" target="#b45">Zhao et al., 2010;</ref><ref type="bibr" target="#b10">Coster and Kauchak, 2011;</ref><ref type="bibr" target="#b43">Xu et al., 2014</ref><ref type="bibr" target="#b42">Xu et al., , 2015</ref>.</p><p>The most relevant work uses bilingual cor- pora, e.g., <ref type="bibr" target="#b46">Zhao et al. (2008)</ref> and <ref type="bibr" target="#b7">Bannard and Callison-Burch (2005)</ref>, the latter leading to PPDB. Our goals are highly similar to those of the PPDB project, which has also been produced for many languages <ref type="bibr">(Ganitkevitch and CallisonBurch, 2014</ref>) since it only relies on the availability of bilingual text.</p><p>Prior work has shown that PPDB can be used for learning embeddings for words and phrases <ref type="bibr" target="#b14">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b40">Wieting et al., 2015)</ref>. However, when learning sentence embeddings, <ref type="bibr" target="#b41">Wieting and Gimpel (2017)</ref> showed that PPDB is not as effective as sentential paraphrases, espe- cially for recurrent networks. These results are in- tuitive because the phrases in PPDB are short and often cut across constituent boundaries. For sen- tential paraphrases, <ref type="bibr" target="#b41">Wieting and Gimpel (2017)</ref> used a dataset developed for text simplification by <ref type="bibr" target="#b10">Coster and Kauchak (2011)</ref>. It was created by aligning sentences from Simple English and stan- dard English Wikipedia. We compare our data to both PPDB and this Wikipedia dataset.</p><p>Neural machine translation for paraphrastic embedding learning. <ref type="bibr" target="#b36">Sutskever et al. (2014)</ref> trained NMT systems and visualized part of the space of the source language encoder for their English→French system. <ref type="bibr" target="#b20">Hill et al. (2016)</ref> eval- uated the encoders of English-to-X NMT sys- tems as sentence representations, finding them to perform poorly compared to several other meth- ods based on unlabeled data. <ref type="bibr" target="#b28">Mallinson et al. (2017)</ref> adapted trained NMT models to produce sentence similarity scores in semantic evaluations. They used pairs of NMT systems, one to translate an English sentence into multiple foreign transla- tions and the other to then translate back to En- glish. Other work has used neural MT architec- tures and training settings to obtain better word embeddings ( <ref type="bibr">Hill et al., 2014a,b)</ref>.</p><p>Our approach differs in that we only use the NMT system to generate training data for train- ing sentence embeddings, rather than use it as the source of the model. This permits us to decouple decisions made in designing the NMT architecture from decisions about which models we will use for learning sentence embeddings. Thus we can benefit from orthogonal work in designing neural architectures to embed sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Machine Translation</head><p>We now describe the NMT systems we use for generating data for learning sentence embeddings.</p><p>In our experiments, we use three encoder-decoder NMT models: Czech→English, French→English, and German→English.</p><p>We used Groundhog 3 as the implementation of the NMT systems for all experiments. We gener- ally followed the settings and training procedure from previous work ( <ref type="bibr" target="#b6">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b33">Sennrich et al., 2016a</ref>). As such, all networks have a hidden layer size of 1000 and an embedding layer size of 620. During training, we used Adadelta <ref type="bibr" target="#b44">(Zeiler, 2012)</ref>, a minibatch size of 80, and the training set was reshuffled between epochs. We trained a network for approximately 7 days on a single GPU (TITAN X), then the embedding layer was fixed and training continued, as sug- gested by <ref type="bibr" target="#b22">Jean et al. (2015)</ref>, for 12 hours. Addi- tionally, the softmax was calculated over a filtered list of candidate translations. Following <ref type="bibr" target="#b22">Jean et al. (2015)</ref>, during decoding, we restrict the softmax layers' output vocabulary to include: the 10000 most common words, the top 25 unigram transla- tions, and the gold translations' unigrams.</p><p>All systems were trained on the available train- ing data from the WMT15 shared translation task (15.7 million, 39.2 million, and 4.2 million sen- tence pairs for CS→EN, FR→EN, and DE→EN,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Czech</head><p>French German Europarl 650,000 2,000,000 2,000,000 Common Crawl 160,000 3,000,000 2,000,000 News Commentary 150,000 200,000 200,000 UN - 12,000,000 - 10 9 French-English - 22,000,000 - CzEng 14,700,000 - -   <ref type="bibr" target="#b13">and Chen, 2010)</ref>, News Commentary v10, the 10 9 French-English corpus, and CzEng 1.0 ( <ref type="bibr" target="#b9">Bojar et al., 2016)</ref>. A breakdown of the sizes of these corpora can be found in Ta- ble 3. The data was pre-processed using standard pre-processing scripts found in Moses ( <ref type="bibr" target="#b26">Koehn et al., 2007)</ref>. Rare words were split into sub-word units, following <ref type="bibr" target="#b34">Sennrich et al. (2016b)</ref>. BLEU scores on the WMT2015 test set for each NMT system can be seen in <ref type="table" target="#tab_1">Table 3</ref>. To produce paraphrases we use "back- translation", i.e., we use our X→English NMT systems to translate the non-English sentence in each training sentence pair into English. We directly use the bitext on which the models were trained. This could potentially lead to pairs in which the reference and translation match exactly, if the model has learned to memorize the reference translations seen during training. However, in practice, since we have so much bitext to draw from, we can easily find data in which they do not match exactly.</p><p>Thus our generated data consists of pairs of En- glish references from the bitext along with the NMT-produced English back-translations. We use beam search with a width of 50 to generate mul- tiple translations for each non-English sentence, each of which is a candidate paraphrase for the En- glish reference.</p><p>Example outputs of this process are in <ref type="table">Table 1</ref>, showing some rich paraphrase phenomena in the data. These examples show non-trivial phrase sub- stitutions ("there is a long way to go" and "much still needs to be done"), sentences being merged and simplified, and sentences being rearranged.</p><p>For examples of erroneous paraphrases that can be generated by this process, see <ref type="table" target="#tab_13">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models and Training</head><p>Our goal is to compare our paraphrase dataset to other datasets by using each to train sentence em- beddings, keeping the models and learning proce- dure fixed. So we select models and a loss function from prior work ( <ref type="bibr" target="#b39">Wieting et al., 2016b;</ref><ref type="bibr" target="#b41">Wieting and Gimpel, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models</head><p>We wish to embed a word sequence s into a fixed- length vector. We denote the tth word in s as s t , and we denote its word embedding by x t . We fo- cus on two models in this paper. The first model, which we call AVG, simply averages the embed- dings x t of all words in s. The only parameters learned in this model are those in the word em- beddings themselves, which are stored in the word embedding matrix W w . This model was found by <ref type="bibr" target="#b39">Wieting et al. (2016b)</ref> to perform very strongly for semantic similarity tasks.</p><p>The second model, the GATED RECURRENT AV- ERAGING NETWORK (GRAN) <ref type="bibr" target="#b41">(Wieting and Gimpel, 2017)</ref>, combines the benefits of AVG and long short-term memory (LSTM) recurrent neural net- works <ref type="bibr" target="#b21">(Hochreiter and Schmidhuber, 1997)</ref>. It first uses an LSTM to generate a hidden vector, h t , for each word s t in s. Then h t is used to com- pute a gate that is elementwise-multiplied with x t , resulting in a new hidden vector a t for each step t:</p><formula xml:id="formula_0">a t = x t σ(W x x t + W h h t + b)<label>(1)</label></formula><p>where W x and W h are parameter matrices, b is a parameter vector, and σ is the elementwise logis- tic sigmoid function. After all a t have been gener- ated for a sentence, they are averaged to produce the embedding for that sentence. The GRAN re- duces to AVG if the output of the gate is always 1. This model includes as learnable parameters those of the LSTM, the word embeddings, and the addi- tional parameters in Eq.</p><p>(1). We use W c to denote the "compositional" parameters, i.e., all parame- ters other than the word embeddings. Our motivation for choosing these two models is that they both work well in this transfer learning setting ( <ref type="bibr" target="#b39">Wieting et al., 2016b</ref>) and they are archi- tecturally similar with one crucial difference: only the GRAN takes into account word order. This difference plays an important role in the effective- ness of the different filtering methods as explored in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We follow the training procedure of <ref type="bibr" target="#b40">Wieting et al. (2015)</ref> and <ref type="bibr" target="#b39">Wieting et al. (2016b)</ref>. The training data is a set S of paraphrastic pairs s 1 , s 2 and we optimize a margin-based loss:</p><formula xml:id="formula_1">min Wc,Ww 1 |S| s 1 ,s 2 ∈S max(0, δ − cos(g(s1), g(s2)) + cos(g(s1), g(t1))) + max(0, δ − cos(g(s1), g(s2)) + cos(g(s2), g(t2))) +λcWc 2 +λwWw initial −Ww 2</formula><p>where g is the model (AVG or GRAN), δ is the margin, λ c and λ w are regularization parameters, W w initial is the initial word embedding matrix, and t 1 and t 2 are "negative examples" taken from a mini-batch during optimization. The intuition is that we want the two texts to be more similar to each other (cos(g(s 1 ), g(s 2 ))) than either is to their respective negative examples t 1 and t 2 , by a margin of at least δ. To select t 1 and t 2 , we choose the most similar sentence in some set (other than those in the given pair). For simplicity we use the mini-batch for this set, i.e., we choose t 1 for a given s 1 , s 2 as follows:</p><formula xml:id="formula_2">t 1 = argmax t:t,··∈S b \{{s 1 ,s 2 } cos(g(s 1 ), g(t))</formula><p>where S b ⊆ S is the current mini-batch. That is, we want to choose a negative example t i that is similar to s i according to the current model. The downside is that we may occasionally choose a phrase t i that is actually a true paraphrase of s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We now investigate how best to use our generated paraphrase data for training universal paraphras- tic sentence embeddings. We consider 10 data sources: Common Crawl (CC), Europarl (EP), and News Commentary (News) from all 3 lan- guage pairs, as well as the 10 9 French-English data (Giga). We extract 150,000 reference/back- translation pairs from each data source. We use 100,000 of these to mine for training data for our sentence embedding models, and the remaining 50,000 are used as train/validation/test data for the reference classification and language models de- scribed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation</head><p>We evaluate the quality of a paraphrase dataset by using the experimental setting of <ref type="bibr" target="#b39">Wieting et al. (2016b)</ref>. We use the paraphrases as training data to create paraphrastic sentence embeddings, using the cosine of the embeddings as the measure of se- mantic relatedness, then evaluate the embeddings on the SemEval semantic textual similarity (STS) tasks from 2012 to <ref type="bibr" target="#b4">(Agirre et al., 2012</ref>, the SemEval 2015 Twitter task ( <ref type="bibr" target="#b42">Xu et al., 2015)</ref>, and the SemEval 2014 SICK Seman- tic Relatedness task ( <ref type="bibr" target="#b29">Marelli et al., 2014</ref>). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. As our test set, we report the average Pearson's r over these 22 sentence similarity tasks. <ref type="bibr">4</ref> As devel- opment data, we use the 2016 STS tasks ( <ref type="bibr" target="#b2">Agirre et al., 2016)</ref>, where the tuning criterion is the av- erage Pearson's r over its 5 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>For fair comparison among different datasets and dataset filtering methods described below, we use only 24,000 training examples for nearly all exper- iments. Different filtering methods produce dif- ferent amounts of training data, and using 24,000 examples allows us to keep the amount of train- ing data constant across filtering methods. It also allows us to complete these several thousand ex- periments in a reasonable amount of time. In Sec- tion 5.8 below, we discuss experiments that scale up to larger amounts of training data.</p><p>We use</p><formula xml:id="formula_3">PARAGRAM-SL999</formula><p>embed- dings ( <ref type="bibr" target="#b40">Wieting et al., 2015)</ref> to initialize the word embedding matrix (W w ) for both models. For all experiments, we fix the mini-batch size to 100, λ w to 0, λ c to 0, and the margin δ to 0.4. We train AVG for 20 epochs, and the GRAN for 3, since it converges much faster. For optimization we use Adam ( <ref type="bibr" target="#b23">Kingma and Ba, 2014</ref>) with a learning rate of 0.001.</p><p>We compare to two data resources used in pre- vious work to learn paraphrastic sentence em- beddings. The first is phrase pairs from PPDB, used by <ref type="bibr">Wieting</ref>    <ref type="bibr" target="#b41">Wieting and Gimpel (2017)</ref>. We refer to this data source as "SimpWiki". We refer to our back-translated data as "NMT".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dataset Comparison</head><p>We first compare datasets, randomly sampling 24,000 sentence pairs from each of PPDB, Simp- Wiki, and each of our NMT datasets. The only hyperparameter to tune for this experiment is the stopping epoch, which we tune based on our de- velopment set. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. We find that the NMT datasets are all effec- tive as training data, outperforming PPDB in all cases when using the GRAN. There are excep- tions when using AVG, for which PPDB is quite strong. This is sensible because AVG is not sen- sitive to word order, so the fragments in PPDB do not cause problems. However, when using the GRAN, which is sensitive to word order, the NMT data is consistently better than PPDB. It of- ten exceeds the performance of training on the SimpWiki data, which consists entirely of human- written sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Filtering Methods</head><p>Above we showed that the NMT data is better than PPDB when using a GRAN and often as good as SimpWiki. Since we have access to so much more NMT data than SimpWiki (which is limited to fewer than 200k sentence pairs), we next ex- periment with several approaches for filtering the NMT data. We first consider filtering based on length, described in Section 5.5. We then con- sider filtering based on several quality measures designed to find more natural and higher-quality translations, described in Section 5.6. Finally, we consider several measures of diversity. By diver- sity we mean here a measure of the lexical and syntactic difference between the reference and its paraphrase. We describe these experiments in Sec- tion 5.7. We note that these filtering methods are not all mutually exclusive and could be combined, though in this paper we experiment with each in- dividually and leave combination to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Length Filtering</head><p>We first consider filtering candidate sentence pairs by length, i.e., the number of tokens in the trans- lation. The tunable parameters are the upper and lower bounds of the translation lengths.</p><p>We experiment with a partition of length ranges, showing the results in <ref type="table" target="#tab_5">Table 5</ref>. These results are averages across all language pairs and data sources of training data for each length range shown. We find it best to select NMT data where the trans- lations have between 0 and 10 tokens, with per- formance dropping as sentence length increases. This is true for both the GRAN and AVG models. We do the same filtering for the SimpWiki data, though the trend is not nearly as strong. There- fore this is unlikely due to the nature of the eval- uation data, and may be due to machine transla- tion quality dropping as sentence length increases. This trend appears even though the datasets with higher ranges have more tokens of training data, since only the number of training sentence pairs is kept constant across configurations.</p><p>We then tune the length range using our de- velopment data, considering the following length ranges: <ref type="bibr">[0,</ref><ref type="bibr">10]</ref>, <ref type="bibr">[0,</ref><ref type="bibr">15]</ref>, <ref type="bibr">[0,</ref><ref type="bibr">20]</ref>, <ref type="bibr">[0,</ref><ref type="bibr">30]</ref>, <ref type="bibr">[0,</ref><ref type="bibr">100]</ref>, <ref type="bibr">[10,</ref><ref type="bibr">20]</ref>, <ref type="bibr">[10,</ref><ref type="bibr">30]</ref>, <ref type="bibr">[10,</ref><ref type="bibr">100]</ref>, <ref type="bibr">[15,</ref><ref type="bibr">25]</ref>, <ref type="bibr">[15,</ref><ref type="bibr">30]</ref>, <ref type="bibr">[15,</ref><ref type="bibr">100]</ref>, <ref type="bibr">[20,</ref><ref type="bibr">30]</ref>, <ref type="bibr">[20,</ref><ref type="bibr">100]</ref>, <ref type="bibr">[30,</ref><ref type="bibr">100]</ref>. We tune over ranges as well as language, data source, and stopping epoch, each time training on 24,000 sen- tence pairs. We report the average test results over all languages and datasets in   The tuned length ranges are short for both NMT and SimpWiki. The distribution of lengths in the NMT and SimpWiki data is fairly similar. The 10 NMT datasets all have mean translation lengths between 22 and 28 tokens. The data has fairly large standard deviations (11-25 tokens) indicat- ing that there are some very long translations in the data. SimpWiki has a mean length of 24.2 and a standard deviation of 13.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Quality Filtering</head><p>We also consider filtering based on several mea- sures of the "quality" of the back-translation:</p><p>• Translation Cost: We use the cost (negative log likelihood) of the translation from the NMT system, divided by the number of tokens in the translation.  <ref type="table">Table 7</ref>: Quality filtering test results after tun- ing quality hyperparameters on development data (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pear- son's r × 100).</p><p>of 0.1. For the language model, we tune an up- per bound on the perplexity of the translations among the set {25, 50, 75, 100, 150, 200, ∞}. For the classifier, we tune the minimum probability of being a reference over the range [0, 0.9] using in- crements of 0.1. <ref type="table">Table 7</ref> shows average test results over all lan- guages and datasets after tuning hyperparameters on our development data for each. The translation cost and language model are not helpful for filter- ing, as random selection outperforms them. Both methods are outperformed by the reference classi- fier, which slightly outperforms random selection when using the stronger GRAN model. We now discuss further how we trained the reference clas- sifier and the data characteristics that it reveals. We did not experiment with quality filtering for SimpWiki since it is human-written text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Reference/Translation Classification</head><p>We experiment with predicting whether a given sentence is a reference or a back-translation, hy- pothesizing that generated sentences with high probabilities of being references are of higher quality. We train two kinds of binary classifiers, one using an LSTM and the other using word av- eraging, followed by a softmax layer. We select 40,000 reference/translation pairs for training and 5,000 for each of validation and testing. A single example is a sentence with label 1 if it is a refer- ence translation and 0 if it is a translation.</p><p>In training, we consider the entire k-best list as examples of translations, selecting one trans- lation to be the 0-labeled example. We either do this randomly or we score each sentence in the k-best list using our model and select the one with the highest probability of being a reference as the 0-labeled example. We tune this choice as well as an L 2 regularizer on the word embeddings (tuned over {10 −5 , 10 −6 , 10 −7 , 10 −8 , 0}). We use PARAGRAM-SL999 embeddings (Wieting et al.,  <ref type="table">Table 8</ref>: Results of reference/translation clas- sification (accuracy×100). The highest score in each column is in boldface. Final two columns show accuracies of positive (reference) and nega- tive classes, respectively.</p><note type="other">Model Lang. Data Test Acc. + Acc. -Acc.</note><p>2015) to initialize the word embeddings for both models. Models were trained by minimizing cross entropy for 10 epochs using Adam with learning rate 0.001. We performed this procedure sepa- rately for each of the 10 language/data pairs. The results are shown in <ref type="table">Table 8</ref>. While per- formance varies greatly across data sources, the LSTM always outperforms the word averaging model. For our translation-reference classifica- tion, we note that our results can be further im- proved. We also trained models on 90,000 exam- ples, essentially doubling the amount of data, and the results improved by about 2% absolute on each dataset on both the validation and testing data.</p><p>Analyzing Reference Classification. We in- spected the output of our reference classifier and noted a few qualitative trends which we then ver- ified empirically. First, neural MT systems tend to use a smaller vocabulary and exhibit more re- stricted use of phrases. They correspondingly tend to show more repetition in terms of both words and longer n-grams. This hypothesis can be verified empirically in several ways. We do so by calculat- ing the entropy of the unigrams and trigrams for both the references and the translations from our 150,000 reference-translation pairs. <ref type="bibr">5</ref> We also cal- culate the repetition percentage of unigrams and  trigrams in both the references and translations. This is defined as the percentage of words that are repetitions (i.e., have already appeared in the sentence). For unigrams, we only consider words consisting of at least 3 characters.</p><p>The results are shown in <ref type="table" target="#tab_10">Table 9</ref>, in which we subtract the translation value from the reference value for each measure. The translated text has lower n-gram entropies and higher rates of repeti- tion. This appears for all datasets, but is strongest for common crawl and French-English 10 9 .</p><p>We also noticed that translations are less likely to use rare words, instead willing to use a larger se- quence of short words to convey the same mean- ing. We found that translations were sometimes more vague and, unsurprisingly, were more likely to be ungrammatical.</p><p>We check whether our classifier is learning these patterns by computing the reference prob- abilities P (R) of 100,000 randomly sampled translation-reference pairs from each dataset (the same used to train models). We then compute the correlation between our classification score and different metrics: the repetition rate of the sentence, the average inverse-document frequency (IDF) of the sentence, <ref type="bibr">6</ref> and the translation length.</p><p>The results are shown in <ref type="table" target="#tab_12">Table 10</ref>. Negative cor- relations with repetitions indicates that fewer repe- titions lead to higher P (R). A positive correlation with average IDF indicates that P (R) rewards the use of rare words. Interestingly, negative correla- tion with length suggests that the classifier prefers <ref type="bibr">6</ref> Wikipedia was used to calculate the frequencies of the tokens. All tokens were lowercased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Spearman   more concise sentences. <ref type="bibr">7</ref> We show examples of these phenomena in <ref type="table" target="#tab_13">Table 11</ref>. The first two exam- ples show the tendency of NMT to repeat words and phrases. The second two show how they tend to use sequences of common words ("put at risk") rather than rare words ("endangering").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Diversity Filtering</head><p>We consider several filtering criteria based on measures that encourage particular amounts of disparity between the reference and its back- translation:</p><p>• n-gram Overlap: Our n-gram overlap mea- sures are calculated by counting n-grams of a given order in both the reference and translation, then dividing the number of shared n-grams by the total number of n-grams in the reference or translation, whichever has fewer. We use three n-gram overlap scores (n ∈ {1, 2, 3}).</p><p>• BLEU Score: We use a smoothed sentence- level BLEU variant from <ref type="bibr" target="#b30">Nakov et al. (2012)</ref> that uses smoothing for all n-gram lengths and also smooths the brevity penalty.  For both methods, the tunable hyperparam- eters are the upper and lower bounds for the above scores. We tune over the cross product of lower bounds {0, 0.1, 0.2, 0.3} and upper bounds {0.6, 0.7, 0.8, 0.9, 1.0}. Our intuition is that the best data will have some amount of n-gram over- lap, but not too much. Too much n-gram overlap will lead to pairs that are not useful for learning.</p><p>The results are shown in <ref type="table" target="#tab_0">Table 12</ref>, for both mod- els and for both NMT and SimpWiki. We find that the diversity filtering methods lead to consistent improvements when training on SimpWiki. We believe this is because many of the sentence pairs in SimpWiki are near-duplicates and these filtering methods favor data with more differences.</p><p>Diversity filtering can also help when selecting NMT data, though the differences are smaller. We do note that unigram overlap is the strongest filter- ing strategy for AVG. When looking at the thresh- old tuning, the best lower bounds are often 0 or 0.1 and the best upper bounds are typically 0.6-0.7, in- dicating that sentence pairs with a high degree of word overlap are not useful for training. We also find that the GRAN benefits more from filtering based on higher-order n-gram overlap than AVG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Scaling Up</head><p>Unlike the SimpWiki data, which is naturally lim- ited and only available for English, we can scale our approach. Since we use data on which the NMT systems were trained and perform back- translation, we can easily produce large training sets of paraphrastic sentence pairs for many lan- guages and data domains, limited only by the availability of bitext.</p><p>To test this, we took the tuned filtering methods and language/data pairs (according to our devel- opment dataset only), and trained them on more data. These were CC-CS for GRAN and CC-DE  for AVG. We also trained each model on the same number of sentence pairs from SimpWiki. <ref type="bibr">8</ref> We also compare to PPDB XL, and since PPDB has fewer tokens per example, we use enough PPDB data so that it has at least as many tokens as the SimpWiki data used in the experiment. 9 <ref type="table" target="#tab_1">Table 13</ref> shows clear improvements when us- ing more training data, providing evidence that our approach can scale to larger datasets. The NMT data surpasses SimpWiki for the GRAN, while the SimpWiki and NMT data perform similarly for AVG. PPDB is outperformed by both data sources for both models. Even when we train on all 52M tokens in PPDB XXL, AVG only reaches 66.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We showed how back-translation can be used to generate effective training data for paraphras- tic sentence embeddings. We explored filtering strategies that improve the generated data; in do- ing so, we identified characteristics that distin- guish NMT output from references. Our hope is that these results can enable learning paraphrastic sentence embeddings with powerful neural archi- tectures across many languages and domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset sizes (numbers of sentence pairs) 
for data domains used for training NMT systems. 

Language 
% BLEU 
Czech→English 
19.7 
French→English 
20.1 
German→English 
28.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BLEU scores on the WMT2015 test set. 

respectively). The training data included: Eu-
roparl v7 (Koehn, 2005), the Common Crawl cor-
pus, the UN corpus (Eisele </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Test results (average Pearson's r × 100 
over 22 STS datasets) using a random selection of 
24,000 examples from each data source. 

subsumes all smaller ones. The pairs in PPDB are 
sorted by a confidence measure and so the smaller 
sets contain higher precision paraphrases. We use 
PPDB XL in this paper, which consists of fairly 
high precision paraphrases. The other data source 
is the aligned Simple English / standard English 
Wikipedia data developed by Coster and Kauchak 
(2011) and used for learning paraphrastic sentence 
embeddings by </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 .</head><label>6</label><figDesc>We com- pare to a baseline that draws a random set of data, showing that length-based filtering leads to gains of nearly half a point on average across our test sets.</figDesc><table>Length Range 
Data 
Model 
0-10 10-20 20-30 30-100 

SimpWiki 
GRAN 67.4 
67.7 
67.1 
67.3 
AVG 
65.9 
65.7 
65.6 
65.9 

NMT 
GRAN 66.6 
66.5 
66.0 
64.8 
AVG 
65.7 
65.6 
65.3 
65.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Test correlations for our models when 
trained on sentences with particular length ranges 
(averaged over languages and data sources for the 
NMT rows). Results are on STS datasets (Pear-
son's r × 100). 

NMT 
SimpWiki 
Filtering Method 
GRAN AVG GRAN AVG 
None (Random) 
66.9 
65.5 
67.2 
65.8 
Length 
67.3 
66.0 
67.4 
66.2 
Tuned Len. Range [0,10] [0,10] [0,10] [0,15] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Length filtering test results after tuning length ranges on development data (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pearson's r × 100).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Differences in entropy and repetition of 
unigrams/trigrams in references and translations. 
Negative values indicate translations have a higher 
value, so references show consistently higher en-
tropies and lower repetition rates. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="true"><head>Table 10 : Spearman's ρ between our reference classifier probability and various measures.</head><label>10</label><figDesc></figDesc><table>Sentence 
P (R) 
R: Room was comfortable and the staff at the 
front desk were very helpful. 
1.0 

T: The staff were very nice and the room was very 
nice and the staff were very nice. 
&lt; 0.01 

R: The enchantment of your wedding day, cap-
tured in images by Flore-Ael Surun. 
0.98 

T: The wedding of the wedding, put into images 
by Flore-Ael A. 
&lt; 0.01 

R: Mexico and Sweden are longstanding support-
ers of the CTBT. 
1.0 

T: Mexico and Sweden have been supporters of 
CTBT for a long time now. 
0.06 

R: We thought Mr Haider ' s Austria was endan-
gering our freedom. 
1.0 

T: We thought that our freedom was put at risk by 
Austria by Mr Haider. 
0.09 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Illustrative examples of references (R) 
and back-translations (T), along with probabilities 
from the reference classifier. See text for details. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table>Diversity filtering test results after tun-
ing filtering hyperparameters on development data 
(averaged over languages and data sources for the 
NMT rows). Results are on STS datasets (Pear-
son's r × 100). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 13 :</head><label>13</label><figDesc></figDesc><table>Test results with more training data. 
More data helps both AVG and GRAN to match or 
surpass training on SimpWiki. Both comfortably 
surpass PPDB. The number of training examples 
used is in parentheses. 

</table></figure>

			<note place="foot" n="1"> Generated paraphrases and code are available at http: //ttic.uchicago.edu/ ˜ wieting.</note>

			<note place="foot" n="2"> For example, CzEng 1.6 (Bojar et al., 2016) contains a billion words across its 8 domains.</note>

			<note place="foot" n="3"> Available at https://github.com/sebastienj/LV_groundhog.</note>

			<note place="foot" n="4"> Statistical significance testing is nontrivial due to averaging Pearson&apos;s r so we leave it to future work.</note>

			<note place="foot" n="5"> We randomly selected translations from the beam search.</note>

			<note place="foot" n="7"> This is noteworthy because the average sentence length of translations and references is not significantly different.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research used resources of the Argonne Lead-ership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357. We thank the developers of Theano (Theano Development Team, 2016) and NVIDIA Corporation for donating GPUs used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>English, Spanish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>SemEval-2015 task 2: Semantic textual similarity</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
		<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Paraphrasing with bilingual parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting paraphrases from a parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 39th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ybovick´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Sudarikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dušan</forename><surname>Variš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 19th International Conference on Text, Speech, and Dialogue (TSD)</title>
		<meeting>19th International Conference on Text, Speech, and Dialogue (TSD)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple english wikipedia: a new text simplification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="665" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">350</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWP</title>
		<meeting>of IWP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MultiUN: A multilingual corpus from united nation documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eisele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh conference on International Language Resources and Evaluation (LREC&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The multilingual paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PPDB: The Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">KenLM: faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6448</idno>
		<title level="m">Coline Devin, and Yoshua Bengio. 2014a. Embedding word similarity with neural machine translation</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0718</idno>
		<title level="m">Coline Devin, and Yoshua Bengio. 2014b. Not all neural embeddings are born equal</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Machine Translation Summit</title>
		<meeting>the 10th Machine Translation Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimizing for sentence-level BLEU+1 yields short translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1979" to="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monolingual machine translation for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the ACL (TACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting recurrent networks for paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Extracting lexically divergent paraphrases from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="435" to="448" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Leveraging multiple MT engines for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1326" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pivot approach for extracting paraphrase patterns from bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="780" to="788" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
