<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deciphering Related Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Sciences Institute</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deciphering Related Languages</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2513" to="2518"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a method for translating texts between close language pairs. The method does not require parallel data, and it does not require the languages to be written in the same script. We show results for six language pairs: Afrikaans/Dutch, Bosnian/Serbian, Danish/Swedish, Mace-donian/Bulgarian, Malaysian/Indonesian, and Polish/Belorussian. We report BLEU scores showing our method to outperform others that do not use parallel data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical Natural Language Processing (NLP) tools often need large amounts of training data in order to achieve good performance. This limits the use of current NLP tools to a few resource- rich languages. Assume an incident happens in an area with a low-resource language, known as the Incident Language (IL). For a quick response, we need to build NLP tools with available data, as finding or annotating new data is expensive and time consuming. For many languages this means that we only have a small amount of often out-of- domain parallel data (e.g. a Bible or Ubuntu man- ual), some monolingual data and almost no anno- tation such as part of speech tags.</p><p>Fortunately, many low-resource languages have one or more higher-resource, closely Related Lan- guages (RL). Examples of such IL/RL pairs are Afrikaans/Dutch and Bosnian/Serbian. A natural idea is to use RL resources to improve the task for IL. But this requires some kind of conversion between RL and IL. Assume the required NLP ca- pability is named entity tagging. If we can con- vert RL to IL, we can convert all RL training data along with annotations into IL and train the tagger for IL. Or, if we can convert IL to RL we can use the potentially existing RL named entity tagger on converted IL data and project back the tags.</p><p>Following this idea, <ref type="bibr" target="#b2">Currey et al. (2016)</ref> use a rule-based translation system to convert Italian and Portuguese into Spanish, to improve Span- ish (here, IL) language modeling, <ref type="bibr" target="#b16">Nakov and Ng (2009)</ref> convert RL/English parallel data to IL/English where both RL and IL have Latin or- thography to improve IL/English machine trans- lation. <ref type="bibr" target="#b7">Hana et al. (2006)</ref> use cognates to adapt Spanish resources to Brazilian Portuguese to train a part-of-speech tagger. <ref type="bibr" target="#b13">Mann and Yarowsky (2001)</ref> use Spanish/Portuguese cog- nates to convert an English/Spanish lexicon to En- glish/Portuguese. These works prove the useful- ness of RL data to improve NLP for IL, but they are designed for specific tasks and IL/RL pairs.</p><p>In this paper we propose a universal method for translating texts between closely related lan- guages. We assume that IL and RL are mostly cognates, having roughly the same word order. Our method is orthography-agnostic for alphabetic systems, and crucially, it does not need any paral- lel data. From now on, we talk about converting RL to IL, but the method does not distinguish be- tween RL and IL; as mentioned above, each direc- tion of translation can have its own potential uses.</p><p>To translate RL to IL, we train a character-based cipher model and connect it to a word-based lan- guage model. The cipher model is trained in a noisy channel model where a character language model produces IL characters and the model con- verts them to RL. Expectation Maximization is used to train the model parameters to maximize the likelihood of a set of RL monolingual data. At decoding time, the cipher model reads the RL text character by character in which words are separated by a special character, and produces a weighted lattice of characters representing all the possible translations for each of the input tokens.  The word-based language model takes this lat- tice and produces a sequence of output words that maximize the language model score times the ci- pher model score. <ref type="figure" target="#fig_1">Figure 1</ref> depicts this process.</p><p>Our cipher models one-to-one, one-to-two and two-to-one character mappings. This allows us to handle cases like Cyrillic 'q' and Latin 'ch', and also subtle differences in pronunciation between RL and IL like Portuguese 'justi√ßa' and Spanish 'justicia'. Using a character-based cipher model provides the flexibility to generate unseen words. In other words, the vocabulary is limited by the decoding LM, not the cipher model. Separation of training and decoding language models enables us to train the decoding LM on as much data as is available without worrying about training speed or memory issues. We can also transliterate out of vocabulary words by spelling out the best path produced by cipher model in case no good match is found for a token in the decoding LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous work on translation between related lan- guages can be categorized into three groups: Systems for specific language pairs such as Czech-Slovak ( <ref type="bibr" target="#b6">Haji et al., 2000</ref>), Turkish-Crimean Tatar <ref type="bibr" target="#b1">(Cicekli, 2002</ref>), Irish-Scottish Gaelic <ref type="bibr" target="#b25">(Scannell, 2006</ref>), and Indonesian-Malaysian ( <ref type="bibr" target="#b12">Larasati and Kubo, 2010)</ref>. Another similar trend is trans- lation between dialects of the same language like Arabic dialects to standard Arabic ( <ref type="bibr" target="#b8">Hitham et al., 2008;</ref><ref type="bibr" target="#b24">Sawaf, 2010;</ref><ref type="bibr" target="#b23">Salloum and Habash, 2010)</ref>. Also, work has been done on translating back the Romanized version of languages like Greeklish to <ref type="bibr">Greek (Chalamandaris et al., 2006</ref>) and Arabizi to <ref type="bibr">Arabic (May et al., 2014</ref>). These methods can- not be applied to our problem because time and resources are limited to build a translation system for the specific language pair.</p><p>Machine learning systems that use parallel data: These methods cover a broader range of languages but require parallel text between related languages. They include character-level machine translation ( <ref type="bibr" target="#b27">Vilar et al., 2007;</ref><ref type="bibr" target="#b26">Tiedemann, 2009</ref>) or combination of word-level and character-level ma- chine translation <ref type="bibr" target="#b17">(Nakov and Tiedemann, 2012)</ref> between related languages.</p><p>Use of non-parallel data: Cognates can be extracted from monolingual data and used as a parallel lexicon ( <ref type="bibr" target="#b7">Hana et al., 2006;</ref><ref type="bibr" target="#b13">Mann and Yarowsky, 2001;</ref><ref type="bibr" target="#b11">Kondrak et al., 2003</ref>). However, our task is whole-text transformation, not just cog- nate extraction.</p><p>Unsupervised deciphering methods, which re- quire no parallel data, have been used for bilin- gual lexicon extraction and machine translation. Word-based deciphering systems ignore sub-word similarities between related languages ( <ref type="bibr" target="#b10">Koehn and Knight, 2002;</ref><ref type="bibr" target="#b22">Ravi and Knight, 2011b;</ref><ref type="bibr" target="#b18">Nuhn et al., 2012;</ref><ref type="bibr" target="#b3">Dou and Knight, 2012;</ref><ref type="bibr" target="#b19">Ravi, 2013)</ref>. <ref type="bibr" target="#b5">Haghighi et al. (2008)</ref> and <ref type="bibr" target="#b15">Naim and Gildea (2015)</ref> propose models that can use orthographic similarities. However, the model proposed by <ref type="bibr" target="#b15">(Naim and Gildea, 2015</ref>) is only capable of pro- ducing a parallel lexicon and not translation. Fur- thermore, both systems require the languages to have the same orthography and their vocabulary is limited to what they see during training.</p><p>Character-based decipherment is the model we use for solving this problem. Character-based decipherment has been previously applied to problems like solving letter substitution ciphers ( <ref type="bibr" target="#b9">Knight et al., 2006;</ref><ref type="bibr" target="#b21">Ravi and Knight, 2011a)</ref> or transliterating Japanese katakana into English ( <ref type="bibr" target="#b20">Ravi and Knight, 2009)</ref>, but not for translating full texts between related languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Translating RL to IL</head><p>We learn a character-based cipher model for trans- lating RL to IL. At decoding, this model is com- bined with a word based IL language model to pro- duce IL text from RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cipher Model</head><p>Our noisy-channel cipher model converts a se- quence of IL characters s 1 , ..., s n to a sequence of RL characters t 1 , ..., t m . It is a WFST composed of three components <ref type="figure">(Figure 2</ref>):</p><p>WFST1 is a one-to-one letter substitution model. For each IL character s it writes one RL character t with probability p 1 (t|s). </p><formula xml:id="formula_0">s ‚Ä≤ ,œµ,1 ... œµ,t,p 3 œµ,œµ,Œ≥ œµ,œµ,Œ± œµ,œµ,Œ≤ œµ,t,p 1 œµ,t,p 21 œµ,t ‚Ä≤ ,p 22 _,_,1</formula><p>Figure 2: Part of the cipher model correspond- ing to reading IL character s from start state. The same pattern repeats for any IL character. After reading s, the model goes to WFST1, WFST2, or WFST3 with respective probability Œ±(s), Œ≤(s), or Œ≥(s). In WFST1, the model produces each RL character t with probability p 1 (t|s). In WFST2, the model produces each two RL characters t and t ‚Ä≤ with probability p 21 (t|s) and p 22 (t ‚Ä≤ |s). In WFST3, the model reads each IL character s ‚Ä≤ and produces each RL character t with probability p 3 (t|ss ‚Ä≤ ). From the last state of WFST1, WFST2, and WFST3, the model returns to the start state without reading or writing.The model has a loop on start state that reads and writes space.</p><p>WFST2 is a one-to-two letter substitution model. For each IL character s, it writes two RL characters t and t ‚Ä≤ with respective probabili- ties p 21 (t|s) and p 22 (t ‚Ä≤ |s).</p><p>We assume p 22 (t ‚Ä≤ |s) is independent of t. As a result we can estimate p(tt ‚Ä≤ |s) = p(t|s)p(t ‚Ä≤ |t, s) ‚âÉ p 21 (t|s)p 22 (t ‚Ä≤ |s) as modeled in WFST2. This simplification is required to make the model practicable. Otherwise, the size of the cipher model would become cubic in the number of RL and IL characters, and combining it with a language model would make the system unfeasi- bly large for training.</p><p>WFST3 is a two-to-one letter substitution ci- pher. For each IL character s, it reads another IL character s ‚Ä≤ with probability 1, and then writes one RL character t with probability p 3 (t|ss ‚Ä≤ ). As we will discuss in Section 3.2 we train p 3 directly from p 21 and p 22 , hence the cubic number of pa- rameters does not cause a problem.</p><p>The start state reads each IL character s and goes to WFST1, WFST2, or WFST3 with respec- tive probability Œ±(s), Œ≤(s), or Œ≥(s). The last state of each component returns to start without reading or writing anything. The start state also reads and writes space with probability one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the Model</head><p>The cipher model described in Section 3.1 is much more flexible than a one-to-one letter substitution cipher. A few thousand sentences of RL mono- lingual data is not enough to train the model as a whole, and more training data makes the pro- cess too slow to be practical. Hence, we break the full model into WFST1, WFST2, and WFST3 and train the parameters of each component, i.e. p 1 , p 21 and p 22 , and p 3 in separate steps. A final step trains the probability of moving into each of the components, i.e. Œ±, Œ≤, and Œ≥. Each step of the training uses EM algorithm to maximize the likelihood of 500 sentences of RL text in a noisy channel model where a fixed 5- gram character based IL language model (trained on 5000 IL sentences) produces an IL text charac- ter by character and the cipher model converts RL characters into RL (top section of <ref type="figure" target="#fig_1">Figure 1</ref>).</p><p>Step one: We set Œ±(s) = 1 and Œ≤(s) = Œ≥(s) = 0 and train p 1IL‚ÜíRL (t|s) for each IL character s and each RL character t. In parallel we re- verse RL and IL and train p 1RL‚ÜíIL (s|t) for each RL character t and each IL character s. We use p 1 (t|s) = 1 2 (p 1IL‚ÜíRL (t|s) + p 1RL‚ÜíIL (s|t)) to set WFST1 parameters in the next steps.</p><p>Step two: We set Œ±(s) = Œ≤(s) = 1 2 and Œ≥(s) = 0, fix p 1 and train p 21IL‚ÜíRL (t|s) and p 22IL‚ÜíRL (t ‚Ä≤ |s) for each IL character s and each pair of RL characters t and t ‚Ä≤ . In parallel we reverse RL and IL and train p 21RL‚ÜíIL (s|t) and p 22RL‚ÜíIL (s ‚Ä≤ |t) for each RL character t and each pair of IL characters s and s ‚Ä≤ .</p><p>Step three: Our cipher model has to decide af- ter reading one IL character if it will perform a one-to-one, one-to-two or two-to-one mapping. In the first two scenarios the model has enough infor- mation to decide, but for the two-to-one mapping the model has to decide before reading the sec- ond IL character. For instance, consider convert- ing Bosnian to Serbian. When the model reads the character "c" it has to decide between one-to-one, one-to-two and two-to-one mappings. A good de- cision will be two-to-one mapping because "ch" maps to q, hence the system learns a large Œ≥ for character "c" but the same Œ≥ applies to any other afr dut bel pol bos srb dan swe mkd bul mal ind #sent. 0.9M 4M 1.9M 4M 0.8M 1.6M 4M 4M 0.9M 2.1M 0.4M 4M #tok. 19M 74M 26M 58M 17M 29M 71M 70M 16M 33M 8M 75M <ref type="table">Table 1</ref>: Size of monolingual data available for each language. IL and RL are presented in pairs, IL first. character that follows "c" which is not desirable.</p><p>One way to overcome this problem is to change the model to make the decision after reading two IL characters, but this will over-complicate the model. We use a simpler trick instead. We compute p 3IL‚ÜíRL (t|ss ‚Ä≤ ) from p 21RL‚ÜíIL (s|t) and p 22RL‚ÜíIL (s ‚Ä≤ |t) using Bayes rule:</p><formula xml:id="formula_1">p 3 (t|ss ‚Ä≤ ) = p(ss ‚Ä≤ |t)p(t) p(ss ‚Ä≤ ) ‚âÉ p 21 (s|t)p 22 (s ‚Ä≤ |t)p(t) p(ss ‚Ä≤ ) (1)</formula><p>The estimate is based on our assumption from the previous step that p 22 (s ‚Ä≤ |t) is independent of s. For each RL character t we compute the empirical probability p(t) from monolingual data and p(ss ‚Ä≤ ) is the normalization factor.</p><p>We set p 3 parameters using equation <ref type="formula">(1)</ref>, but be- fore normalizing we manually prune the probabili- ties. If for IL characters s and s ‚Ä≤ there exists no RL character t such that p 21 (s|t)p 22 (s ‚Ä≤ |t)p(t) &gt; 0.01 we assume that ss ‚Ä≤ does not map to any RL char- acter. Otherwise, we only keep RL characters for which p 21 (s|t)p 22 (s ‚Ä≤ |t)p(t) &gt; 0.01 and then ap- ply the normalization.</p><p>Step four: In the final step we fix p 1 , p 21 , p 22 , and p 3 to the trained values and train Œ±(s), Œ≤(s), and Œ≥(s) for each IL character s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>In the decoding step we compose the cipher WFST with an IL word based language model WFST and find the best path for the input sen- tence in the resulting WFST (bottom section of <ref type="figure" target="#fig_1">Figure 1</ref>). If the best path has a high enough score the model outputs the corresponding IL to- ken. Otherwise it outputs the highest scored char- acter sequence produced by the cipher model as and OOV. In our experiments we use 1-gram and 2-gram language models trained on all the existing IL monolingual data <ref type="table">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>We collect data for six pairs of related languages: Afrikaans(afr) / Dutch(dut), Bosnian(bos) / Ser- bian(srb), Danish(dan) / Swedish(swe), Macedo- nian(mkd) / Bulgarian(bul), Malaysian(mal) / In- donesian(ind), and Polish(pol) / Belorussian(bel).</p><p>For each language, we download the monolin- gual data from Leipzig corpora ( <ref type="bibr" target="#b4">Goldhahn et al., 2012)</ref>. The domain of the data is news, web, and Wikipedia. We consider the language with more data as RL and the one with less data as IL. <ref type="table">Table 1</ref> shows the size of available data for each language.</p><p>We also extract the list of alphabets for each lan- guage from Wikipedia, and collect the Universal Declaration of Human Rights (UDHR) for each IL and RL. We manually sentence align these docu- ments and get 104 sentences and about 1.5K to- kens per language. We use these documents for testing the conversion accuracy.</p><p>We tokenize and lowercase all the monolin- gual, parallel and UDHR data with Moses scripts. We remove all non-alphabetic characters from each text according to the alphabet extracted from Wikipedia. This includes numbers, punctuations, and rare/old characters that are not considered as official characters of the language. We keep all the accented variations of characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We translate the UDHR between the related lan- guages using the following methods: Copy: Copying the text. This is not applicable for languages with different orthography. LS: One-to-one Letter Substitution cipher. This is equivalent to using WFST1 without a decoding language model. LS+1g LM: One-to-one letter substitution cipher with a 1-gram word language model at decoding. PM+1g LM, PM+2g LM: The Proposed Method with respectively 1-gram and 2-gram word lan- guage model at decoding.</p><p>Results are reported for both directions of trans- lation in <ref type="table" target="#tab_0">Tables 2, and 3</ref>. For all the language pairs except Malaysian(mal) / Indonesian(ind), the pro- posed method is the best model with a large mar- gin. Malaysian/Indonesian is a special case where, although the languages have a different vocabulary and a slightly different grammar, they have a com- mon alphabet, and almost all of their cognates are exactly the same. See <ref type="figure" target="#fig_3">Figure 3</ref> for an example. As a result the proposed method cannot learn much more than copying. afr‚Üídut bel‚Üípol bos‚Üísrb dan‚Üíswe mkd‚Üíbul mal‚Üíind Copy 1.9 / 29.1 -/ - -/ - 1.2 / 17.6 5.6 / 34.2 10.0 / 42.7 LS 1.9 / 29.1 0.0 / 7.9 33.2 / 59.4 1.3 / 20.7 9.1 / 39.1 10.0 / 42.7 LS+1g LM 2.9 / 33.3 0.7 / 15.1 32.8 / 59.2 4.5 / 31.3 9.1 / 39.1 10.3 / 43.1 PM+1g LM 4.1 / 36.3 0.0 / 21.8 39.9 / 64.6 6.4 / 36.5 11.8 / 43.3 10.4 / 42.8 PM+2g LM 4.3 / 36.2 1.9 / 24.2 39.2 / 64.4 6.9 / 38.8 11.9 / 43.0 10.4 / 42.8   afr: alle menslike wesens word vry met gelyke --waardigheid en regte a2d:</p><p>alle menslike wezens werd vrij met gelijke --waardigheid en rechte dut:</p><p>alle mensen ------worden vrij en gelijk in waardigheid en rechten afr2en: all human beings are free with equal --dignity and rights a2d2en: all human beings were free with equal --dignity and straight dut2en: all people ------are free and equal in dignity and rights</p><p>Figure 4: First sentence of the first article of UDHR in Afrikaans (afr), Dutch (dut) and its conversion from Afrikaans to Dutch using PM+2-gram LM (a2d), along with their translations to English.</p><p>The proposed method translates between Ser- bian (srb) and Bosnian (bos) almost perfectly. For other pairs, we translate between a quarter and half of the words correctly, but we get few of the higher n-grams. <ref type="figure">Figure 4</ref> visualizes the conversion of the first sentence of the first article of UDHR from Afrikaans (afr) to Dutch (dut) using PM+2g LM (4.3 BLEU4, 36.2 BLEU1). Observe that 4 out of 10 tokens are translated correctly, close to the 36.2 BLEU1 score, and there is no 3 or 4- gram match. For other tokens except "menslike" the translation is either correct but non-existent in the dutch sentence (wezens = beings, met = with) or has a meaning similar enough that can be useful in the downstream applications (werd = were v.s. worden = are, gelijke = equal(noun) v.s. gelijk = equal(adjective), rechte = straight/right v.s. rechten = rights). The token "menslike" in a2d is an OOV. The model is not able to convert "menslike" (afr) to "mensen" (dut). The language model does not accept other potential conversions and passes out "menslike" (a2d) as the best output of the cipher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we present a method for translat- ing texts between closely related languages with potentially different orthography, without needing any parallel data. The only requirement is a few thousand lines of monolingual data for each lan- guage and a word language model for the target. Our experiments on six language pairs show the proposed method outperforms others that do not use parallel data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The process used for training the cipher model and decoding RL text to IL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: First sentence of the first article of UDHR in Malaysian (mal) and Indonesian (ind). These languages have a different vocabulary, but their cognates (shown in bold) are exact matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores for IL-to-RL translation of UDHR text. Format is BLEU4/BLEU1. Polish / 
Belorussian and Serbian / Bosnian have different orthographies hence copying is not applicable. 

dut‚Üíafr 
pol‚Üíbel srb‚Üíbos swe‚Üídan bul‚Üímkd ind‚Üímal 
Copy 
1.9 / 25.0 -/-
-/ -
1.2 / 18.7 5.6 / 33.5 10.0 / 41.6 
LS 
2.13 / 26.5 0.0 / 12.8 33.3 / 60.6 1.3 / 20.7 5.94 / 34.6 10.0 / 41.6 
LS+1g LM 3.07 / 27.6 0.7 / 19.7 33.0 / 60.5 3.8 / 32.7 6.9 / 37.6 10.1 / 41.7 
PM+1g LM 3.9 / 29.4 1.3 / 23.7 42.3 / 67.8 7.7 / 41.1 9.4 / 40.6 10.1 / 41.7 
PM+2g LM 5.2 / 31.2 1.9 / 25.2 42.3 / 67.8 7.6 / 41.2 10.2 / 41.3 10.0 / 41.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc>BLEU scores for RL-to-IL translation of UDHR text. Format is BLEU4/BLEU1. Polish / Belorussian and Serbian / Bosnian have different orthographies hence copying is not applicable.</figDesc><table>mal: semua manusia dilahirkan bebas 
dan samarata dari segi kemuliaan dan hakhak 
ind: semua orang 
dilahirkan merdeka dan mempunyai martabat dan hakhak yang sama 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by DARPA contract HR0011-15-C-0115. The authors would like to thank Marjan Ghazvininejad, Ulf Hermjakob and Jonathan May for their comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">All Greek to me! An automatic Greeklish to Greek transliteration system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aimilios</forename><surname>Chalamandaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Athanassios Protopapas</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Proc. LREC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A machine translation system between a pair of closely related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilyas</forename><surname>Cicekli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCIS</title>
		<meeting>ISCIS</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using related languages to enhance statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Karakanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale decipherment for out-of-domain machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building large monolingual dictionaries at the Leipzig corpora collection: From 100 to 200 Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Goldhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Quasthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klein</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine translation of very close languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Haji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubo</forename><surname>Vladislav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ANLP</title>
		<meeting>ANLP</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tagging Portuguese with a Spanish tagger using cognates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirka</forename><surname>Hana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL workshop on CrossLanguage Knowledge Induction</title>
		<meeting>ACL workshop on CrossLanguage Knowledge Induction</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Chris Brew, and Luiz Amaral</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Abo Bakr Hitham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Shaalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziedan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INFOS</title>
		<meeting>INFOS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised analysis for decipherment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishit</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a translation lexicon from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL workshop on Unsupervised lexical acquisition</title>
		<meeting>ACL workshop on Unsupervised lexical acquisition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cognates can improve statistical translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study of Indonesian-to-Malaysian MT system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Septina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Larasati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kubo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MALINDO workshop</title>
		<meeting>MALINDO workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multipath translation lexicon induction via bridge languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gideon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Arabizi-English social media statistical machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Benjira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Feature-based decipherment for large vocabulary machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iftekhar</forename><surname>Naim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation for resource-poor languages using related resource-rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining word-level and character-level models for machine translation between closely-related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√∂rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deciphering foreign language by combining language models and context vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable decipherment for machine translation via hash sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning phoneme mappings for transliteration without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian inference for Zodiac and other homophonic ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dialectal to standard Arabic paraphrasing to improve ArabicEnglish statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Salloum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL workshop on algorithms and resources for modeling of dialects and language varieties</title>
		<meeting>ACL workshop on algorithms and resources for modeling of dialects and language varieties</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Arabic dialect handling in hybrid machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sawaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine translation for closely related language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Scannell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC Workshop on Strategies for developing machine translation for minority languages</title>
		<meeting>LREC Workshop on Strategies for developing machine translation for minority languages</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Character-based PSMT for closely related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√∂rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EAMT</title>
		<meeting>EAMT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Can we translate letters?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><forename type="middle">T</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL workshop on Statistical Machine Translation</title>
		<meeting>ACL workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
