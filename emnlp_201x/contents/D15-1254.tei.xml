<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Arabic Dialect Classification with Social Media Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>feihuang@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Facebook Inc</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Arabic Dialect Classification with Social Media Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Arabic dialect classification has been an important and challenging problem for Arabic language processing, especially for social media text analysis and machine translation. In this paper we propose an approach to improving Arabic dialect classification with semi-supervised learning: multiple classifiers are trained with weakly supervised, strongly supervised, and unsupervised data. Their combination yields significant and consistent improvement on two different test sets. The dialect classification accuracy is improved by 5% over the strongly supervised classifier and 20% over the weakly supervised classifier. Furthermore, when applying the improved dialect classifier to build a Modern Standard Arabic (MSA) language model (LM), the new model size is reduced by 70% while the English-Arabic translation quality is improved by 0.6 BLEU point.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As more and more users share increasing amount of information on various social media platforms <ref type="bibr">(Facebook, Twitter, etc.</ref>), text analysis for so- cial media language is getting more important and challenging. When people share their sto- ries, opinions, post comments or tweets on so- cial media platforms, they frequently use collo- quial languages, which are more similar to spo- ken languages. In addition to typical natural lan- guage processing problems, the informal nature of social media languages presents additional chal- lenges, such as frequent spelling errors, improper casing, internet slang, spontaneity, dis-fluency and ungrammatical utterances <ref type="bibr" target="#b5">(Eisenstein, 2014)</ref>. Di- alect classification and dialect-specific processing are extra challenges for languages such as Arabic and Chinese.</p><p>Considering Arabic as an example: there are big differences between MSA and various dialec- tal Arabic: MSA is the standardized and literary variety of Arabic used in writing and in most for- mal speech. <ref type="bibr">1</ref> It is widely used in government pro- ceedings, newspapers and product manuals. Many research and linguistic resources for Arabic nat- ural language processing are based on MSA. For example, most existing Arabic-English bilingual data are MSA-English parallel sentences. The di- alect Arabic has more varieties: 5 major dialects are spoken in different regions of the Arab world: Egyptian, Gulf, Iraqi, <ref type="bibr">Levantine and Maghrebi (Zaidan and Callison-Burch, 2011</ref>). These dialects differ in morphologies, grammatical cases, vocab- ularies and verb conjugations. These differences call for dialect-specific processing and modeling when building Arabic automatic speech recogni- tion (ASR) systems or machine translation (MT) systems. Therefore, identification and classifica- tion of Arabic text is fundamental for building so- cial media Arabic speech and language processing systems.</p><p>In order to build better MT systems between Arabic and English, we first analyze the distri- bution of different Arabic dialects appearing on a very large scale social media platform, as well as their effect on Arabic-English machine translation. We propose several methods to improve the dialect classification accuracy by training models with distant supervision: a weakly supervised model is trained with data whose labels are automati-cally assigned based on authors' geographical in- formation. A strongly supervised model is trained with manually annotated data. More importantly, semi-supervised learning on large amount of unla- beled data effectively increases the classification accuracy. We also combine different classifiers to achieve even bigger improvement. When eval- uated on two test sets, the widely adopted Ara- bic Online Commentary (AOC) corpus and a test set created from the social media domain <ref type="bibr">(Facebook)</ref>, our methods demonstrate an absolute 20% improvement over the weakly supervised classi- fier, and 5% over the strongly supervised classi- fier. Furthermore, the improved classifier is ap- plied on large amount of Arabic social media text to filter out non-MSA data. An LM trained with the cleaned data is used for English-Arabic (MSA) translation. Compared with the baseline model trained with the unfiltered data, the MSA LM re- duces the training data by 85%, model size by 70%, and it brings 0.6 BLEU point ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) gain in MT.</p><p>The rest of the paper is organized as follows: in section 2 we review previous research on this topic. In section 3 we analyze the dialect distri- bution and its impact on social media data transla- tion. We present the problem formulation in sec- tion 4. In section 5 we introduce two supervised classifiers trained with weakly and strongly la- beled data. We describe different semi-supervised learning methods in section 6, followed by the combination of multiple classifiers in section 7. In section 8 we show the experimental results on di- alect classification as well as machine translation. The paper finishes with discussions and conclu- sion in section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous research on Arabic dialect identification focused on two problems: spoken dialect classi- fication for speech recognition <ref type="bibr" target="#b12">((Novotney et al., 2011</ref>) and ( <ref type="bibr" target="#b11">Lei and Hansen, 2011)</ref>), and writ- ten text dialect classification mostly for machine translation. ( <ref type="bibr" target="#b8">Habash and Rambow, 2006</ref>), <ref type="bibr" target="#b9">(Habash et al., 2008)</ref>, <ref type="bibr" target="#b4">(Diab et al., 2010)</ref> and <ref type="bibr" target="#b6">(Elfardy and Diab, 2012</ref>) developed annotation guidelines and morphology analyzer for Arabic dialect.</p><p>( <ref type="bibr" target="#b17">Zaidan and Callison-Burch, 2011</ref>) created the AOC data set by extracting reader commentary from online Arabic newspaper forums. The se- lected Arabic sentences are manually labeled with one of 4 dialect labels with the help of crowd sourcing: Egyptian, Gulf, Iraqi and Levantine. A dialect classifier using unigram features is trained from the labeled data. In the BOLT (Broad Oper- ational Language Translation) project, translation from dialectal Arabic (especially Egyptian Arabic) to English is a main problem. ( <ref type="bibr" target="#b7">Elfardy and Diab, 2013)</ref> uses the same labeled AOC data to generate token-based features and perplexity-based features for sentence level dialect identification between MSA and Egyptian Arabic. ( <ref type="bibr" target="#b16">Tillmann et al., 2014</ref>) trained feature-rich linear classifier based on lin- ear SVM then evaluated the classification between MSA and Egyptian Arabic, reporting 1.4% im- provement. All these experiments are based on the AOC corpus. The characteristics and distribution of the Arabic dialects could be different for online social media data. <ref type="bibr" target="#b2">(Darwish et al., 2014</ref>) selected Twitter data and developed models taking consid- eration of lexical, morphological, and phonologi- cal information from different dialects, then classi- fied Egyptian and MSA Arabic tweets. <ref type="bibr" target="#b1">(Cotterell and Callison-Burch, 2014</ref>) collected dialect data covering Iraqi and Maghrebi Arabic from Twitter as well.</p><p>When translating Arabic dialect into English, <ref type="bibr" target="#b15">(Sawaf, 2010)</ref> and <ref type="bibr" target="#b14">(Salloum and Habash, 2011</ref>) normalized dialect words into MSA equivalents considering character-and morpheme-level fea- tures, then translated the normalized input with MSA Arabic-English MT system. ( <ref type="bibr" target="#b19">Zbib et al., 2012</ref>) used crowd sourcing to build Levantine- English and Egyptian-English parallel data. Even with small amount of parallel corpora for each di- alect, they obtained significant gains (6-7 BLEU pts) over a baseline MSA-English MT system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Social Media Arabic Dialect Distribution and Translation</head><p>The population speaking a dialect does not nec- essarily reflect its popularity on internet and so- cial media. Many factors, such as a country's social-economic development status, internet ac- cess and government policy, play important roles.</p><p>To understand the distribution of Arabic dialects on social media, we select data from the largest so- cial media platform, Facebook. There are around one billion users sharing content in 60+ languages every day. The Arabic content comes from dif- ferent regions of the Arabic world, representative enough for our analysis. We randomly select 2700 The result is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Not surprisingly, MSA is the most widely used, accounting for 58% of sen- tences. Besides that, Egyptian Arabic is the most frequent dialect (34%), followed by Levantine and Gulf. Maghrebi is the least frequent. There are other sentences which are not labeled as Arabic dialect, such as classical Arabic, verses from the Quran, foreign words and their transliterations, etc.</p><p>We also investigate the effect of different di- alects on Arabic-English translation. We ask hu- mans to translate the Arabic sentences into En- glish to create reference translations. We build a phrase-based Arabic-English MT system with 1M sentence pairs selected from MSA Arabic-English parallel corpora (UN corpus, Arabic news corpus, etc.). <ref type="bibr">3</ref> The training and decoding procedures are similar to those described in ( <ref type="bibr" target="#b10">Koehn et al., 2007)</ref>. More details about the MT system are given in section 8. We group the source Arabic sentences into different subsets based on their dialect labels, then translate them with the MT system. We mea- sure the BLEU score for each subset, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. As expected the MSA subset has the highest BLEU scores <ref type="formula">(18)</ref>, followed by the Gulf dialect, which is somewhat similar to MSA. The translation of the Egyptian and Levantine dialects is more challenging, with BLEU scores around 10- 12, even though they are 40% of the total Arabic data. To improve Arabic-English MT quality, in- creasing the bilingual data coverage for these two dialects should be most effective, as seen in (Zbib et al., 2012). Because the Maghrebi dialect sam- ple size is too small, we do not report its BLEU score. From these experiments, we further appre- ciate the importance of accurately identifying Ara- bic dialect and building dialect-specific translation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem Formulation</head><p>In this section we present the general framework of dialect classification. Given a sentence S = {w 1 , w 2 , .., w l } generated by user u, its dialect class label d * is determined based on the following functions:</p><formula xml:id="formula_0">d * = arg max i P (d i |S, u),</formula><p>where the probability function is defined accord- ing to the following exponential model:</p><formula xml:id="formula_1">P (d i |S, u) = exp k λ k f k (d i , ·) j exp k λ k f k (d j , ·) d * = arg max i k λ k f k (d i , ·)</formula><p>.</p><formula xml:id="formula_2">Here f k (d i , ·)</formula><p>is the k-th feature function. For ex- ample. f (d i , u) models the likelihood of writing dialect d i by user u given the user's profile infor- mation. f (d i , S) models the likelihood of generat- ing sentence S with d i 's n-gram language model:</p><formula xml:id="formula_3">f (d i , S) = log p(S|d i ) = l k=1 log p d i (w k |w k−1 , ..., w k−n+1 ).</formula><p>This framework allows the incorporation of rich feature functions such as geographical, lexical, morphological and n-gram information, as seen in previous work <ref type="bibr" target="#b17">((Zaidan and Callison-Burch, 2011)</ref> , <ref type="bibr" target="#b2">(Darwish et al., 2014</ref>), ( <ref type="bibr" target="#b16">Tillmann et al., 2014)</ref> and <ref type="bibr" target="#b7">(Elfardy and Diab, 2013)</ref>). However, in this paper we focus on training classifiers with weakly and strongly labeled data, as well as semi-supervised learning methods. So we only choose the geo- graphical and text-based features. Exploration of other features will be reported in another paper.</p><p>Previous research <ref type="bibr" target="#b18">(Zaidan and Callison-Burch, 2014)</ref> indicated that the unigram model obtains the best accuracy in dialect classification. How- ever, ( <ref type="bibr" target="#b16">Tillmann et al., 2014</ref>) and ( <ref type="bibr" target="#b2">Darwish et al., 2014</ref>) exploited more sophisticated text features that lead to better accuracy on selected test set. In our experiments, we find that the unigram model does outperform bigram and trigram models, so we stick to the unigram features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Supervised Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning with Weakly Labeled Data</head><p>In the chosen social media platform, each user is associated with a unique profile, which includes user-specific information such as the age and gen- der of the user, the country where s/he is from, etc.. As different Arabic dialects are spoken in differ- ent countries, one approach is to classify a post's dialect type based on the author's country, assum- ing that there is at least a major dialect spoken in each country. This approach is not highly accu- rate, because the user's country information may be missing or inaccurate; one dialect may be spo- ken in multiple countries (for example, Egyptian is very popular in different regions of the Arabic world) and multiple dialects may be spoken in the same country; the user can post in MSA instead of dialect Arabic or a mixture of both. However, using data from certain countries as the "approxi- mate" dialect training data, we can train a baseline classifier. As the training data labels are inferred from user profiles instead of manually annotated, such data is called weakly labeled data.</p><p>According to the dialect map shown in <ref type="figure" target="#fig_2">Fig.3</ref>, we group the social media posts into the following 5 dialects according to the author's country: <ref type="table">Table 1</ref> shows the number of words for each di- alect group. Considering the dialect distribution in the social media platform (shown in <ref type="figure" target="#fig_0">Figure 1</ref>), we focus on the classification of MSA (msa) and 3 Arabic dialects: Egyptian (egy), Gulf (gul) and Levantine (lev).</p><note type="other">1. Egyptian: Egypt 2. Gulf : Saudi Arabia, United Arab Emirate, Qatar, Bahrain, Oman, Yemen 3. Levantine: Syrian, Jordan, Palestinian, Lebanese 4. Iraqi: Iraq 5. Maghrebi: Algeria, Libya, Tunisia, Morocco</note><p>We train an n-gram model for each dialect from the collected data. To train the MSA model, we select sentences from Arabic UN corpus and news collections. All the dialect and MSA models share the same vocabulary, thus perplexity can be com- pared properly. At classification time, given an in- put sentence, the classifier computes the perplex- ity for each dialect type and choose the one with minimum perplexity as the label. <ref type="table">Table 1</ref>: Corpus size (word count) of weakly and strongly labeled data for supervised learning. The weakly labeled dialect data is from Facebook based on users' country information. The strongly labeled data is manually annotated from the AOC corpus.</p><formula xml:id="formula_4">Dialect Weakly Labeled Strongly Labeled egy 22M 0.45M gul 6M 0.34M lev 8M 0.45M msa 27M 1.34M iraqi 3M 0.01M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning with Strongly Labeled Data</head><p>In the AOC corpus, every sentence's dialect type is labeled by human annotators. As these labels are gold labels, the AOC corpus is strongly labeled data. Because of the high cost of manual annota- tion, the strongly labeled data is much less than the weakly labeled data, but the higher quality makes it possible to train a better classifier. <ref type="table">Table 1</ref> shows the corpus size. Although over 50% data is MSA. Egyptian, Gulf and Levantine dialects still have significant presence while the Iraqi dialect has the least labeled data. Such distribution is consistent with what we observed from the social media data. Using these strongly labeled data, we can train a classifier that significant outperforms the weakly supervised classifier.</p><p>6 Semi-supervised Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Self-training</head><p>Given the small amount of gold labeled data from the AOC corpus and large amount of unlabeled data from the social media platform, a natural combination is semi-supervised learning. In other words, by applying the strongly supervised clas- sifier on the unlabeled data, we can obtain "auto- matically labeled" dialect data that could further improve the classification accuracy. From the so- cial media platform we select additional Arabic posts with a total of 646M words. The sizes of the newly created dialect corpora are shown in Ta- ble 2. Notice that the MSA data accounts for more than 75% of all the labeled data. We train a new classifier with these additional data. As the new labels are only from the original strong classifier, this is self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Co-Training</head><p>Another approach for automatic labeling is co- training <ref type="bibr" target="#b0">(Blum and Mitchell, 1998</ref>). With two classifier C 1 and C 2 classifying the same input sentence S with labels l 1 and l 2 , S is labeled as l only if l 1 = l 2 = l. In other words, a sentence is labeled and used to train a model only when the two classifiers agree. In our experiment we use both the weakly and strongly supervised classifiers to classify the same unlabeled data. <ref type="table">Table 2</ref> lists the sizes of the dialect corpora from co-training. Compared with the self-training approach, the co- training method filters out 25% data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Data Filtering</head><p>Because of domain mismatch, even the strongly supervised classifier does not achieve very high accuracy on the social media test set, thus there is Dialect Self-training <ref type="table">Co-training Filter  egy  73M  54M  21M  gul  46M  5.1M  2.7M  lev  34M  11.7M  2.5M  msa  493M  406M  139M  All  646M  476M  165M   Table 2</ref>: The size of dialect corpora from semi- supervised learning.</p><p>lots of noise in the automatically labeled data. To filter this noise, we only keep the sentences whose minimum perplexity score (corresponding to the winning dialect label) is smaller than any other perplexity score by a margin. Lower perplexity means higher probability of generating the sen- tence from the dialect model. In other words, sen- tence S is assigned with label l and used in model re-training if and only if perp l (S) &lt; perp k (S) × threshold, for k = l. The threshold is selected to optimize the classification accuracy on a tuning set. <ref type="table">Table 2</ref> also shows the corpora size after fil- tering. We can see that the filtered dialect is only a quarter of the self-training data. We will com- pare the three semi-supervised learning methods and evaluate the gains to dialect classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Classifier Combination</head><p>Now we have 3 types of classifiers:</p><p>1. The weakly supervised classifier trained with data whose labels are automatically assigned according to author's country; 2. The strongly supervised classifier trained with human labeled data; 3. The semi-supervised classifier trained with automatically classified data, with different data selection methods.</p><p>How should we combine them to further improve the classification accuracy? One approach is data combination: simply adding all the training data together to train a uni- fied n-gram model for each dialect. This exper- iment is straightforward but the performance is suboptimal because the classifier will be domi- nated by the model with the most training data, even though its accuracy may not be the best.</p><p>The second approach is model combination: we compute the model scores of the weakly su- pervised (w), strongly supervised (s) and semi- supervised (e) classifiers, then combines them with linear interpolation:</p><formula xml:id="formula_5">p(S|d i ) = m={w,m,e} w m p m (S|d i )</formula><p>As the dialect n-gram perplexity is computed sep- arately, the model weights w m can be tuned. In our experiments we optimize them with a tuning set from all the dialects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Dialect Classification</head><p>We already described the training data for super- vised and semi-supervised classifiers in previous sections. In this section we will compare their di- alect classification accuracies. We select two test sets: 9.5K sentences from the AOC corpus as the AOC test set and 2.3K sentences from the Face- book data set as the FB test set 4 . Both test sets have the dialect of each sentence labeled by hu- man. The accuracy is computed as the percentage of sentences whose classified label is the same as the human label. 90% of the AOC labeled data are used for training the strongly supervised clas- sifier, and the remaining 10% data containing 9.5K sentences is for evaluation. We also keep 200 sen- tences from the AOC corpus as the development set to tune the model combination parameters.  <ref type="table">Table 3</ref>: Arabic dialect classification accuracies with the weakly and strongly supervised classi- fiers, as well as the semi-supervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>In <ref type="table">Table 3</ref> we show the overall classification ac- curacies of different models on both test sets. No- tice that the weakly supervised classifier trained with 68M words obtains 68% accuracy on the AOC test set and 48% on the FB test set (row 1), which is not much higher. However, considering this classifier is trained without any human labeled dialect data, the performance is expected and can be improved with better training data and models. The strongly supervised classifier (row 2), which is trained with much less human labeled data (only 2.6M words), outperforms the weak classifier by 15%. Such a difference is consistently observed in both test tests. This confirms the significant bene- fits from the gold labeled data.</p><p>We apply the strong classifier to large amount of unlabeled data, and train several semi-supervised classifiers with these automatically labeled data. The best result is obtained with the co-training strategy, which brings significant improvement over the strongly supervised model: 2.8-4.6% (row 3), as the label noise is effectively reduced among the agreed labels from two supervised clas- sifiers. Finally, combining all three classifiers (row 1, 2 and 3) with model combination achieves the best result: about 5% improvement of the strong baseline and 20% over the weak baseline. These results demonstrate the effectiveness of combin- ing labeled and unlabeled data obtained from so- cial media platform.  <ref type="table">Table 4</ref>: Comparison of semi-supervised learning and combination methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>With semi-supervised learning, we evaluate three data selection methods: self-training, co- training and data filtering. The results are shown in <ref type="table">Table 4</ref>. Compared with the strong classifier baseline, the self-training method improves by 1% -2.4%, the co-training method improves by 2.8- 4.6%, and the data filtering method improves by 1.7-1.8%. The co-training method is the most ef- fective for both test sets because the information are from two independent classifiers. Data filter- ing is more effective for the AOC test set (which has the same domain as the baseline model) but less so for the FB test set because valuable in- domain data are filtered out.</p><p>In the same table we also compare the results from model and data combination: one from the semi-supervised co-training and the other from the strongly supervised learning. On the AOC test set, the data concatenation method is significantly  worse than the model interpolation method. Its ac- curacy is even lower than that of the supervised classifier (82.1% vs. 83.4%). However the gap is much smaller on the FB test set. The automati- cally labeled data is much more than the human labeled data, thus it dominates the combined train- ing data set, which is not a good match for the AOC test data, but is more relevant to the FB test data. In both cases, the model combination obtains better classification accuracies, where the super- vised model is assigned higher weights (0.9) and the semi-supervised model is used for smoothing, therefore the combined model is able to improve over the strong classifier.</p><p>We further analyze the classification precision for each type of dialect on both test sets in <ref type="figure" target="#fig_4">Figure  4</ref>. <ref type="figure" target="#fig_4">Figure 4a</ref> shows the result on the AOC test set. The number after the dialect type (in the parenthe- sis) is the number of sentences from that dialect. Precisions increase from the weakly supervised to the strongly supervised to the semi-supervised classifier, and the combined classifier generally outperforms all three classifiers, except for the Gulf dialect. However, considering the smaller percentage of the Gulf dialect, we still observe significant improvement overall. <ref type="figure" target="#fig_4">Figure 4b</ref> shows the result on the FB test set, where the MSA and Egyptian dialects are much more frequent than the Levantine and Gulf dialects. Improving classifica- tion on the MSA and Egyptian dialect (especially MSA) will be very helpful. We notice that the su- pervised classifier improves over the unsupervised classifier by a large margin on the MSA and Gulf dialects, but performs worse on the Egyptian and Levantine dialects. This is different from the result in the AOC test set, where the supervised classifier consistently improves over the unsupervised clas- sifier. One reason is that in the AOC test set, the training and test data are from the same corpus, thus the supervised training from in-domain data is very effective. For the FB test set, the strongly labeled data and the test data mismatch in genre and topics. The automatically labeled data is less similar to the dialect test set, thus it is less effective for the Egyptian and Levantine dialects. This fur- ther confirms the necessity of combining informa- tion from multiple sources. The combined classi- fier performs significantly better for the MSA and Gulf dialect, but slightly worse for the Egyptian and Levantine dialects. The overall result is still positive.</p><p>We also compare our approach with other di- alect classification methods on the AOC corpus, which is commonly used so the results are com- parable. Most previous work focus on the clas- sification of MSA vs. EGY dialect, and report the accuracies from 85.3% <ref type="bibr" target="#b7">(Elfardy and Diab, 2013)</ref>, 87.9% <ref type="bibr" target="#b18">(Zaidan and Callison-Burch, 2014</ref>) to 89.1% ( <ref type="bibr" target="#b16">Tillmann et al., 2014</ref>), adding morpho- logical features, using word-based unigram-model and linear SVM models. Our MSA vs. EGY dialect classification accuracy is 92.0%, the best known result on this test set. We do not use more sophisticated features; the improvement is just from the mined unlabeled data and the com- bination of different classifiers. On the FB test set, our strongly supervised classifier is the same as <ref type="bibr" target="#b18">(Zaidan and Callison-Burch, 2014</ref>), both using word-based unigram model. We see 5% gain with the combined classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Machine Translation</head><p>The motivation of this research is to handle chal- lenges from Arabic dialects to improve machine translation quality. For example, using the dialect classifier output one can build dialect-specific Arabic-English MT systems. Given an Arabic sen- tence, the system first identifies its dialect type, then translates with the corresponding MT system. When building English-to-Arabic (MSA) transla- tion systems for social media translation, the target LM trained from in-domain data is very helpful to improve the translation quality. Considering that the Arabic in-domain data contains lots of dialects, an effective dialect classifier helps filter out dialect Arabic and only keep the MSA to train a cleaner LM.</p><p>Because of the limited bilingual resources of di- alect Arabic-English, we will focus on English- Arabic MT system first. In this experiment, the training data for the English-Arabic MT system is 1M parallel sentences selected from publicly available Arabic-English bilingual data <ref type="bibr">(LDC, OPUS)</ref>. Because none of the parallel corpora is for social media translation, we select a subset closer to the social media domain by maximizing the n-gram coverage on the test domain. The de- velopment and test sets contain 700 and 892 En- glish sentences, respectively. These sentences are translated into MSA by human translators. We ap- ply the standard SMT system building procedures: pre-processing, automatic word alignment, phrase extraction, parameter tuning with MERT, and de- coding with a typical phrase-based decoder simi- lar to ( <ref type="bibr" target="#b10">Koehn et al., 2007)</ref>. The LM is trained with the target side of the parallel data, plus 200M in- domain Arabic sentences.</p><p>Using the above combined dialect classifier, we label the dialect type of each sentence in the in- domain data, filter out any non-MSA sentences and re-train the target LM. Again to keep the in- domain data clean, we also apply the threshold- based data filtering. As shown in <ref type="table">Table 5</ref>, the dialect filtering reduces the LM training data by 85%, which corresponds to 70% less memory footprint. Thanks to the cleaner LM, the transla- tion quality is also improved by 0.6 BLEU point. 5 <ref type="bibr">5</ref> Due to the challenging nature of social media data, and the lack of in-domain training data, the BLEU score is much lower than the one in news translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All</head><p>Arabic <ref type="table">Data  Filtered  MSA data  number of  sentences  200M  30M   memory  footprint  23G  6.6G</ref> BLEU score (1-reference) 12.52 13.14 <ref type="table">Table 5</ref>: Cleaned MSA LM after dialect filtering for English-Arabic(MSA) translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion and Conclusion</head><p>Existing Arabic dialect classification methods solely rely on textural features, be they n-gram language model or morphology/POS-based fea- tures. This paper utilizes authors' geographical information to train a weakly supervised dialect classifier. Using the weakly and strongly super- vised classifiers to classify and filter unlabeled data leads to several improved semi-supervised classifiers. The combination of all three signif- icantly improves the Arabic dialect classification accuracy on both in-domain and out-of-domain test sets: 20% absolute improvement over the weak baseline and 5% absolute over the strong baseline. After applying the proposed classifier to filter out Arabic dialect data, and building a cleaned MSA LM, we observe 70% model size reduction with 0.6 BLEU point gain in English- Arabic translation quality. In future work, we would like to explore more user-specific information for dialect classification, apply the classifier for Arabic-to-English MT sys- tems, and extend the approach to a larger family of languages and dialects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of various Arabic dialect on the social media platform</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,226.77,126.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU scores of different Arabic dialects in Arabic-English translation. The MT model is trained with mostly MSA-English parallel data.</figDesc><graphic url="image-2.png" coords="3,307.28,62.81,208.63,126.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Arabic dialect map, from (Zaidan and Callison-Burch, 2011).</figDesc><graphic url="image-3.png" coords="4,72.00,62.81,226.78,146.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>a) Result on the AOC test set. (b) Result on the Facebook test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification precisions by dialect. The number in parenthesis is the number of sentences from each dialect.</figDesc><graphic url="image-4.png" coords="7,72.00,62.81,226.77,150.40" type="bitmap" /></figure>

			<note place="foot" n="1"> http://en.wikipedia.org/wiki/Modern_ Standard_Arabic</note>

			<note place="foot" n="2"> The data was annotated by a translation service provider under confidentiality agreement. 3 Because existing Arabic-English bilingual corpora do not include parallel data from social media domain, increasing training data size does not increase the translation quality.</note>

			<note place="foot" n="4"> The FB test set is available for download at https://www.facebook.com/groups/2419174607/ 10153205046974608/.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multi-dialect, multi-genre corpus of informal written arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Verifiably effective arabic dialect identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on</title>
		<meeting>the 2014 Conference on</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1465" to="1468" />
		</imprint>
	</monogr>
	<note>Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Colaba: Arabic dialect annotation and processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC Workshop on Semitic Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
	<note>Mohamed Altantawy, and Yassine Benajiba</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Identifying regional dialects in online social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simplified guidelines for the creation of large scale dialectal arabic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heba</forename><surname>Elfardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentence level dialect identification in arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heba</forename><surname>Elfardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="456" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Magead: a morphological analyzer and generator for the arabic dialects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guidelines for annotation of arabic dialectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reem</forename><surname>Kanjawi-Faraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC Workshop on HLT &amp; NLP within the Arabic world</title>
		<meeting>the LREC Workshop on HLT &amp; NLP within the Arabic world</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="49" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dialect classification via text-independent training and testing for arabic, spanish, and chinese. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised arabic dialect adaptation with self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Novotney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Salloum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties</title>
		<meeting>the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arabic dialect handling in hybrid machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sawaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the 9th Conference of the Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Alonaizan</surname></persName>
		</author>
		<title level="m">Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, chapter Improved SentenceLevel Arabic Dialect Classification</title>
		<meeting>the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, chapter Improved SentenceLevel Arabic Dialect Classification</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics and Dublin City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The arabic online commentary dataset: an annotated dataset of informal arabic with high dialectal content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arabic dialect identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine translation of arabic dialects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Malchiodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stallard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Omar F Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callisonburch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
