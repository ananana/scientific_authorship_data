<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Chinese Word Representations From Glyphs Of Characters</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Ray</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan University No</orgName>
								<address>
									<addrLine>1, Sec. 4, Roosevelt Road</addrLine>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan University No</orgName>
								<address>
									<addrLine>1, Sec. 4, Roosevelt Road</addrLine>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Chinese Word Representations From Glyphs Of Characters</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="264" to="273"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose new methods to learn Chinese word representations. Chi-nese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by con-volutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chi-nese and made them public.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>No matter which target language it is, high quality word representations (also known as word "em- beddings") are keys to many natural language processing tasks, for example, sentence classi- fication <ref type="bibr" target="#b7">(Kim, 2014)</ref>, question answering ( <ref type="bibr" target="#b25">Zhou et al., 2015)</ref>, machine translation ( <ref type="bibr" target="#b18">Sutskever et al., 2014</ref>), etc. Besides, word-level representations are building blocks in producing phrase-level ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>) and sentence-level ( <ref type="bibr" target="#b8">Kiros et al., 2015)</ref> representations.</p><p>In this paper, we focus on learning Chinese word representations. A Chinese word is com- posed of characters which contain rich seman- tics. The meaning of a Chinese word is often re- lated to the meaning of its compositional charac- ters. Therefore, Chinese word embedding can be enhanced by its compositional character embed- dings ( <ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Xu et al., 2016)</ref>. Further- more, a Chinese character is composed of several graphical components. Characters with the same component share similar semantic or pronuncia- tion. When a Chinese user encounters a previ- ously unseen character, it is instinctive to guess the meaning (and pronunciation) from its graph- ical components, so understanding the graphical components and associating them with semantics help people learning Chinese. Radicals 1 are the graphical components used to index Chinese char- acters in a dictionary. By identifying the radical of a character, one obtains a rough meaning of that character, so it is used in learning Chinese word embedding ( <ref type="bibr" target="#b21">Yin et al., 2016</ref>) and character embed- ding ( <ref type="bibr" target="#b17">Sun et al., 2014;</ref><ref type="bibr" target="#b10">Li et al., 2015)</ref>. However, other components in addition to radicals may con- tain potentially useful information in word repre- sentation learning.</p><p>Our research begins with a question: Can ma- chines learn Chinese word representations from glyphs of characters? By exploiting the glyphs of characters as images in word representation learn- ing, all the graphical components in a character are considered, not limited to radicals. In our proposed methods, we render character glyphs to fixed-size grayscale images which are referred to as "character bitmaps", as illustrated in <ref type="figure">Fig.1</ref>. A similar idea was also used in ( <ref type="bibr" target="#b11">Liu et al., 2017</ref>) to help classifying wikipedia article titles into 12 cat- egories. We use a convAE to extract character fea- tures from the bitmap to represent the glyphs. It is also possible to represent the glyph of a char- acter by the graphical components in it. We do not choose this way because there is no unique way to decompose a character, and directly learn- ing representation from bitmaps is more straight- forward. Then we use the models parallel to <ref type="bibr">Skipgram (Mikolov et al., 2013a</ref>) or GloVe ( <ref type="bibr">Penning-ton et al., 2014</ref>) to learn word representations from the character glyph features. Although we only consider traditional Chinese characters in this pa- per, and the examples given below are based on the traditional characters, the same ideas and methods can be applied on the simplified characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rendered bitmaps 60 pixels</head><p>Characters Glyphs (As printed in PDF file) 60 pixels <ref type="figure">Figure 1</ref>: A Chinese character is represented as a fixed-size gray-scale image which is referred to as "character bitmap" in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background Knowledge and Related Works</head><p>To give a clear illustration of our own work, we briefly introduce the representative methods of word representation learning in Section 2.1. In Section 2.2, we will introduce some of the linguis- tic properties of Chinese, and then introduce the methods that utilize these properties to improve word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Representation Learning</head><p>Mainstream research of word representation is built upon the distributional hypothesis, that is, words with similar contexts share similar mean- ings. Usually a large-scale corpus is used, and word representations are produced from the co- occurrence information of a word and its con- text. Existing methods of producing word rep- resentations could be separated into two fami- lies ( <ref type="bibr" target="#b9">Levy et al., 2015)</ref>: count-based family <ref type="bibr" target="#b19">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b1">Bullinaria and Levy, 2007)</ref>, and prediction-based family. Word representations can be obtained by training a neural-network- based models ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b5">Collobert et al., 2011</ref>). The representative methods are briefly in- troduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">CBOW and Skipgram</head><p>Both continuous bag-of-words (CBOW) model and Skipgram model train with words and con- texts in a sliding local context window <ref type="bibr" target="#b14">(Mikolov et al., 2013a</ref>). Both of them assign each word w i with an embedding w i . CBOW predicts the word given its context embeddings, while Skip- gram predicts contexts given the word embed- ding. Predicting the occurrence of word/context in CBOW and Skipgram models could be viewed as learning a multi-class classification neural net- work (the number of classes is the size of vocab- ulary). In ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>), the authors in- troduced several techniques to improve the perfor- mance. Negative sampling is introduced to speed up learning, and subsampling frequent words is introduced to randomly discard training examples with frequent words (such as "the", "a", "of"), and has an effect similar to the removal of stop words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">GloVe</head><p>Instead of using local context windows, <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> proposed GloVe model. Train- ing GloVe word representations begins with cre- ating a co-occurrence matrix X from a corpus, where each matrix entry X ij represents the counts that word w j appears in the context of word w i . In ( <ref type="bibr" target="#b16">Pennington et al., 2014)</ref>, the authors used a harmonic weighting function for co-occurrence count, that is, word-context pairs with distance d contributes 1 d to the global co-occurrence count. Let w i be the word representation of word w i , and˜wand˜ and˜w j be the word representation of word w j as context, GloVe model minimizes the loss:</p><formula xml:id="formula_0">i,j∈ non−zero entries of X f (X ij )( w T i ˜ w j +b i + ˜ b j −log(X ij )),</formula><p>where b i is the bias for word w i , and˜band˜and˜b j is the bias for context w j . A weighting function f (X ij ) is introduced because the authors consider rare co- occurrence word-context pairs carry less informa- tion than frequent ones, and their contributions to the total loss should be decreased. The weighting function f (X ij ) is defined as below. It depends on the co-occurrence count, and the authors set pa- rameters x max = 100, α = 0.75.</p><formula xml:id="formula_1">f (X ij ) = ( X ij xmax ) α if X ij &lt; x max 1 otherwise</formula><p>In the GloVe model, each word has 2 represen- tations w and˜wand˜ and˜w. The authors suggest using w + ˜ w as the word representation, and reported improve- ments over using w only. A Chinese word is composed of a sequence of characters. The meanings of some Chinese words are related to the composition of the meanings of their characters. For example, "戰艦" (battleship), is composed of two characters, "戰" (war) and "艦" (ship). More examples are given in <ref type="figure" target="#fig_0">Fig. 2</ref>. To improve Chinese word representations with sub-word information, character-enhanced word embedding (CWE) <ref type="bibr" target="#b2">(Chen et al., 2015</ref>) in Sec- tion 2.2.2 is proposed. A Chinese character is composed of several graphical components. Characters with the same component share similar semantic or phonetic properties. In a Chinese dictionary characters with similar coarse semantics are grouped into cate- gories for the ease of searching. The common graphical component which relates to the common semantic is chosen to index the category, known as a radical. Examples are given in <ref type="figure" target="#fig_1">Fig. 3</ref>. There are three radicals in row (A), and their semantic meanings are in row (B). In each column, there are five characters containing each radical. It is easy to find that the characters having the same radical have meanings related to the radical in some as- pect. A radical can be put in different positions in a character. For example, in rows (C-1) to (C-4), the radicals are at the left hand side of a charac- ter, but in row (C-5), the radicals are at the bot- tom. The shape of a radical can be different in different positions. For example, the third radi- cal which represents "water" or "liquid" has dif- ferent forms when it is at the left hand side or the bottom of a character. Because radicals serve as a strong semantic indicator of a character, multi- granularity embedding (MGE) ( <ref type="bibr" target="#b21">Yin et al., 2016</ref>) in Section 2.2.3 incorporates radical embeddings in learning word representation. Usually the components other than radicals de- termine the pronunciation of the characters, but in some cases they also influence the meaning of a character. Two examples are given in <ref type="figure" target="#fig_2">Fig. 4</ref> 2 . Both characters in <ref type="figure" target="#fig_2">Fig. 4</ref> have the same radical "亻" (means humans) at the left hand side, but the graphical components at the right hand side also have semantic meanings related to the characters. Considering the left character "伐" (means at- tack). Its right component "戈" means "weapon", and the meaning of the character "伐" is the com- position of the meaning of its two components (a human with a weapon). None of the previous word embedding approach considers all the components of Chinese characters in our best knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improving Chinese Word Representation Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">The Chinese Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Character-enhanced Word Embedding (CWE)</head><p>The main idea of CWE is that word embedding is enhanced by its compositional character embed- dings. CWE predicts the word from both word and character embeddings of contexts, as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref> (a). For word w i , the CWE word embedding w cwe i has the following form:</p><formula xml:id="formula_2">w cwe i = w i + 1 |C(i)| c j ∈C(i) c j where</formula><p>w i is the word embedding, c j is the embed- ding of the j-th character in w i , and C(i) is the set of compositional characters of word w i . Mean value of CWE word embeddings of contexts are then used to predict the word w i . Sometimes one character has several different meanings, this is known as the ambiguity problem. To deal with this, each character is assigned with a bag of embeddings. During training, one of the embeddings is picked to form the modified word embedding. The authors proposed three methods to decide which embedding is picked: position- based, cluster-based, and non-parametric cluster- based character embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Multi-granularity Embedding (MGE)</head><p>Based on CBOW and CWE, ( <ref type="bibr" target="#b21">Yin et al., 2016)</ref> proposed MGE, which predicts target word with its radical embeddings and modified word embed- dings of context in CWE, as shown in <ref type="figure" target="#fig_3">Fig.5 (b)</ref>. There is no ambiguity of radicals, so each radi- cal is assigned with one embedding r. We denote r k as the radical embedding of character c k . MGE predicts the target word w i with the following hid- den vector:</p><formula xml:id="formula_3">h i = 1 |C(i)| c k ∈C(i) r k + 1 |W (i)| w j ∈W (i) w cwe j</formula><p>, where W(i) is the set of contexts words of w i , w cwe j is the CWE word embedding of w j . MGE picks character embeddings with the position- based method in CWE, and picks radical embed- dings according to a character-radical index built from a dictionary during training. When non- compositional word is encountered, only the word embedding is used to form h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We first extract glyph features from bitmaps with the convAE in Section 3.1. The glyph features are used to enhance the existing word representation learning models in Section 3.2. In Section 3.3, we try to learn word representations directly from the glyph features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Character Bitmap Feature Extraction</head><p>A convAE <ref type="bibr" target="#b13">(Masci et al., 2011</ref>) is used to reduce the dimensions of rendered character bitmaps and capture high-level features. The architecture of the convAE is shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. The convAE is com- posed of 5 convolutional layers in both encoder and decoder. The stride larger than one is used in- stead of pooling layers. Convolutional and decon- volutional layers on the same level share the same kernel. The input image is a 60×60 8-bit grayscale bitmap, and the encoder extracts 512-dimensional feature. The feature of character c k from the en- coder is refer to as character glyph feature g k in the paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Glyph-Enhanced Word Embedding</head><p>(GWE)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Enhanced by Context Word Glyphs</head><p>We modify CWE model based on CBOW in Sec- tion 2.2.2 to incorporate context character glyph features (ctxG). This modified word embedding w ctxG i of word w i has the form:</p><formula xml:id="formula_4">w ctxG i = w i + 1 |C(i)| c j ∈C(i) ( c j + g j ),</formula><p>where C(i) is the compositional characters of w i and g j is the glyph feature of c j . The model pre- dicts target word w i from ctxG word embeddings of contexts, as shown in <ref type="figure" target="#fig_5">Fig.7</ref>. The parameters in the convAE are pre-trained, thus not jointly learned with embeddings w and c, so character glyph features g are fixed during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Enhanced by Target Word Glyphs</head><p>Here we propose another variant. In this model, the model structure is the same as in <ref type="figure" target="#fig_5">Fig.7</ref>. The difference lies in the hidden vector used to pre- dict the target word. Instead of adding mean value of character glyph features of the contexts, it adds mean value of glyph feature of the target word (tarG), as shown in <ref type="figure">Fig.8</ref> Figure 8: Illustration of exploiting target word glyphs. Mean value of character glyph features of target words help predicting target word itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Directly Learn From Character Glyph Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">RNN-Skipgram</head><p>We learn word representation w i directly from the sequence of character glyph features { g k , c k ∈ C(i)} of word w i , with the objective of Skip- gram. As in <ref type="figure" target="#fig_6">Fig.9</ref>, a 2-layer Gated Recurrent Units (GRU) (Cho et al., 2014) network followed by 2 fully connected ELU (Clevert et al., 2015) layers produces word representation w i from input se- quence { g k } of word w i .</p><p>w i is then used to predict the contexts of w i . In the training we use nega- tive sampling and subsampling on frequent words from ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">RNN-GloVe</head><p>We modify GloVe model to directly learn from character glyph features as in <ref type="figure">Fig.10</ref>. We feed character glyph feature sequence { g k , c k ∈ C(i)}, { g k , c k ∈ C(j)} of word w i and context w j to a shared GRU network. Outputs of GRU are then fed to two different fully connected ELU layers to produce word representations w i and˜wand˜ and˜w j . The inner product of w i and˜wand˜ and˜w j is the prediction of log co-occurrence log(X ij ). We apply the same loss function with weights in GloVe. We follow ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>) and use w i + ˜ w i for evaluations of word representation.  <ref type="figure">Figure 10</ref>: Model architecture of RNN-GloVe. A shared GRU network and 2 different sets of fully connected ELU layers produce w i and˜wand˜ and˜w j . Inner product of w i and˜wand˜ and˜w j is the prediction of log co- occurrence log(X ij ).</p><p>word, LDC2003T09). All foreign words, numeri- cal words, and punctuations were removed. Word segmentation was performed using open source python package jieba 3 . In all 316,960,386 seg- mented words, we extracted 8780 unique charac- ters, and used a true type font (BiauKai) to render each character glyph to a 60×60 8-bit grayscale bitmap. Furthermore, We removed words whose frequency &lt;= 25, leaving 158,565 unique words as the vocabulary set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extracting Visual Features of Character Bitmap</head><p>Inspired by <ref type="bibr" target="#b22">(Zeiler et al., 2011</ref>), layer-wise train- ing was applied to our convAE. From lower level to higher, the kernel of each layer is trained indi- vidually, with other kernels frozen for 100 epochs. Loss function is the Euclidean distance between input and reconstructed bitmap, and we added l1 regularization to the activations of convolution layers. We chose Adagrad as the optimizing algo- rithm, and set batch size = 20 and learning rate = 0.001. The comparison between the input bitmaps and their reconstructions is shown in <ref type="figure" target="#fig_7">Fig 11.</ref> The in- put bitmaps are in the upper row, while the recon- structions are in the lower row. We further visual- ized the extracted character glyph features with t- SNE ( <ref type="bibr" target="#b12">Maaten and Hinton, 2008)</ref>. Part of the visu- alization result is shown in <ref type="figure" target="#fig_0">Fig. 12. From Fig. 12</ref>, we found that the characters with the same compo- nents are clustered. The result shows that the fea- tures extracted by the convAE are capable of ex- pressing the graphical information in the bitmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details of Word Representations</head><p>We used CWE code 4 to implement both CBOW and Skipgram, along with the CWE. The num- ber of multi-embedding was set to 3. We mod- ified the CWE code to produce GWE represen- tations. For CBOW, Skipgram, CWE, GWE and RNN-Skipgram, we used the following hyperpa- rameters. Context window was set to 5 to both sides of a word. We used 10 negative samples, and threshold t of subsampling was set to 10 −5 .</p><p>Since Yin at al. did not publish their code, we followed their paper and reproduced the MGE model. We created the mapping between charac- ters and radicals from the Unihan database <ref type="bibr">5</ref> . Each character corresponds to one of the 214 radicals in this dataset, and the same hyperparameters were used in training as above. Note that we did not separate non-compositional words during training as the original CWE and MGE did.</p><p>We used the GloVe code 6 to train the baseline GloVe vectors. In construction of co-occurrence matrix for GloVe and RNN-GloVe, we followed the parameter settings of x max = 100 and α = 0.75 in ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>). Context win- dow was 5 words to the both sides of a word, and harmonic weighting was used on co-occurrence counts. For the RNN-GloVe model, we removed entries whose value &lt; 0.5 to speed up training.</p><p>RNN-Skipgram and RNN-GloVe generated 200-dimensional word embeddings, while other models generated 512-dimensional word embed- dings.</p><p>To encourage further research, we published our convAE and embedding models on github <ref type="bibr">7</ref> . Eval- uation datasets were also uploaded, whose details will be explained in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Similarity</head><p>A word similarity test contains multiple word pairs and their human annotated similarity scores. Word representations are considered good if the calculated similarity and human annotated scores have a high rank correlation. We computed the Spearman's correlation between human annotated scores and cosine similarity of word representa- tions.</p><p>Since there is little resource for traditional Chi- nese, we translated WordSim-240 and WordSim- 296 datasets provided by <ref type="bibr" target="#b2">(Chen et al., 2015)</ref>. Note that this translation is non-trivial. Some frequent words are considered out-of-vocabulary (OOV) due to the different usage between the simplified and traditional. For example, "butter" is translated to "黃油" in simplified, but "奶油" in traditional. Besides, we manually translated SimLex-999 ( <ref type="bibr" target="#b6">Hill et al., 2016</ref>) to traditional Chinese, and used it as the third testing dataset. We also made these datasets public along with our code.</p><p>When calculating similarities, word pairs con- taining OOVs were removed. In <ref type="table">Table 1</ref> only show the results of position-based character embeddings here because the results of cluster- based character embeddings are worse in the ex- periments. We found that CWE only consis- tently improved the performance on SimLex-999 for both CBOW and Skipgram probably because SimLex-999 contains more words that could be understood from their compositional characters.</p><p>On SimLex-999, we observed that CWE was bet- ter with CBOW than Skipgram. We think the rea- son is that CBOW+CWE predicts the target word with the mean value of all character embeddings in the context, thus has a less noisy feature; however Skipgram+CWE uses character embeddings of an individual word. This noisy feature could cause negative effects on predicting the target word. The GWEs were learned based on CWE in two ways. "ctxG" represents using glyph features of context words, while "tarG" represents using glyph fea- tures of target words. The glyph features improved CWE on WordSim-240 and SimLex-999, but not WordSim-296. As for MGE results, we were not able to repro- duce the performance in ( <ref type="bibr" target="#b21">Yin et al., 2016)</ref>. We list possible reasons as below: we did not separate non-compositional word during training (charac- ter and radical embeddings are not used for these words), and the we created character-radical index from different data source. We conjecture that the first to be the most crucial factor in reproducing MGE.</p><p>The results of RNN-Skipgram and RNN-GloVe are also in <ref type="table">Table 1</ref>. Their results are not compara- ble with CBOW and Skipgram. From the results, we conclude that it is not easy to produce word representations directly from glyphs. We think the reason is that RNN representations are depen- dent on each other. Updating model parameters for word w i would also change the word represen- tation of word w j . As a result it is much more difficult to train such models.</p><p>We further inspect the impact of glyph fea- tures by doing significance test <ref type="bibr">8</ref> between proposed methods and existing ones. The p-values of the tests are given in <ref type="table">Table 2</ref>. We found only "tarG" method has a p-value less than 0.05 over CWE.  <ref type="table">Table 2</ref>: p-values of significance tests between proposed methods and existing ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word Analogy</head><p>An analogy problem has the following form: "king":"queen" = "man":"?", and "woman" is answer to "?". By answering the ques- tion correctly, the model is considered capable of expressing semantic relationships. Further- more, the analogy relation could be expressed by vector arithmetic of word representations as shown in <ref type="bibr" target="#b15">(Mikolov et al., 2013b</ref>  As in the previous subsection, we translated the word analogy dataset in <ref type="bibr" target="#b2">(Chen et al., 2015)</ref> to traditional. The dataset contains 3 groups of analogy problems: capitals of countries, (China) states/provinces of cities, and family relations. Considering that most capital and city names do not relate to the meaning of their compositional characters, and that we did not separate non- compositional word in our experiments, we pro- posed a new analogy dataset composed of jobs and places (job&amp;place). Nonetheless, there might be multiple corresponding places for a single job. For instance, A "doctor" could be in a "hospital" or "clinic". In this job&amp;place dataset, we provide a set of places for each job. The model is considered to answer correctly as long as the predicted word is in this set.</p><p>We take the mean of all word representa- tions of places (mean( w places 1 )) for the first job (job 1 ), and find the place for another job (job 2 ) by calculating w i such that w i = arg max</p><formula xml:id="formula_5">w cos( w, mean( w places 1 )− w job 1 + w job 2 ).</formula><p>The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. we observed CWE only improved accuracy only for the family group. The results are not surprising. The words of family relations are compositional in Chinese, however capital and city names are usually not. We observed that GWE further improved CWE for words in the family group. From <ref type="table" target="#tab_5">Table 3</ref>, we found that glyph features are helpful when the characters can enhance word representations. This is very reasonable because glyph features are fruit- ful representations of characters. If character in- formation does not play a role in learning word representations, character glyphs may not be use- ful. The same phenomenon is observed in <ref type="table">Table 1</ref>.</p><p>In our job&amp;place, we still observed that GWE improving CWE, however both CWE and GWE were slightly worse than CBOW. We also ob- served that Skipgram-based methods became worse than CBOW-based methods, while in all previous evaluation Skipgram-based methods are consistently better.</p><p>The results of RNN-Skipgram and RNN-GloVe are still poor. We observe that the word represen- tations learned from RNN can no longer be ex- pressed by vector arithmetic. The reason is still under investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head><p>To further probe the effect of glyph features, we show the following word pairs in SimLex-999 whose calculated cosine similarities are higher based on GWE models than CWE. The pairs may not look alike, but their components share related semantics. For example, in "伶俐" (clever), the component "利"(sharp) is compositional to the meaning of "俐"(acute), describing someone with a sharp mind. Other examples show the ability to associate semantics with radicals.  We also provide several counter-examples. Be- low are some word pairs which are not similar, however GWE methods produces higher similarity than CBOW or CWE. Take "山峰" (mountain) and "蜂蜜" (honey) as example. Since they share no  common characters, the only thing in common is the component "夆", and we assume this to be the reason for the higher similarity. Also note that in the pair "無趣" (boring) and "好笑" (funny), the CWE similarity is also higher. We conclude that the character "無" (none) is not strong enough, so the character "趣" (fun) overrides the word "無 趣" (boring), thus a higher score was mistakenly assigned.</p><formula xml:id="formula_6">Models 詞 (word) &amp; 字典 (dictionary) 椅子 (chair) &amp; 板凳 (bench) CBOW</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This work is a pioneer in enhancing Chinese word representations with character glyphs. The char- acter glyph features are directly learned from the bitmaps of characters by convAE. We then pro- posed 2 methods in learning Chinese word repre- sentations: the first is to use character glyph fea- tures as enhancement; the other is to directly learn word representation from sequences of glyph fea- tures. In experiments, we found the latter totally infeasible. Training word representations with RNN without word and character information is challenging. Nonetheless, the glyph features im- proved the character-enhanced Chinese word rep- resentations, especially on the word analogy task related to family. The results of exploiting character glyph fea- tures in word representation learning was ordi- nary. Perhaps the co-occurrence information in the corpus plays a bigger role than glyph fea- tures. Nonetheless, the idea to treat each Chi- nese character as image is innovative. As more character-level models( <ref type="bibr" target="#b24">Zheng et al., 2013;</ref><ref type="bibr" target="#b7">Kim, 2014;</ref><ref type="bibr" target="#b23">Zhang et al., 2015)</ref> are proposed in the NLP field, we believe glyph features could serve as an enhancement, and we will further examine the ef- fect of glyph features on other tasks, such as word segmentation, POS tagging, dependency parsing, or downstream tasks such as text classification, or document retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of compositional Chinese words. Still, the reader should keep in mind that NOT all Chinese words are compositional (related to the meanings of its compositional characters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Some examples of radicals and the characters containing them. In rows (C-1) to (C-4), the radicals are at the left hand side of the character, while in row (C-5), the radicals are at the bottom, and may have different of shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Both characters in the figure have the same radical "亻" (means humans) at the left hand side, but their meanings are the composition of the graphical components at the right hand side and their radical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model comparison of Characterenhanced Word Embedding (CWE) and Multigranularity Embedding (MGE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The architecture of convAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Illustration of exploiting context word glyphs. Mean value of character glyph features in the context is added to the hidden vector that predicts target word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Model architecture of RNN-Skipgram model. Produced word representation w i is used to predict the context of word w i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The input bitmaps of convAE and their reconstructions. The input bitmaps are in the upper row, while the reconstructions are in the lower row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Parts of t-SNE visualization of character glyph features. Most of the characters in the ovals share the same components.</figDesc><graphic url="image-196.png" coords="7,239.78,199.78,54.54,84.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy of analogy problems for capi-
tals of countries, (China) states/provinces of cities, 
family relations, and our proposed job&amp;place 
(J&amp;P) dataset. The higher the values, the better 
the results. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Case study on word pairs in SimLex-999. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Counter examples to which GWE meth-
ods give higher similarity scores than CBOW or 
CWE. 

</table></figure>

			<note place="foot" n="1"> https://en.wikipedia.org/wiki/ Radical_(Chinese_characters)</note>

			<note place="foot" n="2"> The two example characters here have the same glyphs in the traditional and simplified Chinese characters.</note>

			<note place="foot" n="3"> https://github.com/fxsjy/jieba</note>

			<note place="foot" n="4"> https://github.com/Leonard-Xu/CWE 5 http://unicode.org/charts/unihan.html 6 https://github.com/stanfordnlp/GloVe 7 https://github.com/ray1007/GWE</note>

			<note place="foot" n="8"> We followed the method described in https:// stats.stackexchange.com/questions/17696/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior research methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph P</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint learning of character and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan-Bo</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Component-enhanced chinese character embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="829" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning character-level compositionality with visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neu</surname></persName>
		</author>
		<idno>abs/1704.04859</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stacked convolutional autoencoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cires¸ancires¸an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Radical-enhanced chinese character embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improve chinese word embeddings by exploiting internal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1041" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-granularity chinese word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongchao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="981" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Matthew D Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning continuous word embedding with metadata for question retrieval in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
