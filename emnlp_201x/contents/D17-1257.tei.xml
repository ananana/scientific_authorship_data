<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
							<email>libofang@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
							<email>tliu@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
							<email>tangbuzhou@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Global Scientific Information and Computing Center</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
							<email>arogers@cs.uml.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2421" to="2431"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The number of word embedding models is growing every year. Most of them are based on the co-occurrence information of words and their contexts. However, it is still an open question what is the best definition of context. We provide a system-atical investigation of 4 different syntactic context types and context representations for learning word embeddings. Comprehensive experiments are conducted to evaluate their effectiveness on 6 extrinsic and intrinsic tasks. We hope that this paper , along with the published code, would be helpful for choosing the best context type and representation for a given task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there is a growing interest in word em- bedding models, where words are embedded into low-dimensional (dense) real-valued vectors. The trained word embeddings can be directly used for solving intrinsic tasks like word similarity and word analogy. They are also helpful for solv- ing extrinsic tasks, such as part-of-speech tagging, chunking, named entity recognition <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b7">Collobert et al., 2011</ref>) and text clas- sification ( <ref type="bibr" target="#b18">Kim, 2014)</ref>.</p><p>The training objectives of word embedding models are based on the Distributional Hypoth- esis <ref type="bibr" target="#b14">(Harris, 1954</ref>) that can be stated as follows: "words that occur in similar contexts tend to have similar meanings". In most word embedding mod- els, the "context" is defined as the words which precede and follow the target word within some fixed distance ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b35">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b32">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b41">Pennington et al., 2014</ref>). Among them, Global Vectors (GloVe) proposed by <ref type="bibr" target="#b41">Pennington et al. (2014)</ref>, Contin- uous Skip-Gram (CSG) <ref type="bibr">1</ref> and Continuous Bag- Of-Words (CBOW) proposed by <ref type="bibr" target="#b33">Mikolov et al. (2013b)</ref> achieve state-of-the-art results on a range of linguistic tasks, and scale to corpora with bil- lions of words.</p><p>The traditional sparse vector-space models have explored many different types of context. <ref type="bibr" target="#b8">Curran (2004)</ref>; <ref type="bibr" target="#b38">Pad√≥ and Lapata (2007)</ref>; <ref type="bibr" target="#b5">Clark (2012)</ref> have discussed a set of context definitions beyond simple linear context. For example, a sentence or document could be used as the boundary instead of window size. Contextual words could be associat- ed with their relative sides (left/right) or position- s (+1/-2) to the target word. They could also be associated with part-of-speech or grammatical re- lation labels. The weight of each contextual word can be explicitly defined. Moreover, words that are connected to target word in dependency parse  tree can be considered as context. Recent word embedding models have also ex- plored some of the above context types. <ref type="bibr" target="#b22">Levy and Goldberg (2014b)</ref>; <ref type="bibr" target="#b26">Ling et al. (2015)</ref>  <ref type="bibr">2</ref> im- prove CSG and CBOW by introducing position- aware context representation. <ref type="bibr" target="#b21">Levy and Goldberg (2014a)</ref> propose dependency-based context (DEP- S) for CSG.</p><p>However, different types of syntactic contex- t have not been systematically compared for dif- ferent word embeddings. This paper explores two context types (linear or DEPS) and two context representations (bound or unbound), as shown in <ref type="table">Table 1</ref>. Three popular word embedding models (CBOW, GloVe, and CSG) are compared on word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text clas- sification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several studies directly compare different word embedding models. <ref type="bibr" target="#b20">Lai et al. (2016)</ref> compare 6 word embedding models using different cor- pora and hyper-parameters. Nayak and Man- ning (2016) provide a set of evaluations, along with an online tool, for word embedding models. <ref type="bibr" target="#b23">Levy and Goldberg (2014c)</ref> show the theoretical equivalence of CSG and PPMI matrix factoriza- tion. <ref type="bibr" target="#b24">Levy et al. (2015)</ref> further discuss the con- nections between 4 word embedding models (PP- MI, SVD, CSG, GloVe) and re-evaluate them with the same hyper-parameters. <ref type="bibr" target="#b44">Suzuki and Nagata (2015)</ref> investigate different configurations of CS- G and GloVe and merge them together. <ref type="bibr" target="#b48">Yin and Schutze (2016)</ref> propose 4 ensemble methods and show their effectiveness over individual ones.</p><p>There is also research evaluating different con- text types in learning word embeddings. <ref type="bibr" target="#b15">Heylen et al. (2008)</ref> compares dependency-based and lin- ear vector space model for finding semantical- ly related nouns in Dutch. <ref type="bibr" target="#b45">Vulic and Korhonen (2016)</ref> compare CSG and dependency-based mod- els on various languages. Their results suggest that dependency-based models are better at detecting functional similarity in English, although that does not necessarily hold for other languages. <ref type="bibr" target="#b1">Bansal et al. (2014)</ref> show that DEPS context is preferable to linear context on parsing task. <ref type="bibr" target="#b31">Melamud et al. (2016)</ref> investigate the performance of CSG, DEP- S and a substitute-based word embedding model ( <ref type="bibr" target="#b47">Yatbaz et al., 2012)</ref>  <ref type="bibr">3</ref> , which shows that differen- t types of intrinsic tasks have clear preference for particular types of contexts. On the other hand, for extrinsic tasks, the optimal context types need to be carefully tuned on specific dataset.</p><p>The contribution of this study is that in addition to linear and dependency-based context we also consider bound and unbound context representa- tions, as will be described below. Furthermore, we systematically evaluate three word embedding models: CSG, CBOW and GLoVe.</p><p>C is thus However, odel; con- d the num- ally larger generalize ords con- pendency- ntexts cap- -word con- ence " <ref type="bibr">Ausscope".</ref> his is the other neu- e k around duced: the r w.  where lbl is the type of the dependency relation be- tween the head and the modifier (e.g. nsubj, dobj, prep with, amod) and lbl ‚àí1 is used to mark the inverse-relation. Relations that include a preposi- tion are "collapsed" prior to context extraction, by directly connecting the head and the object of the preposition, and subsuming the preposition itself into the dependency label. An example of the de- pendency context extraction is given in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Notice that syntactic dependencies are both more inclusive and more focused than bag-of- words. They capture relations to words that are far apart and thus "out-of-reach" with small win- dow bag-of-words (e.g. the instrument of discover is telescope/prep with), and also filter out "coinci- dental" contexts which are within the window but not directly related to the target word (e.g. Aus- tralian is not used as the context for discovers). In addition, the contexts are typed, indicating, for ex- ample, that stars are objects of discovery and sci- entists are subjects. We thus expect the syntactic contexts to yield more focused embeddings, cap- turing more functional and less topical similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head><p>We experiment with 3 training conditions: BOW5 (bag-of-words contexts with k = 5), BOW2 (same, with k = 2) and DEPS (dependency-based syntactic contexts). We modified word2vec to support arbitrary contexts, and to output the con- text embeddings in addition to the word embed- dings. For bag-of-words contexts we used the original word2vec implementation, and for syn- tactic contexts, we used our modified version. The negative-sampling parameter (how many negative contexts to sample for every correct one) was 15. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Embeddings Models</head><p>In this section, we first introduce different contexts in detail, and discuss their strengths and weakness- es. We then show how CSG, CBOW and GloVe can be generalized to use these contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Types</head><p>There are many different types of context, both on document and sentence level. For syntactic contexts, the current literature discusses mainly the linear (used in most word embedding models) and dependency-based contexts (DEPS ( <ref type="bibr" target="#b21">Levy and Goldberg, 2014a)</ref>). Linear context is defined as the positional neighbors of the target word in texts. DEPS context is defined as the syntactic neighbors of the target word based on dependency parse tree, as shown in <ref type="figure" target="#fig_1">Figure 1</ref> 4 .</p><p>Compared to the linear context, DEPS context can capture more relevant words that are further away from the target word in the text. For ex- ample in <ref type="figure" target="#fig_1">Figure 1</ref>, linear context does not include the word-context pair "discovers telescope", while DEPS context contains this information. DEP- S context can also exclude some uninformative word-context pairs like "with star" and "telescope with".</p><p>Note that dependency parsing is time- consuming.</p><p>Despite its parallelizability, our implementation still takes nearly a month to finish dependency parsing for the Wikipedia corpus on a 32-core machine. it is only fair to compare linear and DEPS context if we ignore the time com- plexity. it is also worth noting that part-of-speech labels are required when performing dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Representations</head><p>In the original CSG, CBOW and GloVe model- s, contexts are represented by words without any additional information. <ref type="bibr">Ling</ref>   <ref type="table">Table 2</ref>: Illustration of bound and unbound rep- resentations under linear and DEPS context types. This example is based on <ref type="figure" target="#fig_1">Figure 1</ref>, and the target word is "discovers".</p><p>CSG and CBOW by introducing position-bound words, where each contextual word is associated with their relative position to the target word. This allows CSG and CBOW to distinguish different sequential positions and capture the structural in- formation from the context. We refer to methods that bind positional information with the contextu- al word as bound (context) representation, as op- posed to unbound (context) representation where contextual words are treated the same irrespective of their positions with regards to the target word. The original DEPS uses "bound" representation by default: each word is associated with its depen- dency relation to the target word. In this paper, we also investigate the simpler context representation where no dependency relation is associated with a word. This enables a fair comparison with conven- tional models like CSG, CBOW and GloVe, since they do not use bound representation either. An example of different syntactic context types and context representations is shown in <ref type="table">Table 2</ref>.</p><p>Intuitively, bound representation should work better than unbound representation, since it uses information about relative word positions. How- ever, this is not always the case in practice. An obvious drawback is that bound representation is more sparse than unbound representation, espe- cially for DEPS context type. In our data, there were 47 dependency relations in dependency parse tree. Although not every combination of depen- dency relations and words appear in the word- context pair collection, in practice it still enlarges the contextual words' vocabulary about 5 times.</p><p>Both syntactic context types (linear and DEPS) and the choice of context representations (bound and unbound) have a dramatic effect on the word embeddings. Bound linear representation transfer- s each contextual word into a new one, and the Linear (window size 1) DEPS <ref type="table">Table 3</ref>: Illustration of collection P , M and M for sentence "australian scientist discovers star with telescope". Unbound representation is used in this example. Words in the collections are Bold. . word-context pairs are changed completely. DEP- S, as compared to the linear contexts, increases the likelihood that the contextual words are in a mean- ingful relation with the target word, although some words captured by DEPS would also be found in the linear contexts if the window is wide enough. For example, in <ref type="table">Table 2</ref>, "scientist" and "star" are considered as the contextual words of "discovers" in both linear and DEPS context types.</p><formula xml:id="formula_0">P (australian, scientist) (scientist, australian) (scientist, discovers) (discovers, scientist) (discovers, star) . . . (australian, scientist) (scientist, australian) (scientist, discovers) (discovers, scientist) (discovers, star) (discovers, telescope) . . . M (australian, scientist) (scientist, australian, discovers) (discovers, scientist, star) . . . (australian, scientist) (scientist, australian, discovers) (discovers, scientist, star, telescope) . . . M (australian, scientist, 1) (scientist, australian, 1) (scientist, discovers, 1) (discovers, scientist, 1) (discovers, star, 1) . . . (australian, scientist, 1) (scientist, australian, 1) (scientist, discovers, 1) (discovers, scientist, 1) (discovers, star, 1) (discovers, telescope, 1) . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalization</head><p>Let P be a collection of word-context pairs. P can be merged based on the words to form a collection M with size of |V |, where V is the vocabulary. Each element (w, c 1 , c 2 , .., c nw ) ‚àà M is word w and its contexts, where n w is the number of word w's contexts. P can also be merged based on both words and contexts to form a collection M . Each element (w, c, #(w, c)) ‚àà M is the word w, con- text c, and the times they appear in collection P . An example of these collections is shown in Ta- ble 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Generalized Bag-Of-Words</head><p>The objective function of Generalized Bag-Of- Words (GBOW) is defined as:</p><formula xml:id="formula_1">(w,c 1 ,..,cn w )‚ààM log p w nw i=1 ci (1)</formula><p>With negative sampling technique, the log prob- ability is calculated by:</p><formula xml:id="formula_2">log œÉ w ¬∑ nw i=1 ci ‚àí K k=1 log œÉ wN k ¬∑ nw 1=i ci (2)</formula><p>where œÉ is the sigmoid function, K is the negative sampling size, w and c is the vector for word w and c respectively. The negatively sampled word w N k is randomly selected on the basis of its uni- gram distribution ( #(w) w #(w) ) ds , where #(w) is the number of times that word w appears in the cor- pus, and ds is the distribution smoothing hyper- parameter which is usually defined as 0.75.</p><p>Note that with negative sampling technique, both GBOW and original CBOW (Mikolov et al., 2013a) will learn two sets of embeddings (word embeddings and context embeddings). In the o- riginal CBOW, the context embeddings can also be considered as word embeddings, since the vo- cabulary set of words and contexts are the same. However, for bound context, the words (i.e. scien- tist) and contexts (i.e. scientist/nsubj) are quite d- ifferent. It is necessary to distinguish conditioned and conditioning variables. For example, in <ref type="figure" target="#fig_1">Fig- ure 1</ref>, the context "scientist/nsubj" can only be predicted by word "discovers". However, most of the word is connected to several contextual word- s. Due to this, the sum of contextual word em- beddings should be used for predicting the target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Generalized Skip-Gram</head><p>For generalized Skip-Gram (GSG), the definition is more straightforward and the objective function actually needs no specification ( <ref type="bibr" target="#b22">Levy and Goldberg, 2014b</ref>). Nonetheless, in order to make it consistent with our GBOW, we also specify the conditioned and conditioning variables in the ob- jective function:</p><formula xml:id="formula_3">(w,c)‚ààP log p (w| c) = (w,c)‚ààP log œÉ ( w ¬∑ c) ‚àí K k=1 log œÉ ( wN k ¬∑ c)<label>(3)</label></formula><p>Note that this generalization does not change the nature of the models for linear context. In our pilot experiments on word analogy and word sim- ilarity, the performance of both GSG and GBOW is almost identical to their original versions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">GloVe</head><p>Unlike GSG and GBOW, GloVe explicitly opti- mizes a log-bilinear regression model based on word co-occurrence matrix. Since GloVe is al- ready a very generalized model, with the previous defined collection M , the final objective function is written as:</p><formula xml:id="formula_4">(w,c)‚ààM f (#(w, c))( w ¬∑ c + bw + bc ‚àí log #(w, c))<label>(4)</label></formula><p>where b w and b c are biases for word and contex- t. f is a non-decreasing weighting function and ensures that large #(w, c) is not over-weighted.</p><p>Note that the inputs of GSG, GBOW and Glove are the collections P , M and M respectively. Once the corpus and hyper-parameters are fixed, these collections (and thus the learned word em- beddings) are determined only by the choice of context types and representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the effectiveness of different syntactic context types and context representations on word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text clas- sification tasks. In this section we describe our models, and then report and discuss the experi- mental results on each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Embeddings</head><p>Previously, the word2vecf toolkit <ref type="bibr">5</ref>   ) is used for dependency parsing. After parsing, tokens are converted to lowercase. Words and contexts that appear fewer than 100 times in the collection P are ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Similarity Task</head><p>Word similarity task aims at producing semantic similarity scores of word pairs, which are com- pared with the human scores using Spearman's correlation. The cosine distance is used for gener- ating similarity scores between two word vectors. We use the WordSim353 ( <ref type="bibr" target="#b11">Finkelstein et al., 2001</ref>) dataset, divided into similarity and relatedness cat- egories ( <ref type="bibr" target="#b49">Zesch et al., 2008;</ref><ref type="bibr" target="#b0">Agirre et al., 2009</ref>). Previous research ( <ref type="bibr" target="#b21">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b31">Melamud et al., 2016)</ref> concluded that compared to linear context, DEPS context can capture more functional similarity (e.g. tiger/cat) rather than topical similarity (relatedness) (e.g. tiger/jungle). However, their experiments do not distinguish the <ref type="bibr">7</ref> Negative sampling size is set to 5 for SG and 2 for CBOW. Distribution smoothing is set to 0.75. No dynamic context or "dirty" sub-sampling is used. The window size is fixed to 2. The number of iterations is set to 2, 5 and 30 for SG, CBOW and GloVe respectively.    effect of different context representations: un- bound representation is used for linear contex- t ( <ref type="bibr" target="#b33">Mikolov et al., 2013b</ref>), while bound represen- tation is used for dependency-based context ( <ref type="bibr" target="#b21">Levy and Goldberg, 2014a</ref>). Moreover, only CSG mod- el is considered.</p><p>We revisit those claims with more systematical experiments. As shown in the top-left sub-figure of <ref type="figure" target="#fig_3">Figure 2</ref>, DEPS does outperform the linear con- textin GSG and GloVe in the similarity section of WordSim353, confirming its ability to capture functional similarity. However, the advantage of DEPS does not fully transfer to GBOW. Although bound DEPS context for GBOW is still the best performer, unbound DEPS context performs the worst, which shows the importance of bound vs unbound representation.</p><p>Note that the results are also reversed on Word- Sim353 relatedness section (the right subfigure of <ref type="figure" target="#fig_3">Figure 2</ref>), which shows that linear context is more suitable for capturing topical similarity.</p><p>Overall, DEPS context type does not get all the credit for capturing functional similarity. Contex- t representations play an important role for word similarity task. it is only safe to say that DEP- S context captures functional similarity with the "help" of bound representation. In contrast, lin- ear context type captures topical similarity with the "help" of unbound representation. However, the above findings come with a ma- jor caveat: a lot seems to depend on the particular dataset, in addition to the model and context type. We experimented with MEN dataset ( <ref type="bibr" target="#b3">Bruni et al., 2012</ref>), Mechanical Turk dataset (Radinsky et al., 2011), Rare Words dataset ( <ref type="bibr" target="#b28">Luong et al., 2013)</ref>, and SimLex-999 dataset ( <ref type="bibr" target="#b16">Hill et al., 2016</ref>) (Ta- ble 4), and we were not able to observe uniform trends even for datasets that are supposed to cap- ture the same relation -like the similarity part of WordSim353, Rare Words and SimLex.</p><p>Still, some models do favor a certain contex- t type for both similarity and relatedness: e.g. GBOW favors linear unbound contexts, while GLoVE in most cases prefers DEPS over the linear context. In case of GCG, however, context type needs to be optimized for the particular dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Analogy Task</head><p>Word analogy task aims at answering the question- s like "a is to a' as b is to ?", such as "Lon- don is to Britain as Tokyo is to Japan". We fol- low the evaluation protocol in <ref type="bibr" target="#b22">Levy and Goldberg (2014b)</ref>, which answers the questions using LR- Cos method ( . LRCos shows significant improvement over the traditional vec- tor offset method. We use BATS analogy dataset ( ) in our experiments.</p><p>As shown in <ref type="figure" target="#fig_6">Figure 3</ref>, context representation plays an important role in word analogy task. The choice of context representation (bound or un- bound) actually has much larger impact than the choice of context type (linear or DEPS). The re- sults on Encyclopedia category are perhaps the most evident. The performance of unbound lin- ear context and unbound DEPS context is similar. However, for most models and categories, bound representation seems to outperform unbound rep- resentation. When bound representation is used, the performance drops around 5 ‚àí 15 percent for DEPS context in terms of accuracy. This is con- sistent with the findings of <ref type="bibr" target="#b21">Levy and Goldberg (2014a)</ref>, who report that DEPS context did not work well for the analogy task.</p><p>As shown in <ref type="table" target="#tab_7">Table 5</ref>, we have also experiment- ed on two much smaller datasets: MSR analogy dataset ( <ref type="bibr" target="#b34">Mikolov et al., 2013c)</ref>, and Google analo- gy dataset <ref type="bibr" target="#b32">(Mikolov et al., 2013a</ref>) (with semantic and syntactic questions). They also show that the choice of context representation has more impact than the choice of context type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">POS, Chunking and NER Tasks</head><p>Although intrinsic evaluations like word similar- ity and word analogy tasks could provide direc- t insights about different context types and repre- sentations, they have certain methodological prob- lems ( , and the exper- imental results above cannot be directly translated to the typical uses of word embeddings in down- stream tasks ( <ref type="bibr" target="#b42">Schnabel et al., 2015;</ref><ref type="bibr" target="#b27">Linzen, 2016;</ref><ref type="bibr" target="#b4">Chiu et al., 2016</ref>). Thus extrinsic tasks should also be considered.</p><p>In this subsection, we evaluate the effective- ness of different word embedding models with dif- ferent contexts on Part-of-Speech Tagging (POS), Chunking <ref type="bibr">8</ref> and Named Entity Recognition (NER) tasks <ref type="bibr">9</ref> . For these tasks, a NLP system assigns la- bels to elements of texts. Note that in practice, one should NOT use DEPS context for POS-tagging and chunking tasks, since their labels are used in  As shown in <ref type="figure">Figure 4</ref> and <ref type="table" target="#tab_10">Table 6</ref>, GSG, GBOW and GloVe exhibit overall similar trends. When the same context type is used, bound represen- tation outperforms unbound representation on all tasks. Sequence labeling tasks are not sensitive to <ref type="bibr">10</ref> The implementation by scikit is used http:// scikit-learn.org/ syntax. For bound representation, the ignorance of syntax becomes beneficial, since it decreases the amount of noise and sparsity.</p><p>Moreover, DEPS context type works slight- ly better than linear context type in most cases. These results suggest that unbound linear contex- t (as in traditional CSG and CBOW) may not be the best choice of input word vectors for sequence labeling. Bound representations should always be used and DEPS context type is also worth consid- ering. Again, similar to the word analogy task, GloVe is more sensitive to different context repre- sentations than Skip-Gram and CBOW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Text Classification Task</head><p>Finally, we evaluate the effectiveness of differen- t word embedding models with different syntactic contexts on text classification task. Text classifi- cation is one of the most popular and well-studied tasks in natural language processing. Recently, deep neural networks achieve state-of-the-art re- sults on this task <ref type="bibr" target="#b18">Kim, 2014;</ref><ref type="bibr" target="#b9">Dai and Le, 2015</ref>   <ref type="bibr" target="#b40">(Pang and Lee, 2005</ref>), customer product re- views (CR) ( <ref type="bibr" target="#b36">Nakagawa et al., 2010)</ref>, and subjec- tivity/objectivity classification (SUBJ) ( <ref type="bibr" target="#b39">Pang and Lee, 2004</ref>). The other 2 datasets are document- level with multiple sentences: full-length movie review (RT-2k) ( <ref type="bibr" target="#b39">Pang and Lee, 2004)</ref>, and IMDB movie review (IMDB) <ref type="bibr" target="#b29">(Maas et al., 2011)</ref>  <ref type="bibr">11</ref> .</p><p>As shown in <ref type="table" target="#tab_12">Table 7</ref>, pre-trained word embed- dings outperform random word embeddings by a large margin. This strengthens the previous claim that pre-trained word embeddings are highly use- ful for text classification <ref type="bibr" target="#b17">(Iyyer et al., 2015;</ref><ref type="bibr" target="#b25">Li et al., 2017)</ref>. Unlike in the other tasks, in text clas- sification all models exhibit similar performance. Text classification has less focus on syntax and function similarity. Because of that, models with bound representation perform worse than those with unbound representation on almost all dataset- s except CR. Models with DEPS context type and linear context type are comparable. These obser- vations suggest that simple unbound linear context type (as in traditional CSG and CBOW) is still the best choice of pre-training word embeddings for text classification, which is already used in most studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper provides a first systematical investiga- tion of different syntactic context types (linear vs <ref type="bibr">11</ref> Please see <ref type="bibr" target="#b46">Wang and Manning (2012)</ref> for more detailed introduction and pre-processing of these datasets. dependency-based) and different context represen- tations (bound vs unbound) for learning word em- beddings. We evaluate GSG, GBOW and GloVe models on intrinsic property analysis tasks (word similarity and word analogy), sequence labeling tasks (POS, Chunking and NER) and text classi- fication task.</p><p>We find that most tasks have clear preference for different context types and representations. Context representation plays a more importan- t role than context type for learning word embed- dings. Only with the "help" of bound representa- tion does DEPS context capture functional similar- ity. Word analogies seem to prefer unbound rep- resentation, although performance varies by ques- tion type No matter which syntactic context type is used, bound representation is essential for se- quence labeling tasks, which benefits from its a- bility of capturing functional similarity. GSG with unbound linear context is still the best choice for text classification task. Linear context is sufficient for capturing topical similarity compared to more labor-intensive DEPS context. Words' position in- formation is generally useless for text classifica- tion, which makes bound representation contribute less to this task.</p><p>In the spirit of transparent and reproducible ex- periments, the word2vecPM toolkit 12 is pub- lished along with this paper. We hope researcher- s will take advantage of the code for further im- provements and applications to other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Basic</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependency-based context extraction example. Top: preposition relations are collapsed into single arcs, making telescope a direct modifier of discovers. Bottom: the contexts extracted for each word in the sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of dependency parse tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Correlation results for similarity and relatedness categories on WordSim353 (word similarity) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Averaged accuracy results for all Inflections, Derivation, Encyclopedia and Lexicography categories on BATS word analogy dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 : Numerical results on word similarity datasets. Best results in group are marked Bold.</head><label>4</label><figDesc></figDesc><table>Model 
Context 
Context 
Google Google MSR 
Inflectional Derivational Encyclopedia Lexicography 
Type Representation Sem 
Syn 
morphology morphology 

GSG 
linear 
unbound 
.708 
.639 .642 
.678 
.110 
.242 
.083 
bound 
.702 
.454 .653 
.668 
.111 
.208 
.099 

dep 
unbound 
.716 
.661 .644 
.691 
.122 
.253 
.095 
bound 
.600 
.307 .600 
.668 
.112 
.170 
.099 

GBOW 
linear 
unbound 
.628 
.566 .601 
.618 
.096 
.201 
.074 
bound 
.602 
.376 .569 
.572 
.091 
.157 
.081 

dep 
unbound 
.573 
.553 .520 
.496 
.094 
.216 
.076 
bound 
.495 
.248 .516 
.563 
.086 
.126 
.078 

GloVe 
linear 
unbound 
.471 
.719 .454 
.425 
.033 
.226 
.054 
bound 
.502 
.218 .542 
.559 
.044 
.129 
.095 

dep 
unbound 
.513 
.700 .525 
.491 
.043 
.227 
.063 
bound 
.402 
.121 .525 
.446 
.033 
.093 
.083 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Numerical results on word analogy datasets. Best results in group are marked Bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Numerical results on Part-of-Speech Tag-
ging, Chunking and Named Entity Recognition 
tasks. Best results in group are marked Bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head></head><label></label><figDesc>). They often need pre-trained word embeddings as inputs to improve their per- formances. Similarly to the previous evaluation of sequence labeling tasks, instead of building com- plex deep neural networks, we use a simpler clas- sification method called Neural Bag-of-Words (Li et al., 2017) to directly evaluate the word em- beddings: texts are first represented by the sum of their word vectors, then a Logistic Regression Classifier (the same as that in previous subsection)</figDesc><table>Model 

Context Context Sentence-level Document-level 
Type 
Rep. MR CR Subj RT-2k IMDB 

GSG 
linear 
unbound 76.1 78.3 90.9 83.5 
85.2 
bound 75.3 79.0 90.4 82.2 
85.2 

dep 
unbound 76.0 77.7 90.7 84.8 
85.1 
bound 75.0 77.5 90.0 84.7 
84.5 

GBOW 
linear 
unbound 74.9 77.9 90.4 82.0 
85.0 
bound 74.1 77.8 90.3 80.7 
84.1 

dep 
unbound 75.0 77.6 90.1 82.4 
84.9 
bound 73.5 78.2 89.9 80.7 
83.4 

GloVe 
linear 
unbound 73.4 76.7 89.6 79.2 
83.5 
bound 73.2 77.5 90.0 79.8 
83.4 

dep 
unbound 74.0 77.7 89.5 81.3 
83.5 
bound 72.5 76.7 88.8 79.2 
83.5 
random word embeddings 63.9 72.8 79.9 72.2 
77.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Accuracy results on 5 text classification 
datasets. Best results in group are Bold 

is built upon these text representations for classifi-
cation. 
Different word embedding models are evaluated 
on 5 text classification datasets. The first 3 dataset-
s are sentence-level: short movie review sentiment 
(MR) </table></figure>

			<note place="foot" n="1"> Many researches refer to Continuous Skip-Gram as SG. However, in order to distinguish linear (continuous) context and DEPS (dependency-based) context, we refer it as CSG.</note>

			<note place="foot" n="2"> In these two papers, the description of position-aware (bound) context are quite different. However, their ideas are actually identical.</note>

			<note place="foot" n="3"> We do not consider this type of context, since in our pilot studies it performed consistently worse than the other two context types. The same observation is also made by Melamud et al. (2016); Vulic and Korhonen (2016).</note>

			<note place="foot" n="8"> CoNLL 2000 shared task http://www.cnts.ua. ac.be/conll2000/chunking 9 CoNLL 2003 shared task http://www.cnts.ua. ac.be/conll2003/ner</note>

			<note place="foot" n="12"> The current version can be found at https:// github.com/libofang/word2vecPM. The entire code is also planned to be re-written in Python and integrated into VSMlib (http://vsm.blackbird.pw/) for easier use.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the authors of the word2vecf toolk-it and the accompanied evaluation scripts ( <ref type="bibr" target="#b24">Levy et al., 2015</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Marius Pas¬∏caPas¬∏ca, and Aitor Soroa. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="809" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R√©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intrinsic evaluation of word vectors fails to predict extrinsic performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vector space models of lexical meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook ofContemporary Semantics second edition</title>
		<imprint>
			<publisher>Wiley-Blackwell</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">From distributional to semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Richard Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Word embeddings, analogies, and machine learning: Beyond king-man + woman = queen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intrinsic evaluations of word embeddings: What can we do better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 1st Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>The 1st Workshop on Evaluating Vector Space Representations for NLP<address><addrLine>Berlin, Germany. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analogy-based detection of morphological and semantic relations with word embeddings: What works and what doesnt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of naacl-hlt</title>
		<meeting>naacl-hlt</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
	<note>Aleksandr Drozd, and Satoshi Matsuoka</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modelling word similarity: an evaluation of automatic synonymy extraction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Heylen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Geeraerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Speelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How to generate a good word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural bag-of-n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3067" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Issues in evaluating semantic spaces using word analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal Linzen</surname></persName>
		</author>
		<idno>abs/1606.07736</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The role of context types and dimensionality in learning word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1030" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>ab- s/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dependency tree-based sentiment classification using crfs with hidden variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="786" to="794" />
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating word embeddings using a representative suite of practical tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pad√≥</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A word at a time: computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<editor>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch</editor>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A unified learning framework of skip-grams and global vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">186</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Is &quot;universal syntax&quot; universally useful for learning distributed word representations? In ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">518</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning syntactic categories using paradigmatic representations of word context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Mehmet Ali Yatbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="940" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning word meta-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Using wiktionary for computing semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="861" to="866" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
