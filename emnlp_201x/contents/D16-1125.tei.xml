<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning about Pragmatics with Neural Listeners and Speakers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning about Pragmatics with Neural Listeners and Speakers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1173" to="1182"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a model for contrastively describing scenes, in which context-specific behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation , our model uses a simple feature-driven architecture (here a pair of neural &quot;lis-tener&quot; and &quot;speaker&quot; models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated without demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to 69% using existing techniques.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We present a model for describing scenes and ob- jects by reasoning about context and listener behav- ior. By incorporating standard neural modules for image retrieval and language modeling into a prob- abilistic framework for pragmatics, our model gen- erates rich, contextually appropriate descriptions of structured world representations.</p><p>This paper focuses on a reference game RG played between a listener L and a speaker S.</p><p>1. Reference candidates r 1 and r 2 are re- vealed to both players.</p><p>2. S is secretly assigned a random target t ∈ {1, 2}.</p><p>3. S produces a description d = S(t, r 1 , r 2 ), which is shown to L.</p><p>4. L chooses c = L(d, r 1 , r 2 ). . This description mentions a tree, the distinguishing object present in (a) but not in (b), and situates it with respect to other objects and events in the scene. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example drawn from a standard captioning dataset ( <ref type="bibr" target="#b20">Zitnick et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Both players win if</head><p>In order for the players to win, S's description d must be pragmatic: it must be informative, fluent, concise, and must ultimately encode an understand- ing of L's behavior. In <ref type="figure" target="#fig_0">Figure 1</ref>, for example, the owl is wearing a hat and the owl is sitting in the tree are both accurate descriptions of the target image, but only the second allows a human listener to suc- ceed with high probability. RG is the focus of many papers in the computational pragmatics literature: it provides a concrete generation task while eliciting a broad range of pragmatic behaviors, including con- versational implicature <ref type="bibr" target="#b1">(Benotti and Traum, 2009)</ref> and context dependence ( <ref type="bibr" target="#b15">Smith et al., 2013)</ref>. Exist- ing computational models of pragmatics can be di- vided into two broad lines of work, which we term the direct and derived approaches.</p><p>Direct models (see Section 2 for examples) are based on a representation of S. They learn prag- matic behavior by example. Beginning with datasets annotated for the specific task they are trying to solve (e.g. examples of humans playing RG), direct models use feature-based architectures to predict ap- propriate behavior without a listener representation. While quite general in principle, such models re- quire training data annotated specifically with prag- matics in mind; such data is scarce in practice.</p><p>Derived models, by contrast, are based on a repre- sentation of L. They first instantiate a base listener L0 (intended to simulate a na¨ıvena¨ıve, non-pragmatic listener). They then form a reasoning speaker S1, which chooses a description that causes L0 to behave correctly. Existing derived models cou- ple hand-written grammars and hand-engineered lis- tener models with sophisticated inference proce- dures. They exhibit complex behavior, but are re- stricted to small domains where grammar engineer- ing is practical.</p><p>The approach we present in this paper aims to capture the best aspects of both lines of work. Like direct approaches, we use machine learning to ac- quire a complete grounded generation model from data, without domain knowledge in the form of a hand-written grammar or hand-engineered listener model. But like derived approaches, we use this learning to construct a base model, and embed it within a higher-order model that reasons about lis- tener responses. As will be seen, this reasoning step allows the model to make use of weaker supervision than previous data-driven approaches, while exhibit- ing robust behavior in a variety of contexts.</p><p>Our goal is to build a derived model that scales to real-world datasets without domain engineering. In- dependent of the application to RG, our model also belongs to the family of neural image captioning models that have been a popular subject of recent study ( <ref type="bibr" target="#b18">Xu et al., 2015)</ref>. Nevertheless, our approach appears to be:</p><p>• the first such captioning model to reason explicitly about listeners</p><p>• the first learned approach to pragmatics that re- quires only non-pragmatic training data</p><p>Following previous work, we evaluate our model on RG, though the general architecture could be ap- plied to other tasks where pragmatics plays a core role. Using a large dataset of abstract scenes like the one shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we run a series of games with humans in the role of L and our system in the role of S. We find that the descriptions generated by our model result in correct interpretation 17% more often than a recent learned baseline system. We use these experiments to explore various other aspects of computational pragmatics, including tradeoffs be- tween adequacy and fluency, and between computa- tional efficiency and expressive power. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Direct pragmatics As an example of the direct approach mentioned in the introduction, <ref type="bibr" target="#b3">FitzGerald et al. (2013)</ref> collect a set of human-generated re- ferring expressions about abstract representations of sets of colored blocks. Given a set of blocks to describe, their model directly learns a maximum- entropy distribution over the set of logical expres- sions whose denotation is the target set. Other re- search, focused on referring expression generation from a computer vision perspective, includes that of <ref type="bibr" target="#b11">Mao et al. (2015)</ref> and <ref type="bibr" target="#b10">Kazemzadeh et al. (2014)</ref>.</p><p>Derived pragmatics Derived approaches, some- times referred to as "rational speech acts" models, include those of <ref type="bibr" target="#b15">Smith et al. (2013)</ref>, <ref type="bibr" target="#b17">Vogel et al. (2013)</ref>, <ref type="bibr" target="#b6">Golland et al. (2010)</ref>, and <ref type="bibr" target="#b12">Monroe and Potts (2015)</ref>. These couple template-driven language gen- eration with probabilistic or game-theoretic reason- ing frameworks to produce contextually appropriate language: intelligent listeners reason about the be- havior of reflexive speakers, and even higher-order speakers reason about these listeners. <ref type="bibr">Experiments (Frank et al., 2009)</ref> show that derived approaches ex- plain human behavior well, but both computational and representational issues restrict their application to simple reference games. They require domain- specific engineering, controlled world representa- tions, and pragmatically annotated training data.</p><p>An extensive literature on computational prag- matics considers its application to tasks other than RG, including instruction following <ref type="bibr" target="#b0">(Anderson et al., 1991)</ref> and discourse analysis ( <ref type="bibr" target="#b9">Jurafsky et al., 1997</ref>).</p><p>Representing language and the world In addi- tion to the pragmatics literature, the approach pro- posed in this paper relies extensively on recently de- veloped tools for multimodal processing of language and unstructured representations like images. These includes both image retrieval models, which select an image from a collection given a textual descrip- tion ( <ref type="bibr" target="#b16">Socher et al., 2014)</ref>, and neural conditional lan- guage models, which take a content representation and emit a string ( <ref type="bibr" target="#b2">Donahue et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our goal is to produce a model that can play the role of the speaker S in RG. Specifically, given a target referent (e.g. scene or object) r and a dis- tractor r , the model must produce a description d that uniquely identifies r. For training, we have ac- cess to a set of non-contrastively captioned referents {(r i , d i )}: each training description d i is generated for its associated referent r i in isolation. There is no guarantee that d i would actually serve as a good referring expression for r i in any particular context. We must thus use the training data to ground lan- guage in referent representations, but rely on reason- ing to produce pragmatics.</p><p>Our model architecture is compositional and hi- erarchical. We begin in Section 3.2 by describing a collection of "modules": basic computational prim- itives for mapping between referents, descriptions, and reference judgments, here implemented as lin- ear operators or small neural networks. While these modules appear as substructures in neural architec- tures for a variety of tasks, we put them to novel use in constructing a reasoning pragmatic speaker. Section 3.3 describes how to assemble two base models: a literal speaker, which maps from refer- ents to strings, and a literal listener, which maps from strings to reference judgments. Section 3.4 de- scribes how these base models are used to imple- ment a top-level reasoning speaker: a learned, prob- abilistic, derived model of pragmatics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Formally, we take a description d to consist of a se- quence of words  <ref type="figure">Figure 2</ref>: Diagrams of modules used to construct speaker and listener models. "FC" is a fully-connected layer (a matrix multi- ply) and "ReLU" is a rectified linear unit. The encoder modules (a,b) map from feature representations (in gray) to embeddings (in black), while the ranker (c) and describer modules (d) re- spectively map from embeddings to decisions and strings.</p><formula xml:id="formula_0">d 1 , d 2 , . . . , d n , drawn</formula><p>sentence (for purposes of this paper, a vector of in- dicator features on n-grams). These two views-as a sequence of words d i and a feature vector f (d)- form the basis of module interactions with language. Referent representations are similarly simple. Be- cause the model never generates referents-only conditions on them and scores them-a vector- valued feature representation of referents suffices. Our approach is completely indifferent to the na- ture of this representation. While the experiments in this paper use a vector of indicator features on objects and actions present in abstract scenes <ref type="figure" target="#fig_0">(Fig- ure 1</ref>), it would be easy to instead use pre-trained convolutional representations for referring to natural images. As with descriptions, we denote this feature representation f (r) for referents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modules</head><p>All listener and speaker models are built from a kit of simple building blocks for working with multi- modal representations of images and text:</p><p>1. a referent encoder E r 2. a description encoder E d 3. a choice ranker R 4. a referent describer D These are depicted in <ref type="figure">Figure 2</ref>, and specified more formally below. All modules are parameterized by weight matrices, written with capital letters W 1 , W 2 , etc.; we refer to the collection of weights for all modules together as W .</p><p>Encoders The referent and description encoders produce a linear embedding of referents and descrip- tions in a common vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referent encoder:</head><p>E r (r) = W 1 f (r) (1) Description encoder:</p><formula xml:id="formula_1">E d (d) = W 2 f (d) (2)</formula><p>Choice ranker The choice ranker takes a string encoding and a collection of referent encodings, as- signs a score to each (string, referent) pair, and then transforms these scores into a distribution over ref- erents. We write R(e i |e −i , e d ) for the probability of choosing i in contrast to the alternative; for exam- ple, R(e 2 |e 1 , e d ) is the probability of answering "2" when presented with encodings e 1 and e 2 .</p><formula xml:id="formula_2">s 1 = w 3 ρ(W 4 e 1 + W 5 e d ) s 2 = w 3 ρ(W 4 e 2 + W 5 e d ) R(e i |e −i , e d ) = e s i e s 1 + e s 2<label>(3)</label></formula><p>(Here ρ is a rectified linear activation function.)</p><p>Referent describer The referent describer takes an image encoding and outputs a description us- ing a (feedforward) conditional neural language model. We express this model as a distribution p(d n+1 |d n , d &lt;n , e r ), where d n is an indicator fea- ture on the last description word generated, d &lt;n is a vector of indicator features on all other words pre- viously generated, and e r is a referent embedding. This is a "2-plus-skip-gram" model, with local posi- tional history features, global position-independent history features, and features on the referent being described. To implement this probability distribu- tion, we first use a multilayer perceptron to com- pute a vector of scores s (one s i for each vocabulary item):</p><formula xml:id="formula_3">s = W 6 ρ(W 7 [d n , d &lt;n , e i ])</formula><p>. We then normal- ize these to obtain probabilities:</p><formula xml:id="formula_4">p i = e s i / j e s j . Finally, p(d n+1 |d n , d &lt;n , e r ) = p d n+1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Base models</head><p>From these building blocks, we construct a pair of base models. The first of these is a literal listener Desc. encoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref. encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref. encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranker</head><p>Ref. decoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref. encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Literal listener (L0)</head><p>Literal speaker (S0)</p><p>Reasoning speaker (S1) S0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L0</head><p>Sampler <ref type="figure">Figure 3</ref>: Schematic depictions of models. The literal listener L0 maps from descriptions and reference candidates to ref- erence decisions. The literal speaker S0 maps directly from scenes to descriptions, ignoring context, while the reasoning speaker uses samples from S0 and scores from both L0 and S0 to produce contextually-appropriate captions.</p><p>L0, which takes a description and a set of referents, and chooses the referent most likely to be described. This serves the same purpose as the base listener in the general derived approach described in the intro- duction. We additionally construct a literal speaker S0, which takes a referent in isolation and outputs a description. The literal speaker is used for efficient inference over the space of possible descriptions, as described in Section 3.4. L0 is, in essence, a retrieval model, and S0 is neural captioning model. Both of the base models are probabilistic: L0 pro- duces a distribution over referent choices, and S0 produces a distribution over strings. They are de- picted with shaded backgrounds in <ref type="figure">Figure 3</ref>.</p><p>Literal listener Given a description d and a pair of candidate referents r 1 and r 2 , the literal listener em- beds both referents and passes them to the ranking module, producing a distribution over choices i.</p><formula xml:id="formula_5">e d = E d (d) e 1 = E r (r 1 ) e 2 = E r (r 2 ) p L0 (i|d, r 1 , r 2 ) = R(e i |e −i , e d )<label>(4)</label></formula><p>That is, p L0 (1|d, r 1 , r 2 ) = R(e 1 |e 2 , e d ) and vice- versa. This model is trained contrastively, by solving the following optimization problem:</p><formula xml:id="formula_6">max W j log p L0 (1|d j , r j , r )<label>(5)</label></formula><p>Here r is a random distractor chosen uniformly from the training set. For each training exam- ple (r i , d i ), this objective attempts to maximize the probability that the model chooses r i as the referent of d i over a random distractor. This contrastive objective ensures that our ap- proach is applicable even when there is not a naturally-occurring source of target-distractor pairs, as previous work <ref type="bibr" target="#b6">(Golland et al., 2010;</ref><ref type="bibr" target="#b12">Monroe and Potts, 2015</ref>) has required. Note that this can also be viewed as a version of the loss described by <ref type="bibr" target="#b14">Smith and Eisner (2005)</ref>, where it approximates a likeli- hood objective that encourages L0 to prefer r i to ev- ery other possible referent simultaneously.</p><p>Literal speaker As in the figure, the literal speaker is obtained by composing a referent encoder with a describer, as follows:</p><formula xml:id="formula_7">e = E r (f (r)) p S0 (d|r) = D d (d|e)</formula><p>As with the listener, the literal speaker should be un- derstood as producing a distribution over strings. It is trained by maximizing the conditional likelihood of captions in the training data:</p><formula xml:id="formula_8">max W i log p S0 (d i |r i )<label>(6)</label></formula><p>These base models are intended to be the minimal learned equivalents of the hand-engineered speak- ers and hand-written grammars employed in previ- ous derived approaches ( <ref type="bibr" target="#b6">Golland et al., 2010</ref>). The neural encoding/decoding framework implemented by the modules in the previous subsection provides a simple way to map from referents to descriptions and descriptions to judgments without worrying too much about the details of syntax or semantics. Past work amply demonstrates that neural conditional language models are powerful enough to generate fluent and accurate (though not necessarily prag- matic) descriptions of images or structured represen- tations (Donahue et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reasoning model</head><p>As described in the introduction, the general derived approach to pragmatics constructs a base listener and then selects a description that makes it behave correctly. Since the assumption that listeners will behave deterministically is often a poor one, it is common for such derived approaches to implement probabilistic base listeners, and maximize the prob- ability of correct behavior.</p><p>The neural literal listener L0 described in the pre- ceding section is such a probabilistic listener. Given a target i and a pair of candidate referents r 1 and r 2 , it is natural to specify the behavior of a reasoning speaker as simply:</p><formula xml:id="formula_9">max d p L0 (i|d, r 1 , r 2 )<label>(7)</label></formula><p>At a first glance, the only thing necessary to im- plement this model is the representation of the literal listener itself. When the set of possible utterances comes from a fixed vocabulary ( <ref type="bibr" target="#b17">Vogel et al., 2013)</ref> or a grammar small enough to exhaustively enumer- ate ( <ref type="bibr" target="#b15">Smith et al., 2013</ref>) the operation max d in Equa- tion 7 is practical.</p><p>For our purposes, however, we would like the model to be capable of producing arbitrary utter- ances. Because the score p L0 is produced by a discriminative listener model, and does not factor along the words of the description, there is no dy- namic program that enables efficient inference over the space of all strings.</p><p>We instead use a sampling-based optimization procedure. The key ingredient here is a good pro- posal distribution from which to sample sentences likely to be assigned high weight by the model lis- tener. For this we turn to the literal speaker S0 described in the previous section. Recall that this speaker produces a distribution over plausible de- scriptions of isolated images, while ignoring prag- matic context. We can use it as a source of candi- date descriptions, to be reweighted according to the expected behavior of L0. The full specification of a sampling neural reasoning speaker is as follows:</p><formula xml:id="formula_10">1. Draw samples d 1 , . . . d n ∼ p S0 (·|r i ). 2. Score samples: p k = p L0 (i|d k , r 1 , r 2 ). 3. Select d k with k = arg max p k .</formula><p>While primarily to enable efficient inference, we can also use the literal speaker to serve a differ- ent purpose: "regularizing" model behavior towards choices that are adequate and fluent, rather than ex- ploiting strange model behavior. Past work has re-stricted the set of utterances in a way that guaran- tees fluency. But with an imperfect learned listener model, and a procedure that optimizes this listener's judgments directly, the speaker model might acci- dentally discover the kinds of pathological optima that neural classification models are known to ex- hibit ( <ref type="bibr" target="#b7">Goodfellow et al., 2014</ref>)-in this case, sen- tences that cause exactly the right response from L0, but no longer bear any resemblance to human lan- guage use. To correct this, we allow the model to consider two questions: as before, "how likely is it that a listener would interpret this sentence cor- rectly?", but additionally "how likely is it that a speaker would produce it?"</p><p>Formally, we introduce a parameter λ that trades off between L0 and S0, and take the reasoning model score in step 2 above to be:</p><formula xml:id="formula_11">p k = p S0 (d k |r i ) λ · p L0 (i|d k , r 1 , r 2 ) 1−λ (8)</formula><p>This can be viewed as a weighted joint probability that a sentence is both uttered by the literal speaker and correctly interpreted by the literal listener, or al- ternatively in terms of Grice's conversational max- ims <ref type="bibr" target="#b8">(Grice, 1970)</ref>: L0 encodes the maxims of qual- ity and relation, ensuring that the description con- tains enough information for L to make the right choice, while S0 encodes the maxim of manner, en- suring that the description conforms with patterns of human language use. Responsibility for the maxim of quantity is shared: L0 ensures that the model doesn't say too little, and S0 ensures that the model doesn't say too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate our model on the reference game RG described in the introduction. In particular, we con- struct instances of RG using the Abstract Scenes Dataset introduced by <ref type="bibr" target="#b19">Zitnick and Parikh (2013)</ref>. Example scenes are shown in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure">Figure  4</ref>. The dataset contains pictures constructed by hu- mans and described in natural language. Scene rep- resentations are available both as rendered images and as feature representations containing the identity and location of each object; as noted in Section 3.1, we use this feature set to produce our referent rep- resentation f (r). This dataset was previously used for a variety of language and vision tasks (e.g. <ref type="bibr" target="#b13">Ortiz et al. (2015)</ref>, <ref type="bibr" target="#b20">Zitnick et al. (2014)</ref>). It consists of 10,020 scenes, each annotated with up to 6 captions.</p><p>The abstract scenes dataset provides a more chal- lenging version of RG than anything we are aware of in the existing computational pragmatics literature, which has largely used the TUNA corpus of isolated object descriptions ( <ref type="bibr" target="#b5">Gatt et al., 2007)</ref> or small syn- thetic datasets <ref type="bibr" target="#b15">(Smith et al., 2013)</ref>. By contrast, the abstract scenes data was generated by humans look- ing at complex images with numerous objects, and features grammatical errors, misspellings, and a vo- cabulary an order of magnitude larger than TUNA. Unlike previous work, we have no prespecified in- domain grammar, and no direct supervision of the relationship between scene features and lexemes.</p><p>We perform a human evaluation using Amazon Mechanical Turk. We begin by holding out a de- velopment set and a test set; each held-out set con- tains 1000 scenes and their accompanying descrip- tions. For each held-out set, we construct two sets of 200 paired (target, distractor) scenes: All, with up to four differences between paired scenes, and Hard, with exactly one difference between paired scenes. (We take the number of differences between scenes to be the number of objects that appear in one scene but not the other.)</p><p>We report two evaluation metrics. Fluency is determined by showing human raters isolated sen- tences, and asking them to rate linguistic quality on a scale from 1-5. Accuracy is success rate at RG: as in <ref type="figure" target="#fig_0">Figure 1</ref>, humans are shown two images and a model-generated description, and asked to select the image matching the description.</p><p>In the remainder of this section, we measure the tradeoff between fluency and accuracy that results from different mixtures of the base models (Sec- tion 4.1), measure the number of samples needed to obtain good performance from the reasoning lis- tener (Section 4.2), and attempt to approximate the reasoning listener with a monolithic "compiled" lis- tener (Section 4.3). In Section 4.4 we report final accuracies for our approach and baselines.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">How good are the base models?</head><p>To measure the performance of the base models, we draw 10 samples d jk for a subset of 100 pairs (r 1,j , r 2,j ) in the Dev-All set. We collect human flu- ency and accuracy judgments for each of the 1000 total samples. This allows us to conduct a post-hoc search over values of λ: for a range of λ, we com- pute the average accuracy and fluency of the high- est scoring sample. By varying λ, we can view the tradeoff between accuracy and fluency that results from interpolating between the listener and speaker model-setting λ = 0 gives samples from p L0 , and λ = 1 gives samples from p S0 . <ref type="figure" target="#fig_2">Figure 5</ref> shows the resulting accuracy and fluency for various values of λ. It can be seen that relying entirely on the listener gives the highest accuracy but degraded fluency. However, by adding only a very small weight to the speaker model, it is possible to achieve near-perfect fluency without a substantial decrease in accuracy. Example sentences for an in- dividual reference game are shown in <ref type="figure" target="#fig_2">Figure 5</ref>; in- creasing λ causes captions to become more generic. For the remaining experiments in this paper, we take λ = 0.02, finding that this gives excellent perfor- mance on both metrics.</p><p>On the development set, λ = 0.02 results in an average fluency of 4.8 (compared to 4.8 for the lit- eral speaker λ = 1). This high fluency can be con- firmed by inspection of model samples <ref type="figure">(Figure 4</ref>). We thus focus on accuracy or the remainder of the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">How many samples are needed?</head><p>Next we turn to the computational efficiency of the reasoning model. As in all sampling-based infer- ence, the number of samples that must be drawn from the proposal is of critical interest-if too many samples are needed, the model will be too slow to use in practice. Having fixed λ = 0.02 in the pre- ceding section, we measure accuracy for versions of the reasoning model that draw 1, 10, 100, and 1000 samples. Results are shown in <ref type="table" target="#tab_1">Table 1</ref>. We find that gains continue up to 100 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Is reasoning necessary?</head><p>Because they do not require complicated inference procedures, direct approaches to pragmatics typi- cally enjoy better computational efficiency than de- rived ones. Having built an accurate derived speaker, can we bootstrap a more efficient direct speaker?</p><p>To explore this, we constructed a "compiled" speaker model as follows: Given reference candi- dates r 1 and r 2 and target t, this model produces embeddings e 1 and e 2 , concatenates them together into a "contrast embedding" [e t , e −t ], and then feeds this whole embedding into a string decoder mod- ule. Like S0, this model generates captions without the need for discriminative rescoring; unlike S0, the contrast embedding means this model can in prin- ciple learn to produce pragmatic captions, if given access to pragmatic training data. Since no such training data exists, we train the compiled model on     <ref type="table" target="#tab_2">Table 2</ref>: Success rates at RG on abstract scenes. "Literal" is a captioning baseline corresponding to the base speaker S0. "Contrastive" is a reimplementation of the approach of <ref type="bibr" target="#b11">Mao et al. (2015)</ref>. "Reasoning" is the model from this paper. All differences between our model and baselines are significant (p &lt; 0.05, Binomial).</p><p>captions sampled from the reasoning speaker itself.</p><p>This model is evaluated in <ref type="table">Table 3</ref>. While the distribution of scores is quite different from that of the base model (it improves noticeably over S0 on scenes with 2-3 differences), the overall gain is negligible (the difference in mean scores is not sig- nificant). The compiled model significantly under- performs the reasoning model. These results sug- gest either that the reasoning procedure is not easily approximated by a shallow neural network, or that example descriptions of randomly-sampled training pairs (which are usually easy to discriminate) do not provide a strong enough signal for a reflex learner to recover pragmatic behavior.  <ref type="table">Table 3</ref>: Comparison of the "compiled" pragmatic speaker model with literal and explicitly reasoning speakers. The mod- els are evaluated on subsets of the development set, arranged by difficulty: column headings indicate the number of differences between the target and distractor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Final evaluation</head><p>Based on the following sections, we keep λ = 0.02 and use 100 samples to generate predictions. We evaluate on the test set, comparing this Reason- ing model S1 to two baselines: Literal, an image captioning model trained normally on the abstract scene captions (corresponding to our L0), and Con- trastive, a model trained with a soft contrastive ob- jective, and previously used for visual referring ex- pression generation ( <ref type="bibr" target="#b11">Mao et al., 2015)</ref>.</p><p>Results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Our reasoning model outperforms both the literal baseline and pre- vious work by a substantial margin, achieving an im- provement of 17% on all pairs set and 15% on hard </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an approach for learning to gen- erate pragmatic descriptions about general referents, even without training data collected in a pragmatic context. Our approach is built from a pair of sim- ple neural base models, a listener and a speaker, and a high-level model that reasons about their outputs in order to produce pragmatic descriptions. In an evaluation on a standard referring expression game, our model's descriptions produced correct behavior in human listeners significantly more often than ex- isting baselines. It is generally true of existing derived approaches to pragmatics that much of the system's behavior re- quires hand-engineering, and generally true of di- rect approaches (and neural networks in particular) that training is only possible when supervision is available for the precise target task. By synthesiz- ing these two approaches, we address both prob- lems, obtaining pragmatic behavior without domain knowledge and without targeted training data. We believe that this general strategy of using reasoning to obtain novel contextual behavior from neural de- coding models might be more broadly applied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample output from our model. When presented with a target image (a) in contrast with a distractor image (b), the model generates a description (c). This description mentions a tree, the distinguishing object present in (a) but not in (b), and situates it with respect to other objects and events in the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Tradeoff between speaker and listener models, controlled by the parameter λ in Equation 8. With λ = 0, all weight is placed on the literal listener, and the model produces highly discriminative but somewhat disfluent captions. With λ = 1, all weight is placed on the literal speaker, and the model produces fluent but generic captions.</figDesc><graphic url="image-3.png" coords="7,72.00,41.08,226.79,170.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>prefer L0) 0.0 a hamburger on the ground 0.1 mike is holding the burger (prefer S0) 0.2 the airplane is in the sky</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Captions for the same pair with varying λ. Changing λ alters both the naturalness and specificity of the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(Figure 4 :Figure 4 :</head><label>44</label><figDesc>Figure 4: Figure 4: Four randomly-chosen samples from our model. For each, the target image is shown on the left, the distractor image is shown on the right, and description generated by the model is shown below. All descriptions are fluent, and generally succeed in uniquely identifying the target scene, even when they do not perfectly describe it (e.g. (c)). These samples are broadly representative of the model's performance (Table 2).</figDesc><graphic url="image-10.png" coords="8,111.24,173.30,80.27,64.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 6: Descriptions of the same image in different contexts. When the target scene (b) is contrasted with the left (a), the system describes a bat; when the target scene is contrasted with the right (c), the system describes a snake.</figDesc><graphic url="image-14.png" coords="9,74.76,59.45,71.60,57.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>(a) referent encoder Er (b) description encoder Ed (c) choice ranker R</head><label></label><figDesc></figDesc><table>from a vo-
cabulary of known size. For encoding, we also as-
sume access to a feature representation f (d) of the 

sentence 

FC 
FC 

ReLU 
Sum 
FC 
Softmax 

ReLU 
FC 
Softmax 
FC 

ngram 
features 
desc 
ref 
features 
referent 

referent 

wordn 

word&lt;n 
wordn+1 

choice 

referent 

desc 

(d) referent describer D 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : S1 accuracy vs. number of samples.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 ).</head><label>2</label><figDesc></figDesc><table>Dev acc. (%) Test acc. (%) 

Model 
All 
Hard 
All 
Hard 

Literal (S0) 
66 
54 
64 
53 
Contrastive 
71 
54 
69 
58 
Reasoning (S1) 83 
73 
81 
68 

</table></figure>

			<note place="foot" n="1"> Models, human annotations, and code to generate all tables and figures in this paper can be found at http://github. com/jacobandreas/pragma.</note>

			<note place="foot" n="2"> For comparison, a model with hand-engineered pragmatic behavior-trained using a feature representation with indicators on only those objects that appear in the target image but not the distractor-produces an accuracy of 78% and 69% on all and hard development pairs respectively. In addition to performing slightly worse than our reasoning model, this alternative approach relies on the structure of scene representations and cannot be applied to more general pragmatics tasks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">Gurman</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwyneth</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Kowtko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Miller</surname></persName>
		</author>
		<title level="m">The HCRC map task corpus. Language and speech</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A computational account of comparative implicatures for a spoken dialogue agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciana</forename><surname>Benotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="4" to="17" />
		</imprint>
	</monogr>
	<note>Proceedings of the Eighth International Conference on Computational Semantics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning distributions over logical forms for referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Informative communication in word production and word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Noah D Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual conference of the cognitive science society</title>
		<meeting>the 31st annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1228" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating algorithms for the generation of referring expressions using a balanced corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ielka</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Sluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
	<note>Proceedings of the Eleventh European Workshop on Natural Language Generation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Logic and conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herbert P Grice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic detection of discourse structure for speech recognition and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Ess-Dykema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02283</idno>
		<title level="m">Generation and comprehension of unambiguous object descriptions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning in the Rational Speech Acts model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 20th Amsterdam Colloquium</title>
		<meeting>20th Amsterdam Colloquium<address><addrLine>Amsterdam, December. ILLC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to interpret and describe abstract scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis Gilberto Mateos</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1505" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning and using language via recursive pragmatic reasoning about other agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nathaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3039" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emergence of Gricean maxims from multi-agent decision theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bodoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1072" to="1081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bringing semantics into focus using visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3009" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adopting abstract images for semantic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
