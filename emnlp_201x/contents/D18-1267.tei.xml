<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentence Compression for Arbitrary Languages via Multilingual Pivoting</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2453</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
							<email>J.Mallinson@ed.ac.uk, {rsennric,mlap}@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentence Compression for Arbitrary Languages via Multilingual Pivoting</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2453" to="2464"/>
							<date type="published">October 31-November 4, 2018. 2018. 2453</date>
						</imprint>
					</monogr>
					<note>Compression dataset for English, German, and French which can be used to evaluate com-pression models across languages and genres.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models. Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length. Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release 1 MOSS, a new parallel Multilingual</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization <ref type="bibr" target="#b23">(Jing, 2000;</ref><ref type="bibr" target="#b33">Madnani et al., 2007;</ref><ref type="bibr" target="#b51">Woodsend and Lapata, 2010;</ref><ref type="bibr" target="#b2">Berg-Kirkpatrick et al., 2011</ref>), subtitle generation ( <ref type="bibr" target="#b50">Vandeghinste and Pan, 2004;</ref><ref type="bibr" target="#b32">Luotolahti and Ginter, 2015)</ref>, and the display of text on small-screens <ref type="bibr" target="#b12">(Corston-Oliver, 2001</ref>).</p><p>The bulk of research on sentence compression has focused on a simplification of the task in- volving exclusively word deletion <ref type="bibr" target="#b27">(Knight and Marcu, 2002;</ref><ref type="bibr" target="#b38">Riezler et al., 2003;</ref><ref type="bibr" target="#b48">Turner and Charniak, 2005;</ref><ref type="bibr" target="#b35">McDonald, 2006;</ref><ref type="bibr" target="#b9">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b10">Cohn and Lapata, 2009)</ref>, whereas a few approaches view sentence compression as a more general text rewriting problem ( <ref type="bibr" target="#b17">Galley and McKeown, 2007;</ref><ref type="bibr" target="#b51">Woodsend and Lapata, 2010;</ref><ref type="bibr" target="#b11">Cohn and Lapata, 2013)</ref>. Irrespective of how the compression task is formulated, most previ- ous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, in- cluding sentence compression ( <ref type="bibr" target="#b39">Rush et al., 2015;</ref><ref type="bibr" target="#b14">Filippova et al., 2015;</ref><ref type="bibr" target="#b25">Kikuchi et al., 2016)</ref>. <ref type="bibr" target="#b14">Filippova et al. (2015)</ref> focus on deletion-based sentence compression which they model as a sequence labeling problem using a re- current neural network with long short-term mem- ory units (LSTM; Hochreiter and Schmidhuber 1997). <ref type="bibr" target="#b39">Rush et al. (2015)</ref> capture the full gamut of rewrite operations drawing insights from encoder- decoder models recently proposed for machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>.</p><p>Neural network-based approaches are data- driven, relying on the ability of recurrent archi- tectures to learn continuous features without re- course to preprocessing tools or syntactic infor- mation (e.g., part-of-speech tags, parse trees). In order to achieve good performance, they re- quire large amounts of training data, in the re- gion of millions of long-short sentence pairs. <ref type="bibr">2</ref> Existing compression datasets are several orders of magnitude smaller. For example, the Ziff- Davis corpus <ref type="bibr" target="#b27">(Knight and Marcu, 2002</ref>) con- tains 1,067 sentences and originated from a col- lection of news articles on computer products. <ref type="bibr" target="#b9">Clarke and Lapata (2008)</ref> create two manual cor- pora sampled from written (1,433 sentences) and spoken sources (1,370 sentences). Cohn and La- pata (2013) elicit manual compressions for 625 sentences taken from newspaper articles. More recently, <ref type="bibr" target="#b47">Toutanova et al. (2016)</ref> crowdsource a larger corpus which contains manual compres- sions for single and multiple sentences (about 26,000 pairs of source and compressed texts).</p><p>Since large scale compression datasets do not occur naturally, they must be somehow approx- imated, e.g., by pairing headlines with the first sentence of a news article ( <ref type="bibr" target="#b15">Filippova and Altun, 2013;</ref><ref type="bibr" target="#b39">Rush et al., 2015)</ref>. As a result, the train- ing corpus construction process must be repeated and reconfigured for new languages and domains (e.g., many headline-first sentence pairs are spu- rious and need to be filtered using language and domain specific heuristics). And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions.</p><p>Our work addresses the paucity of data for sen- tence compression models. We argue that multi- lingual corpora are a rich source for learning a va- riety of rewrite rules across languages and that ex- isting neural machine translation (NMT) models <ref type="bibr" target="#b46">(Sutskever et al. 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al. 2015</ref>) can be easily adapted to the compression task through bilingual pivoting <ref type="bibr" target="#b34">(Mallinson et al., 2017</ref>) coupled with methods which decode the output sequence to a desired length (e.g., subject to language and genre requirements). We obtain compressions by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length ( <ref type="bibr" target="#b25">Kikuchi et al., 2016)</ref>. Our model can be trained for any language as long as a bilingual corpus is available, and can perform arbitrary rewrites while taking advantage of multiple pivots if these exist.We also demon- strate that models trained on multilingual data per- form well out-of-domain.</p><p>Although our approach does not employ com- pression corpora for training, for evaluation pur- poses, we create MOSS, a new Multilingual Compression dataset for English, French, and Ger- man. MOSS is a parallel corpus containing doc- uments from the European parliament proceed- ings, TED talks, news commentaries, and the EU bookshop. Each document is written in English, French, and German, and compressed by native speakers of the respective language who process a document at a time. We obtain five compres- sions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources <ref type="bibr" target="#b9">(Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b11">Cohn and Lapata, 2013;</ref><ref type="bibr" target="#b31">de Loupy et al., 2010</ref>) our corpus is cu- rated manually, however it differs from <ref type="bibr" target="#b47">Toutanova et al. (2016)</ref> in that it contains compressions for individual sentences, not documents.</p><p>There has been relatively little interest in com- pressing languages other than English. A few models have been proposed for Japanese ( <ref type="bibr" target="#b22">Hori and Furui, 2004;</ref><ref type="bibr" target="#b20">Hirao et al., 2009;</ref><ref type="bibr" target="#b18">Harashima and Kurohashi, 2012)</ref>, including a neural network model ( <ref type="bibr" target="#b19">Hasegawa et al., 2017</ref>) which repurposes Filippova and Altun's (2013) data construction method for Japanese. There is a compression cor- pus available for French (de <ref type="bibr" target="#b31">Loupy et al., 2010)</ref>, however, we are not aware of any modeling work on this language. Overall, there are no standard- ized datasets in languages other than English, ei- ther for training or testing.</p><p>Our contributions in this work are three-fold: a novel application of bilingual pivoting to sen- tence compression; corroborated by empirical re- sults showing that our model scales across lan- guages and text genres without additional supervi- sion over and above what is available in the bilin- gual parallel data; and the release of a multilin- gual, multi-reference compression corpus which can be effectively used to gain insight in the compression task and facilitate further research in compression modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pivot-based Neural Compression</head><p>In our pivot-based sentence compression model an input sequence is first translated into a for- eign language, and then back into the source lan- guage. Unlike previous paraphrasing pivoting models <ref type="bibr" target="#b34">(Mallinson et al., 2017)</ref>, we parameterize our translation models with a length feature, which allows us to produce compressed output. We de- fine two models, performing compression in one step or alternatively in two steps which affords more flexibility in model output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NMT Background</head><p>In the neural encoder-decoder framework for MT ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b46">Sutskever et al., 2014</ref>), an encoder takes in a source X = (x 1 , ..., x T x ) of length T x and the decoder generates a target se- quence (y 1 , ..., y T y ) of length T y . Let h i be the hid- den state of the source symbol at position i, ob- tained by concatenating the forward and backward encoder RNN hidden states,</p><formula xml:id="formula_0">h i = [ − → h i ; ← − h i ].</formula><p>We de- viate from previous work ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b46">Sutskever et al., 2014</ref>) in that we initialize the de- coder with the average of the hidden states, fol- lowing :</p><formula xml:id="formula_1">s 0 = tanh(W init ∑ T x i=1 h i T x )<label>(1)</label></formula><p>where W init is a learnt parameter. Our decoder is a conditional recurrent neural network, specifically a gated recurrent unit (GRU, <ref type="bibr" target="#b6">Cho et al., 2014</ref>) with attention, which we denote as cGRU att . cGRU att takes as input the previous hidden state s j−1 , the source annotations C = h 1 , ..., h T x , and the previ- ously decoded symbol y j−1 in order to update its hidden state s j , which is used to decode symbol y j at position j:</p><formula xml:id="formula_2">s j = cGRU att (s j−1 , y j−1 ,C)<label>(2)</label></formula><p>cGRU att consists of three components. The first combines the previously decoded symbol y j−1 and the previous hidden state s j−1 to generate an inter- mediate representation s j . The attention mecha- nism, AT T , inputs the entire context set C along with intermediate hidden state s j in order to com- pute the context vector c j :</p><formula xml:id="formula_3">c j = AT T (C, s j ) = T x ∑ i α i j h i<label>(3)</label></formula><formula xml:id="formula_4">α i j = exp(e i j ) ∑ T x k=1 exp(e k j )<label>(4)</label></formula><formula xml:id="formula_5">e i j = f (s j , h i )<label>(5)</label></formula><p>Where α i j is the normalized alignment weight be- tween the source symbol at position i and the tar- get symbol at position j, and f is a feedfoward neural network. Finally, we generate s j , the hidden state of cGRU att , by using the intermediate representa- tion s j and the context vector c j . Given s j , y j−1 , and c j the output probability p(y j |s j , y j−1 , c j ) is computed using a feedforward neural network with a softmax activation. We define the proba- bility of sequence y as:</p><formula xml:id="formula_6">P(y|x; θ) = T y ∏ j=1 p(y j |s j , y j−1 , c j )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Length Control</head><p>To be able to produce compressed sentences, we parameterize our model with a length vector which allows to control the output length. Our approach is similar to the LenInit model of <ref type="bibr" target="#b25">Kikuchi et al. (2016)</ref>, however we use a GRU instead of an LSTM. The hidden state of the decoder consists of the average of the encoder's hidden states but also a length vector LV , a learnt parameter, which is scaled by the desired target length T y . We there- fore rewrite Equation (1) as follows:</p><formula xml:id="formula_7">s 0 = tanh W init ∑ T x i=1 h i T x ; LV · T y<label>(7)</label></formula><p>As such we now define our model as:</p><formula xml:id="formula_8">P(y|x, T y ; θ)<label>(8)</label></formula><p>During training, the target length is set to T y = T y . However, at test time, the target length generally varies according to the domain, genre, and lan- guage at hand. We determine the target length ex- perimentally based on a small validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pivoting</head><p>Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source lan- guage to the target by taking advantage of paths through an intermediate language. The idea dates back at least to <ref type="bibr" target="#b24">Kay (1997)</ref>, who observed that am- biguities in translating from one language onto an- other may be resolved if a translation into some third language is available, and has met with suc- cess in phrase-based SMT ( <ref type="bibr" target="#b52">Wu and Wang, 2007;</ref><ref type="bibr" target="#b49">Utiyama and Isahara, 2007</ref>) and more recently in neural MT systems <ref type="bibr" target="#b16">(Firat et al., 2016)</ref>. We use pivoting to provide a path from a source English sentence, via an intermediate foreign lan- guage, to English in a compressed form. We pro- pose to extend <ref type="bibr">Mallinson et al.'s (2017)</ref> approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, F x = { f 1 , ..., f K }. The probability of generating compression y = y 1 ...y T y is decomposed as:</p><formula xml:id="formula_9">P(y|x) = F x ∑ f P(y| f ; − → θ ) · P( f |x; ← − θ )<label>(9)</label></formula><p>which we approximate as the tokenwise weighted average of the pivots:</p><formula xml:id="formula_10">P(y|x) ≈ T y ∏ j=1 F x ∑ f P(y j |y &lt; j , f )P( f |x)<label>(10)</label></formula><p>where y &lt; j = y 1 , ...y j . To ensure a probability dis- tribution, we normalize the K-best list F x , such that the translation probabilities sum to one. We use beam search to decode tokens by conditioning on multiple pivoting sentences. The results with the best decoding scores are considered candidate compressions. To ensure the model produces compressed out- put, we extend the pivoting approach in two ways. In single step compression, one of the translation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.6 CR 0.8 CR EncDec</head><p>Figure 1: Histograms of output lengths at three compression rates (CR) compared to a vanilla encoder- decoder system which does not manipulate output length. German is used as pivot for English, and English as pivot for French and German. models is parameterized with length information:</p><formula xml:id="formula_11">P(y|x, T y ) ≈ F ∑ f P(y| f , T y ; − → θ ) · P( f |x; ← − θ )</formula><p>In dual-step compression, we parameterize both translation models with length information:</p><formula xml:id="formula_12">P(y|x, T y , T y ) ≈ F ∑ f P(y| f , T y ; − → θ )·P( f |x, T y ; ← − θ )</formula><p>We find that dual-compression performs better when the system is expected to drastically com- press the source sentence (e.g., in a headline gen- eration task). Imposing a high compression ra- tio from the start tends to produce unintelligible text. The model attempts to reduce the length of the source at all costs, even at the expense of be- ing semantically faithful to the input. Performing two moderate compressions in succession reduces both length and content conservatively and as a re- sult produces more meaningful text. In <ref type="figure">Figure 1</ref> we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions. We show the output of a single-step compression model on three languages initialized with varying compres- sion rates 3 (see Section 4 for details on how the models were trained and tested). The compression rate (CR) is used to determine length parameter of Equation <ref type="formula" target="#formula_8">(8)</ref>:</p><formula xml:id="formula_13">T y = T x ·CR<label>(11)</label></formula><p>The figure shows how the output length varies compared to a vanilla encoder-decoder system which uses pivoting to backtranslate the source</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head><p>French German On the very day that the earthquake struck, the European Council asked the High Representative and the Commis- sion to mobilise all appropriate assis- tance.</p><p>Le jour même du tremblement de terre, le Conseil européen a demandédemandé`demandéà la haute représentante etàet`età la Commission de mobiliser toute l'aide appropriée.</p><p>Am gleichen Tag, an dem das Erdbeben ausbrach, ersuchte der Europäische Rat die Hohe Vertreterin und die Kom- mission um die Mobilisierung aller angemessenen Hilfe. Assistance was mobilized on the very day of the earthquake.</p><p>Le Conseil européen a demandédemandé`demandéà la haute reprsentante etàet`età la Commission de mobiliser l'aide.</p><p>Europa erbrachte Hilfe noch am selben Tag.</p><p>We're at a tipping point in human his- tory, a species poised between gaining the stars and losing the planet we call home.</p><p>L'histoire humaine estàest`està un tournant. Notre espèce hésitè a toucher lesétoilesles´lesétoiles où a perdre la planète qui est la sienne.</p><p>Wir stehen vor einem historischen Wen- depunkt: zwischen dem Griff nach den Sternen und dem Verlust unseres Heimatplaneten. We're at tipping point in human history, poised between gaining the stars and losing the Earth.</p><p>L'humanité estàest`està un tourt. Notre espèce a envie desétoilesdes´desétoiles ou perdre sa planète.</p><p>Wir sind vor einem historischen Wen- depunkt: zwischen dem Griff nach Ster- nen und Verlust unseres Planeten. Surveys undertaken by the World Bank in developing countries show that when poor people are asked to name the three most important concerns they face good health is always mentioned.</p><p>Les were asked to compress while preserving the most important information, ensuring the sentences re- mained grammatical and meaning preserving. An- notators were encouraged to use any rewriting op- erations that seemed appropriate, e.g., to delete words, add new words, substitute them, or reorder them. Annotation proceeded on a document-by- document basis, line-by-line. Crowdworkers com- pressed the first twenty lines of each document and we elicited five compression per document. Exam- ple compressions are shown in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="table" target="#tab_2">Table 2</ref> presents various statistics on our cor- pus. As can be seen, Europarl contains the longest sentences across languages (see column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between. We also observe that crowdworkers compress the least when it comes to TED (see column CR), which is not surprising given the brevity of the utterances. Overall, French speakers seem more conservative when shortening sentences compared to English and German. In general, compression rates are genre dependent, they range from 0.58 (for En- glish Europarl) to 0.84 (for German TED). We also examined the degree to which crowdworkers para- phrase the source sentence using Translation Edit Rate (TER; <ref type="bibr" target="#b45">Snover et al., 2006</ref>), a measure com- monly used to automatically evaluate the quality of machine translation output. We used TER to compute the (average) number of edits required to change a long sentence to shorter output. We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert long to short sentences. We observe that crowdworkers perform a fair amount of rewriting across corpora and lan- guages. The most frequent rewrite operations are deletions followed by substitutions, shifts, and in- sertions.</p><note type="other">enquêtes menées par la Banque mondiale dans les pays en développement montrent que, quand on demande aux populations pauvres de nommer les trois défis les plus importants qu'ils rencontrent, leur "bonne santé" fait toujours partie de cette liste. Umfragen der Weltbank in Entwick- lungsländern zeigen, wenn man Arme nach den drei wichtigsten Anliegen fragt, die sie beschäftigen, wird "Gesundheit" immer genannt. World Bank surveys in developing countries show poor people always name good health as an important con- cern. Quand on demande aux populations pauvres de nommer les trois défis les plus importants qu'ils rencontrent, leur "bonne santé" fait toujours partie de la liste. Umfragen in Entwicklungslndern zeigen, dass bei Armen das wichtigste Anliegen Gesundheit ist.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Neural Machine Translation Training Nema- tus ( ) was used as the ma- chine translation system for all our experiments. We generally used the default settings and training procedures as specified within Nematus. All net- works have a hidden layer size of 1,000, and an embedding layer size of 512. In addition, layer normalization ( <ref type="bibr" target="#b0">Ba et al., 2016</ref>) was used. Dur- ing training, we used ADAM ( <ref type="bibr" target="#b26">Kingma and Ba, 2014</ref>), a minibatch size of 80, and the training set was reshuffled between epochs. We also employed early stopping.   <ref type="bibr">German→English (31.19)</ref>. German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated mono- lingual training data, from the news domain, ( <ref type="bibr" target="#b41">Sennrich et al., 2016a</ref>) in training for the German systems.</p><p>The data was pre-processed using standard scripts found in MOSES ( <ref type="bibr" target="#b29">Koehn et al., 2007)</ref>. Rare words were split into sub-word units, using byte pair encoding (BPE; <ref type="bibr" target="#b42">Sennrich et al. 2016b</ref>). The BPE operations are shared between language directions.</p><p>We experimented with various model variants using one or multiple pivots. The compression rate (see Equation <ref type="formula" target="#formula_8">(8)</ref>) was tuned experimentally on the validation set which consists of one doc- ument from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly comparable to those shown in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="bibr">4</ref> BLEU scores were calculated using mteval-v13a.pl.</p><p>Comparison Systems We compared our model against ABS, a sequence-to-sequence attention- based model, developed by <ref type="bibr" target="#b39">Rush et al. (2015)</ref>. This model was trained on a monolingual dataset extracted from the Annotated English Gigaword corpus <ref type="bibr" target="#b36">(Napoles et al., 2011</ref>). The dataset con- sists of approximately 4 million pairs of the first sentence from each source document and its head- line. We also trained LenInit ( <ref type="bibr" target="#b25">Kikuchi et al., 2016)</ref> on the same corpus which is conceptually sim- ilar to ABS but additionally controls the output length using a length embedding vector (as de- scribed in Section 2.2). <ref type="bibr">5</ref> Unfortunately, we could not train these models for French or German, since there are no monolingual sentence compression datasets available at a similar scale. An obvious workaround is to translate Gigaword to French and German and then train compression models on the translated data. As the quality of the translation is relatively poor, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or Ger- man. Finally, we include a prefix (Pfix) baseline which does not perform any rewriting but simply truncates the source sentence so that it matches the compression ratio of the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>MOSS Evaluation We assessed model perfor- mance using three automatic metrics which rep- resent different aspects of the compression task and have been found to correlate well with hu- man judgments ( <ref type="bibr" target="#b47">Toutanova et al., 2016;</ref><ref type="bibr" target="#b8">Clarke and Lapata, 2006</ref>). These include a recall met- ric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four 6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1). We used the Stanford neural network parser <ref type="bibr" target="#b4">(Chen and Manning, 2014</ref>) to obtain dependency triples. <ref type="table" target="#tab_4">Table 3</ref>(a) reports results on English with a model which controls the output length (L) and uses either a single pivot (SP; K = 1) or multi- ple pivots (MP; K = 10). We experimented with French (fr) or German (de) as pivot languages. All pivot-based models perform compression in a single step (see Section 2.3). Dual-step compres-  pivot languages: English (en), French (fr), German (de); ABS ( <ref type="bibr" target="#b39">Rush et al., 2015)</ref> and <ref type="bibr">LenInit (Kikuchi et al., 2016)</ref> are sequence-to-sequence models trained on Gigaword; Gold is inter-annotator agreement.</p><formula xml:id="formula_14">English RS-R D2-R R2-F1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head><p>French German ABS Europe urged to help quake victims. Le Conseil Européen demande une aide pour les victimes du tremblement de terre.</p><p>Europäischer Rat sucht Hilfen für Quiz-Opfer.</p><p>SP The European Council called on the High Representative and the Commis- sion to mobilise all appropriate assis- tance.</p><p>Le Conseil Européen a demandé au Haut Représentant etàet`età la Commission de mobiliser l'assistance.</p><p>Am selben Tag forderte der Eu- ropäische Rat die Hohe Vertreterin und die Kommission auf, jede Hilfe.</p><p>ABS Advance for Sunday July a new look at the world.</p><p>Un tournant pour le tournant. Die Stars der Stars und die Stars.</p><p>SP We are at a turning point in human history and losing the planet we call home.</p><p>L'histoire de l'humanité estàest`està la croisée des chemins et de l'histoire.</p><p>Zwischen dem Griff der Sterne und dem Verlust unseres Planeten stehen wir vor. ABS Poor people ask to name the three most important concerns.</p><p>Les enquêtes de la Banque mondiale révèlent que la santé fait toujours par- tie de la liste.</p><p>Weltbank-Umfragen zeigen arme Menschen in Entwicklungsländern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SP Polls conducted by the World Bank</head><p>show that when poor people are asked to mention the three main concerns.</p><p>Les enquêtes menées par la Banque mondiale dans les pays en développement montrent que, lorsqu'on demande aux pauvres de nommer les trois plus grandséfisgrands´grandséfis.</p><p>Wenn man die Armen nach den drei Hauptanliegen fragt, werden sie gefordert. <ref type="table">Table 4</ref>: System output for the example source sentences in <ref type="table" target="#tab_0">Table 1.</ref> sion obtained inferior results which we omit for the sake of brevity. As can be seen, models which use a single pivot are better than those using mul- tiple ones (German is better than French; see SP de vs SP f r ). More pivots might introduce noise at the expense of translation quality.</p><p>Overall, pivot-based models outperform ABS and LenInit. This is perhaps to be expected since these models are tested on out of domain data with different vocabulary and writing conventions; MOSS does not contain any newspaper articles. Unfortunately, it is not possible to train ABS and LenInt on in-domain data as compression data only exists for the headlines-first sentences pairs. As an upper bound, we also report how well hu- mans agree with each other, treating one (ran- domly selected) reference as system output and computing how it agrees with the rest (row Gold in <ref type="table" target="#tab_4">Table 3</ref>). All models lag significantly behind human performance on this task. Tables 3(b) and 3(c) report results on French and German, respectively. For these languages, we obtained best results with English as pivot, using a single-step compression model. ABS and LenInit perform poorly when trained directly on transla- tions of Gigaword into French and German; their performance improves considerably when they are trained on the Gigaword and used to compress En- glish translations of French or German (ABS en , LenInit en ). Again, we observe that our models (SP L,en , MP L,en ) outperform the comparison sys- tems across all metrics and that using a single pivot yields better compressions. Example com- pressions are given in <ref type="table">Table 4</ref> where we show out- put produced by ABS and SP for each language (see the supplementary material for more exam- ples). Finally, notice that automatic scores for the prefix baseline across languages are misleadingly high, since it simply repeats the source sentence up to a fixed length without performing any rewriting.   <ref type="table">Table 6</ref>: Statistics of model output (SP L ) on MOSS (aggregated across domains): length of source (SL) and target (TL), compression rate (CR), TER scores, and number of insertions (Ins), deletions (Del), substitutions (Sub), and shifts (Shft).</p><p>We also elicited human judgments through the Crowdflower platform. We asked crowdworkers to rate the grammaticality of the target compressions and whether they preserved the most important in- formation from the source. In both cases, they used a five-point rating scale where a high num- ber indicates better performance. We randomly selected 25 sentences from each corpus from the test portion of MOSS, i.e., 100 long-short sen- tence pairs per language. We compared compres- sions generated by our model (SP L ), with ABS models for the three languages, the prefix base- line, and (randomly selected) gold-standard ref- erence (Ref) compressions from MOSS. All sys- tems used the length parameter to allow compar- isons with approximately the same compression rates. We collected five ratings per compression. Our results are summarized in <ref type="table" target="#tab_6">Table 5</ref>. We show mean ratings for grammaticality (Gram), impor- tance (Imp) and their combination (column Avg). Across languages our model (SP L ) significantly (p &lt; 0.05) outperforms comparison systems (Pfix, ABS) on both dimensions of grammaticality and importance (significance tests were performed us- ing a student t-test). All systems are significantly worse (p &lt; 0.05) than the human reference com- pressions.</p><p>Finally, in <ref type="table">Table 6</ref> we analyze the output of our best model (SP L ) using the same statistics we applied to the human compressions (see <ref type="table" target="#tab_2">Ta- ble 2</ref>). As can be seen, the model generally com- pressess more aggressively and applies more ed- Models RS-R D2-R R2-F1 R1-R R2-R RL-R  <ref type="bibr" target="#b39">(Rush et al., 2015)</ref> 26.55 7.06 22.05 ABS+ ( <ref type="bibr" target="#b39">Rush et al., 2015)</ref> 28.18 8.49 23.81 RAS (  28.97 8.26 24.06 LenInit 8 ( <ref type="bibr" target="#b25">Kikuchi et al., 2016</ref><ref type="bibr">) 25.87 8.27 23.24 LenEmb (Kikuchi et al., 2016</ref> 26.73 8.40 23.88 its than the crowdworkers (both compression rates and TER scores are higher for all three languages). Although the rate of deletions is similar to hu- mans, insertions, substitutions and shifts happen to a greater extent for our model, indicating that it performs a good amount of paraphrasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUC-2004 Evaluation</head><p>Besides MOSS, we eval- uated our model on the benchmark DUC-2004 task-1 dataset. In this task, the aim is to create a very short summary (75 bytes) for a document. The evaluation set consists of 500 source docu- ments (from the New York Times and Associated Press Wire services) each paired with four human- written (reference) summaries. We follow previ- ous work <ref type="bibr" target="#b39">(Rush et al., 2015;</ref>) in compressing the first sentence of the document and presenting this as the summary. To make the evaluation unbiased to length, the output of all sys- tems is cut-off after 75-characters and no bonus is given for shorter summaries.</p><p>Our results are shown in <ref type="table" target="#tab_8">Table 7</ref>. To com- pare with existing methods, we also report ROUGE <ref type="bibr" target="#b30">(Lin, 2004</ref>) unigram and bigram overlap <ref type="bibr" target="#b30">(Lin, 2004</ref>) and the longest common subsequence (ROUGE-L). <ref type="bibr">9</ref> We employed a dual step com- pression model (see Section 2) as preliminary ex- periments showed that it was superior to single- stage variants. We compared single and multi- ple pivot models against existing ABS and ABS+ <ref type="bibr" target="#b39">(Rush et al., 2015</ref>), two encoder-decoder models trained on the English Gigaword. ABS+ applies minimum error rate (MERT) training as a copy- <ref type="bibr">7</ref> Our ABS implementation obtains R1-R 25.03, R2-R 8.40, and RL-R: <ref type="bibr">22.35</ref> 8 Our LenInit implementation obtains R1-R 29.26, R2-R 9.56, and RL-R 25.70 <ref type="bibr">9</ref> We used ROUGE version 1.5.5 with the original DUC-2004 ROUGE parameters. Source King Norodom Sihanouk has declined requests to chair a summit of Cambodia's top political leaders, saying the meeting would not bring any progress in deadlocked negotiations to form a government. SP L,de King Norodom Sihanouk has refused to chair Cambodia summit. Gold</p><p>Sihanouk refuses to chair Cambodian political summit at home or abroad. Source Cambodia's ruling party responded Tuesday to criticisms of its leader in the U.S. Congress with a lengthy defense of strongman Hun Sen's human rights record. SP L,de Cambodia's ruling party responded Tuesday to criticism of its leader in the US.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Cambodian party defends leader Hun Sen against criticism of U.S. House. Source The Swiss government has ordered no investigation of possible bank accounts belonging to former Chilean dictator Augusto Pinochet, a spokesman said Wednesday. SP L,de Swiss government ordered no inquiry into possible bank accounts of former Chilean dictator Augusto. Gold Switzerland joins charges against Pinochet but avoids bank probe. ing mechanism. LenEmb and LenInit include a length parameter ( <ref type="bibr" target="#b25">Kikuchi et al., 2016)</ref>, whereas RAS uses a specialized recurrent neural network architecture <ref type="bibr" target="#b13">(Elman, 1990</ref>). We also report how well DUC-2004 abstractors agree with each other (row Gold in <ref type="table" target="#tab_8">Table 7</ref>). Example compressions are given in <ref type="table" target="#tab_9">Table 8</ref>, where we show output produced by SP L,de and a human reference (see the supple- mentary material for further examples).</p><p>Using automatic metrics we see that our model generally performs worse compared to these sys- tems and that German is the best pivot for English. Although the objective of this paper is not to ob- tain state-of-the-art scores on this evaluation set, it is interesting to see that our model is able to compress out-of-domain. We do not have access to headline-first sentence pairs, while all compar- ison systems do. We also elicited human judg- ments on the compressions of 100 lead sentences whose documents were randomly selected from the DUC-2004 test set. We compared the prefix baseline, our model (SP L,de ), ABS+ ( <ref type="bibr" target="#b39">Rush et al., 2015)</ref>, <ref type="bibr">LenEmb (Kikuchi et al., 2016</ref>), Topiary ( <ref type="bibr" target="#b53">Zajic et al., 2004</ref>), and a randomly selected ref- erence. Topiary came top in almost all measures in the DUC-2004 evaluation; it first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic key- words. Crowdworkers rated grammaticality and importance, using a five-point scale; we collected five ratings per compression.</p><p>As shown in <ref type="table" target="#tab_11">Table 9</ref> ABS+ has the lead with our system following suit. In terms of grammaticality, ABS+ and SP L,de are not significantly different from the gold standard or from each other (Pfix, Topiary, and LenEmb are significantly worse than Gold; p &lt; 0.05). In terms of importance, pairwise differences between systems and the gold standard are not significant. Overall, we observe that SP L,de performs comparably to ABS+ even though it was  not trained on any compression specific data. In- spection of system output reveals that our model performs more paraphrasing than comparison sys- tems (a conclusion also confirmed by the statistics in <ref type="table">Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we have shown that multilingual cor- pora can be used to bootstrap compression mod- els across languages and text genres. Our ap- proach adapts existing neural machine translation machinery to the compression task coupled with methods which decode the output to a desired length. An interesting direction for future work would be to train our model using reinforcement learning ( <ref type="bibr" target="#b37">Ranzato et al., 2016;</ref><ref type="bibr" target="#b54">Zhang and Lapata, 2017</ref>) in order to control the compression output more directly. Moreover, although we do not use any direct supervision in our experiments, it would be interesting to incorporate it as a means of do- main adaptation ( <ref type="bibr" target="#b5">Cheng et al., 2016</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Examples of crowdsourced compressions (in italics) from the MOSS corpus. Sentences shown (in order of appearance) from Europarl, TED, and News Commentary corpora.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>MOSS statistics across corpora and lan-
guages: length of source (SL) and target sentence 
(TL), compression rate (CR), TER scores, and 
number of insertions (Ins), deletions (Del), sub-
stitutions (Sub), and shifts (Shft). 

We used up to four encoder-decoder NMT 
models in our experiments (BLEU scores 4 
shown in parentheses): English→French (27.03), 
French→English (29.14), English→German 
(28.3), and </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Automatic evaluation on MOSS; S/MP: single/multiple pivot models; L: length parameter; 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Mean ratings elicited by humans on 
MOSS; Avg is the average rating of grammatical-
ity and importance. 

SL 
TL CR TER Ins Del Sub Shft 
English 19.69 12.31 0.63 0.65 0.10 6.68 2.14 0.44 
French 21.26 14.98 0.70 0.67 0.29 5.71 3.36 0.61 
German 18.23 12.51 0.68 0.67 0.16 6.38 2.94 0.50 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>DUC-2004 results (75 char length cap); 
results for comparison systems are taken from 
their respective papers. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>System output for DUC-2004. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Mean ratings elicited by humans on 
DUC-2004; Avg is the average rating of grammat-
icality and importance. 

</table></figure>

			<note place="foot" n="1"> Publicly available for download at https://github. com/Jmallins/MOSS</note>

			<note place="foot" n="2"> Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million.</note>

			<note place="foot" n="3"> The term refers to the percentage of words retained from the source sentence in the compression. language (Mallinson et al., 2017). We can see that the majority of sentences are generated with length close to the desired compression rate. 3 The MOSS Dataset For evaluation purposes, we created a multilingual sentence compression corpus in English, German, and French. The corpus was collated from existing document and sentence aligned multilingual datasets which vary both in terms of topic and genre. We sampled five documents each from: 1. Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment). 2. The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues. 3. The EU bookshop corpus (Skadin¸ˇSkadin¸Skadin¸ˇ s et al., 2014) contains publications from European institutions covering a variety of topics such as refugees, gender equality, and travel. 4. The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs). We obtained compressions using the Crowdflower platform. Crowdworkers were given instructions that explained the task and defined sentence compression with the aid of examples. They</note>

			<note place="foot" n="5"> We used our own implementation of ABS and LenInit which on DUC-2004 obtained ROUGE scores similar to those published in Rush et al. (2015) and Kikuchi et al. (2016). 6 We add a begin-of-sentence marker at the start of the candidate and reference sentences.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wit 3 : Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 16 th Conference of the European Association for Machine Translation (EAMT)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semisupervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1965" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Models for sentence compression: A comparison across domains, training requirements and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="273" to="381" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentence compression as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="637" to="674" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An abstractive approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text Compaction for Display on Very Small Screens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Corston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Workshop on Automatic Summarization</title>
		<meeting>the NAACL Workshop on Automatic Summarization<address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zero-resource translation with multi-lingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Yarman</forename><surname>Fatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Vural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="268" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference</title>
		<meeting>the Main Conference<address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="180" to="187" />
		</imprint>
	</monogr>
	<note>Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flexible Japanese sentence compression by relaxing unit constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Japanese sentence compression with a large training dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="281" to="286" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A syntax-free approach to japanese sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="826" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speech summarization: an approach through word extraction and a method for evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentence Reduction for Automatic Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ANLP</title>
		<meeting>the 6th ANLP<address><addrLine>Seattle,WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="310" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The proper place of men and machines in language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="23" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: a probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Machine Translation Summit</title>
		<meeting>the 10th Machine Translation Summit<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A french human reference corpus for multidocument summarization and sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>De Loupy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Guégan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somara</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel Torres</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the 7th International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sentence compression for automatic subtitling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Nordic Conference for Computational Linguistics</title>
		<meeting>the 20th Nordic Conference for Computational Linguistics<address><addrLine>Vilnius, Lithuania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mutiple alternative sentence compressions for automatic text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Document Understanding Conference</title>
		<meeting>the 2007 Document Understanding Conference<address><addrLine>Rochester, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Necip Fazil Ayan, and Jimmy Lin</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Paraphrastic sentence compression with a character-based metric: Tightening without deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Monolingual Text-To-Text Generation</title>
		<meeting>the Workshop on Monolingual Text-To-Text Generation<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><forename type="middle">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT/NAACL</title>
		<meeting>the HLT/NAACL<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raivis</forename><surname>Skadin¸ˇskadin¸skadin¸ˇs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<imprint>
			<pubPlace>Roberts Rozis, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Billions of parallel words for free: Building and using the EU bookshop corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiga</forename><surname>Deksne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference of the Association for Machine Translation of the Americas</title>
		<meeting>the 7th Conference of the Association for Machine Translation of the Americas<address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="340" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised learning for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenine</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A comparison of pivot methods for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sentence compression for automated subtitling: A hybrid approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vandeghinste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Text Summarization</title>
		<meeting>the ACL Workshop on Text Summarization<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="89" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automatic generation of story highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pivot language approach for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bbn/umd at duc-2004: Topiary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Workshop on Document Understanding</title>
		<meeting>the NAACL Workshop on Document Understanding<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
