<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Attentions for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center</orgName>
								<address>
									<addrLine>IBM 1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center</orgName>
								<address>
									<addrLine>IBM 1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">T.J. Watson Research Center</orgName>
								<address>
									<addrLine>IBM 1101 Kitchawan Rd</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised Attentions for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2283" to="2288"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the &quot;true&quot; alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) has gained pop- ularity in recent two years (e.g. ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b8">Jean et al., 2015;</ref><ref type="bibr" target="#b13">Luong et al., 2015;</ref><ref type="bibr" target="#b15">Mi et al., 2016b;</ref>, especially for the attention- based models of <ref type="bibr" target="#b1">Bahdanau et al. (2014)</ref>.</p><p>The attention model plays a crucial role in NMT, as it shows which source word(s) the model should focus on in order to predict the next target word. However, the attention or alignment quality of NMT is still very low ( <ref type="bibr" target="#b14">Mi et al., 2016a;</ref><ref type="bibr" target="#b17">Tu et al., 2016)</ref>.</p><p>In this paper, we alleviate the above issue by uti- lizing the alignments (human annotated data or ma- chine alignments) of the training set. Given the alignments of all the training sentence pairs, we add an alignment distance cost to the objective func- tion. Thus, we not only maximize the log translation probabilities, but also minimize the alignment dis- tance cost. Large-scale experiments over Chinese- to-English on various test sets show that our best method for a single system improves the transla- tion quality significantly over the large vocabulary NMT system (Section 5) and beats the state-of-the- art syntax-based system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>As shown in <ref type="figure">Figure 1</ref>, attention-based NMT <ref type="bibr" target="#b1">(Bahdanau et al., 2014</ref>) is an encoder-decoder network. the encoder employs a bi-directional recurrent neu- ral network to encode the source sentence x = (x 1 , ..., x l ), where l is the sentence length (includ- ing the end-of-sentence eos), into a sequence of hidden states h = (h 1 , ..., h l ), each h i is a concate- nation of a left-to-right − → h i and a right-to-left ← − h i . Given h, the decoder predicts the target transla- tion by maximizing the conditional log-probability of the correct translation</p><formula xml:id="formula_0">y * = (y * 1 , ...y * m )</formula><p>, where m is the sentence length (including the end-of- sentence). At each time t, the probability of each word y t from a target vocabulary V y is:</p><formula xml:id="formula_1">p(y t |h, y * t−1 ..y * 1 ) = g(s t , y * t−1 ),<label>(1)</label></formula><p>where g is a two layer feed-forward neural network over the embedding of the previous word y * t−1 , and the hidden state s t . The s t is computed as:</p><formula xml:id="formula_2">s t = q(s t−1 , y * t−1 , H t )<label>(2)</label></formula><formula xml:id="formula_3">H t = l i=1 (α t,i · ← − h i ) l i=1 (α t,i · − → h i ) ,<label>(3)</label></formula><p>where q is a gated recurrent units, H t is a weighted sum of h; the weights, α, are computed with a two layer feed-forward neural network r:</p><formula xml:id="formula_4">α t,i = exp{r(s t−1 , h i , y * t−1 )} l k=1 exp{r(s t−1 , h k , y * t−1 )}<label>(4)</label></formula><p>We put all α t,i (t = 1...m, i = 1...l) into a matrix A , we have a matrix (alignment) like (c) in <ref type="figure">Figure 2</ref>, where each row (for each target word) is a probabil- ity distribution over the source sentence x. The training objective is to maximize the condi- tional log-probability of the correct translation <ref type="figure">Figure 1</ref>: The architecture of attention-based NMT ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). The source sentence x = (x1, ..., x l ) with length l, x l is an end-of-sentence token eos on the source side. The reference translation is y * = (y * 1 , ..., y * m ) with length m, similarly, y * m is the target side eos.</p><formula xml:id="formula_5">y * ↵ t1 ↵ tl s t1 s t … o t y 1 … … y |V y | A tj … … H t = l X i=1 (↵ ti · h i ) l X i=1 (↵ ti · ! h i ) x 1 x l h 1 h l ! h l ! h 1 … … … x 2 ! h 2 h 2 x 1 x l h 1 h l ! h l ! h 1 … … … h j ! h j x j … … … ↵ t2 y ⇤ t1 y ⇤ t e t,1 e t,j e t,l ↵ t,j = exp(e t,j ) P l i=1 exp(e t,i ) s t</formula><p>← − hi and − → hi are bi-directional encoder states. αt,j is the attention probability at time t, position j. Ht is the weighted sum of encoding states. st is a hidden state. ot is an output state. Another one layer neural network projects ot to the target output vocabulary, and conducts softmax to predict the probability distribution over the output vocabulary. The attention model (the right box) is a two layer feedforward neural network, At,j is an intermediate state, then another layer converts it into a real number et,j, the final attention probability at position j is αt,j.</p><p>given x with respect to the parameters θ</p><formula xml:id="formula_6">θ * = arg max θ N n=1 m t=1 log p(y * n t |x n , y * n t−1 ..y * n 1 ),<label>(5)</label></formula><p>where n is the n-th sentence pair (x n , y * n ) in the training set, N is the total number of pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Alignment Component</head><p>The attentions, α t,1 ...α t,l , in each step t play an im- portant role in NMT. However, the accuracy is still far behind the traditional MaxEnt alignment model in terms of alignment F1 score ( <ref type="bibr" target="#b15">Mi et al., 2016b;</ref><ref type="bibr" target="#b17">Tu et al., 2016</ref>). Thus, in this section, we explicitly add an alignment distance to the objective function in Eq. 5. The "truth" alignments for each sentence pair can be from human annotated data, unsupervised or supervised alignments (e.g. GIZA++ <ref type="bibr" target="#b16">(Och and Ney, 2000</ref>) or MaxEnt ( <ref type="bibr" target="#b7">Ittycheriah and Roukos, 2005)</ref>).</p><p>Given an alignment matrix A for a sentence pair (x, y) in <ref type="figure">Figure 2</ref> (a), where we have an end-of- source-sentence token eos = x l , and we align all the unaligned target words (y * 3 in this example) to eos, also we force y * m (end-of-target-sentence) to be aligned to x l with probability one. Then we con- duct two transformations to get the probability dis- tribution matrices ((b) and (c) in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simple Transformation</head><p>The first transformation simply normalizes each row. <ref type="figure">Figure 2</ref> (b) shows the result matrix A * . The last column in red dashed lines shows the alignments of the special end-of-sentence token eos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Smoothed Transformation</head><p>Given the original alignment matrix A, we create a matrix A * with all points initialized with zero. Then, for each alignment point A t,i = 1, we update A * by adding a Gaussian distribution, g(µ, σ), with a window size w (t-w, ... t ... t+w). Take the A 1,1 = 1 for example, we have A * 1,1 += 1, A * 1,2 += 0.61, and A * 1,3 += 0.14 with w=2, g(µ, σ)=g(0, 1). Then we normalize each row and get (c). In our experiments, we use a shape distribution, where σ = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Objectives</head><p>Alignment Objective: Given the "true" alignment A * , and the machine attentions A produced by NMT model, we compute the Euclidean distance bewteen A * and A .  </p><formula xml:id="formula_7">d(A , A * ) = m t=1 l i=1 (A t,i − A * t,i ) 2 .<label>(6)</label></formula><formula xml:id="formula_8">x 1 x 2 x l y ⇤ 1 y ⇤ 2 y ⇤ m y ⇤ 3 y ⇤ 4 y ⇤ 5 x 3 x 4</formula><formula xml:id="formula_9">x 1 x 2 x 3 x 4 0 0 0.57 0 0.26 1 (a) (b) (c) 0 0 1 0 0 1 x 5 x l x 5 x l x 5</formula><p>Figure 2: Alignment transformation. A special token, eos, is introduced to the source sentence, we align all the unaligned target words (y * 3 in this case) to eos. (a): the original alignment matrix A from GIZA++ or MaxEnt aligner. (b): simple normalization by rows (probability distribution over the source sentence x). (c): smoothed transformation followed by normalization by rows, and typically, we always align end-of-source-sentence x l to end-of-target-sentence ym by probability one.</p><p>NMT Objective: We plug Eq. 6 to Eq. 5, we have</p><formula xml:id="formula_10">θ * = arg max θ N n=1 m t=1 log p(y * n t |x n , y * n t−1 ..y * n 1 ) − d(A n , A * n ) .<label>(7)</label></formula><p>There are two parts: translation and alignment, so we can optimize them jointly, or separately (e.g. we first optimize alignment only, then optimize transla- tion). Thus, we divide the network in <ref type="figure">Figure 1</ref> into alignment A and translation T parts:</p><p>• A: all networks before the hidden state s t ,</p><p>• T: the network g(s t , y * t−1 ).</p><p>If we only optimize A, we keep the parameters in T unchanged. We can also optimize them jointly J. In our experiments, we test different optimization strategies. , and introduced a combined objective that takes into account both translation directions (source-to-target and target-to-source) and an agree- ment term between the two alignment directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>By contrast, our approach directly uses and op- timizes NMT parameters using the "supervised" alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Preparation</head><p>We run our experiments on Chinese to English task.  <ref type="table">Table 1</ref>: Single system results in terms of (TER-BLEU)/2 (T-B, the lower the better) on 5 million Chinese to English training set.</p><p>BP denotes the brevity penalty. NMT results are on a large vocabulary (300k) and with UNK replaced. The second column shows different alignments (Zh → En (one direction), GDFA ("grow-diag-final-and"), and MaxEnt (Ittycheriah and <ref type="bibr" target="#b7">Roukos, 2005)</ref>. A, T, and J mean optimize alignment only, translation only, and jointly. Gau. denotes the smoothed transformation.</p><p>from 'fast align' <ref type="bibr" target="#b5">(Dyer et al., 2013</ref>). The maximum length of a source phrase is 4. In the training time, we add the reference in order to make the translation reachable.</p><p>The Cov. LVNMT system is a re-implementation of the enhanced NMT system of <ref type="bibr" target="#b14">Mi et al. (2016a)</ref>, which employs a coverage embedding model and achieves better performance over large vocabulary NMT <ref type="bibr" target="#b8">Jean et al. (2015)</ref>. The coverage embedding dimension of each source word is 100.</p><p>Following <ref type="bibr" target="#b8">Jean et al. (2015)</ref>, we dump the align- ments, attentions, for each sentence, and replace UNKs with the word-to-word translation model or the aligned source word.</p><p>Our SMT system is a hybrid syntax-based tree-to- string model <ref type="bibr" target="#b20">(Zhao and Al-onaizan, 2008</ref>), a simpli- fied version of the joint decoding ( <ref type="bibr" target="#b12">Liu et al., 2009;</ref><ref type="bibr" target="#b3">Cmejrek et al., 2013</ref>). We parse the Chinese side with Berkeley parser, and align the bilingual sen- tences with GIZA++ and MaxEnt. and extract Hi- ero and tree-to-string rules on the training set. Our two 5-gram language models are trained on the En- glish side of the parallel corpus, and on monolin- gual corpora (around 10 billion words from Giga- word (LDC2011T07), respectively.As suggested by <ref type="bibr" target="#b19">Zhang (2016)</ref>, NMT systems can achieve better re- sults with the help of those monolingual corpora. In this paper, our NMT systems only use the bilingual data. We tune our system with PRO ( <ref type="bibr" target="#b6">Hopkins and May, 2011</ref>) to minimize (TER-BLEU)/2 1 on the de- velopment set. <ref type="table">Table 1</ref> shows the translation results of all sys- tems. The syntax-based statistical machine trans- lation model achieves an average (TER-BLEU)/2 of 13.36 on three test sets. The Cov. LVNMT system achieves an average (TER-BLEU)/2 of 14.24, which is about 0.9 points worse than Tree-to-string SMT system. Please note that all systems are single sys- tems. It is highly possible that ensemble of NMT systems with different random seeds can lead to bet- ter results over SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation Results</head><p>We test three different alignments:</p><p>• Zh → En (one direction of GIZA++),</p><p>• GDFA (the "grow-diag-final-and" heuristic merge of both directions of GIZA++),</p><p>• MaxEnt (trained on 67k hand-aligned sen- tences).</p><p>The alignment quality improves from Zh → En to MaxEnt. We also test different optimization strate- gies: J (jointly), A (alignment only), and T (trans- lation model only). A combination, A → T, shows that we optimize A only first, then we fix A and only update T part. Gau. denotes the smoothed trans- formation (Section 3.2). Only the last row uses the smoothed transformation, all others use the simple transformation.</p><p>Experimental results in <ref type="table">Table 1</ref> show some in- teresting results. First, with the same alignment, J joint optimization works best than other optimiza- tion strategies (lines 3 to 6). Unfortunately, break- ing down the network into two separate parts (A and T) and optimizing them separately do not help (lines 3 to 5). We have to conduct joint optimization J in order to get a comparable or better result (lines 3, 5 and 6) over the baseline system.</p><p>Second, when we change the training alignment seeds (Zh → En, GDFA, and MaxEnt) NMT model does not yield significant different results (lines 6 to 8).</p><p>Third, the smoothed transformation (J + Gau.) gives some improvements over the simple transfor- mation (the last two lines), and achieves the best result (1.2 better than LVNMT, and 0.3 better than Tree-to-string). In terms of BLEU scores, we con- duct the statistical significance tests with the sign- test of <ref type="bibr" target="#b4">Collins et al. (2005)</ref>, the results show that the improvements of our J + Gau. over LVNMT are significant on three test sets (p &lt; 0.01).</p><p>At last, the brevity penalty (BP) consistently gets better after we add the alignment cost to NMT objec- tive. Our alignment objective adjusts the translation length to be more in line with the human references accordingly. <ref type="table" target="#tab_1">Table 2</ref> shows the alignment F1 scores on the align- ment test set (447 hand aligned sentences). The MaxEnt model is trained on 67k hand-aligned sen- tences, and achieves an F1 score of 75.96. For NMT systems, we dump the alignment matrixes and con- vert them into alignments with following steps. For each target word, we sort the alphas and add the max probability link if it is higher than 0.  And we further boost the score to 50.97 by tuning alignment and translation jointly (J in line 7). Inter- estingly, the system using MaxEnt produces more alignments in the output, and results in a higher re- call. This suggests that using MaxEnt can lead to a sharper attention distribution, as we pick the align- ment links based on the probabilities of attentions, the sharper the distribution is, more links we can pick. We believe that a sharp attention distribution is a great property of NMT. Again, the best result is J + Gau. in the last row, which significantly improves the F1 by 5 points over the baseline Cov. LVNMT system. When we use MaxEnt alignments, J + Gau. smoothing gives us about 1.7 points gain over J system. So it looks in- teresting to run another J + Gau. over GDFA align- ment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Alignment Results</head><p>Together with the results in <ref type="table">Table 1</ref>, we conclude that adding the alignment cost to the training ob- jective helps both translation and alignment signif- icantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we utilize the "supervised" alignments, and put the alignment cost to the NMT objective function. In this way, we directly optimize the at- tention model in a supervised way. Experiments show significant improvements in both translation and alignment tasks over a very strong LVNMT sys- tem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>In order to improve the attention or alignment ac- curacy, Cheng et al. (2016) adapted the agreement- based learning (Liang et al., 2006; Liang et al., 2008)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>systems, the full vocabulary size of the training set is 300k. In the training procedure, we use AdaDelta (Zeiler, 2012) to update model parameters with a mini-batch size 80. Following Mi et al. (2016a), the output vocabulary for each mini-batch or sentence is a sub-set of the full vo- cabulary. For each source sentence, the sentence- level target vocabularies are union of top 2k most frequent target words and the top 10 candidates of the word-to-word/phrase translation tables learned MT06 MT08 avg. single system News Web BP BLEU T-B BP BLEU T-B BP BLEU T-B T-B Tree-to-string 0.95 34.93 9.45 0.94 31.12 12.90 0.90 23.45 17.72 13.36 Cov. LVNMT (Mi et al., 2016b) 0.92 35.59 10.71 0.89 30.18 15.33 0.97 27.48 16.67 14.24</figDesc><table>The training corpus consists of approximately 5 mil-
lion sentences available within the DARPA BOLT 
Chinese-English task. The corpus includes a mix of 
newswire, broadcast news, and webblog. We do not 
include HK Law, HK Hansard and UN data. The 
Chinese text is segmented with a segmenter trained 
on CTB data using conditional random fields (CRF). 
Our development set is the concatenation of sev-
eral tuning sets (GALE Dev, P1R6 Dev, and Dev 
12) initially released under the DARPA GALE pro-
gram. The development set is 4491 sentences in to-
tal. Our test sets are NIST MT06 (1664 sentences) 
, MT08 news (691 sentences), and MT08 web (666 
sentences). 
For all NMT +Alignment 
Zh → En 
A → J 
0.95 35.71 10.38 0.93 30.73 14.98 0.96 27.38 16.24 13.87 
A → T 
0.95 28.59 16.99 0.92 24.09 20.89 0.97 20.48 23.31 20.40 
A → T → J 
0.95 35.95 10.24 0.92 30.95 14.62 0.97 26.76 17.04 13.97 
J 
0.96 36.76 9.67 0.94 31.24 14.80 0.96 28.35 15.61 13.36 
GDFA 
J 
0.96 36.44 10.16 0.94 30.66 15.01 0.96 26.67 16.72 13.96 

MaxEnt 
J 
0.95 36.80 9.49 0.93 31.74 14.02 0.96 27.53 16.21 13.24 
J + Gau. 
0.96 36.95 9.71 0.94 32.43 13.61 0.97 28.63 15.80 13.04 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>2 .</head><label>2</label><figDesc></figDesc><table>If we only 
tune the alignment component (A in line 3), we im-
prove the alignment F1 score from 45.76 to 47.87. 

system 
pre. rec. 
F1 
MaxEnt 
74.86 77.10 75.96 
Cov LVNMT (Mi et al., 2016b) 51.11 41.42 45.76 

+Alignment 
Zh → En 

A 
50.88 45.19 47.87 
A → J 
53.18 49.37 51.21 
A → T 
50.29 44.90 47.44 
A → T → J 53.71 49.33 51.43 
J 
54.29 48.02 50.97 
GDFA 
J 
53.88 48.25 50.91 

MaxEnt 
J 
44.42 55.25 49.25 
J + Gau. 
48.90 55.38 51.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Alignment F1 scores of different models.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The metric used for optimization in this work is (TERBLEU)/2 to prevent the system from using sentence length alone to impact BLEU or TER. Typical SMT systems use target word count as a feature and it has been observed that BLEU can be optimized by tweaking the weighting of the target word count with no improvement in human assessments of translation quality. Conversely, in order to optimize TER shorter sentences can be produced. Optimizing the combination of metrics alleviates this effect (Arne Mauser and Ney, 2008).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank the anonymous reviewers for their useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic evaluation measures for statistical machine translation system optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Sasa Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Agreement-based joint training for bidirectional attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flexible and efficient hypergraph interactions for joint hierarchical and forest-to-string decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cmejrek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="545" to="555" />
		</imprint>
	</monogr>
	<note>Seattle. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kucerova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A maximum entropy word aligner for arabic-english machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT &apos;05: Proceedings of the HLT and EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards zero unknown word in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2016</title>
		<meeting>IJCAI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Agreementbased learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint decoding with multiple translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
	<note>ACL &apos;09</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Portugal, September. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A coverage embedding model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>August</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL &apos;00</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics, ACL &apos;00<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Coveragebased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalizing local and non-local word-reordering patterns for syntaxbased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
