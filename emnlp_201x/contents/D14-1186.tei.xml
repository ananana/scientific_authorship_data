<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Learning of Chinese Words, Terms and Keywords</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Learning of Chinese Words, Terms and Keywords</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1774" to="1778"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous work often used a pipelined framework where Chinese word segmen-tation is followed by term extraction and keyword extraction. Such framework suffers from error propagation and is unable to leverage information in later modules for prior components. In this paper, we propose a four-level Dirichlet Process based model (DP-4) to jointly learn the word distributions from the corpus, domain and document levels simultaneously. Based on the DP-4 model, a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results. Meanwhile, terms and keywords are acquired in the sampling process. Experimental results have shown the effectiveness of our method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For Chinese language which does not contain ex- plicitly marked word boundaries, word segmenta- tion (WS) is usually the first important step for many Natural Language Processing (NLP) tasks including term extraction (TE) and keyword ex- traction (KE). Generally, Chinese terms and key- words can be regarded as words which are repre- sentative of one domain or one document respec- tively. Previous work of TE and KE normally used the pipelined approaches which first conducted WS and then extracted important word sequences as terms or keywords.</p><p>It is obvious that the pipelined approaches are prone to suffer from error propagation and fail to leverage information for word segmentation from later stages. Here, we provide one example in the disease domain, to demonstrate the common prob- lems in current pipelined approaches and propose the basic idea of our joint learning of words, terms and keywords. This is a correctly segmented Chinese sen- tence. The document containing the example sen- tence mainly talks about the property of "{ • (heparinoid)" which can be regarded as one key- word of the document. At the same time, the word @•ÏÇ(thrombocytopenia) appears fre- quently in the disease domain and can be treated as a domain-specific term.</p><p>However, for such a simple sentence, current segmentation tools perform poorly. The segmen- tation result with the state-of-the-art Conditional Random Fields (CRFs) approach ( <ref type="bibr" target="#b13">Zhao et al., 2006</ref>) is as follows:</p><formula xml:id="formula_0">@•(blood platelet) Ï(reduction) Ç(symptom) {(of same kind) •(liver) (always) sû(relation)</formula><p>where @•ÏÇ is segmented into three com- mon Chinese words and {• is mixed with its neighbors.</p><p>In a text processing pipeline of WS, TE and KE, it is obvious that imprecise WS results will make the overall system performance unsatisfy- ing. At the same time, we can hardly make use of domain-level and document-level information col- lected in TE and KE to promote the performance of WS. Thus, one question comes to our minds: can words, terms and keywords be jointly learned with consideration of all the information from the corpus, domain, and document levels?</p><p>Recently, the hierarchical Dirichlet process (HDP) model has been used as a smoothed bigram model to conduct word segmentation <ref type="bibr" target="#b1">(Goldwater et al., 2006;</ref><ref type="bibr" target="#b2">Goldwater et al., 2009</ref>). Meanwhile, one strong point of the HDP based models is that they can model the diversity and commonality in multiple correlated corpora <ref type="bibr" target="#b8">(Ren et al., 2008;</ref><ref type="bibr" target="#b11">Xu et al., 2008;</ref><ref type="bibr" target="#b12">Zhang et al., 2010;</ref><ref type="bibr" target="#b4">Li et al., 2012;</ref><ref type="bibr" target="#b0">Chang et al., 2014</ref>). Inspired by such existing work, we propose a four-level DP based model,  2 DP-4 Model <ref type="bibr" target="#b1">Goldwater et al. (2006</ref>) applied the HDP model on the word segmentation task. In essence, Goldwa- ter's model can be viewed as a bigram language model with a unigram back-off. With the lan- guage model, word segmentation is implemented by a character-based Gibbs sampler which repeat- edly samples the possible word boundary posi- tions between two neighboring words, conditioned on the current values of all other words. How- ever, Goldwater's model can be deemed as mod- eling the whole corpus only, and does not distin- guish between domains and documents. To jointly learn the word information from the corpus, do- main and document levels, we extend Goldwater's model by adding two levels (domain level and doc- ument level) of DPs, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Description</head><p>M DPs (H m w ;1 ≤ m ≤ M ) are designed specif- ically to word w to model the bigram distribu- tions in each domain and these DPs share an overall base measure H w , which is drawn from DP (α 0 , G 1 ) and gives the bigram distribution for the whole corpus. Assuming the m th domain in- cludes N m documents, we use H </p><formula xml:id="formula_1">m j i ∼ H m j w (1 ≤ i ≤ N j m )</formula><p>. Thus, our four-level DP model can be summarized formally as follows:</p><formula xml:id="formula_2">G 1 ∼ DP (α 0 , G 0 ) ; H w ∼ DP (α 1 , G 1 ) H m w ∼ DP (α 2 , H w ) ; H m j w ∼ g α 3 , H m w , H m −j w w m j i |w i−1 = w ∼ H d w</formula><p>Here, we provide for our model the Chinese Restaurant Process (CRP) metaphor, which can create a partition of items into groups. In our model, the word type of the previous word w i−1 corresponds to a restaurant and the current word w i corresponds to a customer. Each domain is analogous to a floor in a restaurant and a room de- notes a document. Now, we can see that there are |V | restaurants and each restaurant consists of M floors. The m th floor contains N m rooms and each room has an infinite number of tables with infinite seating capacity. Customers enter a specific room on a specific floor of one restaurant and seat them- selves at a table with the label of a word type. Dif- ferent from the standard HDP, each customer sits at an occupied table with probability proportional to both the numbers of customers already seated there and the numbers of customers with the same word type seated in the neighboring rooms, and at an unoccupied table with probability proportional to both the constant α 3 and the probability that the customers with the same word type are seated on the same floor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Inference</head><p>It is important to build an accurate G 0 which de- termines the prior word distribution p 0 (w). Sim- ilar to the work of <ref type="bibr" target="#b5">Mochihashi et al. (2009)</ref>, we consider the dependence between characters and calculate the prior distribution of a word w i using the string frequency statistics <ref type="bibr" target="#b3">(Krug, 1998)</ref>:</p><formula xml:id="formula_3">p 0 (w i ) = n s (w i ) n s (.)<label>(1)</label></formula><p>where n s (w i ) counts the character string com- posed of w i and the symbol "." represents any word in the vocabulary V . Then, with the CRP metaphor, we can obtain the expected word unigram and bigram distributions on the corpus level according to G 1 and H w :</p><formula xml:id="formula_4">p 1 (w i ) = n (w i ) + α 0 p 0 (w i ) n (.) + α 0 (2) p 2 (w i |w i−1 = w) = n w (w i ) + α 1 p 1 (w i ) n w (.) + α 1<label>(3)</label></formula><p>where the subscript numbers indicate the corre- sponding DP levels. n(w i ) denotes the number of w i and n w (w i ) denotes the number of the bigram &lt; w, w i &gt; occurring in the corpus. Next, we can easily get the bigram distribution on the domain level by extending to the third DP.</p><formula xml:id="formula_5">p m 3 (w i |w i−1 = w) = n m w (w i ) + α 2 p 2 (w i |w i−1 ) n m w (.) + α 2<label>(4)</label></formula><p>where n m w (w i ) is the number of the bigram &lt; w, w i &gt; occurring in the m th domain.</p><p>To model the bigram distributions on the docu- ment level, it is beneficial to consider the influence of related documents in the same domain ( <ref type="bibr" target="#b10">Wan and Xiao, 2008</ref> </p><formula xml:id="formula_6">t d j m w (w i ) = n d j m w (w i )+ k s(d j m [k], d j m )n d j m [k] w (w i )<label>(5)</label></formula><p>where n</p><formula xml:id="formula_7">d j m [k] w</formula><p>(w i ) denotes the count of the bigram &lt; w, w i &gt; occurring in d j m <ref type="bibr">[k]</ref>. Next, we model the bigram distribution in d j m as a DP with the base measure H m w :</p><formula xml:id="formula_8">p d j m 4 (w i |w i−1 = w) = t d j m w (w i ) + α 3 p m 3 (w i |w i−1 ) t d j m w (.) + α 3 (6)</formula><p>With CRP, we can also easily estimate the un- igram probabilities p m 3 (w i ) and p d j m 4 (w i ) respec- tively on the domain and document levels, through combining all the restaurants.</p><p>To measure whether a word is eligible to be a term, the score function T H m (·) is defined as:</p><formula xml:id="formula_9">T H m (w i ) = p m 3 (w i ) p 1 (w i )<label>(7)</label></formula><p>This equation is inspired by the work of Nazar (2011), which extracts terms with consideration of both the frequency in the domain corpus and the frequency in the general reference corpus. Similar to Eq. 7, we define the function KH d j m (·) to judge whether w i is an appropriate keyword.</p><formula xml:id="formula_10">KH d j m (w i ) = p d j m 4 (w i ) p 1 (w i )<label>(8)</label></formula><p>During each sampling, we make use of Eqs. (7) and (8) to identify the most possible terms and keywords. Once a word is identified as a term or keyword, it will drop out of the sampling pro- cess in the following iterations. Its CRP explana- tion is that some customers (terms and keywords) find their proper tables and keep sitting there after- wards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence-wise Gibbs Sampler</head><p>The character-based Gibbs sampler for word seg- mentation ( <ref type="bibr" target="#b1">Goldwater et al., 2006</ref>) is extremely slow to converge, since there exists high correla- tion between neighboring words. Here, we intro- duce the sentence-wise Gibbs sampling technique as well as efficient dynamic programming strat- egy proposed by <ref type="bibr" target="#b5">Mochihashi et al. (2009)</ref>. The basic idea is that we randomly select a sentence in each sampling process and use the Viterbi al- gorithm <ref type="bibr" target="#b9">(Viterbi, 1967)</ref> to find the optimal seg- mentation results according to the word distribu- tions derived from other sentences. Different from Mochihashi's work, once terms or keywords are identified, we do not consider them in the segmen- tation process. Due to space limitation, the algo- rithm is not detailed here and can be referred in ( <ref type="bibr" target="#b5">Mochihashi et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Setting</head><p>It is indeed difficult to find a standard evaluation corpus for our joint tasks, especially in different domains. As a result, we spent a lot of time to col- lect and annotate a new corpus 1 composed of ten domains (including Physics, Computer, Agricul- ture, Sports, Disease, Environment, History, Art, Politics and Economy) and each domain is com- posed of 200 documents. On average each doc- ument consists of about 4800 Chinese characters. For these 2000 documents, three annotators have manually checked the segmented words, terms and keywords as the gold standard results for evalu- ation. As we know, there exists a large amount of manually-checked segmented text for the gen- eral domain, which can be used as the training data for further segmentation. As with other nonpara- metric Bayesian models ( <ref type="bibr" target="#b1">Goldwater et al., 2006;</ref><ref type="bibr" target="#b5">Mochihashi et al., 2009</ref>), our DP-4 model can be easily amenable to semi-supervised learning by imposing the word distributions of the segmented text on the corpus level. The news texts pro- vided by Peking University (named PKU corpus) 2 is used as the training data. This corpus contains about 1,870,000 Chinese characters and has been manually segmented into words.</p><p>In our experiments, the concentration coeffi- cient (α 0 ) is finally set to 20 and the other three (α 1∼3 ) are set to 15. The parameter K which con- trols the number of similar documents is set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Evaluation</head><p>The following baselines are implemented for com- parison of segmentation results: (1) Forward max- imum matching (FMM) algorithm with a vocab- ulary compiled from the PKU corpus; (2) Re- verse maximum matching (RMM) algorithm with the compiled vocabulary; (3) Conditional Random Fields (CRFs) 3 based supervised algorithm trained from the PKU corpus; (4) HDP based semi- supervised algorithm <ref type="bibr" target="#b1">(Goldwater et al., 2006</ref>) us-ing the PKU corpus. The strength of <ref type="bibr" target="#b5">Mochihashi et al. (2009)</ref>'s NPYLM based segmentation model is its speed due to the sentence-wise sam- pling technique, and its performance is similar to <ref type="bibr" target="#b1">Goldwater et al. (2006)</ref>'s model. Thus, we do not consider the NPYLM based model for compari- son here. Then, the segmentation results of FMM, RMM, CRF, and HDP methods are used respec- tively for further extracting terms and keywords. We use the mutual information to identify the can- didate terms or keywords composed of more than two segmented words. As for DP-4, this recogni- tion process has been done implicitly during sam- pling. To measure the candidate terms or key- words, we refer to the metric in Nazar (2011) to calculate their importance in some specific domain or document.</p><p>The metrics of F 1 and the out-of-vocabulary Recall (OOV-R) are used to evaluate the segmenta- tion results, referring to the gold standard results. The second and third columns of <ref type="table" target="#tab_3">Table 1</ref> show the F 1 and OOV-R scores averaged on the 10 domains for all the compared methods. Our method sig- nificantly outperforms FMM, RMM and HDP ac- cording to t-test (p-value ≤ 0.05). From the seg- mentation results, we can see that the FMM and RMM methods are highly dependent on the com- piled vocabulary and their identified OOV words are mainly the ones composed of a single Chinese character. The HDP method is heavily influenced by the segmented text, but it also exhibits the abil- ity of learning new words. Our method only shows a slight advantage over the CRF approach. We check our segmentation results and find that the performance of the DP-4 model is depressed by the identified terms and keywords which may be composed of more than two words in the gold standard results, because the DP-4 model always treats the term or keyword as a single word. For example, in the gold standard, "-W ‡((Lingnan Culture)" is segmented into two words "-W" and " ‡ ", "p n ¥ ã(data interface)" is segmented into "pn" and "¥ã" and so on. In fact, our seg- mentation results correctly treat "-W ‡" and "p n¥ã" as words.</p><p>To evaluate the TE and KE performance, the top 50 (TE-50) and 100 (TE-100) accuracy are mea- sured for the identified terms of one domain, while the top 5 (KE-5) and 10 (KE-10) accuracy for the keywords in one document, are shown in the right four columns of <ref type="table" target="#tab_3">Table 1</ref>. We can see that DP-4 performs significantly better than all the other methods in TE and KE results.</p><p>As for the ten domains, we find our approach behaves much better than the other approaches on the following three domains: Disease, Physics and Computer. It is because the language of these three domains is much different from that of the general domain (PKU corpus), while the rest do- mains are more similar to the general domain.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a four-level DP based model to construct the word distributions from the cor- pus, domain and document levels simultaneously, through which Chinese words, terms and key- words can be learned jointly and effectively. In the future, we plan to explore how to combine more features such as part-of-speech tags into our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Example: @•ÏÇ(thrombocytopenia) (with) {• (heparinoid) (have) sû(relation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: DP-4 Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>N m ) to model the bigram distribution of the i th document in the domain. Usually, given a do- main, the bigram distributions of different docu- ments are not conditionally independent and simi- lar documents exhibit similar bigram distributions. Thus, the bigram distribution of one document is generated according to both the bigram distribu- tion of the domain and the bigram distributions of other documents in the same domain. That is, H m j w ∼ g(α 3 , H m w , H m −j w ) where H m −j w repre- sents the bigram distributions of the documents in the m th domain except the j th document. Assum- ing the j th document in the m th domain contains N j m words, each word is drawn according to H m j w . That is, w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of WS, TE and KE Perfor-
mance (averaged on the 10 domains). 

</table></figure>

			<note place="foot" n="1"> Nine domains are from http://www.datatang. com/data/44139 and we add an extra Disease domain. 2 http://icl.pku.edu.cn 3 We adopt CRF++(http://crfpp.googlecode. com/svn/trunk/doc/index.html)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the three anonymous reviewers for their helpful comments. This work was par-tially supported by National High <ref type="figure">Technology</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inducing word sense with automatically learned hidden concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaohong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014</title>
		<meeting>COLING 2014<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual dependencies in unsupervised word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="54" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">String frequency: A cognitive motivating factor in coalescence, language processing, and linguistic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Krug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of English Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="286" to="320" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Update summarization using a multilevel hierarchical dirichlet process model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling<address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1603" to="1618" />
		</imprint>
	</monogr>
	<note>Mumbai</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian unsupervised word segmentation with nested pitman-yor language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th</title>
		<meeting>the Joint Conference of the 47th</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A statistical approach to term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogelio</forename><surname>Nazar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of English Studies</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="182" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IJES</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The dynamic hierarchical dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="824" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single document keyphrase extraction using neighborhood knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="855" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dirichlet process based evolutionary clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="648" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1079" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An improved chinese word segmentation system with conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1082117</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
