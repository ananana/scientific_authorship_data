<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
							<email>sbowman@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics † Stanford NLP Group ‡ Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
							<email>angeli@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics † Stanford NLP Group ‡ Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
							<email>cgpotts@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics † Stanford NLP Group ‡ Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics † Stanford NLP Group ‡ Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics † Stanford NLP Group ‡ Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entail-ment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classi-fiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The semantic concepts of entailment and contra- diction are central to all aspects of natural lan- guage meaning <ref type="bibr" target="#b10">(Katz, 1972;</ref><ref type="bibr" target="#b26">van Benthem, 2008)</ref>, from the lexicon to the content of entire texts. Thus, natural language inference (NLI) -charac- terizing and using these relations in computational systems ( <ref type="bibr" target="#b7">Fyodorov et al., 2000;</ref><ref type="bibr" target="#b3">Condoravdi et al., 2003;</ref><ref type="bibr" target="#b0">Bos and Markert, 2005;</ref><ref type="bibr" target="#b4">Dagan et al., 2006;</ref><ref type="bibr" target="#b15">MacCartney and Manning, 2009</ref>) -is essential in tasks ranging from information retrieval to seman- tic parsing to commonsense reasoning.</p><p>NLI has been addressed using a variety of tech- niques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground for approaches employing distributed word and phrase representations. Distributed representa- tions excel at capturing relations based in similar- ity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., <ref type="bibr" target="#b24">Socher et al. 2013</ref>), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr">Weston et al., 2015b;</ref><ref type="bibr" target="#b30">Weston et al., 2015a</ref>). In a SemEval 2014 task aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities <ref type="bibr" target="#b17">(Marelli et al., 2014a)</ref>.</p><p>Our ultimate objective is to provide an empiri- cal evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an as- sessment. They are generally too small for train- ing modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeter- minacies of event and entity coreference that sig- nificantly impact annotation quality.</p><p>To address this, this paper introduces the Stan- ford Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entail- ment, contradiction, and semantic independence. At 570,152 sentence pairs, SNLI is two orders of magnitude larger than all other resources of its type. And, in contrast to many such resources, all of its sentences and labels were written by hu- mans in a grounded, naturalistic context. In a sepa- rate validation phase, we collected four additional judgments for each label for 56,941 of the exam- ples. Of these, 98% of cases emerge with a three- annotator consensus, and 58% see a unanimous consensus from all five annotators.</p><p>In this paper, we use this corpus to evaluate A man inspects the uniform of a figure in some East Asian country. contradiction</p><formula xml:id="formula_0">C C C C C</formula><p>The man is sleeping An older and younger man smiling. neutral N N E N N Two men are smiling and laughing at the cats play- ing on the floor.</p><p>A black race car starts up in front of a crowd of people. contradiction</p><formula xml:id="formula_1">C C C C C</formula><p>A man is driving down a lonely road.</p><p>A soccer game with multiple males playing. entailment</p><formula xml:id="formula_2">E E E E E</formula><p>Some men are playing a sport.</p><p>A smiling costumed woman is holding an um- brella. neutral N N E C N A happy woman in a fairy costume holds an um- brella. <ref type="table">Table 1</ref>: Randomly chosen examples from the development section of our new corpus, shown with both the selected gold labels and the full set of labels (abbreviated) from the individual annotators, including (in the first position) the label used by the initial author of the pair.</p><p>a variety of models for natural language infer- ence, including rule-based systems, simple lin- ear classifiers, and neural network-based models. We find that two models achieve comparable per- formance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997). We further evaluate the LSTM model by taking advantage of its ready sup- port for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A new corpus for NLI</head><p>To date, the primary sources of annotated NLI cor- pora have been the Recognizing Textual Entail- ment (RTE) challenge tasks. 1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical mod- els of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed rep- resentations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowl- edge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data ( <ref type="bibr">Marelli et al. 2014a, §6)</ref>. The Denotation Graph entailment set ( <ref type="bibr" target="#b33">Young et al., 2014</ref>) contains millions of examples of en- tailments between sentences and artificially con- structed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup- 1 http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool plementary training data. Outside the domain of sentence-level entailment,  intro- duce a large corpus of semi-automatically anno- tated entailment examples between subject-verb- object relation triples, and the second release of the Paraphrase Database ( <ref type="bibr" target="#b21">Pavlick et al., 2015</ref>) in- cludes automatically generated entailment anno- tations over a large corpus of pairs of words and short phrases.</p><p>Existing resources suffer from a subtler issue that impacts even projects using only human- provided annotations: indeterminacies of event and entity coreference lead to insurmountable in- determinacy concerning the correct semantic la- bel <ref type="bibr">(de Marneffe et al. 2008 §4.3;</ref><ref type="bibr" target="#b18">Marelli et al. 2014b</ref>). For an example of the pitfalls surround- ing entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sen- tences refer to the same single event, but could also be reasonably labeled as neutral if that as- sumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present prob- lems. If we opt not to assume that events are coreferent, then we will only ever find contradic- tions between sentences that make broad univer- sal assertions, but if we opt to assume coreference, new counterintuitive predictions emerge. For ex- ample, Ruth Bader Ginsburg was appointed to the US Supreme Court and I had a sandwich for lunch today would unintuitively be labeled as a contra- diction, rather than neutral, under this assumption.</p><p>Entity coreference presents a similar kind of in- determinacy, as in the pair A tourist visited New York and A tourist visited the city. Assuming coreference between New York and the city justi- fies labeling the pair as an entailment, but with- out that assumption the city could be taken to refer to a specific unknown city, leaving the pair neu- tral. This kind of indeterminacy of label can be re- solved only once the questions of coreference are resolved.</p><p>With SNLI, we sought to address the issues of size, quality, and indeterminacy. To do this, we employed a crowdsourcing framework with the following crucial innovations. First, the exam- ples were grounded in specific scenarios, and the premise and hypothesis sentences in each exam- ple were constrained to describe that scenario from the same perspective, which helps greatly in con- trolling event and entity coreference. <ref type="bibr">2</ref> Second, the prompt gave participants the freedom to produce entirely novel sentences within the task setting, which led to richer examples than we see with the more proscribed string-editing techniques of ear- lier approaches, without sacrificing consistency. Third, a subset of the resulting sentences were sent to a validation task aimed at providing a highly re- liable set of annotations over the same data, and at identifying areas of inferential uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data collection</head><p>We used Amazon Mechanical Turk for data col- lection. In each individual task (each HIT), a worker was presented with premise scene descrip- tions from a pre-existing corpus, and asked to supply hypotheses for each of our three labels- entailment, neutral, and contradiction-forcing the data to be balanced among these classes.</p><p>The instructions that we provided to the work- ers are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Below the instructions were three fields for each of three requested sen- tences, corresponding to our entailment, neutral, and contradiction labels, a fourth field (marked optional) for reporting problems, and a link to an FAQ page. That FAQ grew over the course of data collection. It warned about disallowed tech- niques (e.g., reusing the same sentence for many different prompts, which we saw in a few cases), provided guidance concerning sentence length and We will show you the caption for a photo. We will not show you the photo. Using only the caption and what you know about the world:</p><p>• Write one alternate caption that is definitely a true description of the photo. Example: For the caption "Two dogs are running through a field." you could write "There are animals outdoors."</p><p>• Write one alternate caption that might be a true description of the photo. Example: For the cap- tion "Two dogs are running through a field." you could write "Some puppies are running to catch a stick."</p><p>• Write one alternate caption that is definitely a false description of the photo. Example: For the caption "Two dogs are running through a field." you could write "The pets are sitting on a couch." This is different from the maybe correct category because it's impossible for the dogs to be both running and sitting. complexity (we did not enforce a minimum length, and we allowed bare NPs as well as full sen- tences), and reviewed logistical issues around pay- ment timing. About 2,500 workers contributed. For the premises, we used captions from the Flickr30k corpus ( <ref type="bibr" target="#b33">Young et al., 2014</ref>), a collection of approximately 160k captions (corresponding to about 30k images) collected in an earlier crowd- sourced effort. <ref type="bibr">3</ref> The captions were not authored by the photographers who took the source images, and they tend to contain relatively literal scene de- scriptions that are suited to our approach, rather than those typically associated with personal pho- tographs (as in their example: Our trip to the Olympic Peninsula). In order to ensure that the la- bel for each sentence pair can be recovered solely based on the available text, we did not use the im- ages at all during corpus collection. <ref type="table">Table 2</ref> reports some key statistics about the col- lected corpus, and <ref type="figure" target="#fig_2">Figure 2</ref> shows the distributions of sentence lengths for both our source hypotheses and our newly collected premises. We observed that while premise sentences varied considerably in length, hypothesis sentences tended to be as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data set sizes: Training pairs 550,152 Development pairs 10,000 Test pairs 10,000</head><p>Sentence length: Premise mean token count 14.1 Hypothesis mean token count 8.3</p><p>Parser output:</p><p>Premise 'S'-rooted parses 74.0% Hypothesis 'S'-rooted parses 88.9% Distinct words (ignoring case) 37,026 <ref type="table">Table 2</ref>: Key statistics for the raw sentence pairs in SNLI. Since the two halves of each pair were collected separately, we report some statistics for both.</p><p>short as possible while still providing enough in- formation to yield a clear judgment, clustering at around seven words. We also observed that the bulk of the sentences from both sources were syn- tactically complete rather than fragments, and the frequency with which the parser produces a parse rooted with an 'S' (sentence) node attests to this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data validation</head><p>In order to measure the quality of our corpus, and in order to construct maximally useful test- ing and development sets, we performed an addi- tional round of validation for about 10% of our data. This validation phase followed the same basic form as the Mechanical Turk labeling task used to label the SICK entailment data: we pre- sented workers with pairs of sentences in batches of five, and asked them to choose a single label for each pair. We supplied each pair to four an- notators, yielding five labels per pair including the label used by the original author. The instructions were similar to the instructions for initial data col- lection shown in <ref type="figure" target="#fig_0">Figure 1</ref>, and linked to a similar FAQ. Though we initially used a very restrictive qualification (based on past approval rate) to se- lect workers for the validation task, we nonethe- less discovered (and deleted) some instances of random guessing in an early batch of work, and subsequently instituted a fully closed qualification restricted to about 30 trusted workers. For each pair that we validated, we assigned a gold label. If any one of the three labels was cho- sen by at least three of the five annotators, it was   <ref type="table" target="#tab_2">12  13  43314  15994  13  14  38121  11047  14  15  33183  7601  15  16  27621  5312  16  17  23250  3732  17  18  20247  2631  18  19  18513  1878  19  20  16386  1325  20  21  13746  911  21  22  12066  642  22  23  9183  449  23  24  7131  357  24  25  6198  217  25  26  5007  168  26  27  3963  138  27  28  3438  84  28  29  2631  67  29  30  1959  46  30  31  1956  26  31  32  1434  31  32  33  1086  23  33  34  912  16  34  35  897  19  35  36  774  8  36  37  453  12  37  38  618  4  38  39  291  5  39  40  330  2  40  41  249  4  41  42  180  2  42  43  225  1  43  44  162  1  44  45  108  1  48  46  87  1  51  47  60  2  55  48  36  1  56  49  90  1  60  50  21  1  62  51  66  52  51  53  36  54  24  55  63  56  18  57  15  58  6  59  27  60  6  61  3  62  3  63  3  64  6  65  3  66  3  67  6  68  6  69  18  70  15  71  3  72</ref>   chosen as the gold label. If there was no such con- sensus, which occurred in about 2% of cases, we assigned the placeholder label '-'. While these un- labeled examples are included in the corpus dis- tribution, they are unlikely to be helpful for the standard NLI classification task, and we do not in- clude them in either training or evaluation in the experiments that we discuss in this paper.</p><p>The results of this validation process are sum- marized in <ref type="table" target="#tab_2">Table 3</ref>. Nearly all of the examples received a majority label, indicating broad con- sensus about the nature of the data and categories. The gold-labeled examples are very nearly evenly distributed across the three labels. The Fleiss κ scores (computed over every example with a full five annotations) are likely to be conservative given our large and unevenly distributed pool of annotators, but they still provide insights about the levels of disagreement across the three semantic classes. This disagreement likely reflects not just the limitations of large crowdsourcing efforts but also the uncertainty inherent in naturalistic NLI. Regardless, the overall rate of agreement is ex- tremely high, suggesting that the corpus is suffi- ciently high quality to pose a challenging but real- istic machine learning task. <ref type="table">Table 1</ref> shows a set of randomly chosen validated examples from the development set with their la- bels. Qualitatively, we find the data that we col- lected draws fairly extensively on commonsense knowledge, and that hypothesis and premise sen- tences often differ structurally in significant ways, suggesting that there is room for improvement be- yond superficial word alignment models. We also find the sentences that we collected to be largely General: Validated pairs 56,951 Pairs w/ unanimous gold label 58.3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The distributed corpus</head><p>Individual annotator label agreement: Individual label = gold label 89.0% Individual label = author's label 85.8%</p><p>Gold  <ref type="bibr" target="#b11">Klein and Manning, 2003)</ref>, trained on the stan- dard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to im- prove the parse quality of the descriptive sentences and noun phrases found in the descriptions.</p><note type="other">label/author's label agreement: Gold label = author's label 91.2% Gold label = author's label 6.8% No gold label (no 3 labels match) 2.0% Fleiss κ: contradiction 0.77 entailment 0.72 neutral 0.60 Overall 0.70</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our data as a platform for evaluation</head><p>The most immediate application for our corpus is in developing models for the task of NLI. In par-  ticular, since it is dramatically larger than any ex- isting corpus of comparable quality, we expect it to be suitable for training parameter-rich models like neural networks, which have not previously been competitive at this task. Our ability to evaluate standard classifier-base NLI models, however, was limited to those which were designed to scale to SNLI's size without modification, so a more com- plete comparison of approaches will have to wait for future work. In this section, we explore the per- formance of three classes of models which could scale readily: (i) models from a well-known NLI system, the Excitement Open Platform; (ii) vari- ants of a strong but simple feature-based classi- fier model, which makes use of both unlexicalized and lexicalized features, and (iii) distributed repre- sentation models, including a baseline model and neural network sequence models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Excitement Open Platform models</head><p>The first class of models is from the Excitement Open Platform (EOP, <ref type="bibr" target="#b20">Padó et al. 2014;</ref><ref type="bibr" target="#b16">Magnini et al. 2014</ref>)-an open source platform for RTE re- search. EOP is a tool for quickly developing NLI systems while sharing components such as com- mon lexical resources and evaluation sets. We evaluate on two algorithms included in the dis- tribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP's full suite of lexical resources. Our initial goal was to better understand the dif- ficulty of the task of classifying SNLI corpus in- ferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set ( <ref type="bibr" target="#b8">Giampiccolo et al., 2007)</ref>. We report results in <ref type="table" target="#tab_4">Table 4</ref>. Each of the models was separately trained on the training set of each corpus. All models are evaluated only on 2-class entailment. To convert 3-class problems like SICK and SNLI to this setting, all instances of contradic- tion and unknown are converted to nonentailment. This yields a most-frequent-class baseline accu- racy of 66% on SNLI, and 71% on SICK. This is intended primarily to demonstrate the difficulty of the task, rather than necessarily the performance of a state-of-the-art RTE system. The edit dis- tance algorithm tunes the weight of the three case- insensitive edit distance operations on the train- ing set, after removing stop words. In addition to the base classifier-based system distributed with the platform, we train a variant which includes in- formation from WordNet <ref type="bibr" target="#b19">(Miller, 1995)</ref> and Verb- Ocean ( <ref type="bibr" target="#b2">Chklovski and Pantel, 2004</ref>), and makes use of features based on tree patterns and depen- dency tree skeletons ( <ref type="bibr" target="#b28">Wang and Neumann, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lexicalized Classifier</head><p>Unlike the RTE datasets, SNLI's size supports ap- proaches which make use of rich lexicalized fea- tures. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these features in lieu of more involved lan- guage understanding. Our classifier implements 6 feature types; 3 unlexicalized and 3 lexicalized:</p><p>1. The BLEU score of the hypothesis with re- spect to the premise, using an n-gram length between 1 and 4. 2. The length difference between the hypothesis and the premise, as a real-valued feature. 3. The overlap between words in the premise and hypothesis, both as an absolute count and a percentage of possible overlap, and both over all words and over just nouns, verbs, ad- jectives, and adverbs. 4. An indicator for every unigram and bigram in the hypothesis. 5. Cross-unigrams: for every pair of words across the premise and hypothesis which share a POS tag, an indicator feature over the two words. 6. Cross-bigrams: for every pair of bigrams across the premise and hypothesis which share a POS tag on the second word, an in- dicator feature over the two bigrams.</p><p>We report results in  for removing all lexicalized features. On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and an- other from using the very sparse cross-bigram fea- tures. The latter result suggests that there is value in letting the classifier automatically learn to rec- ognize structures like explicit negations and adjec- tive modification. A similar result was shown in <ref type="bibr" target="#b27">Wang and Manning (2012)</ref> for bigram features in sentiment analysis. It is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations. Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence embeddings and NLI</head><p>SNLI is suitably large and diverse to make it pos- sible to train neural network models that produce distributed representations of sentence meaning. In this section, we compare the performance of three such models on the corpus. To focus specif- ically on the strengths of these models at produc- ing informative sentence representations, we use sentence embedding as an intermediate step in the NLI classification task: each model must produce a vector representation of each of the two sen- tences without using any context from the other sentence, and the two resulting vectors are then passed to a neural network classifier which pre- dicts the label for the pair. This choice allows us to focus on existing models for sentence embedding, and it allows us to evaluate the ability of those models to learn useful representations of mean- ing (which may be independently useful for sub- sequent tasks), at the cost of excluding from con-  <ref type="table">Tables 6 and 7</ref>, two identical copies of the model are run with the two sentences as input, and their outputs are used as the two 100d inputs shown here. sideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level.</p><p>Our neural network classifier, depicted in <ref type="figure" target="#fig_3">Fig- ure 3</ref> (and based on a one-layer model in <ref type="bibr" target="#b1">Bowman et al. 2015</ref>), is simply a stack of three 200d tanh layers, with the bottom layer taking the con- catenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself.</p><p>We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sen- tence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>).</p><p>The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, <ref type="bibr" target="#b22">Pennington et al. 2014</ref>) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural net- work layer to map these 300d embeddings into the lower-dimensional phrase and sentence em- bedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta <ref type="bibr" target="#b34">(Zeiler, 2012</ref>) minibatch SGD un- til performance on the development set stops im- proving. We applied L2 regularization to all mod- els, manually tuning the strength coefficient λ for each, and additionally applied dropout ( <ref type="bibr" target="#b25">Srivastava et al., 2014</ref>) to the inputs and outputs of the sen-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence model</head><p>Train Test  <ref type="table">Table 6</ref>: Accuracy in 3-class classification on our training and test sets for each model. tence embedding models (though not to its internal connections) with a fixed dropout rate. All mod- els were implemented in a common framework for this paper, and the implementations will be made available at publication time.</p><p>The results are shown in <ref type="table">Table 6</ref>. The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier- while the sum of words model can use pretrained word embeddings to better handle rare words, it lacks even the rudimentary sensitivity to word or- der that the lexicalized model's bigram features provide. Of the two RNN models, the LSTM's more robust ability to learn long-term dependen- cies serves it well, giving it a substantial advan- tage over the plain RNN, and resulting in perfor- mance that is essentially equivalent to the lexical- ized classifier on the test set (LSTM performance near the stopping iteration varies by up to 0.5% between evaluation steps). While the lexicalized model fits the training set almost perfectly, the gap between train and test set accuracy is relatively small for all three neural network models, suggest- ing that research into significantly higher capacity versions of these models would be productive. <ref type="figure">Figure 4</ref> shows a learning curve for the LSTM and the lexicalized and unlexicalized feature-based models. It shows that the large size of the corpus is crucial to both the LSTM and the lexicalized model, and suggests that additional data would yield still better performance for both. In addi- tion, though the LSTM and the lexicalized model show similar performance when trained on the cur- rent full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrar- ily structured representations of sentence mean- ing may give it an advantage over the more con- strained lexicalized model on still larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis and discussion</head><p>We were struck by the speed with which the lexicalized classifier outperforms its unlexicalized 73.95 76.78 78. <ref type="table" target="#tab_2">22   30   40   50   60   70   80   1  10  100</ref> 1,000 10,000 100,000 1,000,000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% Accuracy</head><p>Training pairs used (log scale)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unlexicalized</head><p>Lexicalized LSTM <ref type="figure">Figure 4</ref>: A learning curve showing how the baseline classifiers and the LSTM perform when trained to convergence on varied amounts of train- ing data. The y-axis starts near a random-chance accuracy of 33%. The minibatch size of 64 that we used to tune the LSTM sets a lower bound on data for that model. counterpart. With only 100 training examples, the cross-bigram classifier is already performing bet- ter. Empirically, we find that the top weighted features for the classifier trained on 100 examples tend to be high precision entailments; e.g., playing → outside (most scenes are outdoors), a banana → person eating. If relatively few spurious entail- ments get high weight-as it appears is the case- then it makes sense that, when these do fire, they boost accuracy in identifying entailments.</p><p>There are revealing patterns in the errors com- mon to all the models considered here. Despite the large size of the training corpus and the distri- butional information captured by GloVe initializa- tion, many lexical relationships are still misana- lyzed, leading to incorrect predictions of indepen- dent, even for pairs that are common in the train- ing corpus like beach/surf and sprinter/runner. Semantic mistakes at the phrasal level (e.g., pre- dicting contradiction for A male is placing an order in a deli/A man buying a sandwich at a deli) indicate that additional attention to composi- tional semantics would pay off. However, many of the persistent problems run deeper, to inferences that depend on world knowledge and context- specific inferences, as in the entailment pair A race car driver leaps from a burning car/A race car driver escaping danger, for which both the lex- icalized classifier and the LSTM predict neutral. In other cases, the models' attempts to shortcut this kind of inference through lexical cues can lead them astray. Some of these examples have quali- ties reminiscent of Winograd schemas <ref type="bibr" target="#b32">(Winograd, 1972;</ref><ref type="bibr" target="#b13">Levesque, 2013)</ref>. For example, all the mod- els wrongly predict entailment for A young girl throws sand toward the ocean/A girl can't stand the ocean, presumably because of distributional associations between throws and can't stand.</p><p>Analysis of the models' predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntac- tic subject, following information structuring con- ventions associating subjects and topics in English ( <ref type="bibr" target="#b29">Ward and Birner, 2004</ref>). Our annotators generally followed suit, writing sentences that, while struc- turally diverse, share topic/focus (theme/rheme) structure with their premises. This promotes a coherent, situation-specific construal of each sen- tence pair. This is information that our models can easily take advantage of, but it can lead them astray. For instance, all of them stumble with the amusingly simple case A woman prepares ingre- dients for a bowl of soup/A soup bowl prepares a woman, in which prior expectations about paral- lelism are not met. Another headline example of this type is A man wearing padded arm protec- tion is being bitten by a German shepherd dog/A man bit a dog, which all the models wrongly di- agnose as entailment, though the sentences report two very different stories. A model with access to explicit information about syntactic or semantic structure should perform better on cases like these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transfer learning with SICK</head><p>To the extent that successfully training a neural network model like our LSTM on SNLI forces that model to encode broadly accurate representations of English scene descriptions and to build an en- tailment classifier over those relations, we should expect it to be readily possible to adapt the trained model for use on other NLI tasks. In this section, we evaluate on the SICK entailment task using a simple transfer learning method <ref type="bibr" target="#b23">(Pratt et al., 1991)</ref> and achieve competitive results.</p><p>To perform transfer, we take the parameters of the LSTM RNN model trained on SNLI and use them to initialize a new model, which is trained from that point only on the training portion of SICK. The only newly initialized parameters are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training sets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Test</head><p>Our data only 42.0 46.7 SICK only 100.0 71.3 Our data and SICK (transfer) 99.9 80.8 <ref type="table">Table 7</ref>: LSTM 3-class accuracy on the SICK train and test sets under three training regimes.</p><p>softmax layer parameters and the embeddings for words that appear in SICK, but not in SNLI (which are populated with GloVe embeddings as above).</p><p>We use the same model hyperparameters that were used to train the original model, with the excep- tion of the L2 regularization strength, which is re-tuned. We additionally transfer the accumula- tors that are used by AdaDelta to set the learn- ing rates. This lowers the starting learning rates, and is intended to ensure that the model does not learn too quickly in its first few epochs after trans- fer and destroy the knowledge accumulated in the pre-transfer phase of training.</p><p>The results are shown in <ref type="table">Table 7</ref>. Training on SICK alone yields poor performance, and the model trained on SNLI fails when tested on SICK data, labeling more neutral examples as contradic- tions than correctly, possibly as a result of subtle differences in how the labeling task was presented. In contrast, transferring SNLI representations to SICK yields the best performance yet reported for an unaugmented neural network model, surpasses the available EOP models, and approaches both the overall state of the art at 84.6% ) and the 84% level of interannota- tor agreement, which likely represents an approx- imate performance ceiling. This suggests that the introduction of a large high-quality corpus makes it possible to train representation-learning models for sentence meaning that are competitive with the best hand-engineered models on inference tasks.</p><p>We attempted to apply this same transfer evalu- ation technique to the RTE-3 challenge, but found that the small training set (800 examples) did not allow the model to adapt to the unfamiliar genre of text used in that corpus, such that no training con- figuration yielded competitive performance. Fur- ther research on effective transfer learning on small data sets with neural models might facilitate improvements here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Natural languages are powerful vehicles for rea- soning, and nearly all questions about meaning- fulness in language can be reduced to questions of entailment and contradiction in context. This sug- gests that NLI is an ideal testing ground for the- ories of semantic representation, and that training for NLI tasks can provide rich domain-general se- mantic representations. To date, however, it has not been possible to fully realize this potential due to the limited nature of existing NLI resources. This paper sought to remedy this with a new, large- scale, naturalistic corpus of sentence pairs labeled for entailment, contradiction, and independence. We used this corpus to evaluate a range of models, and found that both simple lexicalized models and neural network models perform well, and that the representations learned by a neural network model on our corpus can be used to dramatically improve performance on a standard challenge dataset. We hope that SNLI presents valuable training data and a challenging testbed for the continued application of machine learning to semantic representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The instructions used on Mechanical Turk for data collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7</head><label>7</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The distribution of sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The neural network classification architecture: for each sentence embedding model evaluated in Tables 6 and 7, two identical copies of the model are run with the two sentences as input, and their outputs are used as the two 100d inputs shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics for the validated pairs. The au-
thor's label is the label used by the worker who 
wrote the premise to create the sentence pair. A 
gold label reflects a consensus of three votes from 
among the author and the four annotators. 

fluent, correctly spelled English, with a mix of 
full sentences and caption-style noun phrase frag-
ments, though punctuation and capitalization are 
often omitted. 
The corpus is available under a CreativeCom-
mons Attribution-ShareAlike license, the same li-
cense used for the Flickr30k source captions. It 
can be downloaded at: 
nlp.stanford.edu/projects/snli/ 

Partition We distribute the corpus with a pre-
specified train/test/development split. The test 
and development sets contain 10k examples each. 
Each original ImageFlickr caption occurs in only 
one of the three sets, and all of the examples in the 
test and development sets have been validated. 

Parses The distributed corpus includes parses 
produced by the Stanford PCFG Parser 3.5.2 
(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>2-class test accuracy for two simple 
baseline systems included in the Excitement Open 
Platform, as well as SICK and RTE results for a 
model making use of more sophisticated lexical 
resources. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 ,</head><label>5</label><figDesc>along with abla- tion studies for removing the cross-bigram fea- tures (leaving only the cross-unigram feature) and System SNLI SICK Train Test Train Test</figDesc><table>Lexicalized 
99.7 78.2 90.4 77.8 
Unigrams Only 93.1 71.6 88.1 77.0 
Unlexicalized 
49.4 50.4 69.9 69.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>3-class accuracy, training on either our 
data or SICK, including models lacking cross-
bigram features (Feature 6), and lacking all lexical 
features (Features 4-6). We report results both on 
the test set and the training set to judge overfitting. 

</table></figure>

			<note place="foot" n="2"> Issues of coreference are not completely solved, but greatly mitigated. For example, with the premise sentence A dog is lying in the grass, a worker could safely assume that the dog is the most prominent thing in the photo, and very likely the only dog, and build contradicting sentences assuming reference to the same dog.</note>

			<note place="foot" n="3"> We additionally include about 4k sentence pairs from a pilot study in which the premise sentences were instead drawn from the VisualGenome corpus (under construction; visualgenome.org). These examples appear only in the training set, and have pair identifiers prefixed with vg in our corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge support from a Google Faculty Research Award, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filter-ing of Text (DEFT) Program under Air Force Re-search Laboratory (AFRL) contract no. FA8750-13-2-0040, the National Science Foundation un-der grant no. IIS 1159679, and the Department of the Navy, Office of Naval Research, under grant no. N00014-10-1-0109. Any opinions, find-ings, and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of Google, Bloomberg L.P., DARPA, AFRL NSF, ONR, or the US government. We also thank our many ex-cellent Mechanical Turk contributors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognising textual entailment with logical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recursive neural networks can learn logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VerbOcean: Mining the web for fine-grained semantic verb relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entailment, intensionality and text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleo</forename><surname>Condoravdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Valeria De Paiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">G</forename><surname>Stolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the HLT-NAACL 2003 Workshop on Text Meaning</title>
		<meeting>of the HLT-NAACL 2003 Workshop on Text Meaning</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding contradictions in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">N</forename><surname>Rafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kucera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
		<respStmt>
			<orgName>Brown corpus manual. Brown University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A natural logic inference system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Fyodorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoad</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissim</forename><surname>Francez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on Inference in Computational Semantics</title>
		<meeting>of the 2nd Workshop on Inference in Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>of the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semantic Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerrold</forename><forename type="middle">J</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Harper &amp; Row</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Illinois-LH: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On our best behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focused entailment graphs for open IE propositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An extended model of natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Eighth International Conference on Computational Semantics</title>
		<meeting>of the Eighth International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Excitement Open Platform for textual inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zanoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathrin</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Gil</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Design and realization of a modular architecture for textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Gil</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zanoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charley</forename><surname>Beller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Direct transfer of learned information among neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lorien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><forename type="middle">A</forename><surname>Mostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ace</forename><forename type="middle">A</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A brief history of natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Benthem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Logic, Navya-Nyaya and Applications: Homage to Bimal Matilal</title>
		<editor>M. Chakraborty, B. Löwe, M. Nath Mitra, and S. Sarukki</editor>
		<imprint>
			<publisher>College Publications</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment using sentence similarity based on dependency tree skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Information structure and non-canonical syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Betty</forename><surname>Birner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Pragmatics</title>
		<editor>Laurence R. Horn and Gregory Ward</editor>
		<meeting><address><addrLine>Blackwell, Oxford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="153" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sumit Chopra, and Antoine Bordes. 2015b. Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="191" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
