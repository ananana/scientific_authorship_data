<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Factored Neural Network Model for Characterizing Online Discussions in Vector Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Factored Neural Network Model for Characterizing Online Discussions in Vector Space</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2296" to="2306"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We develop a novel factored neural model that learns comment embeddings in an un-supervised way leveraging the structure of distributional context in online discussion forums. The model links different context with related language factors in the embedding space, providing a way to interpret the factored embeddings. Evaluated on a community endorsement prediction task using a large collection of topic-varying Reddit discussions, the fac-tored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the model captures community style and topic, as well as response trigger patterns.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Massive user-generated content on social media has drawn interests in predicting community reac- tions in the form of virality <ref type="bibr" target="#b12">(Guerini et al., 2011</ref>), popularity <ref type="bibr" target="#b31">(Suh et al., 2010;</ref><ref type="bibr" target="#b15">Hong et al., 2011;</ref><ref type="bibr" target="#b20">Lakkaraju et al., 2013;</ref><ref type="bibr" target="#b34">Tan et al., 2014</ref>), commu- nity endorsement <ref type="bibr" target="#b17">(Jaech et al., 2015;</ref>, persuasive impact ( <ref type="bibr" target="#b0">Althoff et al., 2014;</ref><ref type="bibr" target="#b35">Tan et al., 2016;</ref><ref type="bibr" target="#b38">Wei et al., 2016)</ref>, etc. Many of these studies have analyzed content-agnostic fac- tors such as submission timing and author social status, as well as language factors that underlie the textual content, e.g., the topic and idiosyncrasies of the community. In particular, there is an in- creasing amount of work on online discussion fo- rums such as Reddit that exploits the conversa- tional and community-centric nature of the user- generated content ( <ref type="bibr" target="#b20">Lakkaraju et al., 2013;</ref><ref type="bibr" target="#b0">Althoff et al., 2014;</ref><ref type="bibr" target="#b17">Jaech et al., 2015;</ref><ref type="bibr" target="#b35">Tan et al., 2016;</ref><ref type="bibr" target="#b38">Wei et al., 2016;</ref><ref type="bibr" target="#b13">He et al., 2016a;</ref>, which contrasts with Twitter where the au- thor's social status seems to play a larger role in popularity. This paper focuses on Reddit, using the karma score 1 as a readily available measure of community endorsement.</p><p>Some of the prior work on Reddit investi- gates specific linguistic phenomena (e.g. polite- ness, topic relevance, community style matching) using feature engineering to understand their role in predicting community reactions <ref type="bibr" target="#b0">(Althoff et al., 2014;</ref><ref type="bibr" target="#b17">Jaech et al., 2015)</ref>. In contrast, this pa- per explores methods for unsupervised text em- bedding learning using a model structured so as to provide some interpretability of the results when used in comment endorsement prediction. The model aims to characterize the interdependence of comment on its global context and subsequent responses that is characteristic of multi-party dis- cussions. Specifically, we propose a factored neu- ral model with separate mechanisms for represent- ing global context, comment content and response generation. By factoring the model, we hope un- supervised learning will pick up different compo- nents of interactive language in the resulting em- beddings, which will improve prediction of com- munity reactions.</p><p>Distributed representations of text, or text em- beddings, have achieved great success in many language processing applications, using both su- pervised and unsupervised methods. Unsuper- vised learning, in particular, has been successful at different levels, including words ( <ref type="bibr" target="#b25">Mikolov et al., 2013b</ref>), sentences ( <ref type="bibr" target="#b19">Kiros et al., 2015)</ref>, and docu- ments ( <ref type="bibr" target="#b6">Deerwester et al., 1990;</ref><ref type="bibr" target="#b21">Le and Mikolov, 2014)</ref>. Studies have also shown that the learned embedding captures both syntactic and semantic functions of words ( <ref type="bibr" target="#b24">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b28">Pennington et al., 2014;</ref><ref type="bibr" target="#b22">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b9">Faruqui et al., 2015a</ref>). At the same time, em-beddings are often viewed as uninterpretable -it is difficult to align embedding dimensions to ex- isting semantic or syntactic classes. This con- cern has triggered attempts in developing more interpretable embedding models <ref type="bibr" target="#b10">(Faruqui et al., 2015b)</ref>, which is also a goal of our work. We leverage the fact that the structure of the distribu- tional context impacts what is learned in an unsu- pervised way and include multiple objectives for separating different types of context.</p><p>Here, we are interested in linking two types of context with corresponding language factors learned in the embedding space that may impact comment reception. First, conformity to the topic and the language use of the community tends to make the content better accepted ( <ref type="bibr" target="#b20">Lakkaraju et al., 2013;</ref><ref type="bibr" target="#b34">Tan et al., 2014;</ref><ref type="bibr" target="#b36">Tran and Ostendorf, 2016)</ref>. Those global modes typically influence the au- thor's generation of local content. Second, charac- teristics of a comment can influence the responses it triggers. Clearly, questions and statements will elicit different responses, and comments directed at a particular discussion participant may prompt that individual to respond. Of more interest here are aspects of comments that might elicit minimal response or responses with different sentiments, which are relevant for eventual endorsement.</p><p>The primary contribution of this work is the de- velopment of a factored neural model to jointly learn these aspects of multi-party discussions from a large collection of Reddit comments in an un- supervised fashion. Extending the recent neural attention model ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, the pro- posed model can interpret the learned latent global modes as community-related topic and style. A comment-response generation model component captures aspects of the comment that are response triggers. The multi-factored comment embedding is evaluated on the task of predicting the comment endorsement for three online communities differ- ent in topic trends and writing style. The represen- tation of textual information using our approach consistently outperforms multiple document em- bedding baselines, and analyses of the global modes and response trigger subvectors show that the model learns common communication strate- gies in discussion forums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head><p>To characterize different aspects of language use in a comment, the proposed model factorizes a <ref type="figure">Figure 1</ref>: The structure of the full model omitting output layers, illustrating the computation of at- tention weights for b 2 and d 3 in a comment w 1:4 with its response r 1:4 . Purple circles a k and a j represent scalars computed in (1) and (6), respec- tively. ⊗ and ⊕ are scaling and element-wise ad- dition operators, respectively. Black arrowed lines are connections carrying weight matrices. comment embedding into two sub-vectors, i.e. a local mode vector and a content vector. The lo- cal mode vector, computed as a mixture of global mode vectors, exploits the global context of a comment. In Reddit discussions that we use, the global mode represents the topic and language id- iosyncracies (style) of a particular subreddit. More specific information communicated in the com- ment is captured in the content vector. The gen- eration process of a comment is modeled through a recurrent neural network (RNN) language model (LM) conditioned on local mode and content vec- tors, while the global mode vectors are jointly learned during the training. Moreover, a residual learning architecture ( <ref type="bibr" target="#b14">He et al., 2016b</ref>) is used to extend the RNN LM for separating the informa- tion flow of the mode and the content vectors.</p><p>In addition to the global context, the full model further exploits direct responses to the comment in order to learn better comment embeddings. This is achieved by modeling the generation of comment responses through another RNN LM conditioned on response trigger vectors. The response trigger vectors are computed as mixtures of content vec-tors, with the idea that they will characterize as- pects of the comment that incent others to respond, whether that be information or framing.</p><p>The full model is illustrated in <ref type="figure">Fig. 1</ref>. While the end goal is a joint framework, the model is de- scribed in the following two sub-sections in terms of two components: i) mode vectors for capturing global context, and ii) response trigger vectors for exploiting comment responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mode Vectors</head><p>Using an RNN LM shown in the upper part of <ref type="figure">Fig. 1</ref>, we model the generation process of a word sequence by predicting the next word conditioned on the global context as well as the local con- tent. The global context is encoded in the lo- cal mode vector, computed as a mixture of global mode vectors with mixture weights inferred based on content vectors. The local mode vector indi- cates where the comment fits in terms of what peo- ple in this subreddit generally say. It changes dy- namically with the content vector as the comment generation progresses, considering the possibility of topic shifts or different broad categories of dis- cussion participants.</p><p>Suppose there is a set of K latent global modes with distributed representations m 1:K ∈ R n . For the t-th word w t in a sequence, a local mode vector b t ∈ R n is computed as</p><formula xml:id="formula_0">bt = K k=1 a(ct, m k ) ⊗ m k ,</formula><p>where c t ∈ R n is the content vector for the cur- rent partial sequence w 1:t , ⊗ multiplies a vector by a scalar, and the function a(c t , m k ) outputs a scalar association probability for the current con- tent vector c t and a mode vector m k . The associ- ation function a(c, m k ) is defined as</p><formula xml:id="formula_1">a(c, m k ) = exp(v T tanh(U [c; m k ]) K i=1 exp(v T tanh(U [c; mi])) ,<label>(1)</label></formula><p>where U ∈ R n×2n and v ∈ R n are parameters characterizing the similarity between m k and c. The computation of the association probabil- ity is the well-known attention mechanism (Bah- danau et al., 2015). However, unlike the original attention RNN model where the attended vector is concatenated with the input vector to augment the input to the recurrent layer, we adopt a residual learning approach ( <ref type="bibr" target="#b14">He et al., 2016b</ref>) to learn con- tent vectors. For the t-th word w t in a sequence, the content vector c t under the original attention RNN model is computed as</p><formula xml:id="formula_2">ct = f (Wxt + Gbt−1, ct−1),<label>(2)</label></formula><p>where x t ∈ R d is the word embedding for w t , b t−1 ∈ R n and c t−1 ∈ R n are previous lo- cal mode and content vectors, respectively, W ∈ R n×d and G ∈ R n×n are weight matrices trans- forming the input to the recurrent layer, and f (·, ·) is the recurrent layer activation function. To ad- dress the vanishing gradient issue in RNNs, we use the gated recurrent unit ( <ref type="bibr" target="#b5">Cho et al., 2014</ref>) for the RNN layer, i.e.</p><formula xml:id="formula_3">f (p, q) = (1−u) tanh(p+R[r q])+u q,</formula><p>where is the element-wise multiplication, R is the recurrent weight matrix, and u and r are the update and reset gates, respectively. In this paper, we compute the content vector c t as follows:</p><formula xml:id="formula_4">ct = f (Wxt, Gbt−1 + ct−1).<label>(3)</label></formula><p>Comparing <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_4">(3)</ref>, it can be seen that we first aggregate the local mode vector b t−1 and the content vector c t−1 and treat the resulting vec- tor Gb t−1 + c t−1 as the memory of the recurrent layer. The resulting hidden state vectors from the recurrent layer are content vectors c t 's. The use of residual learning is motivated by the following considerations. The local mode vector b t−1 can be seen as a non-linear transformation of c t−1 into a global mode space parameterized by m 1:K . If the global information carried in b t−1 is residual for generating the following word in the comment, the model only needs to exploit the information in local content c t−1 and learns to zero out the local mode vector b t−1 , i.e. G = 0. <ref type="bibr" target="#b14">He et al. (2016b)</ref> show that the residual learning usually leads to a more well-conditioned model which promises bet- ter generalization ability. Finally, the RNN LM estimates the probability of the (t + 1)-th word w t+1 based on the current local mode vector b t and content vector c t , i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Response Trigger Vectors</head><p>Another important aspect of comments in online discussions is how other participants react to the content. In order to exploit those characteris- tics, we use comment-reply pairs in online discus- sions and build this component upon the encoder- decoder framework with the attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which is illustrated in the lower part of <ref type="figure">Fig. 1</ref>. The decoder is essentially an- other RNN LM conditioned on response trigger vectors aiming at distilling relevant parts of the comment which other people are responding to.</p><p>Let r j denote the j-th word in a reply to a com- ment w 1 , · · · , w T . The decoder RNN LM com- putes a hidden vector h j ∈ R n for r j as follows,</p><formula xml:id="formula_5">hj = f (W † xj + G † dj−1, hj−1),<label>(5)</label></formula><p>where W † ∈ R n×d and G † ∈ R n×n are weight matrices, x j is r j 's word embeddings from a shared embedding dictionary as used by the en- coder RNN LM in Subsection 2.1, and d j−1 ∈ R n and h j−1 ∈ R n are the response trigger vector and hidden vector at the previous time step, re- spectively. The initial hidden vector h 0 is set to be the last content vector c T . With the comment's content vectors c 1 , · · · , c T obtained from the en- coder RNN LM in Subsection 2.1, a response trig- ger vector d j is computed as the mixture:</p><formula xml:id="formula_6">dj = T t=1 a (hj, ct) · ct,<label>(6)</label></formula><p>where a (h j , c t ) is a similar function to a(c t , m k ) defined in (1) with different parameters. Similar to the encoder RNN LM, the decoder RNN LM estimates the probability of the (j + 1)-th word r j+1 in the reply based on the hidden vector h j and the response trigger vector d j , i.e.</p><formula xml:id="formula_7">Pr(rj+1|r1:j) = softmax(Q † [hj; dj]),</formula><p>where Q † ∈ R V ×2n is the weight matrix. Note the decoder RNN only aims at providing additional supervision signals in training the en- coder RNN through a response generation task. At test time, we do not use the responses therefore do not need to run the decoder RNN LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Learning</head><p>The full model is trained by maximizing the log- likelihood of the data, i.e. i log Pr(w</p><formula xml:id="formula_8">(i) 1:T (i) ) + α log Pr(r (i) 1:J (i) |w (i) 1:T (i) ),</formula><p>where the two terms correspond to the log- likelihood of the encoder RNN LM and the de- coder RNN LM, respectively, and α is the hyper parameter which weights the importance of the second term. In our experiments, we let α = 0.1. During the training, each comment-reply pair is used as a training sample. Considering that com- ments may receive a huge number of replies, we keep up to 5 replies for each comment. Due to memory limitations associated with the RNN, we use only the first 50 words of comments and the first 20 words of replies. If a comment has no re- ply, a special token is used. All weights are ran- domly initialized according to N (0, 0.01). The model is optimized using Adam ( <ref type="bibr" target="#b18">Kingma and Ba, 2015</ref>) with an initial learning rate 0.01. Once the validation log-likelihood decreases for the first time, we halve the learning rate at each epoch. The training process is terminated when the validation log-likelihood decreases for the second time. In our experiments, we learn word embeddings of di- mension d = 256 from scratch. The number of modes K is set to 16. A single-layer RNN is used, with the dimension n of hidden layers set to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and Task</head><p>In this paper, we work with Reddit discussion threads, taking advantage of their conversational and community-centric nature as well as the avail- able karma scores. Each thread starts from a post and grows with comments to the post or other comments within the thread, presented as a tree structure. Posts and comments can be voted up or down by readers depending on whether they agree or disagree with the opinion, find it amusing vs. of- fensive, etc. A karma score is computed as the dif- ference between up-votes and down-votes, which has been used as a proxy of community endorse- ment for a Reddit comment. Three popular sub- reddits with different topics and styles are studied 2 AskWomen (814K comments), AskMen (1,057K comments), and Politics (2,180K comments).</p><p>For each subreddit, we randomly split comments by threads into training, validation, and test data, with a 3:1:1 ratio. The vocabulary of each sub- reddit is built on the training set. After removing singletons, the vocabulary sizes are 45K, 52K, and 60K for AskWomen, AskMen, and Politics, respectively. Task: Considering the heavy-tailed Zipfian distri- bution of karma scores, regression with a mean squared error objective may not be informative be- cause low-karma comments dominate the overall objective. Following , we quan- tize comment karma scores into 8 discrete levels and design a task consisting of 7 binary classifica- tion subtasks which individually predict whether a comment's karma is at least level-l for each level l = 1, · · · , 7. This task is sensitive to the order of quantized karma scores, e.g., for the level-6 sub- task, predicting a comment as level-5 or level-7 would lead to different evaluation results such as recall, which is not the case for a standard multi- class classification task. Additionally, compared to a standard multi-class classification task, these subtasks alleviate the unbalanced data issue, al- though higher levels are still more skewed. Evaluation metric: For each level-l binary clas- sification subtask, we compute the F1 score by treating comments at levels lower than l as nega- tive samples and others as positive samples. Note that we only compute F1 scores for l ∈ {1, . . . , 7} since no comment is at a level lower than 0. The averaged F1 scores is used as an indicator of the overall prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate the effectiveness of the factored comment embeddings on the quantized karma prediction task. We use the concatenation of the local mode vector and the content vector at the last time step as the factored comment embed- ding. First, we study the overall prediction perfor- mance of four different classifiers under two set- tings, i.e., using factored comment embeddings or not. Then we compare the factored comment em- beddings inferred from the full model and its two</p><formula xml:id="formula_9">Range Description 0/1</formula><p>Whether the comment author is the user who initiated the thread. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R ≥0</head><p>Relative comment time (in hours) with respect to the original post.</p><p>Relative comment time (in hours) with respect to the parent com- ment. Normalized † number of replies to the comment. Normalized † number of comments in the subtree rooted from the comment.  variants with other kinds of text features using the best type of classifiers. Finally, we carry out error analysis on prediction results of the best classifiers using the factored comment embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classifiers</head><p>The following four types of classifiers are studied:</p><p>• ShallowLR: A standard multi-class logistic re- gression model; • ShallowOR: An ordinal regression model <ref type="bibr" target="#b30">(Rennie and Srebro, 2005</ref>), which can exploit the or-der information of the quantized karma labels; • DeepLR: A feed-forward neural network using the logistic regression objective; • DeepOR: A feed-forward neural network using the ordinal regression objective. These classifiers have different objectives and model complexities, allowing us to study the ro- bustness of the learned comment embeddings. The factored comment embeddings are inferred from the proposed models trained on the same training data but independently with these classifiers.</p><p>As baselines, we train the classifiers using only content-agnostic features, as shown in <ref type="table" target="#tab_0">Table 1</ref>, which have strong correlations with community endorsement ( <ref type="bibr" target="#b17">Jaech et al., 2015;</ref>). In our pilot work, we experimented with several groups of features from (Jaech et al., 2015) to find the content-agnostic features used in our paper. Since Jaech et al. (2015) work on a different task (ranking comments in a short time window), many of the useful content-agnostic features from (Jaech et al., 2015), including k-index, do not give ad- ditional improvement over the selected configura- tion for the karma prediction task.</p><p>All classifiers are trained on the training data for each subreddit independently, with hyper- parameter tuned on the validation data. The penultimate weights are regularized using L 2 and the regularization parameters are selected in {0.0, 0.001, 0.01, 0.1, 1.0}. The number of hid- den layers for deep classifiers are chosen from {1, 2, 3}, and the number of hidden neurons is se- lected from {32, 48, 64}.</p><p>We report the prediction performance on the test data, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. We observe that us- ing comment embeddings consistently improves the performance of these classifiers. While Shal- lowOR significantly outperforms ShallowLR, in- dicating the usefulness of exploiting the order in- formation in quantized karma labels, the differ- ence is much smaller for deep classifiers. Also, deep classifiers consistently outperforms their shallow counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Text Features</head><p>We compare the factored comment embeddings with the following text features:</p><p>• BoW: A sparse bag-of-word representation;</p><p>• LDA: A vector of topic probabilities inferred from the topic modeling ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>); • Doc2Vec: Embeddings inferred from the para- graph vector model ( <ref type="bibr" target="#b21">Le and Mikolov, 2014</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AskMen</head><p>AskWomen Politics level&gt;0 level&gt;1 level&gt;2 level&gt;3 level&gt;4 level&gt;5 level&gt;6</p><p>Figure 3: F1 scores of the DeepOR classifier for individual subtasks. Error bars indicate the im- provement of using the factored comment embed- dings over the classifier using no text features.</p><p>For these models, which do not use RNNs, all words in a comment are used. We use the gen- sim implementations ( ˇ Rehůřek and Sojka, 2010) for both LDA and Doc2Vec. For LDA, the num- ber of topic is selected in {16, 32, 64}, and 32 works the best on the validation set for all sub- reddits. For Doc2Vec, we select the embedding dimension from {32, 64, 128}, and 64 works the best on the validation set for all subreddits. We train the Doc2Vec for 20 epochs, and the learning rate is initialized as 0.025 and decreased by 0.001 at each epoch.</p><p>In addition to the factored comment embed- dings obtained from our full model, we study two variants of the full model: 1) a model trained with- out the mode vector component (Factored\M), which is a normal sequence-to-sequence attention model ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, and 2) a model trained without the response trigger vector compo- nent (Factored\R). All textual representations are used together with the baseline content-agnostic features described previously.</p><p>Since the DeepLR and the DeepOR perform best across all subreddits and they have similar trends, we report results of the DeepOR in Tabel 2. Among all text features, the BoW has the worst av- eraged F1 scores and even hurts the performance for AskWomen, probably due to the data sparsity problem. Both the LDA and the Doc2Vec out- perform the BoW. The Doc2Vec performs slightly better on AskMen and Politics, which might be attributed to the relative larger training data size. The factored comment embeddings derived from the full model consistently achieve better av- eraged F1 scores. It can be observed that the two variants of the full model mostly lead to similar performance as the Doc2Vec, though the Factored\R embeddings usually have higher aver- aged F1 scores than the Factored\M embeddings. These results suggest advantages of jointly model- ing two components, which may drive the model to discover more latent factors and patterns in the data that could be useful for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>In this subsection, we focus on analyzing how fac- tored comment embeddings improve the predic- tion results of the DeepOR classifiers. The F1 scores for individual subtasks are shown in <ref type="figure">Fig. 3</ref>. Note that the higher the level is, the more skewed the task is, i.e. a lower positive ratio. As expected, comments with the lowest endorsement level are easier to classify. Adding comment embeddings primarily boosts the performance of the classifier on the high-endorsement tasks (level &gt; 5, 6) and the low-endorsement tasks (level &gt; 0, 1).</p><p>Confusion matrices for the DeepOR classifier with and without factored comment embeddings are shown in <ref type="figure" target="#fig_3">Fig. 4</ref> for Politics. Using the additional comment embeddings leads to a higher concentration of cell weights near the diagonals, corresponding to errors that mainly confuse neigh- boring levels. Without any text features, the clas- sifier seems to only distinguish four levels. We ob- serve similar trends on AskWomen and AskMen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Analysis</head><p>In this section, we conduct analysis to better understand what the factored model is learning, again using the Politics subreddit. First, we analyze latent global modes learned from the full model. For each global mode, we extract com- ments with top association scores. Note that the model assumes a locally coherent mixture of global modes and updates the mixture for each ob- served word. Thus, each comment receives a se- quence of association probabilities over the global modes. The association score β k between a com- ment w 1:T and Mode-k is then computed as β k = max t∈{1,··· ,T } a(c t , m k ) for k ∈ {1, · · · , K}, where a(c t , m k ) is defined in (1). In <ref type="table">Table 3</ref>, we show examples from the most coherent modes out of the 16 learned modes. Some modes seem to be capturing style (modes 2, 6, and 10), while others are related to topics (modes 7 and 16). Mode-2 captures the style of starting with rhetor- ical question to express negative sentiment and disagreement. Many comments in Mode-6 be- gin with words of drawing attention such as "bull" and "psst". Mode-10 tends to be associated with comments telling a story about a closely re- lated person. Many comments in Mode-7 dis- cuss low salaries, whereas Mode-16 comments discuss politicians or ideology of the Republican.</p><p>The characteristics of examples in modes 2 and 6 suggested that modes might have a loca- tion dependency, so we looked at word positions with the strongest association of each mode, i.e. argmax t∈{1,··· ,T } a(c t , m k ). For each Mode-k, we only keep comments with association score higher than mean(β k ) + std(β k ). <ref type="figure" target="#fig_4">Fig. 5</ref> shows the box plot of locations where the strongest associ- ation happens. It can be seen that modes 2 and 6 usually have the strongest association at the be- ginning of a comment. For modes 3, 8, 15 and 16, the strongest associations occur over a wider span in comments. In addition to the interpretability of Mode-2</p><p>• Oh come on! Really? One can't make that trip and spend maybe half and save the other for milk, bread and things that do spoil? . . .</p><p>• Remind me. How many filibusters did Harry Reid conduct this year? . . .</p><p>• Feckless tyrant? How did you do that with your brain? . . .</p><p>• Seriously? You have to be registered to vote. . . .</p><p>• Holy f*, seriously? This is some heavy duty shit. . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mode-6</head><p>• Bull. Plenty of individuals influence policy by never missing a single chance to vote, no matter how minor the election. . . .</p><p>• Bull. Conservatives hate Obamacare so much because if their constituents got mental health treatment, they'd stop voting Republican.</p><p>• Utter bull s*. Where was the compromise from Obama and the Dems when they pushed through Obamacare without ONE Republican vote. . . .</p><p>• psst. . . it's college • psst-he's "black" -meaning that one of his ancestors is black (as if it's pollutant of some sort).</p><p>Mode-7</p><p>• . . . , I used to work 55+ hours a week, salaried, lower quartile salary to boost. . . .</p><p>• Or possibly that the standard of living between unemployment and the "jobs" that are out there is really insignificant. . . .</p><p>• Where on earth is 7.25 a living wage? If by some miracle you get 40 hours a week that's only $1,160 before taxes. . . .</p><p>• If you have to work 40 hours a week to pay your bills that means you are controlled in your fight for survival. . . .</p><p>• . . . Working 15 hours a week for extra pocket money when you're a teen is easy. Working 50 hours a week at fast food to cover rent, food, . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mode-10</head><p>• . . . Had a guy stalk a trans friend of mine for months trying to terrorize her because . . .</p><p>• A co-worker of mine got audited by the IRS because . . .</p><p>• . . . Some conservative friends of mind wanted to meet up at a coffee house with shittier coffee because the other one was too "liberal". . . .</p><p>• . . . Friend of mine works with mentally unstable and aggressive people as part of some social service. . . .</p><p>• . . . A student of mine asked our own AP about an atheist group and he just flat out said "You kidding me?" . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mode-16</head><p>• . . . These same people will continue to listen to the bullshit that is the Republican Party. And when that happens, they have this twisted reality . . .</p><p>• . . . After spending almost my entire life in Texas and as a Born gain evangelical conservative Republican, I learned my lessons about how completely dishonest and corrupted that entire culture is the hard way. . . . I will never gain ever vote for or support any kind of conservative. . . .</p><p>• . . . has been our greatest embarrassment, but what makes matter even worse is the support he has for re-election. I would not be surprised . . .</p><p>• Well, it is entirely possible that . . . the underlying cause of Limbaugh's attack was that this guy was playing the type of dirty politics . . .</p><p>• . . . this was more of a referendum on the GOP leadership in Congress by Republican voters, because let's face it, they haven't done anything.. . . <ref type="table">Table 3</ref>: Examples of comments associated with the learned global modes for Politics.</p><p>the learned modes as one can get from LDA, these observations suggest that our model may further capture word location effects which may help pre- dicting community endorsement.</p><p>Next, we analyze the response characteristics by examining the response trigger vectors associated with the onset of comment responses, which is a special start-of-reply token. These response trig- ger vectors are clustered into 8 classes via k-means and visualized in <ref type="figure" target="#fig_5">Fig. 6</ref> using t-SNE <ref type="bibr" target="#b23">(van der Maaten and Hinton, 2008)</ref>. For each cluster, we study the karma distribution, as well as comments together with the first reply. Related data statistics and examples are shown in <ref type="figure" target="#fig_3">Fig. A-4</ref> and Tables A- 2&amp;A-3 in the supplementary materials. The hor- izontal dimension seems to be associated with how many replies a comment elicits. The vertical dimension is less interpretable but most clusters have identifiable traits. The far left classes (Class- 1&amp;4) both have few replies and low karma, often two-party exchanges where Class-4 has more neg- ative sentiment. Class-2 comments tend to involve complements, whereas comments in Class-3 usu- ally trigger a reply with but-clause for a contrast and disagreement intent. Comments in Class-5 mostly receive responses expanding on the orig- inal comments. Class-6 has a lot of sarcastic and cynical comments and replies. Comments in Class-7 are mostly anomalous since their first re- sponses were usually <ref type="bibr">[deleted]</ref>. It seems there are multiple response trigger factors in the proposed embedding model, some may reflect dialog acts and others sentiment, any of which may be helpful in predicting community endorsement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The skip-thought vector method ( <ref type="bibr" target="#b19">Kiros et al., 2015</ref>) is most closely related to our work in terms of utilizing context for unsupervised sequence modeling under the sequence-to-sequence frame- work ( <ref type="bibr" target="#b33">Sutskever et al., 2014</ref>). A key difference is the context being exploited. The skip-thought vector method uses surrounding sentences by ab- stracting the skip-gram structure ( <ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>) from word to sequence. In our model, we exploit two types of context that are unique in online discussions: 1) the global context such as community topic and style which is learned in the mode vectors, and 2) the responses to a comment modeled as the response trigger vectors. More- over, we augment our model with the attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) to push the model to distill the relevant information from con- text. The neural attention mechanism has been used for a variety of natural language processing tasks, e.g., machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, question answering ( <ref type="bibr" target="#b32">Sukhbaatar et al., 2015)</ref>, con- stituency parsing <ref type="bibr" target="#b37">(Vinyals et al., 2015)</ref>, social me- dia opinion mining <ref type="bibr" target="#b39">(Yang and Eisenstein, 2017)</ref>. and dependency parsing ( . The attention mechanism developed in this paper for exploiting global modes differs from previous work in that the global modes being attended over are latent rather than explicitly observed, and in that they are learned jointly with the full model.</p><p>Predicting the community endorsement has been studied by using either hand-crafted features <ref type="bibr" target="#b17">(Jaech et al., 2015</ref>) or neural models <ref type="bibr" target="#b40">Zayats and Ostendorf, 2017)</ref>, but all of them focus on supervised learning. Unsupervised learn- ing strategies have been explored for character- izing different factors in language. A hierarchi- cal Dirichlet process model was originally pro- posed for topic variations but has been extended to characterize multiple factors in <ref type="bibr" target="#b16">(Huang and Renals, 2008)</ref>. While much of the Dirichlet modeling work uses multinomial distributions, a loglinear version is introduced in <ref type="bibr" target="#b7">(Eisenstein et al., 2011</ref>). Multi-dimensional structure latent factors in text are modeled by extending the sparsity-promoting topic model in <ref type="bibr" target="#b27">(Paul and Dredze, 2012)</ref>. Our model instead uses a neural network to character- ize latent language factors, where the learned la- tent language factors could have a dependency on word positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper introduces a new factored neural model for unsupervised learning of comment embed- dings leveraging two different types of context in online discussions. By extending the atten- tion mechanism and using residual learning, our method is able to jointly model global context, comment content and response generation. Quan- titative experiments on three different subreddits show that the factored embeddings achieve consis- tent improvement in predicting quantized karma scores over other standard document embedding methods. Analyses on the learned global modes show community-related style and topic character- istics are captured in our model. Also, we observe that response trigger vectors characterize certain aspects of comments that elicit different response patterns.</p><p>A potential future direction is to explore whether the comment embeddings derived from the unsupervised factored neural model can be useful across multiple tasks. Recently, a dataset with dialogue act annotations on Reddit discus- sions is published and can be used for a dialogue act prediction task ( <ref type="bibr" target="#b41">Zhang et al., 2017</ref>). Iden- tifying or ranking persuasive arguments in the ChangeMyView subreddit (as studied in <ref type="bibr" target="#b35">(Tan et al., 2016;</ref><ref type="bibr" target="#b38">Wei et al., 2016)</ref>) and asking for favors in the RandomActsOfPizza subreddit (used in ( <ref type="bibr" target="#b0">Althoff et al., 2014)</ref>) are also interesting for fu- ture work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Averaged F1 scores of different classifiers. Blue bars show the performance using no comment embeddings. Orange bars show the absolute improvement by using factored comment embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Z</head><label></label><figDesc>≥0 Number of comments made by the author. Number of replies to the comment. Number of earlier comments. Number of later comments. Number of sibling comments. Number of comments in the subtree rooted from the comment. Height of the subtree rooted from the comment. Depth of the comment in the tree rooted from the original post.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The confusion matrices for the DeepOR classifier on Politics. The color of cell on the i-th row and the j-th column indicates the percentage of comments with quantized karma level i that are classified as j, and each row is normalized.</figDesc><graphic url="image-3.png" coords="7,309.22,62.81,209.24,156.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The box plot of strongest association positions for each global mode in Politics.</figDesc><graphic url="image-2.png" coords="7,173.31,71.01,119.13,89.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: t-SNE visualization of response trigger vectors clustered using k-means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Content-agnostic features.  † means two 
kinds of normalization are used: 1) zero-mean nor-
malization; 2) divided by the squared-root-rank of 
the feature value in the thread. 

AskWomen AskMen Politics 
Baseline 
53.6% 
49.3% 
51.3% 
BoW 
53.1% 
50.9% 
51.8% 
LDA 
55.3% 
51.1% 
52.5% 
Doc2Vec 
55.2% 
51.7% 
53.0% 
Factored\M 
54.2% 
51.8% 
52.9% 
Factored\R 
55.1% 
51.9% 
53.4% 
Factored 
56.3% 
52.7% 
54.8% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Averaged F1 scores of DeepOR classi-
fiers using different text features. Baseline results 
do not use any text features. 

</table></figure>

			<note place="foot" n="1"> karma = #up-votes-#down-votes. See https:// goo.gl/TnXgCr.</note>

			<note place="foot">Pr(wt+1|w1:t) = softmax(Q(Gbt + ct)), (4) where Q ∈ R V ×n is the weight matrix, and V is the vocabulary size. Note that the model jointly learns all parameters in the RNN together with the mode vectors m 1:K. This differs our model from the context-dependent RNN LM (Mikolov and Zweig, 2012), which is conditioned on a context vector inferred from a pre-trained topic model.</note>

			<note place="foot" n="2"> Comment IDs and labels used in this paper is at https: //github.com/hao-cheng/factored_neural.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper is based on work supported by the DARPA DEFT Program. Views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to ask for a favor: A case study of the success of altruistic request</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. AAAI Conf. Web and Social Media (ICWSM)</title>
		<meeting>Int. AAAI Conf. Web and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bi-directional attention with agreement for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<imprint>
			<biblScope unit="page" from="2204" to="2214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahadanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethhi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse additive generative models of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning latent local conversation modes for predicting community endorsement in online discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Natural Language Process. for Social Media</title>
		<meeting>Int. Workshop Natural Language ess. for Social Media</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Chapter Assoc. for Computational Linguistics (NAACL)</title>
		<meeting>Conf. North American Chapter Assoc. for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse overcomplete word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Annu</surname></persName>
		</author>
		<title level="m">Meeting Assoc. for Computational Linguistics (ACL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring text virality in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Guerini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gozde</forename><surname>Ozba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. AAAI Conf. Web and Social Media (ICWSM)</title>
		<meeting>Int. AAAI Conf. Web and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with a combinatorial action space for predicting popular Reddit threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image reconigtion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting popular messages in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ovidiu</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling topic and role information in meetings using the hierarchical Dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Multimodal Interaction V</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Springer Lecture Notes in Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Talking to the crowd: What do people react to in online discussions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2026" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What&apos;s in a name? Understanding the interplay between titles, content, and communities in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. AAAI Conf. Web and Social Media (ICWSM)</title>
		<meeting>Int. AAAI Conf. Web and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quo</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop at Int. Conf. Learning Representations</title>
		<meeting>Workshop at Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Chapter Assoc. for Computational Linguistics: Human Language Technologies (NAACLHLT)</title>
		<meeting>Conf. North American Chapter Assoc. for Computational Linguistics: Human Language Technologies (NAACLHLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Spoken Language Technologies Workshop</title>
		<meeting>IEEE Spoken Language Technologies Workshop</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorial lda: Sparse multi-dimensional text models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC Workshop New Challenges for NLP Frameworks</title>
		<meeting>LREC Workshop New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Loss functions for preference levels: regression with discrete ordered labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intelligence</title>
		<meeting>Int. Joint Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Want to be retweeted? large scale analytics on factors impacting retweet in Twitter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongwon</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pirolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Second Intern. Conf. Social Computing</title>
		<meeting>IEEE Second Intern. Conf. Social Computing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The effect of wording on message propagation: Topic-and author-controlled natural experiments on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescuniculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Characterizing the language of online communities and its relation to community reception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1030" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ranking argumentative comments in the online forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Overcoming language variation in sentiment analysis with social attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02080</idno>
		<title level="m">Conversation modeling on reddit using a graph-structured LSTM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Characterizing online discussion using coarse discourse sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Culbertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. AAAI Conf. Web and Social Media (ICWSM)</title>
		<meeting>Int. AAAI Conf. Web and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
