<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Importance sampling for unbiased on-demand evaluation of knowledge base population</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">Tejasvi</forename><surname>Chaganty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><forename type="middle">Pradeep</forename><surname>Paranjape</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>{manning}@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Importance sampling for unbiased on-demand evaluation of knowledge base population</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1038" to="1048"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations. Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems. We show that this evaluation is problematic: when a new system predicts a previously unseen relation, it is penalized even if it is correct. This leads to significant bias against new systems, which counterproductively discourages innovation in the field. Our first contribution is a new importance-sampling based evaluation which corrects for this bias by annotating a new system&apos;s predictions on-demand via crowdsourcing. We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task. Our second contribution is an implementation of our method made publicly available as an online KBP evaluation service. We pilot the service by testing diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Harnessing the wealth of information present in unstructured text online has been a long stand- ing goal for the natural language processing com- munity. In particular, knowledge base popula- tion seeks to automatically construct a knowl- edge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question an- swering ( <ref type="bibr" target="#b3">Berant et al., 2013;</ref><ref type="bibr" target="#b10">Fader et al., 2014</ref>; * Authors contributed equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fisher's mother, entertainer Debbie Reynolds, said on</head><p>Twitter on Sunday that her daughter was stabilizing.  <ref type="bibr" target="#b22">Reddy et al., 2014</ref>), automated reasoning <ref type="bibr" target="#b16">(Kalyanpur et al., 2012</ref>) and dialogue ( <ref type="bibr" target="#b11">Han et al., 2015)</ref>.</p><p>Evaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a pooling- based methodology is used in practice to construct datasets, similar to them methodology used in in- formation retrieval <ref type="bibr" target="#b15">(Jones and Rijsbergen, 1975;</ref><ref type="bibr" target="#b12">Harman, 1993)</ref>. For instance, at the annual NIST TAC KBP evaluation, all relations predicted by participating systems are pooled together, anno- tated and released as a dataset for researchers to develop and evaluate their systems on. However, during development, if a new system predicts a previously unseen relation it is considered to be wrong even if it is correct. The discrepancy be- tween a system's true score and the score on the pooled dataset is called pooling bias and is typi- cally assumed to be insignificant in practice <ref type="bibr" target="#b27">(Zobel, 1998</ref>).</p><p>The key finding of this paper contradicts this as- sumption and shows that the pooling bias is actu-ally significant, and it penalizes newly developed systems by 2% F 1 on average (Section 3). Novel improvements, which typically increase scores by less than 1% F 1 on existing datasets, are there- fore likely to be clouded by pooling bias during development. Worse, the bias is larger for a sys- tem which predicts qualitatively different relations systematically missing from the pool. Of course, systems participating in the TAC KBP evaluation do not suffer from pooling bias, but this requires researchers to wait a year to get credible feedback on new ideas. This bias is particularly counterproductive for machine learning methods as they are trained as- suming the pool is the complete set of positives. Predicting unseen relations and learning novel pat- terns is penalized. The net effect is that researchers are discouraged from developing innovative ap- proaches, in particular from applying machine learning, thereby slowing progress on the task.</p><p>Our second contribution, described in Sec- tion 4, addresses this bias through a new evalua- tion methodology, on-demand evaluation, which avoids pooling bias by querying crowdworkers, while minimizing cost by leveraging previous sys- tems' predictions when possible. We then com- pute the new system's score based on the predic- tions of past systems using importance weighting. As more systems are evaluated, the marginal cost of evaluating a new system decreases. We show how the on-demand evaluation methodology can be applied to knowledge base population in Sec- tion 5. Through a simulated experiment on eval- uation data released through the TAC KBP 2015 Slot Validation track, we show that we are able to obtain unbiased estimates of a new systems score's while significantly reducing variance.</p><p>Finally, our third contribution is an implementa- tion of our framework as a publicly available eval- uation service at https://kbpo.stanford. edu, where researchers can have their own KBP systems evaluated. The data collected through the evaluation process could even be valuable for rela- tion extraction, entity linking and coreference, and will also be made publicly available through the website. We evaluate three systems on the 2016 TAC KBP corpus for about $150 each (a fraction of the cost of official evaluation). We believe the public availability of this service will speed the pace of progress in developing KBP systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Humans</head><p>System A System B System C i 1 : (s 1 , r, o 1 , p 1 ) <ref type="figure">Figure 2</ref>: In pooled evaluation, an evaluation dataset is constructed by labeling relation in- stances collected from the pooled systems (A and B) and from a team of human annotators (Hu- mans). However, when a new system (C) is evalu- ated on this dataset, some of its predictions (i 6 ) are missing and can not be fairly evaluated. Here, the precision and recall for C should be 3 3 and 3 4 re- spectively, but its evaluation scores are estimated to be 2 3 and 2 3 . The discrepancy between these two scores is called pooling bias.</p><formula xml:id="formula_0">i 2 : (s 1 , r, o 2 , p 2 ) i 3 : (s 1 , r, o 3 , p 3 ) i 4 : (s 1 , r, o 2 , p 4 ) i 5 : (s 1 , r, o 3 , p 5 ) i 6 : (s 1 , r, o 4 , p 6 ) ×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In knowledge base population, each relation is a triple (SUBJECT, PREDICATE, OBJECT) where SUBJECT and OBJECT are some globally unique entity identifiers (e.g. Wikipedia page titles) and PREDICATE belongm to a specified schema. 1 A KBP system returns an output in the form of re- lation instances (SUBJECT, PREDICATE, OBJECT, PROVENANCE), where PROVENANCE is a descrip- tion of where exactly in the document corpus the relation was found. In the example shown in <ref type="figure">Fig- ure 1</ref>, CARRIE FISHER and DEBBIE REYNOLDS are identified as the subject and object, respec- tively, of the predicate CHILD OF, and the whole sentence is provided as provenance. The prove- nance also identifies that CARRIE FISHER is ref- erenced by Fisher within the sentence. Note that the same relation can be expressed in multiple sen- tences across the document corpus; each of these is a different relation instance.</p><p>Pooled evaluation. The primary source of eval- uation data for KBP comes from the annual TAC KBP competition organized by NIST (Ji et al., <ref type="bibr">1</ref> The TAC KBP guidelines specify a total of 65 predicates (including inverses) such as per:title or org:founded on, etc. Subject entities can be people, or- ganizations, geopolitical entities, while object entities also include dates, numbers and arbitrary string-values like job ti- tles. 2011). Let E be a held-out set of evaluation en- tities. There are two steps performed in parallel: First, each participating system is run on the docu- ment corpus to produce a set of relation instances; those whose subjects are in E are labeled as either positive or negative by annotators. Second, a team of annotators identify and label correct relation in- stances for the evaluation entities E by manually searching the document corpus within a time bud- get ( <ref type="bibr" target="#b9">Ellis et al., 2012</ref>). These labeled relation in- stances from the two steps are combined and re- leased as the evaluation dataset. In the example in <ref type="figure">Figure 2</ref>, systems A and B were used in construct- ing the pooling dataset, and there are 3 distinct re- lations in the dataset, between s 1 and o 1 , o 2 , o 3 .</p><p>A system is evaluated on the precision of its predicted relation instances for the evaluation en- tities E and on the recall of the corresponding pre- dicted relations (not instances) for the same enti- ties (see <ref type="figure">Figure 2</ref> for a worked example). When using the evaluation data during system develop- ment, it is common practice to use the more le- nient anydoc score that ignores the provenance when checking if a relation instance is true. Un- der this metric, predicting the relation (CARRIE FISHER, CHILD OF, DEBBIE REYNOLDS) from an ambiguous provenance like "Carrie Fisher and Debbie Reynolds arrived together at the awards show" would be considered correct even though it would be marked wrong under the official metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Measuring pooling bias</head><p>The example in <ref type="figure">Figure 2</ref> makes it apparent that pooling-based evaluation can introduce a system- atic bias against unpooled systems. However, it has been assumed that the bias is insignificant in practice given the large number of systems pooled in the TAC KBP evaluation. We will now show that the assumption is not valid using data from the TAC KBP 2015 evaluation. <ref type="bibr">2</ref> Measuring bias. In total, there are 70 system submissions from 18 teams for 317 evaluation en- tities (E) and the evaluation set consists of 11,008 labeled relation instances. <ref type="bibr">3</ref> The original evalua-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Median bias Precision</head><p>Recall Macro F1 Official 17.93% 17.00% 15.51% anydoc 2.34% 1.93% 2.05%</p><p>Figure 3: Median pooling bias (difference be- tween pooled and unpooled scores) on the top 40 systems of TAC KBP 2015 evaluation using the official and anydoc scores. The bias is much smaller for the lenient anydoc metric, but even so, it is larger than the largest difference between adjacent systems (1.5% F 1 ) and typical system im- provements (around 1% F 1 ).</p><p>tion dataset gives us a good measure of the true scores for the participating systems. Similar to <ref type="bibr" target="#b27">Zobel (1998)</ref>, which studied pooling bias in informa- tion retrieval, we simulate the condition of a team not being part of the pooling process by removing any predictions that are unique to its systems from the evaluation dataset. The pooling bias is then the difference between the true and unpooled scores.</p><p>Results. <ref type="figure">Figure 3</ref> shows the results of measur- ing pooling bias on the TAC KBP 2015 eval- uation on the F 1 metric using the official and anydoc scores. <ref type="bibr">45</ref> We observe that even with le- nient anydoc heuristic, the median bias (2.05% F 1 ) is much larger than largest difference between adjacently ranked systems (1.5% F 1 ). This ex- periment shows that pooling evaluation is signif- icantly and systematically biased against systems that make novel predictions! sider instances selected in the first part of this process. <ref type="bibr">4</ref> We note that anydoc scores are on average 0.88%F1 larger than the official scores. <ref type="bibr">5</ref> The outlier at rank 36 corresponds to a University of Texas, Austin system that only filtered predictions from other systems and hence has no unique predictions itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">On-demand evaluation with importance sampling</head><p>Pooling bias is fundamentally a sampling bias problem where relation instances from new sys- tems are underrepresented in the evaluation dataset. We could of course sidestep the prob- lem by exhaustively annotating the entire docu- ment corpus, by annotating all mentions of en- tities and checking relations between all pairs of mentions. However, that would be a laborious and prohibitively expensive task: using the interfaces we've developed (Section 6), it costs about $15 to annotate a single document by non-expert crowd- workers, resulting in an estimated cost of at least $1,350,000 for a reasonably large corpus of 90,000 documents <ref type="bibr" target="#b8">(Dang, 2016)</ref>. The annotation effort would cost significantly more with expert annota- tors. In contrast, labeling relation instances from system predictions can be an order of magnitude cheaper than finding them in documents: using our interfaces, it costs only about $0.18 to verify each relation instance compared to $1.60 per instance extracted through exhaustive annotations. We propose a new paradigm called on-demand evaluation which takes a lazy approach to dataset construction by annotating predictions from sys- tems only when they are underrepresented, thus correcting for pooling bias as it arises. In this sec- tion, we'll formalize the problem solved by on- demand evaluation independent of KBP and de- scribe a cost-effective solution that allows us to accurately estimate evaluation scores without bias using importance sampling. We'll then instantiate the framework for KBP in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem statement</head><p>Let X be the universe of (relation) instances, Y ⊆ X be the unknown subset of correct instances, X 1 , . . . X m ⊆ X be the predictions for m sys- tems, and let</p><formula xml:id="formula_1">Y i = X i ∩ Y. Let X = m i=1 X i and Y = m i=1 Y i . Let f (x) def = I[x ∈ Y] and g i (x) = I[x ∈ X i ]</formula><p>, then the precision, π i , and recall, r i , of the set of predictions X i is</p><formula xml:id="formula_2">π i def = E x∼p i [f (x)] r i def = E x∼p 0 [g i (x)],</formula><p>where p i is a distribution over X i and p 0 is a dis- tribution over Y. We assume that p i is known, e.g. the uniform distribution over X i and that we know p 0 up to normalization constant and can sample from it.</p><p>In on-demand evaluation, we can query f (x) (e.g. labeling an instance) or draw a sample from p 0 ; typically, querying f (x) is significantly cheaper than sampling from p 0 . We obtain predic- tion sets X 1 , . . . , X m sequentially as the systems are submitted for evaluation. Our goal is to esti- mate π i and r i for each system i = 1, . . . , m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simple estimators</head><p>We can estimate each π i and r i independently with simple Monte Carlo integration. LetˆXLetˆ LetˆX 1 , . . . , ˆ X m be multi-sets of n 1 , . . . , n j i.i.d. samples from X 1 , . . . , X m respectively, and letˆYletˆ letˆY 0 be a multi- set of n 0 samples drawn from Y. Then, the simple estimators for precision and recall are:</p><formula xml:id="formula_3">ˆ π (simple) i = 1 n i x∈ˆXx∈ˆ x∈ˆX i f (x) ˆ r (simple) i = 1 n 0 x∈ˆYx∈ˆ x∈ˆY 0 g i (x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint estimators 6</head><p>The simple estimators are unbiased but have wastefully large variance because evaluating a new system does not leverage labels acquired for pre- vious systems.</p><p>On-demand evaluation with the joint estimator works as follows: FirstˆYFirstˆ FirstˆY 0 is randomly sampled from Y once when the evaluation framework is launched. For every new set of predictions X m submitted for evaluation, the minimum number of samples n m required to accurately evaluate X m is calculated based on the current evaluation data, ˆ Y 0 andˆXandˆ andˆX 1 , . . . , ˆ X m−1 . Then, the setˆXsetˆ setˆX m is added to the evaluation data by evaluating f (x) on n m sam- ples drawn from X m . Finally, estimates π i and r i are updated for each system i = 1, . . . , m using the joint estimators that will be defined next. In the rest of this section, we will answer the follow- ing three questions:</p><p>1. How can we use all the samplesˆXsamplesˆ samplesˆX 1 , . . . ˆ X m when estimating the precision π i of system i?</p><p>2. How can we use all the samplesˆXsamplesˆ samplesˆX 1 , . . . , ˆ X m withˆYwithˆ withˆY 0 when estimating recall r i ?</p><p>3. Finally, to formˆXformˆ formˆX m , how many samples should we draw from X m given existing sam- ples andˆXandˆ andˆX 1 , . . . , ˆ X m−1 andˆYandˆ andˆY 0 ?</p><p>Estimating precision jointly. Intuitively, if two systems have very similar predictions X i and X j , we should be able to use samples from one to es- timate precision on the other. However, it might also be the case that X i and X j only overlap on a small region, in which case the samples from X j do not accurately represent instances in X i and could lead to a biased estimate. We address this problem by using importance sampling <ref type="bibr" target="#b19">(Owen, 2013)</ref>, a standard statistical technique for estimat- ing properties of one distribution using samples from another distribution. In importance sampling, ifˆXifˆ ifˆX i is sampled from q i , then 1</p><formula xml:id="formula_4">n i x∈ˆXx∈ˆ x∈ˆX i p i (x) q i (x) f (x)</formula><p>is an unbiased esti- mate of π i . We would like the proposal distribu- tion q i to both leverage samples from all m sys- tems and be tailored towards system i. To this end, we first define a distribution over systems j, represented by probabilities w ij . Then, define q i as sampling a j and drawing x ∼ p j ; formally q i (x) = m j=1 w ij p j (x). We note that q i (x) not only significantly differs between systems, but also changes as new systems are added to the evaluation pool. Unfortunately, the standard importance sampling procedure re- quires us to draw and use samples from each dis- tribution q i (x) independently and thus can not ef- fectively reuse samples drawn from different dis- tributions. To this end, we introduce a practical refinement to the importance sampling procedure: we independently draw n j samples according to p j (x) from each of the m systems independently and then numerically integrate over these samples using the weights w ij to "mix" them appropriately to produce and unbiased estimate of π i while re- ducing variance. Formally, we define the joint pre- cision estimator:</p><formula xml:id="formula_5">ˆ π (joint) i def = m j=1 w ij n j x∈ˆXx∈ˆ x∈ˆX j p i (x)f (x) q i (x) ,</formula><p>where eachˆXeachˆ eachˆX j consists of n j i.i.d. samples drawn from p j . It is a hard problem to determine what the op- timal mixing weights w ij should be. However, we can formally verify that if X i and X j are dis- joint, then w ij = 0 minimizes the variance of π i , and if X i = X j , then w ij ∝ n j is opti- mal. This motivates the following heuristic choice which interpolates between these two extremes:</p><formula xml:id="formula_6">w ij ∝ n j x∈X p j (x)p i (x)</formula><p>. Estimating recall jointly. The recall of system i can be expressed can be expressed as a product r i = θν i , where θ is the recall of the pool, which measures the fraction of all positive instances pre- dicted by the pool (any system), and ν i is the pooled recall of system i, which measures the frac- tion of the pool's positive instances predicted by system i. Letting g(x) def = I[x ∈ X], we can de- fine these as:</p><formula xml:id="formula_7">ν i def = E x∼p 0 [g i (x) | x ∈ X] θ def = E x∼p 0 [g(x)].</formula><p>We can estimate θ analogous to the simple recall estimatorˆrestimatorˆ estimatorˆr i , except we use the pool g instead a system g i . For ν i , the key is to leverage the work from estimating precision. We already evaluated f (x) onˆXonˆ onˆX i , so we can computê</p><formula xml:id="formula_8">Y i def = ˆ X i ∩ Y and form the subsetˆYsubsetˆ subsetˆY = m i=1ˆY i=1ˆ i=1ˆY i . ˆ</formula><p>Y is an approx- imation of Y whose bias we can correct through importance reweighting. We then define estima- tors as follows:</p><formula xml:id="formula_9">ˆ ν i def = m j=1 w ij n j x∈ˆYx∈ˆ x∈ˆY j p 0 (x)g i (x) q i (x) m j=1 w ij n j x∈ˆYx∈ˆ x∈ˆY j p 0 (x) q i (x) ˆ r (joint) i def = ˆ θˆνθˆν i ˆ θ def = 1 n 0 x∈ˆYx∈ˆ x∈ˆY 0 g(x).</formula><p>where q i and w ij are the same as before.</p><p>Adaptively choosing the number of samples. Finally, a desired property for on-demand evalu- ation is to label new instances only when the cur- rent evaluation data is insufficient, e.g. when a new set of predictions X m contains many instances not covered by other systems. We can measure how well the current evaluation set covers the predic- tions X m by using a conservative estimate of the variance ofˆπofˆ ofˆπ (joint) m . 7 In particular, the variance ofˆπofˆ ofˆπ (joint) m is a monotonically decreasing function in n m , the number of samples drawn from X m . We can easily solve for the minimum number of samples required to estimatê π (joint) m within a con- fidence interval by using the bisection method <ref type="bibr" target="#b6">(Burden and Faires, 1985)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">On-demand evaluation for KBP</head><p>Applying the on-demand evaluation framework to a task requires us to answer three questions:</p><p>1. What is the desired distribution over system predictions p i ?</p><p>2. How do we label an instance x, i.e. check if x ∈ Y? 3. How do we sample from the unknown set of true instances x ∼ p 0 ?</p><p>In this section, we present practical implementa- tions for knowledge base population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sampling from system predictions</head><p>Both the official TAC-KBP evaluation and the on-demand evaluation we propose use micro- averaged precision and recall as metrics. However, in the official evaluation, these metrics are com- puted over a fixed set of evaluation entities chosen by LDC annotators, resulting in two problems: (a) defining evaluation entities requires human inter- vention and (b) typically a large source of vari- ability in evaluation scores comes from not hav- ing enough evaluation entities (see e.g. <ref type="bibr" target="#b25">(Webber, 2010)</ref>). In our methodology, we replace manu- ally chosen evaluation entities by sampling entities from each system's output according p i . In effect, p i makes explicit the decision process of the anno- tator who chooses evaluation entities. Identifying a reasonable distribution p i is an im- portant implementation decision that depends on what one wishes to evaluate. Our goal for the on- demand evaluation service we have implemented is to ensure that KBP systems are fairly evalu- ated on diverse subjects and predicates, while at the same time, ensuring that entities with multiple relations are represented to measure completeness of knowledge base entries. As a result, we propose a distribution that is inversely proportional to the frequency of the subject and predicate and is pro- portional to the number of unique relations iden- tified for an entity (to measure knowledge base completeness). See Appendix A in the supplemen- tary material for an analysis of this distribution and a study of other potential choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Labeling predicted instances</head><p>We label predicted relation instances by present- ing the instance's provenance to crowdworkers and asking them to identify if a relation holds be- tween the identified subject and object mentions <ref type="figure" target="#fig_2">(Figure 4a</ref>). Crowdworkers are also asked to link the subject and object mentions to their canoni- cal mentions within the document and to pages on Wikipedia, if possible, for entity linking. On av- erage, we find that crowdworkers are able to per- form this task in about 20 seconds, correspond- ing to about $0.05 per instance. We requested 5 crowdworkers to annotate a small set of 200 rela- tion instances from the 2015 TAC-KBP corpus and measured a substantial inter-annotator agreement with a Fleiss' kappa of 0.61 with 3 crowdworkers and 0.62 with 5. Consequently, we take a majority vote over 3 workers in subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sampling true instances</head><p>Sampling from the set of true instances Y is diffi- cult because we can't even enumerate the elements of Y. As a proxy, we assume that relations are identically distributed across documents and have crowdworkers annotate a random subset of doc- uments for relations using an interface we devel- oped <ref type="figure" target="#fig_2">(Figure 4b)</ref>. Crowdworkers begin by iden- tifying every mention span in a document. For each mention, they are asked to identify its type, canonical mention within the document and asso- ciated Wikipedia page if possible. They are then presented with a separate interface to label predi- cates between pairs of mentions within a sentence that were identified earlier.</p><p>We compare crowdsourced annotations against those of expert annotators using data from the TAC KBP 2015 EDL task on 10 randomly chosen docu- ments. We find that 3 crowdworkers together iden- tify 92% of the entity spans identified by expert annotators, while 7 crowdworkers together iden- tify 96%. When using a token-level majority vote to identify entities, 3 crowdworkers identify about 78% of the entity spans; this number does not change significantly with additional crowdwork- ers. We also measure substantial token-level inter- annotator agreement using Fleiss' kappa for iden- tifying typed mention spans (κ = 0.83), canonical mentions (κ = 0.75) and entity links (κ = 0.75) with just three workers. Based on this analysis, we use token-level majority over 3 workers in subse- quent experiments.</p><p>The entity annotation interface is far more in- volved and takes on average about 13 minutes per document, corresponding to about $2.60 per docu- ment, while the relation annotation interface takes on average about $2.25 per document. Because documents vary significantly in length and com- plexity, we set rewards for each document based on the number of tokens (.75c per token) and men- tion pairs (5c per pair) respectively. With 3 work- ers per document, we paid about $15 per document on average. Each document contained an average   , recall (R) and F 1 scores from a pilot run of our evaluation service for ensembles of a rule-based system (R), a logistic classifier (L) and a neural network classifier (N) run on the TAC KBP 2016 document corpus.</p><p>9.2 relations, resulting in a cost of about $1.61 per relation instance. We note that this is about ten times as much as labeling a relation instance. We defer details regarding how documents themselves should be weighted to capture diverse entities that span documents to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>Let us now see how well on-demand evaluation works in practice. We begin by empirically study- ing the bias and variance of the joint estimator pro- posed in Section 4 and find it is able to correct for pooling bias while significantly reducing variance in comparison with the simple estimator. We then demonstrate that on-demand evaluation can serve as a practical replacement for the TAC KBP eval- uations by piloting a new evaluation service we have developed to evaluate three distinct systems on TAC KBP 2016 document corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bias and variance of the on-demand</head><p>evaluation.</p><p>Once again, we use the labeled system predictions from the TAC KBP 2015 evaluation and treat them as an exhaustively annotated dataset. To evaluate the pooling methodology we construct an evalua- tion dataset using instances found by human an- notators and labeled instances pooled from 9 ran- domly chosen teams (i.e. half the total number of participating teams), and use this dataset to evaluate the remaining 9 teams. On average, the pooled evaluation dataset contains between 5,000 and 6,000 labeled instances and evaluates 34 dif- ferent systems (since each team may have submit- ted multiple systems). Next, we evaluated sets of 9 randomly chosen teams with our proposed simple and joint estimators using a total of 5,000 samples: about 150 of these samples are drawn from Y, i.e. the full TAC KBP 2015 evaluation data, and 150 samples from each of the systems being evaluated. We repeat the above simulated experiment 500 times and compare the estimated precision and recall with their true values <ref type="figure" target="#fig_2">(Figure 4</ref>). The simulations once again highlights that the pooled methodology is biased, while the simple and joint estimators are not. Furthermore, the joint estima- tors significantly reduce variance relative to the simple estimators: the median 90% confidence intervals reduce from 0.14 to 0.06 precision and from 0.14 to 0.08 for recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Number of samples required by on-demand evaluation</head><p>Separately, we evaluate the efficacy of the adaptive sample selection method described in Section 4.3 through another simulated experiment. In each trial of this experiment, we evaluate the top 40 systems in random order. As each subsequent sys- tem is evaluated, the number of samples to pick from the system is chosen to meet a target variance and added to the current pool of labeled instances.</p><p>To make the experiment more interpretable, we choose the target variance to correspond with the estimated variance of having 500 samples. <ref type="figure" target="#fig_2">Fig- ure 4</ref> plots the results of the experiment. The number of samples required to estimate systems quickly drops off from the benchmark of 500 sam- ples as the pool of labeled instances covers more systems. This experiment shows that on-demand evaluation using joint estimation can scale up to an order of magnitude more submissions than a simple estimator for the same cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">A mock evaluation for TAC KBP 2016</head><p>We have implemented the on-demand evaluation framework described here as an evaluation service to which researchers can submit their own system predictions. As a pilot of the service, we evaluated three relation extraction systems that also partici- pated in the official 2016 TAC KBP competition.</p><p>Each system uses Stanford CoreNLP ( ) to identify entities, the Illinois Wiki- fier <ref type="bibr" target="#b21">(Ratinov et al., 2011</ref>) to perform entity linking and a combination of a rule-based system (P), a logistic classifier (L), and a neural network classi- fier (N) for relation extraction. We used 15,000 Newswire documents from the 2016 TAC KBP evaluation as our document corpus. In total, 100 documents were exhaustively annotated for about $2,000 and 500 instances from each system were labeled for about $150 each. Evaluating all three system only took about 2 hours. <ref type="figure" target="#fig_2">Figure 4f</ref> reports scores obtained through on- demand evaluation of these systems as well as their corresponding official TAC evaluation scores. While the relative ordering of systems be- tween the two evaluations is the same, we note that precision and recall as measured through on- demand evaluation are respectively higher and lower than the official scores. This is to be ex- pected because on-demand evaluation measures precision using each systems output as opposed to an externally defined set of evaluation entities. Likewise, recall is measured using exhaustive an- notations of relations within the corpus instead of annotations from pooled output in the official eval- uation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>The subject of pooling bias has been extensively studied in the information retrieval (IR) commu- nity starting with <ref type="bibr" target="#b27">Zobel (1998)</ref>, which examined the effects of pooling bias on the TREC AdHoc task, but concluded that pooling bias was not a significant problem. However, when the topic was later revisited, <ref type="bibr" target="#b4">Buckley et al. (2007)</ref> identified that the reason for the small bias was because the sub- missions to the task were too similar; upon repeat- ing the experiment using a novel system as part of the TREC Robust track, they identified a 23% point drop in average precision scores! 8</p><p>Many solutions to the pooling bias problem have been proposed in the context of information retrieval, e.g. adaptively constructing the pool to collect relevant data more cost-effectively <ref type="bibr" target="#b27">(Zobel, 1998;</ref><ref type="bibr" target="#b7">Cormack et al., 1998;</ref><ref type="bibr" target="#b2">Aslam et al., 2006</ref>), or modifying the scoring metrics to be less sen- sitive to unassessed data ( <ref type="bibr" target="#b5">Buckley and Voorhees, 2004;</ref><ref type="bibr" target="#b23">Sakai and Kando, 2008;</ref><ref type="bibr" target="#b2">Aslam et al., 2006</ref>). Many of these ideas exploit the ranking of docu- ments in IR which does not apply to KBP. While both <ref type="bibr" target="#b2">Aslam et al. (2006)</ref> and <ref type="bibr" target="#b26">Yilmaz et al. (2008)</ref> estimate evaluation metrics by using importance sampling estimators, the techniques they propose require knowing the set of all submissions before- hand. In contrast, our on-demand methodology can produce unbiased evaluation scores for new development systems as well.</p><p>There have been several approaches taken to crowdsource data pertinent to knowledge base population ( <ref type="bibr" target="#b24">Vannella et al., 2014;</ref><ref type="bibr" target="#b1">Angeli et al., 2014;</ref><ref type="bibr" target="#b13">He et al., 2015;</ref><ref type="bibr" target="#b17">Liu et al., 2016)</ref>. The most extensive annotation effort is probably <ref type="bibr" target="#b20">Pavlick et al. (2016)</ref>, which crowdsources a knowledge base for gun-violence related events. In contrast to previous work, our focus is on evaluating systems, not collecting a dataset. Furthermore, our main contribution is not a large dataset, but an evalua- tion service that allows anyone to use crowdsourc- ing predictions made by their system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>Over the last ten years of the TAC KBP task, the gap between human and system performance has barely narrowed despite the community's best ef- forts: top automated systems score less than 36% F 1 while human annotators score more than 60%. In this paper, we've shown that the current eval- uation methodology may be a contributing factor because of its bias against novel system improve- ments. The new on-demand framework proposed in this work addresses this problem by obtaining human assessments of new system output through crowdsourcing. The framework is made economi- cally feasible by carefully sampling output to be assessed and correcting for sample bias through importance sampling.</p><p>Of course, simply providing better evaluation scores is only part of the solution and it is clear that better datasets are also necessary. However, the very same difficulties in scale that make eval- uating KBP difficult also make it hard to collect a high quality dataset for the task. As a result, existing datasets ( <ref type="bibr" target="#b1">Angeli et al., 2014;</ref><ref type="bibr" target="#b0">Adel et al., 2016)</ref> have relied on the output of existing sys- tems, making it likely that they exhibit the same biases against novel systems that we've discussed in this paper. We believe that providing a fair and standardized evaluation platform as a service al- lows researchers to exploit such datasets and while still being able to accurately measure their perfor- mance on the knowledge base population task.</p><p>There are many other tasks in NLP that are even harder to evaluate than KBP. Existing evaluation metrics for tasks with a generation component- such as summarization or dialogue-leave much to be desired. We believe that adapting the ideas of this paper to those tasks is a fruitful direction, as progress of a research community is strongly tied to the fidelity of evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Debbie</head><label></label><figDesc>Figure 1: An example describing entities and relations in knowledge base population.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>x</head><label></label><figDesc>= +0.2y 2 − 0.1y + 2.3 (R = 15.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a, b): Interfaces for annotating relations and entities respectively. (c, d): A comparison of bias for the pooling, simple and joint estimators on the TAC KBP 2015 challenge. Each point in the figure is a mean of 500 repeated trials; dotted lines show the 90% quartile. Both the simple and joint estimators are unbiased, and the joint estimator is able to significantly reduce variance. (e): A comparison of the number of samples used to estimate scores under the fixed and adaptive sample selection scheme. Each faint line shows the number of samples used during a single trial, while solid lines show the mean over 100 trials. The dashed line shows a square-root relationship between the number of systems evaluated and the number of samples required. Thus joint estimation combined with adaptive sample selection can reduce the number of labeled annotations required by an order of magnitude. (f): Precision (P ), recall (R) and F 1 scores from a pilot run of our evaluation service for ensembles of a rule-based system (R), a logistic classifier (L) and a neural network classifier (N) run on the TAC KBP 2016 document corpus.</figDesc></figure>

			<note place="foot" n="2"> Our results are not qualitatively different on data from previous years of the shared task. 3 The evaluation set is actually constructed from compositional queries like, &quot;what does Carrie Fisher&apos;s parents do?&quot;: these queries select relation instances that answer the question &quot;who are Carrie Fisher&apos;s parents?&quot;, and then use those answers (e.g. &quot;Debbie Reynolds&quot;) to select relation instances that answer &quot;what does Debbie Reynolds do?&quot;. We only con</note>

			<note place="foot" n="6"> Proofs for claims made in this section can be found in Appendix B of the supplementary material.</note>

			<note place="foot" n="7"> Further details can be found in Appendix B of the supplementary material.</note>

			<note place="foot" n="8"> For the interested reader, Webber (2010) presents an excellent survey of the literature on pooling bias.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yuhao Zhang, Hoa Deng, Eduard Hovy, and Jacob Steinhardt for discus-sions, William E. Webber for his excellent thesis that helped shape this project and the anonymous reviewers for their detailed and pertinent feedback. The first and second authors are supported under DARPA DEFT program under ARFL prime con-tract no. FA8750-13-2-0040.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparing convolutional neural networks to traditional models for slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining distant and partial supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A statistical method for system evaluation using incomplete judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Special Interest Group on Information Retreival (SIGIR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bias and the limits of pooling for large collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimmick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Special Interest Group on Information Retreival</title>
		<imprint>
			<date type="published" when="2007" />
			<publisher>SIGIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retrieval evaluation with incomplete information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Special Interest Group on Information Retreival (SIGIR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Burden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Faires</surname></persName>
		</author>
		<title level="m">Numerical Analysis</title>
		<imprint>
			<publisher>PWS Publishers</publisher>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient construction of large test collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Special Interest Group on Information Retreival</title>
		<imprint>
			<date type="published" when="1998" />
			<publisher>SIGIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<title level="m">Cold start knowledge base population at TAC KBP 2016. Text Analytics Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<title level="m">Linguistic resources for 2012 knowledge base population evaluations. Text Analytics Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting knowledge base to generate responses for natural language dialog listening agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="129" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The first text retrieval conference (trec-1) rockville, md, u.s.a., 4-6 november, 1992. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="411" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Questionanswer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2011 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Analytics Conference</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Report on the need for and provision of an &quot;ideal test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Test Collection</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured data and inference in deepqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Boguraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coppola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fokoue-Nkoutche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Qui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="351" to="364" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective crowd annotation for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The stanford coreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL system demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Monte Carlo theory, methods and examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The gun violence database: A new task and data set for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1018" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Largescale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On information retrieval metrics designed for evaluation with incomplete relevance assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Special Interest Group on Information Retreival (SIGIR)</title>
		<imprint>
			<biblScope unit="page" from="447" to="470" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Validating and extending semantic knowledge bases using video games with a purpose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vannella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scarfini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Toscani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1294" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Measurement in Information Retrieval Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Melbourne</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple and efficient sampling method for estimating AP and NDCG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Special Interest Group on Information Retreival (SIGIR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How reliable are the results of largescale information retrieval experiments?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Special Interest Group on Information Retreival</title>
		<imprint>
			<date type="published" when="1998" />
			<publisher>SIGIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
