<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Sense Induction with Neural biLM and Symmetric Patterns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Amrami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Sense Induction with Neural biLM and Symmetric Patterns</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4860" to="4867"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4860</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An established method for Word Sense Induction (WSI) uses a language model to predict probable substitutes for target words, and induces senses by clustering these resulting substitute vectors. We replace the ngram-based language model (LM) with a recurrent one. Beyond being more accurate, the use of the recurrent LM allows us to effectively query it in a creative way, using what we call dynamic symmetric patterns. The combination of the RNN-LM and the dynamic symmetric patterns results in strong substitute vectors for WSI, allowing to surpass the current state-of-the-art on the SemEval 2013 WSI shared task by a large margin.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We deal with the problem of word sense induc- tion (WSI): given a target lemma and a collection of within-sentence usages it, cluster the usages (instances) according to the different senses of the target lemma. For example, for the sentences:</p><p>(a) We spotted a large bass in the ocean.</p><p>(b) The bass player did not receive the acknowl- edgment she deserves.</p><p>(c) The black sea bass, is a member of the wreck- fish family.</p><p>We would like to cluster (a) and (c) in one group and (b) in another. <ref type="bibr">1</ref> Note that some mentions are ambiguous. For example, (d) matches both the music and the fish senses:</p><p>(d) Bass scales are the worst.</p><p>This calls for a soft clustering, allowing to prob- abilistically associate a given mention to two senses.</p><p>The problem of WSI has been extensively stud- ied with a series of shared tasks on the topic <ref type="bibr" target="#b0">(Agirre and Soroa, 2007;</ref><ref type="bibr" target="#b7">Manandhar et al., 2010;</ref><ref type="bibr" target="#b3">Jurgens and Klapaftis, 2013)</ref>, the latest being <ref type="bibr">SemEval 2013 Task 13 (Jurgens and</ref><ref type="bibr" target="#b3">Klapaftis, 2013)</ref>. Recent state-of-the-art approaches to WSI rely on generative graphical models ( <ref type="bibr" target="#b6">Lau et al., 2013;</ref><ref type="bibr" target="#b13">Wang et al., 2015;</ref><ref type="bibr" target="#b4">Komninos and Manandhar, 2016)</ref>. In these works, the sense is modeled as a latent variable that influences the context of the target word. The later models explicitly differ- entiate between local (syntactic, close to the dis- ambiguated word) and global (thematic, semantic) context features.</p><p>Substitute Vectors <ref type="bibr" target="#b1">Baskaya et al. (2013)</ref> take a different approach to the problem, based on sub- stitute vectors. They represent each instance as a distribution of possible substitute words, as deter- mined by a language model (LM). The substitute vectors are then clustered to obtain senses. <ref type="bibr" target="#b1">Baskaya et al. (2013)</ref> derive their probabilities from a 4-gram language model. Their system (AI- KU) was one of the best performing at the time of SemEval 2013 shared task. Our method is inspired by the AI-KU use of substitution based sense in- duction, but deviate from it by moving to a recur- rent language model. Besides being more accu- rate, this allows us to further improve the quality of the derived substitutions by the incorporation of dynamic symmetric patterns.</p><p>BiLM Bidirectional RNNs were shown to be ef- fective for word-sense disambiguation and lexi- cal substitution tasks ( <ref type="bibr" target="#b8">Melamud et al., 2016;</ref><ref type="bibr" target="#b15">Yuan et al., 2016;</ref><ref type="bibr" target="#b11">Raganato et al., 2017)</ref>. We adopt the ELMo biLM model of <ref type="bibr" target="#b10">Peters et al. (2018)</ref>, which was shown to produce very competitive results for many NLP tasks. We use the pre-trained ELMo biLM provided by <ref type="bibr" target="#b10">Peters et al. (2018)</ref>. <ref type="bibr">2</ref> How- ever, rather than using the LSTM state vectors as suggested in the ELMo paper, we opt instead to use the predicted word probabilities. Moving from continuous and opaque state vectors to dis- crete and transparent word distributions allows far better control of the resulting representations (e.g. by sampling, re-weighting and lemmatizing the words) as well as better debugging opportunities.</p><p>As expected, the move to the neural biLM already outperforms the AI-KU system, and matches the previous state-of-the-art. However, we observe that the substitute vectors do not take into account the disambiguated word itself. We find that this often results in noisy substitutions. As a motivating example, consider the sentence "the doctor recommends oranges for your health". Here, running is a perfectly good substitution, as the "fruitness" of the target word itself isn't rep- resented in the context. We would like the sub- stitutes word distribution representing the target word to take both kinds of information-the con- text as well as the target word-into account.</p><p>Dynamic Symmetric Patterns Our main pro- posal incorporates such information. It is moti- vated by Hearst patterns <ref type="bibr" target="#b2">(Hearst, 1992;</ref><ref type="bibr" target="#b14">Widdows and Dorow, 2002;</ref><ref type="bibr" target="#b12">Schwartz et al., 2015</ref>), and made possible by neural LMs. Neural LMs are better in capturing long-range dependencies, and can handle and predict unseen text by generalizing from similar contexts. Conjunctions, and in partic- ular the word and, are known to combine expres- sions of the same kind. Recently, <ref type="bibr" target="#b12">Schwartz et al. (2015)</ref> used conjunctive symmetric patterns to de- rive word embeddings that excel at capturing word similarity. Similarly, <ref type="bibr" target="#b5">Kozareva et al. (2008)</ref> search for doubly-anchored patterns including the word and in a large web-corpus to improve semantic- class induction. The method of <ref type="bibr" target="#b12">Schwartz et al. (2015)</ref> result in context-independent embeddings, while that of <ref type="bibr" target="#b5">Kozareva et al. (2008)</ref> takes some context into account but is restricted to exact cor- pus matches and thus suffers a lot from sparsity.</p><p>We make use of the rich sequence represen- tation capabilities of the neural biLM to de- rive context-dependent symmetric pattern substi-tutions. Relying on the generalization properties of neural language models and the abundance of the "X and Y" pattern, we present the language model with a dynamically created incomplete pat- tern, and ask it to predict probable completion can- didates. Rather than predicting the word distribu- tion following the doctor recommends , we in- stead predict the distribution following the doctor recommends oranges and . This provides sub- stantial improvement, resulting in state-of-the-art performance on the SemEval 2013 shared task.</p><p>The code for reproducing the experiments and our analyses is available at https://github. com/asafamr/SymPatternWSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given a target word (lemma and its part-of-speech pair), together with several sentences in which the target word is used (instances), our goal is to clus- ter the word usages such that each cluster corre- sponds to a different sense of the target word. Fol- lowing the SemEval 2013 shared task and motivat- ing example (d) from the introduction, we seek a soft (probabilistic) clustering, in which each word instance is assigned with a probability of belong- ing to each of the sense-clusters.</p><p>Our algorithm works in three stages: (1) We first associate each instance with a probability dis- tribution over in-context word-substitutes. This probability distribution is based on a neural biLM (section 2.1). (2) We associate each instance with k representatives, each containing multiple sam- ples from its associated word distributions (section 2.3). (3) Finally, we cluster the representatives and use the hard clustering to derive a soft-clustering over the instances (section 2.4).</p><p>We use the pre-trained neural biLM as a black- box, but use linguistically motivated processing of both its input and its output: we rely on the gen- eralization power of the biLM and query it using dynamic symmetric patterns (section 2.2); and we lemmatize the resulting word distributions.</p><p>Running example In what follows, we demon- strate the algorithm using a running example of in- ducing senses from the word sound, focusing on the instance sentence: I liked the sound of the harpsichord.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">biLM Derived Substitutions</head><p>We follow the ELMo biLM approach ( <ref type="bibr" target="#b10">Peters et al., 2018</ref>) and consider two separately trained language models, a forward model trained for predicting p → (w i |w 1 , ..., w i−1 ) and a backward model p ← (w i |w n , ..., w i+1 ). Rather than combin- ing the two models' predictions into a single dis- tribution, we simply associate the target word with two distributions, one from p → and one from p ← . For convenience, we use LM → (w 1 w 2 ...w i−1 ) to denote the distribution p → (w i |w 1 , ..., w i−1 ) and</p><formula xml:id="formula_0">LM ← ( w i+1 w i+2 ...w n ) to denote p ← (w i |w n , ..., w i+1 ).</formula><p>Context-based substitution In the purely context-based setup (the one used in the AI-KU system) we represent the target word sounds by the two distributions:</p><formula xml:id="formula_1">LM → (&lt;s&gt; I liked the ) LM ← ( of the harpsichord &lt;/s&gt; )</formula><p>The resulting top predictions from each distribu- tion are: {idea:0.12, fact:0.07, article: 0.05, guy: 0.04, concept: 0.02} and {sounds:0.04, version: 0.03, rhythm: 0.03, strings: 0.03, piece: 0.02} respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic Symmetric Patterns</head><p>As discussed in the introduction, conditioning solely on context is ignoring valuable infor- mation. This is evident in the resulting word distributions. We use the coordinative sym- metric pattern X and Y in order to produce a substitutes vector incorporating both the word and its context. Concretely, we represent a target word w i by p → (w |w 1 , ..., w i , and) and p ← (w |w n , ..., w i , and). For our running exam- ple, this translates to: LM → (&lt;s&gt; I liked the sound and ) LM ← ( and sound of the harpsichord . &lt;/s&gt;) with resulting top words: {feel: 0.15, felt: 0.11, thought: 0.07, smell: 0.06, sounds: 0.05} and {sight: 0.16, sounds: 0.11, rhythm: 0.04, tone: 0.03, noise: 0.03}.</p><p>The distributions predicted using the and pat- tern exhibit a much nicer behavior, and incorpo- rate global context (resulting in sensing related substitutes) as well as local and syntactic informa- tion that resulting from the target word itself. Ta- ble 1 compares the context-only and symmetric- pattern substitutes for two senses of the word sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representative Generation</head><p>To perform fuzzy clustering, we follow AI-KU and associate each instance with k representatives, but deviate in the way the representatives are gen- erated. Specifically, each representative is a set of size 2 , containing samples from the forward distribution and samples from the backward dis- tribution. In the symmetric pattern case above, a plausible representative, assuming = 2, would be: {feel, sounds, sight, rhythm} where two words were predicted by each side LM. In this work, we use = 4 and k = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sense Clustering</head><p>After obtaining k representatives for each of the n word instances, we cluster the nk representa- tives into distinct senses and translate this hard- clustering of representatives into a probabilistic clustering of the originating instances.</p><p>Hard-clustering of representatives Let V be the vocabulary obtained from all the representa- tives. We associate each representative with a sparse |V | dimensional bag-of-features vector, and arrange the representatives into a nk × |V | matrix M where each row corresponds to a representa- tive. We now cluster M 's rows into senses. We found it is beneficial to transform the matrix us- ing TF-IDF. Treating each representative as a doc- ument, TF-IDF reduces the weight of uninforma- tive words shared by many representatives. We use agglomerative clustering (cosine distance, av- erage linkage) and induce a fixed number of clus- ters. <ref type="bibr">3</ref> We use sklearn (Pedregosa et al., 2011) for both TF-IDF weighting and clustering.</p><p>Inducing soft clustering over instances After clustering the representatives, we induce a soft- clustering over the instances by associating each instance j to sense i based on the proportion of representatives of j that are assigned to cluster i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Additional Processing</head><p>Lemmatization The WSI task is defined over lemmas, and some target words have morpholog- ical variability within a sense. This is especially common with verb tenses, e.g., "I booked a flight" and "I am booking a flight". As the conjunctive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Only</head><p>Symmetric Pattern Forward dist. Backward dist. Forward dist.</p><p>Backward dist. This is a sound idea, I like it. sad 0.02 bad 0.12 welcome 0.09 funny 0.10 great 0.02 good 0.09 practical 0.03 beautiful 0.05 huge 0.02 great 0.06 comprehensive 0.03 fun 0.04 symmetric pattern favors morphologically-similar words, the resulting substitute vectors for these two sentences will differ, each of them agreeing with the tense of its source instance. To deal with this, we lemmatize the predictions made by the language model prior to adding them to the rep- resentatives. Such removal of morphological in- flection is straightforward when using the word distributions but much less trivial when using raw LM state vectors, further motivating our choice of working with the word distributions. The substan- tial importance of the lemmatization is explored in the ablation experiments in the next section, as well as in the supplementary material. Distribution cutoff and bias Low ranked LM prediction tend to become noisier. We thus con- sider only the top 50 word predicted by each LM, re-normalizing their probabilities to sum to one. Additionally, we ignore the final bias vec- tor during prediction (words are predicted via sof tmax(W x) rather than sof tmax(W x + b)). This removes unconditionally probable (frequent) words from the top LM predictions.</p><note type="other">very 0.02 wonderful 0.05 light 0.02 simple 0.04 lesson 0.02 nice 0.04 balanced 0.02 interesting 0.03 I liked the sound of the harpsichord idea 0.12 sounds 0.04 feel 0.15 sight 0.16 fact 0.07 version 0.03 felt 0.11 sounds 0.11 article 0.05 rhythm 0.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We evaluate our method on the SemEval 2013 Task 13 dataset (Jurgens and Klapaftis, 2013), containing 50 ambiguous words each with roughly 100 in-sentence instances, where each instance is soft-labeled with one or more WordNet senses. Experiment Protocol Due to the stochastic na- ture of the algorithm, we repeat each experiment 30 times and report the mean scores together with the standard deviation.</p><p>Evaluation metrics We follow previous work ( <ref type="bibr" target="#b13">Wang et al., 2015;</ref><ref type="bibr" target="#b4">Komninos and Manandhar, 2016)</ref> and evaluate on two measures: Fuzzy Nor- malized Mutual Information (FNMI) and Fuzzy B-Cubed (FBC) as well as their geometric mean (AVG). Systems We compare against three graphical- model based systems which, as far as we know, represent the current state of the art: MCC-S ( <ref type="bibr" target="#b4">Komninos and Manandhar, 2016)</ref>, Sense-Topic ( <ref type="bibr" target="#b13">Wang et al., 2015</ref>) and unimelb ( <ref type="bibr" target="#b6">Lau et al., 2013</ref>). We also compare against the AI-KU sys- tem. Wang et al. also present a method for dataset enrichment that boosted their model performance. We didn't use the suggested methods and compare ourselves to the vanilla settings, but report the en- richment numbers as well.</p><p>Results <ref type="table" target="#tab_2">Table 2</ref> summarizes the results. Our system using symmetric patterns outperforms all other setups with an AVG score of 25.4, establish- ing a new state-of-the-art on the task.</p><p>Ablation and analysis We perform ablations to explore the contribution of the different compo- nents (Symmetric Patterns (SP), Lemmatization (LEM) and TF-IDF re-weighting). <ref type="figure" target="#fig_0">Figure (1)</ref> shows the results for the entire dataset (ALL, top), as well as broken-down by part-of-speech. All components are beneficial and are needed for ob- taining the best performance in all cases. How- ever, their relative importance differs across parts- of-speech. Adjectives gain the most from the use of the dynamic symmetric patterns, while nouns gain the least. For verbs, the lemmatization is   crucial for obtaining good performance, especially when symmetric patterns are used: using symmet- ric patterns without lemmatization, the mean score drops to 17.0. Lemmatization without symmet- ric patterns achieves a higher mean score of 20.5, while using both yields 22.8. Finally, for nouns it is the TF-IDF scoring that plays the biggest role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We describe a simple and effective WSI method based on a neural biLM and a novel dynamic ap- plication of the X and Y symmetric pattern. The method substantially improves on the state-of-the- art. Our results provide further validation that RNN-based language models contain valuable se- mantic information.</p><p>The main novelty in our proposal is querying the neural LM in a creative way, in what we call dynamic symmetric patterns. We believe that the use of such dynamic symmetric patterns (or more generally dynamic Hearst patterns) will be bene- ficial to NLP tasks beyond WSI.</p><p>In contrast to previous work, we used discrete predicted word distributions rather than the con- tinuous RNN states. This paid off by allowing us to inspect and debug the representation, as well to control it in a meaningful way by injecting linguis- tic knowledge in the form of lemmatization, and by distributional cutoff and TF-IDF re-weighing. We encourage others to consider using explicit, discrete representations when appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics of the SemEval 2013 Task 13 Dataset</head><p>SemEval 2013 Task 13 consists of 50 targets, each has a lemma and a part of speech (20 verbs, 20 nouns and 10 adjectives). We use the dataset only for evaluation. Most targets have around 100 la- beled instances (sentences containing a usage of the target in its designated part of speech together with one or more WordNet senses assigned by hu- man labeler). Exceptions are the targets of trace.n and book.v which have 37 and 22 labeled in- stances accordingly. Leaving out the two anoma- lous targets mentioned above we are left with 4605 instances from 48 targets: 19 verb, 19 noun and 10 adjective targets. We note that the small size of the dataset should make one cautious to draw quick conclusions, yet, our results seem to be consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the Choice of Number of Clusters</head><p>An important statistic of the dataset is the num- ber of senses per target. The average number of senses per target in the dataset is 6.94 (stdev:2.71). Breaking down by part of speech, the means and standard deviations of target senses are: verbs: 5.90 (±1.37), nouns: 7.32 (±2.21), adjectives: 7.11 (±3.54). In this work we follow this statis- tic and always look for 7 clusters. <ref type="figure" target="#fig_1">Figure 2</ref> shows the accuracy as a function of the number of clus- ters. While 7 clusters indeed produces the highest scores, all numbers in the range 4 to 15 produce state-of-the-art results. We leave the selection of per-instance number of clusters to future work.  2 also tells us our system is better at in- ducing senses for adjectives, at least according to task score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Importance of Lemmatization</head><p>The ablation results in the paper indicate that for verbs, using symmetric patterns without lemmati- zation yields poor results. We present the analysis the motivated our use of lemmatization. Consider the samples from the biLM with and without sym- metric patterns, for the instance It was when I was a high-school student that I became convinced of this fact for the first time.</p><p>fw LM, no SP: didn, write, 'd, learnt, start bw LM, no SP: seem, be, grow, be, be fw LM, with SP: went, got, started, wasn, loved bw LM, with SP: 1990s, decade, 1980s, afterwards, changed Another sentence, in another tense: The issue will become more pressing as an estimated 40,000 to 50,000 Chinese, mostly unskilled, come to settle each year.</p><p>fw LM, no SP: be, be, remain, likely, be bw LM, no SP: becoming, grown becoming, much, becomes fw LM, with SP: remains, remain, which, continue, how bw LM, with SP: rising, overseas, booming, abroad, expanded When using the symmetric patterns, the pre- dicted verbs tend to share the tense of the target word.</p><p>This results in targets of different tenses hav- ing nearly distinct distributions, even when the tar- gets share the same sense, splitting the single sense cluster to two (or more) tense clusters. We quan- tify this intuition by computing the correlation be- tween tense and induced clusters (senses), as given by the Normalized Mutual Information (NMI). We measure NMI between verb instance tense in sen- tence and their most probable induced cluster in the different settings, as well as the NMI of the verb instances and the gold clusters. <ref type="table">Table 3</ref> sum- marize the results. We see that in the gold clus- ters there is indeed very little correlation (0.15) between the the tense and the sense. When us- ing SP but not lemmatization (w/o LEM), the cor- relation is substantially higher (0.67). When not using either lemmatization of SP (w/o LEM and SP) the correlation is 0.27, much closer to the gold one. Performing explicit lemmatization naturally reduces the correlation with tense, and using the full model (Final model) results in a correlation to 0.22, close to the gold number of 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some Failure Modes of Dynamic Symmetric Patterns</head><p>While the use of dynamic symmetric patterns im- proves performance and generally produces good substitutes for contextualized words, we also iden- tify some failure modes and unexpected behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common phrases involving conjunctions</head><p>Some target words have a strong prior to appear in common phrases involving a conjunction, causing the strong local pattern to override context-based hints. For example, when the LM is asked to complete ... state and , its prior on church makes it a very probable completion, regardless of context and sense. This phenomena motivated our use TF-IDF for weighing of too common words. Relatedly, a common completion for symmetric patterns is the word then, as and then is a very common phrase. This completion even ignores the target word and could be troublesome if a global, cross-lemma, clustering is attempted.</p><p>Multi word phrases substitutes Sometime the LM does interpret the and as a trigger for a sym- metric relation, but on a chunk extending beyond the target word. For example, when presented with the query The human heart not only makes heart sounds and , the forward LM predicted in its top twenty suggestions the word muscle, fol- lowed by a next-word prediction of movements. That is, the symmetry extends beyond "sounds" to the phrase "heart sounds" which could be substi- tutes by "muscle movements". We didn't specif- ically address this in the current work, but note that restricting the prediction to agree with the tar- get word on part-of-speech and plurality may help in mitigating this. Furthermore, this suggests an exciting direction for moving from single words towards handling of multi-word units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>NMI (mean ± STD) Gold labels 0.15 ± 0.07  <ref type="table">Table 3</ref>: Correlation between tense and sense. NMI is averaged on all verbs, using best matching sense. SP: Symmetric Patterns, LEM: Lemmatizing predictions, ALL: LEM, SP, TFIDF. The bold line show symmetric patterns without lemmatization excessively correlates tense and sense and provides additional validation to our hypothesis, suggesting its essential to lemmatizate when symmetric patterns are used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Ablation break down by part of speech, each part of speech was averaged across run. Bars are mean of means and error bars are standard deviations.</figDesc><graphic url="image-1.png" coords="5,72.00,277.98,218.27,360.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: AVG score by number of clusters.</figDesc><graphic url="image-2.png" coords="7,72.32,524.70,217.62,212.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 2 also tells us our system is better at inducing senses for adjectives, at least according to task score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Evaluation Results on the SemEval 2013 Task 13 Dataset. SW: Embeddings similarity based feature weighting. AAC: Extending instance sentences from their traced source. AUC: Adding similar sentences from the dataset originating corpus. We report our mean scores over 30 runs ± standard deviation</figDesc><table></table></figure>

			<note place="foot" n="1"> This example shows homonymy, a case where the same word form has two distinct meaning. A more subtle case is polysemy, where the senses share some semantic similarity. In &quot;She played a low bass note&quot;, the sense of bass is related to the sense in (b), but distinct from it. The WSI task we tackle in this work deals with both cases.</note>

			<note place="foot" n="2"> We thank the ELMo team for sharing the pre-trained models.</note>

			<note place="foot" n="3"> In this work, we use 7 clusters, which roughly matches the number of senses for each target word in the corpus. Dynamically selecting the number of clusters is left for future work. The effect of changing the number of clusters is explored in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 02: Evaluating word sense induction and discrimination systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, SemEval &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ai-ku: Using substitute vectors and cooccurrence modeling for word sense induction and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osman</forename><surname>Baskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="300" to="306" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics</title>
		<meeting>the 14th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval2013 task 13: Word sense induction for graded and non-graded senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured generative models of continuous features for word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Komninos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3577" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic class learning from the web with hyponym pattern linkage graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1048" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">unimelb: Topic modelling-based word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="307" to="311" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 14: Word sense induction &amp; disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sameer S Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation</title>
		<meeting>the 5th international workshop on semantic evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning generic context embedding with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1156" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Symmetric pattern based word embeddings for improved word similarity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A sense-topic model for word sense induction with unsupervised data enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T Yu</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clement</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A graph model for unsupervised lexical acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beate</forename><surname>Dorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>COLING &apos;02</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised word sense disambiguation with neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Altendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07012</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
