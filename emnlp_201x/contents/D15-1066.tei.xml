<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LDTM: A Latent Document Type Model for Cumulative Citation Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Knowledge Mining Group</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LDTM: A Latent Document Type Model for Cumulative Citation Recommendation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper studies Cumulative Citation Recommendation (CCR)-given an entity in Knowledge Bases, how to effectively detect its potential citations from volume text streams. Most previous approaches treated all kinds of features indifferently to build a global relevance model, in which the prior knowledge embedded in documents cannot be exploited adequately. To address this problem, we propose a latent document type discriminative model by introducing a latent layer to capture the correlations between documents and their underlying types. The model can better adjust to different types of documents and yield flexible performance when dealing with a broad range of document types. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset, and the results demonstrate that this model can yield a significant performance gain in recommendation quality as compared to the state-of-the-art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Bases (KBs), like Wikipedia, are playing increasingly important roles in numerous entity-based information retrieval tasks. Neverthe- less, most KBs are hard to be up-to-date due to their manual maintenances by human editors. As reported in <ref type="bibr" target="#b4">(Frank et al., 2012</ref>), there exists a me- dian time lag of 356 days between the day a news article is published and the time that the news is cited in a Wikipedia article dedicated to the entity concerned by the news. The time lag would be reduced if relevant documents could be automati- cally detected as soon as they are published online * This work was partially performed when the first author was visiting Purdue University and Microsoft Research Asia.</p><p>† Corresponding Author and then recommended to the editors. This task is studied as Cumulative Citation Recommendation (CCR). Formally, given a set of KB entities, CCR is to filter relevant documents from a stream cor- pus and evaluate their citation-worthiness to the target entities. A variety of supervised approaches (e.g., clas- sification, learning to rank) have been employed and achieved promising results ( <ref type="bibr" target="#b9">Wang et al., 2013;</ref>. Nevertheless, most of them leverage all features indiscriminately to build a global relevance model, which leads to unsatisfactory performance. The documents can offer some prior knowledge, which is named as type in this paper. The type is the prior knowledge embedded in the document that im- pacts on the probability of its being recommended to KBs. For instance, when dealing with a docu- ment on "music" topic, we would like to have less weights put on a politician entity because this doc- ument is not likely to related to it, but more often related to musicians or musical bands. Besides, the source of a document impacts on the recom- mendation strategies too. A document from news agencies is more reliable and citable than the one from social websites even if they state an identi- cal story about the target KB entity. Hence we consider two kinds of document features to model the prior type knowledge: (1) topic-based features, and (2) source-based features.</p><p>This paper proposes a latent document type dis- criminative mixture model for CCR. We introduce an intermediate latent layer to model latent docu- ment types and define a joint distribution over the document-entity pairs and latent document-types on the observation data. The aim is to achieve a discriminative mixture model that is expected to outperform the global relevance model.</p><p>To the best of our knowledge, this is the first research work that leverages prior knowledge em- bedded in documents to improve CCR perfor-mance. An extensive set of experiments conducted on TREC-KBA-2013 dataset has demonstrated the effectiveness of the proposed mixture model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discriminative Models for CCR</head><p>Given a set of KB entities E = {e u |u = 1, · · · , M } and a document collection D = {d v |v = 1, · · · , N }, our objective is to es- timate the conditional probability of relevance P (r|e, d) with respect to an entity-document pair</p><formula xml:id="formula_0">(e, d). Each (e, d) is represented as a feature vector f (e, d) = (f 1 (e, d), · · · , f K (e, d)),</formula><p>where K is the dimension of the entity-document fea- ture vector. Moreover, to model the hidden document type, each document is represented as an document-type feature vector</p><formula xml:id="formula_1">g(d) = (g 1 (d), · · · , g L (d))</formula><p>, where L indicates the dimen- sion of the document-type feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Global Model</head><p>This paper utilizes logistic regression to estimate the conditional probability P (r|e, d), where r(r ∈ {1, −1}) is a binary label indicating the relevance of an entity-document pair (e, d). The value of r is 1 if the document d is related to the entity e, oth- erwise r = −1. Formally, the parametric form of</p><formula xml:id="formula_2">P (r=1|e, d) is expressed as P (r=1|e, d) = δ( K i=1 ω i f i (e, d))</formula><p>, where δ(x) is the standard lo- gistic function, ω i is the combination parame- ter for the ith feature. It is easy to derive that for different values of r, the only difference in P (r|e, d) is the sign within the logistic function. Therefore, we adopt the general representation of</p><formula xml:id="formula_3">P (r|e, d)=δ(r K i=1 ω i f i (e, d))</formula><p>. This model is denoted as GM in this paper. Several previous ap- proaches can be deemed as global models adopt- ing different classification functions such as deci- sion trees ( <ref type="bibr" target="#b9">Wang et al., 2013)</ref> and Support Vector Machine (SVM) ( <ref type="bibr" target="#b2">Bonnefoy et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent Document Type Model</head><p>In GM, a fixed set of combination weights (i.e., ω) are learned to optimize the overall performance for all entity-document pairs. However, the best combination strategy for a given pair is not al- ways the best for the others since both the docu- ments and entities are heterogeneous. Therefore, we may benefit from developing a document type dependent model in which we choose the com- bination strategy individually for each document type to optimize the performance for specific doc- ument types. Since it is not feasible to determine the proper combination strategy for each docu- ment type, we need to classify documents into one of several types. The combination strategy is then tuned to optimize average performance for docu- ments within the same type.</p><p>We propose a latent document type model (LDTM) by introducing an intermediate layer to capture the underlying type information in documents. A latent variable z is utilized to indicate which type the combination weights ω z are drawn from. The choice of z is determined by the document d. The joint probability of relevance r and the latent variable z is represented as P (r, z|e, d; α, ω)=P (z|d; α)P (r|e, d, z; ω), where P (z|d; α) is the mixing coefficient, denot- ing the probability of choosing the hidden type z given document d, and α is the corresponding parameter. P (r|e, d, z; ω) denotes the discrimi- native component which takes a logistic function. By marginalizing out z, we obtain</p><formula xml:id="formula_4">P (r|e, d; α, ω) = Nz z P (z|d; α)δ r K i=1 ω zi f i (e, d)<label>(1)</label></formula><p>where ω zi is the weight for the ith entry in the feature vector under the hidden variable z. We adopt a soft-max function 1</p><formula xml:id="formula_5">Z d exp( L j=1 α z j g j (d))</formula><p>to model P (z|d; α), and Z d is the normalization factor that scaled the exponential function to be a probability distribution. In this representation, each document d is denoted by a bag of document type features</p><formula xml:id="formula_6">(g 1 (d), · · · , g L (d))</formula><p>. By plugging the soft-max function into Equation (1), we have</p><formula xml:id="formula_7">P (r|e, d; α, ω)= 1 Z d Nz z=1 exp Lz j=1 α zj g j (d) δ r K i=1 ω zi f i (e, d)<label>(2)</label></formula><p>Suppose entity-document pairs in training set are represented as T ={(d u , e v )}, and R={r uv } de- notes the corresponding relevance judgment of (d u , e v ), where u = 1, · · · , M and v = 1, · · · , N . Assume training instances in T are independently generated, the conditional likelihood of training data is written as</p><formula xml:id="formula_8">P (R|T ) = M u=1 N v=1 P (r uv |e u , d v )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameter Estimation</head><p>The parameters (i.e., ω and α) can be estimated by maximizing the data log-likelihood L(ω, α), which is the form of logarithm of Equation <ref type="formula" target="#formula_8">(3)</ref>. A typical parameter estimation method is to use Expectation-Maximization (EM) algorithm by it- erating E-step and M-step continuously until con- vergence. The E-step is derived by computing the posterior probability of z given d u and e v , which is denoted as P (z|d u , e v ).</p><formula xml:id="formula_9">P (z|d u , e v ) = exp Lz j=1 α zj g j (d u ) δ r uv K i=1 ω zi f i (d u , e v ) z exp Lz j=1 α zj g j (d u ) δ r uv K i=1 ω zi f i (d u , e v )<label>(4)</label></formula><p>In M-step, we can obtain the following parame- ter update rules.</p><formula xml:id="formula_10">ω * z = arg max ωz uv P (z|d u , e v )log δ K i=1 ω zi f i (d u , e v ) α * z = arg max αz u v P (z|d u , e v ) log 1 Z du exp Lz j=1 α zj g j (d u )<label>(5)</label></formula><p>To optimize Equation (5), we employ the minFunc toolkit 1 using Quasi-Newton strategy. We adopt Akaike Information Criteria (AIC) to determine the number of latent variables <ref type="bibr" target="#b3">(Fang et al., 2010)</ref>, which is calculated as 2m − 2L(ω, α), where m is the number of parameters in the model. LDTM holds two advantages over GM. (1) The combination parameters vary across various docu- ment types and hence lead to a gain of flexibility; (2) It offers probabilistic semantics for the latent document types and thus documents can be asso- ciated with multiple types. addition, LDTM requires document-type features (i.e., g(e)) to learn the mixing coefficients in the mixture component.</p><p>Since our goal is not to develop new entity- document features, we adopt the identical entity- document feature set proposed in our previous work ( <ref type="bibr" target="#b9">Wang et al., 2013;</ref><ref type="bibr" target="#b10">Wang et al., 2015a;</ref><ref type="bibr" target="#b11">Wang et al., 2015b</ref>), which has been proved effective.</p><p>In terms of document-type features, we consider two kinds of prior knowledge embedded in doc- uments to model the correlations between docu- ments and their latent types.</p><p>Topic-based features One prior knowledge to model a document's latent type is its intrinsic top- ics. As we have claimed, documents with one or more obvious topics are more likely to be rec- ommended to KB than those without any explicit topic. We capture the underlying topics in docu- ments with word co-occurrences. After removing stop words, we represent each document as a fea- ture vector with the bag-of-words model, where word weights are determined by TF-IDF scheme.</p><p>Source-based features The source of a docu- ment is another prior knowledge to evaluate the probability of the document's being recommended to KBs. We leverage a "bag-of-sources" model to represent each document as source-based feature vector, and term weights are determined by binary occurrence scheme. Please note that the sources are organized hierarchically. For example, main- stream news is a sub-source of news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We utilize TREC-KBA-2013 dataset 2 as our ex- perimental dataset. The dataset is composed of a temporally stream corpus and a target KB entity set. The stream corpus contains nearly 1 billion documents crawled from 10 sources: news, mainstream news, social, weblog, link- ing, arxiv, classified, reviews, forum and meme- tracker <ref type="bibr">3</ref> . The corpus has been split with doc- uments from October 2011 to February 2012 as training instances and the remainder for evalua- tion. We adopt the same training/test range setting in our experiments. The entity set is composed of 121 Wikipedia entities and 20 Twitter entities.</p><p>Each entity-document pair is labeled as one of the 4 relevance levels: (i) Vital, timely informa- tion about the entity's current state, actions, or sit- uation. This would motivate a change to an al- ready up-to-date KB article. (ii) Useful, possibly citable but not timely, e.g., background biography, secondary source information. (iii) Neural, in- formative but not citable, e.g., tertiary source like Wikipedia article itself. and (iv) Garbage, no in- formation about the target entity could be learned from the document, e.g., spam. Annotation details of the dataset are presented in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>: Annotation details of TREC-KBA-2013 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Scenarios</head><p>According to different granularity settings, we evaluate the proposed models in two scenarios: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Methods</head><p>We conduct extensive comparisons with the fol- lowing methods.</p><p>• Global Model (GM). The global discrimina- tive model introduced in section 2.1.</p><p>• Source-based Latent Document Type Model (src LDTM). A variant of LDTM that uti- lizes source-based features as document-type features.</p><p>• Topic-based Latent Document Type Model (topic LDTM). A variant of LDTM that uti- lizes topic-based features as document-type features.</p><p>• Combination Latent Document Type Model (combine LDTM). This approach utilizes source-based and topic-based features to- gether as document-type features. In our ex- perimental setting, we simply union the two feature vectors together into an integral fea- ture vector.</p><p>For reference, we also include three top-ranked ap- proaches in TREC-KBA-2013 track.</p><p>• BIT-MSRA ( <ref type="bibr" target="#b9">Wang et al., 2013)</ref>. A global random forests classification method, the first place approach in TREC-KBA-2013 track.</p><p>• UDEL ( <ref type="bibr" target="#b8">Liu et al., 2013</ref>). An entity-centric query expansion approach, the second place approach in TREC-KBA-2013 track.</p><p>• Official Baseline <ref type="figure">(Frank et al., 2013)</ref>. A strong baseline in which human annotators go through target entities and came up with a list of keywords for filtering vital documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Discussion</head><p>Improving precision is harder than improving re- call for CCR ( <ref type="bibr" target="#b5">Frank et al., 2013</ref>). Therefore, we care more about recommendation quality of CCR. Precision and overall accuracy are adopted as metrics to evaluate different approaches. All the metrics are computed in the test pool of all entity-document pairs. The results are reported in  in the 2nd block of <ref type="table" target="#tab_1">Table 2</ref>, our mixture models achieve higher or competitive precision and ac- curacy in both scenarios considerably. Compared with the official baseline, our best mixture model improves precision about 28%. In both scenarios, the variants of LDTM outperform GM on preci- sion and accuracy, which validates our motivations that (i) introducing document latent types in mix- ture model can enhance the recommendation qual- ity, and (ii) source-based and topic-based features can capture the hidden type information of docu- ments. Moreover, topic LDTM generally performs bet- ter than src LDTM in both scenarios, which meets our expectation because topic-based features have far more dimensions than source-based features. However, even if source-based feature vector holds a few dimensions (10 in our experiments), src LDT improves the precision on the basis of GM. Thus, the precision can be enhanced further if we can develop more valuable features to repre- sent the underlying document types. The combi- nation variant of LDTM achieve the best precision in Vital Only scenario and the best accuracy in Vital + Useful scenario. The na¨ıvena¨ıve combination strategy of two types of features can improve the performance but not stable, so we need find better combination strategies.</p><p>For all variant of the LDTM, the number of latent types determined by AIC are reported in <ref type="table" target="#tab_4">Table 3</ref>. The optimal number of latent types in Vital + Useful is more than that in Vital Only. This reveals that the types of Vital documents for entities have more restrictions than Useful docu- ments, either by topics or by sources. In addition, the optimal number of latent topics is more than that of latent sources, which also follows our in- tuition that topic-based features holding more di- mensions than source-based features. Since we employ a na¨ıvena¨ıve combination strategy for the two types of features, the number of latent types of combine LDTM is more close to topic LDTM, which possesses more features than src LDTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Vital  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There are three kinds of approaches developed for CCR in previous work: query expansion ( <ref type="bibr" target="#b8">Liu et al., 2013;</ref><ref type="bibr" target="#b9">Wang et al., 2013)</ref>, classification such as SVM ( <ref type="bibr" target="#b7">Kjersten and McNamee, 2012)</ref> and Random Forest classifier ( <ref type="bibr" target="#b2">Bonnefoy et al., 2013;</ref>, and learning to rank approaches ( <ref type="bibr" target="#b9">Wang et al., 2013;</ref>). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs ( <ref type="bibr" target="#b12">Zhou and Chang, 2013)</ref>. However, some highly supervised methods re- quire training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global dis- criminative model with all features indifferently.</p><p>We spotlight document-type features and study the impacts of them in discriminative mixture models. Mixture model has been applied and proved effec- tive in multiple information retrieval tasks, such as expert search <ref type="bibr" target="#b3">(Fang et al., 2010</ref>) and federated search <ref type="bibr" target="#b6">(Hong and Si, 2012)</ref>. By learning flexible combination weights for different types of training instances, mixture model can outperform global models with fixed weights for all instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Cumulative Citation Recommendation (CCR) is an important task to automatically detect citation- worthy documents from volume text streams for knowledge base entities. We study CCR as a classification problem and propose a latent docu- ment type model (LDTM) through introducing a latent layer in a discriminative model to capture the correlations between documents and their in- trinsic types. Two variants of LDTM are imple- mented by modeling the latent types with doc- ument source-based and topic-based features re- spectively. Experimental results on TREC-KBA- 2013 dataset demonstrate that our mixture model can improve CCR performance significantly, espe- cially on precision and accuracy, revealing the ad- vantage of LDTM in enhancing recommendation quality of citation-worthy documents.</p><p>For future work, we wish to explore more use- ful document-type features and apply more proper combination strategies to improve the latent docu- ment type model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(i) Vital Only.</head><label></label><figDesc>Only vital entity-document pairs are treated as positive instances. (ii) Vital + Use- ful. Both vital and useful entity-document pairs are treated as positive instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Range 
Vital Useful Neutral Garbage 
Train 2011.10 ∼ 2012.02 1696 
2121 
1030 
1702 
Test 
2012.03 ∼ 2013.02 5630 11579 
3379 
10543 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 . In comparison to the baselines listed</head><label>2</label><figDesc></figDesc><table>Methods 
Vital Only Vital + Useful 
P 
Accu 
P 
Accu 
Official Baseline .171 .175 .540 
.532 
BIT-MSRA 
.214 .445 .589 
.615 
UDEL 
.169 .259 .573 
.579 
GM 
.218 .587 .604 
.565 
src LDTM 
.273 .763 .626 
.607 
topic LDTM 
.293 .755 .643 
.609 
combine LDTM .299 .751 .633 
.611 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Overall results of evaluated methods. 
Best scores are typeset boldface. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of latent types determined by 
AIC. 

</table></figure>

			<note place="foot" n="3"> Features This section presents the two types of features used in the discriminative models. Entitydocument features (i.e., f (e, d)) are used in the discriminative components of GM and LDTM. In 1 http://www.cs.ubc.ca/ ˜ schmidtm/ Software/minFunc.html</note>

			<note place="foot" n="2"> http://trec-kba.org/ kba-stream-corpus-2013.shtml 3 http://www.memetracker.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Fei Sun, Qifan Wang and Chen Shao for their valuable sugges-tions and the anonymous reviewers for their help-ful comments. This work is funded by the Na-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cumulative citation recommendation: classification vs. ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heri</forename><surname>Ramampiaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="941" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-step classification approaches to cumulative citation recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heri</forename><surname>Ramampiaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OAIR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
	<note>Naimdjon Takhirov, and Kjetil Nørvåg</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A weakly-supervised detection of entity central documents in a stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bouvier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative models of integrating document evidence and document-candidate associations for expert search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">P</forename><surname>Mathur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="683" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building an Entity-Centric Stream Filtering Test Collection for TREC 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating stream filtering for entity profile updates for trec 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Triouraneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher R` E</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mixture model with multiple centralized retrieval algorithms for result merging in federated search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzung</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The hltcoe approach to the trec 2012 kba track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Kjersten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A related entity based approach for knowledge base acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Darko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bit and msra at trec kba ccr track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Resorting relevance evidences to cumulative citation recommendation for knowledge base acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web-Age Information Management</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9098</biblScope>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An entity class-depedent discriminative mixture model for cumulative citation recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entity-centric document filtering: boosting feature mapping through meta-features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
