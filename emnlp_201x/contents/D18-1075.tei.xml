<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="702" to="707"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>702</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated , which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic dialogue generation task is of great importance to many applications, ranging from open-domain chatbots ( <ref type="bibr" target="#b5">Higashinaka et al., 2014;</ref><ref type="bibr" target="#b23">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b9">Li et al., 2016</ref><ref type="bibr" target="#b10">Li et al., , 2017a</ref>) to goal-oriented technical support agents <ref type="bibr" target="#b2">(Bordes and Weston, 2016;</ref><ref type="bibr" target="#b0">Asri et al., 2017)</ref>. Recently there is an increasing amount of studies about purely data- driven dialogue models, which learn from large corpora of human conversations without hand- crafted rules or templates. Most of them are based on the sequence-to-sequence (Seq2Seq) frame- work ( <ref type="bibr" target="#b21">Sutskever et al., 2014</ref>) that maximizes the probability of gold responses given the previous dialogue turn. Although such methods offer great promise for generating fluent responses, they still suffer from the poor semantic relevance between inputs and responses ( ). For exam- ple, given "What's your name" as the input, the models generate "I like it" as the output.</p><p>Recently, the neural attention mechanism ( <ref type="bibr" target="#b13">Luong et al., 2015;</ref><ref type="bibr" target="#b22">Vaswani et al., 2017</ref>) has been proved successful in many tasks including neural machine translation ( <ref type="bibr" target="#b15">Ma et al., 2018b</ref>) and abstrac- tive summarization ( , for its ability of capturing word-level dependency by associat- ing a generated word with relevant words in the source-side context. Recent studies ( <ref type="bibr" target="#b16">Mei et al., 2017;</ref><ref type="bibr" target="#b19">Serban et al., 2017)</ref> have applied the at- tention mechanism to dialogue generation to im- prove the dialogue coherence. However, conversa- tion generation is a much more complex and flex- ible task as there are less "word-to-words" rela- tions between inputs and responses. For exam- ple, given "Try not to take on more than you can handle" as the input and "You are right" as the response, each response word can not find any aligned words from the input. In fact, this task re- quires the model to understand the utterance-level dependency, a relation between the whole mean- ings of inputs and outputs. Due to the lack of utterance-level semantic dependency, the conven- tional attention-based methods that simply capture the word-level dependency achieve less satisfying performance in generating high-quality responses.</p><p>To address this problem, we propose a novel Auto-Encoder Matching model to learn utterance-level dependency. First, motivated by <ref type="bibr" target="#b14">Ma et al. (2018a)</ref>, we use two auto-encoders to learn the semantic representations of inputs and re- sponses in an unsupervised style. Second, given the utterance-level representations, the mapping module is taught to learn the utterance-level de- pendency. The advantage is that by explicitly sep- arating representation learning and dependency  learning, the model has a stronger modeling ability compared to traditional Seq2Seq models. Exper- imental results show that our model substantially outperforms baseline methods in generating high- quality responses.</p><p>Our contributions are listed as follows:</p><p>• To promote coherence in dialogue gener- ation, we propose a novel Auto-Encoder Matching model to learn the utterance-level dependency.</p><p>• In our proposed model, we explicitly separate utterance representation learning and depen- dency learning for a better expressive ability.</p><p>• Experimental results on automatic evaluation and human evaluation show that our model can generate much more coherent text com- pared to baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we introduce our proposed model. An overview is presented in Section 2.1. The de- tails of the modules are shown in Sections 2.2, 2.3 and 2.4. The training method is introduced in Sec- tion 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The proposed model contains three modules: an encoder, a decoder, and a mapping module, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>In general, our model is different from the con- ventional sequence-to-sequence models. The en- coder and decoder are both implemented as auto- encoders <ref type="bibr" target="#b1">(Baldi, 2012)</ref>. They learn the internal representations of inputs and target responses, re- spectively. In addition, a mapping module is built to map the internal representations of the input and the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoder</head><p>The encoder E θ is an unsupervised auto-encoder based on Long Short Term Memory Networks (LSTM) <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1996)</ref>. As it is essentially a LSTM-based Seq2Seq model, we name the encoder and decoder of the auto- encoder "source-encoder" and "source-decoder". To be specific, the encoder E θ receives the source text x = {x 1 , x 2 , ..., x n }, and encodes it to an in- ternal representation h, and then decodes h to a new sequence˜xsequence˜ sequence˜x = {˜x{˜x 1 , ˜ x 2 , ..., ˜ x n } for the recon- struction of the input. We extract the hidden state h as the semantic representation. The encoder E θ is trained to reduce the reconstruction loss, whose loss function is defined as follows:</p><formula xml:id="formula_0">J 1 (θ) = − log P (˜ x|x; θ)<label>(1)</label></formula><p>where θ refers to the parameters of the encoder E θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decoder</head><p>Similar to the encoder, our decoder D φ is also a LSTM-based auto-encoder. However, as there is no target text provided in the testing stage, we pro- pose the customized implementation, which is il- lustrated in Section 2.5. Here in the introduction of the decoder, we do not provide the testing de- tails. Similarly, we name the encoder and decoder of the auto-encoder "target-encoder" and "target- decoder". The target-encoder receives the target y = {y 1 , y 2 , ..., y n } and encodes it to a utterance- level semantic representation s, and then decodes s to a new sequence to approximate the target text. The loss function is identical to that of the en- coder:</p><formula xml:id="formula_1">J 2 (φ) = − log P (˜ y|y; φ)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Mapping Module</head><p>As our model is constructed for dialogue genera- tion, we design the mapping module to ensure that the generated response is semantically consistent with the source. There are many matching mod- els that can be used to learn such dependency rela- tions ( <ref type="bibr" target="#b7">Hu et al., 2014;</ref><ref type="bibr" target="#b17">Pang et al., 2016;</ref>). For simplicity, we only use a simple feedforward network for implemen- tation. The mapping module M γ transforms the source semantic representation h to a new repre- sentation t. To be specific, we implement a multi- layer perceptron (MLP) g(·) for M γ and train it by minimizing the L2-norm loss J 3 (γ) of the trans- formed representation t and the semantic represen- tation of target response s:</p><formula xml:id="formula_2">t = g(h) J 3 (γ) = 1 2 t − s 2 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training and Testing</head><p>In the testing stage, given an input utterance, the encoder E θ , the decoder D φ , and the matching module M γ work together to produce a dialogue response. The source-encoder first receives the in- put x and encodes it to a semantic representation h of the source utterance. Then, the mapping mod- ule transforms h to t, a target response represen- tation. Finally, t is sent to the target-decoder for response generation.</p><p>In the training stage, besides the auto-encoder loss and the mapping loss, we also use an end-to- end loss J 4 (θ, φ, γ):</p><formula xml:id="formula_3">J 4 (θ, φ, γ) = − log P (y|x; θ, φ, γ)<label>(4)</label></formula><formula xml:id="formula_4">= − T t=1 log P (y t |x, y 1..t−1 ; θ, φ, γ)<label>(5)</label></formula><p>where x is the source input, y is the target re- sponse, and T is the length of response sequence. The model learns to generate˜ygenerate˜ generate˜y to approximate y by minimizing the reconstruction losses J 1 (θ) and J 2 (φ), the mapping loss J 3 (γ), and the end-to-end loss J 4 (θ, φ, γ). The details are illustrated below:</p><formula xml:id="formula_5">J = λ 1 [J 1 (θ) + J 2 (φ)] + λ 2 J 3 (γ) + λ 3 J 4 (θ, φ, γ)<label>(6)</label></formula><p>where J refers to the total loss, and λ 1 , λ 2 , and λ 3 are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>We conduct experiments on a high-quality dia- logue dataset called DailyDialog built by <ref type="bibr" target="#b11">Li et al. (2017b)</ref>. The dialogues in the dataset reflect our daily communication and cover various topics about our daily life. We split the dataset into three parts with 36.3K pairs for training, 11.1K pairs for validation, and 11.1K pairs for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Details</head><p>For dialogue generation, we set the maximum length to 15 words for each generated sentence. Based on the performance on the validation set, we set the hidden size to 512, embedding size to 64 and vocabulary size to 40K for baseline mod- els and the proposed model. The parameters are updated by the Adam algorithm ( <ref type="bibr" target="#b8">Kingma and Ba, 2014</ref>) and initialized by sampling from the uni- form distribution ([−0.1, 0.1]). The initial learn- ing rate is 0.002 and the model is trained in mini- batches with a batch size of 256. λ 1 and λ 3 are set to 1 and λ 2 is set to 0.01 in Equation (6). It is important to note that for a fair comparison, we re- implement the baseline models with the best set- tings on the validation set. After fixing the hyper- parameters, we combine the training and valida- tion sets together as a larger training set to produce the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We use BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>), to com- pare the performance of different models, and use the widely-used BLEU-4 as our main BLEU score. The results are shown in <ref type="table">Table 1</ref>. The proposed AEM model significantly outperforms the Seq2Seq model. It demonstrates the effec- tiveness of utterance-level dependency on improv- ing the quality of generated text. Furthermore, we find that the utterance-level dependency also benefits the learning of word-level dependency.</p><p>The improvement from the AEM model to the AEM+Attention model 2 is 0.68 BLEU-4 point. It is much more obvious than the improvement from the Seq2Seq model to the Seq2Seq+Attention, which is 0.29 BLEU-4 point. We also report the diversity of the generated re- sponses by calculating the number of distinct un- igrams, bigrams, and trigrams. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>   than that of the Seq2Seq model. Also, it should be noticed that the attention mechanism performs almost the same compared to the AEM model (31.2K vs. 34.6K in terms of Dist-3), which indi- cates that the utterance-level dependency and the word-level dependency are both indispensable for dialogue generation. Therefore, by combining the two dependencies together, the AEM+Attention model achieves the best results. Such improve- ments are expected. With the increase of the rel- evance of the generated text, it gets harder for the model to generate repeated responses. In our experimental results, the number of repetitive "I don't know" in the AEM+Attention model is re- duced by 50% compared to the Seq2Seq model. For dialogue generation, human evaluation is more convincing, so we also report human eval- uation results on the test set. We randomly choose 100 utterances in daily communication style for the human evaluation, each of which is sent to different models to generate responses. The re- sults are distributed to the annotators who have no knowledge about which model the sentence is from. All annotators have linguistic background. They are asked to score the generated responses in terms of fluency and coherence. Fluency rep-  resents whether each sentence is in correct gram- mar. Coherence evaluates whether the generated response is relevant to the input. The score ranges from 1 to 10 (1 is very bad and 10 is very good). To evaluate the overall performance, we use the geometric mean of fluency and coherence as the final evaluation metric. <ref type="table" target="#tab_2">Table 3</ref> shows the results of human evaluation. The inter-annotator agreement is satisfactory con- sidering the difficulty of human evaluation. The Pearson's correlation coefficient is 0.69 on co- herence and 0.57 on fluency, with p &lt; 0.0001. First, it is clear that the AEM model outperforms the Seq2Seq model with a large margin, which proves the effectiveness of the AEM model on generating high quality responses. Second, it is interesting to note that with the attention mech- anism, the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model. It suggests that the utterance-level dependency greatly benefits the learning of word- level dependency. Therefore, it is expected that the AEM+Attention model achieves the best G-score. <ref type="table" target="#tab_3">Table 4</ref> shows the examples generated by the AEM model and the Seq2Seq model. For easy questions (ex. 4 and ex. 5), they both perform well. For hard questions (ex. 1 and ex. 2), the pro- posed model obviously outperforms the Seq2Seq model. It shows that the utterance-level depen- dency learned by the proposed model is useful for handling complex inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Error Analysis</head><p>Although our model achieves the best perfor- mance, there are still several failure cases. We find that the model performs badly for the inputs with unseen words. For instance, given "Bonjour" as the input, it generates "Stay out of here" as the out- put. It shows that the proposed model is sensitive to the unseen utterance representations. Therefore, we would like to explore more approaches to ad- dress this problem in the future work. For exam- ple, the auto-encoders can be replaced by varia- tional auto-encoders to ensure that the distribution of utterance representations is normal, which has a better generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose an Auto-Encoder Match- ing model to learn the utterance-level semantic de- pendency, a critical dependency relation for gener- ating coherent and fluent responses. The model contains two auto-encoders that learn the utter- ance representations in an unsupervised way, and a mapping module that builds the mapping between the input representation and response representa- tion. Experimental results show that the proposed model significantly improves the quality of gener- ated responses according to automatic evaluation and human evaluation, especially in coherence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the Auto-Encoder Matching model. The encoder and decoder are two autoencoders that are responsible for learning the semantic representations. The mapping module is responsible for learning the utterance-level dependency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Input 1 :</head><label>1</label><figDesc>My name is David. What is my name? Seq2Seq: Ours, maria. AEM: Pardon me? It is unreasonable! Input 2: I'm going to kill you! Seq2Seq: I know. I'm a good man. AEM: What?!?!!!! No!! Input 3: I hate you. Seq2Seq: I'm not. AEM: You do? you're a sweet girl. Input 4: How are you? Seq2Seq: Fine. AEM: Fine. I'm good. Input 5: Can you say 'Hello'? Seq2Seq: Yeah, I guess. AEM: Sure, I guess.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. We find that the AEM model achieves significant improvement on the diversity of generated text. The number of unique tri- gram of the AEM model is almost six times more</figDesc><table>Models 
BLEU-1 BLEU-2 BLEU-3 BLEU-4 
Seq2Seq 
12.43 
4.57 
2.69 
1.84 
AEM 
13.55 
4.89 
3.04 
2.16 
Seq2Seq+Attention 13.63 
4.99 
3.05 
2.13 
AEM+Attention 
14.17 
5.69 
3.78 
2.84 

Table 1: BLEU scores for the AEM model and the Seq2Seq model. 

Models 
Dist-1 Dist-2 Dist-3 
Seq2Seq 
0.8K 
2.7K 
5.5K 
AEM 
3.1K 14.8K 31.2K 
Seq2Seq+Attention 2.5K 13.6K 34.6K 
AEM+Attention 
3.3K 23.2K 53.9K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Diversity scores for the AEM model and the Seq2Seq model. Dist-1, Dist-2, and Dist-3 are respectively 
the number of distinct unigrams, bigrams, and trigrams in the generated text. 

Models 
Fluency Coherence G-Score 
Seq2Seq 
6.97 
3.51 
4.95 
AEM 
8.11 
4.18 
5.82 
Seq2Seq+Attention 5.11 
3.30 
4.10 
AEM+Attention 
7.92 
4.97 
6.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Human evaluation results of the AEM model 
and the Seq2Seq model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Examples generated by the AEM model and the Seq2Seq model.</figDesc><table></table></figure>

			<note place="foot" n="1"> The code is available at https://github.com/ lancopku/AMM</note>

			<note place="foot" n="2"> With the additional attention mechanism, the outputs of attention-based decoder and our decoder are combined together to predict the probability of response words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natu-ral Science Foundation of China (No. 61673028). We thank all reviewers for providing the construc-tive suggestions. Xu Sun is the corresponding au-thor of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frames: a corpus for adding memory to goal-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emery</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-15" />
			<biblScope unit="page" from="207" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autoencoders, unsupervised learning, and deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Unsupervised and Transfer Learning-Workshop held at ICML 2011</title>
		<meeting><address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Identifying high-quality chinese news comments based on multi-target text matching model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1808.07191</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016</title>
		<meeting>the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016<address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-24" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards an open-domain conversational system fully based on natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichiro</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyomi</forename><surname>Meguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiaki</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomi</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiro</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="928" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LSTM can solve hard long time lag problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="473" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="2157" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-11-27" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="986" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<idno>abs/1805.03989</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Autoencoder as assistant supervisor: Improving text representation for chinese social media text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1805.04869</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag-of-words as target for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1805.04871</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coherent dialogue with attention-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3252" to="3258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2793" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Philadelphia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002-07-06" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename><surname>Courville ; California</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3288" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dialogue generation with GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-813" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1506.05869</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diversity-promoting gan: A crossentropy based generative adversarial network for diversified text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end offline goal-oriented dialog policy learning via policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
		<idno>abs/1712.02838</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
