<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-Sizing Neural Networks: With Applications to n-gram Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Murray</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Auto-Sizing Neural Networks: With Applications to n-gram Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through ∞,1 and 2,1 regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks have proven to be highly ef- fective at many tasks in natural language. For example, neural language models and joint lan- guage/translation models improve machine trans- lation quality significantly ( <ref type="bibr" target="#b17">Vaswani et al., 2013;</ref><ref type="bibr" target="#b3">Devlin et al., 2014</ref>). However, neural networks can be complicated to design and train well. Many de- cisions need to be made, and performance can be highly dependent on making them correctly. Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments.</p><p>In this paper, we focus on the choice of the sizes of hidden layers. We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that en- courages units to deactivate if not needed, so that they can be removed from the network. Thus, af- ter training with more units than necessary, a net- work is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use.</p><p>Using a neural n-gram language model ( <ref type="bibr" target="#b2">Bengio et al., 2003)</ref>, we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity. The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes. In addition, we show that incorporating these mod- els into a machine translation decoder still results in large BLEU point improvements. The result is that fewer experiments are needed to obtain mod- els that perform well and are correctly sized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Language models are often used in natural lan- guage processing tasks involving generation of text. For instance, in machine translation, the lan- guage model helps to output fluent translations, and in speech recognition, the language model helps to disambiguate among possible utterances.</p><p>Current language models are usually n-gram models, which look at the previous (n − 1) words to predict the nth word in a sequence, based on (smoothed) counts of n-grams collected from training data. These models are simple but very effective in improving the performance of natural language systems.</p><p>However, n-gram models suffer from some lim- itations, such as data sparsity and memory usage. As an alternative, researchers have begun explor- ing the use of neural networks for language mod- eling. For modeling n-grams, the most common approach is the feedforward network of <ref type="bibr" target="#b2">Bengio et al. (2003)</ref>, shown in <ref type="figure">Figure 1</ref>.</p><p>Each node represents a unit or "neuron," which has a real valued activation. The units are orga- nized into real-vector valued layers. The activa- tions at each layer are computed as follows. (We assume n = 3; the generalization is easy.) The two preceding words, w 1 , w 2 , are mapped into lower- dimensional word embeddings,</p><formula xml:id="formula_0">x 1 = A :w 1 x 2 = A :w 2</formula><p>then passed through two hidden layers,</p><formula xml:id="formula_1">y = f (B 1 x 1 + B 2 x 2 + b) z = f (Cy + c)</formula><p>where f is an elementwise nonlinear activation (or transfer) function. Commonly used activation functions are the hyperbolic tangent, logistic func- tion, and rectified linear units, to name a few. Fi- nally, the result is mapped via a softmax to an out- put probability distribution,</p><formula xml:id="formula_2">P (w n | w 1 · · · w n−1 ) ∝ exp([Dz + d] wn ).</formula><p>The parameters of the model are A, B 1 , B 2 , b, C, c, D, and d, which are learned by minimizing the negative log-likelihood of the the training data using stochastic gradient descent (also known as backpropagation) or variants. <ref type="bibr" target="#b17">Vaswani et al. (2013)</ref> showed that this model, with some improvements, can be used effectively during decoding in machine translation. In this pa- per, we use and extend their implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Our method is focused on the challenge of choos- ing the number of units in the hidden layers of a feed-forward neural network. The networks used for different tasks require different numbers of units, and the layers in a single network also re- quire different numbers of units. Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfit- ting. It can also slow down computations with the network, which can be a major concern for many applications such as integrating neural language models into a machine translation decoder.</p><p>Our method starts out with a large number of units in each layer and then jointly trains the net- work while pruning out individual units when pos- sible. The goal is to end up with a trained network words <ref type="figure">Figure 1</ref>: Neural probabilistic language model ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>), adapted from <ref type="bibr" target="#b17">Vaswani et al. (2013)</ref>.</p><formula xml:id="formula_3">w 1 , w 2 input embeddings x 1 , x 2 hidden y hidden z output P (w 3 | w 1 w 2 ) D C B 1 B 2 A</formula><p>that also has the optimal number of units in each layer.</p><p>We do this by adding a regularizer to the ob- jective function. For simplicity, consider a single layer without bias, y = f (Wx). Let L(W) be the negative log-likelihood of the model. Instead of minimizing L(W) alone, we want to mini- mize L(W) + λR(W), where R(W) is a con- vex regularizer. The 1 norm, R(W) = W 1 = i,j |W ij |, is a common choice for pushing pa- rameters to zero, which can be useful for prevent- ing overfitting and reducing model size. However, we are interested not only in reducing the number of parameters but the number of units. To do this, we need a different regularizer.</p><p>We assume activation functions that satisfy f (0) = 0, such as the hyperbolic tangent or rec- tified linear unit (f (x) = max{0, x}). Then, if we push the incoming weights of a unit y i to zero, that is, W ij = 0 for all j (as well as the bias, if any: b i = 0), then y i = f (0) = 0 is independent of the previous layers and contributes nothing to subse- quent layers. So the unit can be removed without affecting the network at all. Therefore, we need a regularizer that pushes all the incoming connec- tion weights to a unit together towards zero.</p><p>Here, we experiment with two, the 2,1 norm and the ∞,1 norm. <ref type="bibr">1</ref> The 2,1 norm on a ma- trix W is</p><formula xml:id="formula_4">R(W) = i W i: 2 = i   j W 2 ij   1 2</formula><p>. <ref type="formula">(1)</ref> (If there are biases b i , they should be included as well.) This puts equal pressure on each row, but within each row, the larger values contribute more, and therefore there is more pressure on larger val- ues towards zero. The ∞,1 norm is</p><formula xml:id="formula_5">R(W) = i W i: ∞ = i max j |W ij |. (2)</formula><p>Again, this puts equal pressure on each row, but within each row, only the maximum value (or val- ues) matter, and therefore the pressure towards zero is entirely on the maximum value(s). <ref type="figure" target="#fig_0">Figure 2</ref> visualizes the sparsity-inducing behav- ior of the two regularizers on a single row. Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimization</head><p>However, this also means that sparsity-inducing regularizers are not differentiable at zero, mak- ing gradient-based optimization methods trickier to apply. The methods we use are discussed in detail elsewhere ( <ref type="bibr" target="#b5">Duchi et al., 2008;</ref><ref type="bibr" target="#b4">Duchi and Singer, 2009)</ref>; in this section, we include a short description of these methods for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proximal gradient method</head><p>Most work on learning with regularizers, includ- ing this work, can be thought of as instances of the proximal gradient method ( <ref type="bibr" target="#b12">Parikh and Boyd, 2014)</ref>. Our objective function can be split into two parts, a convex and differentiable part (L) and a convex but non-differentiable part (λR). In prox- imal gradient descent, we alternate between im- proving L alone and λR alone. Let u be the pa- rameter values from the previous iteration. We compute new parameter values w using:</p><formula xml:id="formula_6">v ← u − ηL(u)<label>(3)</label></formula><p>w ← arg max</p><formula xml:id="formula_7">w 1 2η w − v 2 + λR(w)<label>(4)</label></formula><p>and repeat until convergence. The first update is just a standard gradient descent update on L; the second is known as the proximal operator for λR and in many cases has a closed-form solution. In the rest of this section, we provide some justifica- tion for this method, and in Sections 4.2 and 4.3 we show how to compute the proximal operator for the 2 and ∞ norms. We can think of the gradient descent update (3) on L as follows. Approximate L around u by the tangent plane,</p><formula xml:id="formula_8">¯ L(v) = L(u) + L(u)(v − u)<label>(5)</label></formula><p>and move v to minimize ¯ L, but don't move it too far from u; that is, minimize</p><formula xml:id="formula_9">F (v) = 1 2η v − u 2 + ¯ L(v).</formula><p>Setting partial derivatives to zero, we get</p><formula xml:id="formula_10">∂F ∂v = 1 η (v − u) + L(u) = 0 v = u − ηL(u).</formula><p>By a similar strategy, we can derive the second step (4). Again we want to move w to minimize the objective function, but don't want to move it too far from u; that is, we want to minimize:</p><formula xml:id="formula_11">G(w) = 1 2η w − u 2 + ¯ L(w) + λR(w).</formula><p>Note that we have not approximated R by a tan- gent plane. We can simplify this by substituting in (3). The first term becomes</p><formula xml:id="formula_12">1 2η w − u 2 = 1 2η w − v − ηL(u) 2 = 1 2η w − v 2 − L(u)(w − v) + η 2 L(u) 2</formula><p>and the second term becomes</p><formula xml:id="formula_13">¯ L(w) = L(u) + L(u)(w − u) = L(u) + L(u)(w − v − ηL(u)).</formula><p>The L(u)(w − v) terms cancel out, and we can ignore terms not involving w, giving</p><formula xml:id="formula_14">G(w) = 1 2η w − v 2 + λR(w) + const.</formula><p>which is minimized by the update (4). Thus, we have split the optimization step into two easier steps: first, do the update for L (3), then do the update for λR (4). The latter can often be done exactly (without approximating R by a tangent plane). We show next how to do this for the 2 and ∞ norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">2 and 2,1 regularization</head><p>Since the 2,1 norm on matrices (1) is separable into the 2 norm of each row, we can treat each row separately. Thus, for simplicity, assume that we have a single row and want to minimize</p><formula xml:id="formula_15">G(w) = 1 2η w − v 2 + λw + const.</formula><p>The minimum is either at w = 0 (the tip of the cone) or where the partial derivatives are zero ( <ref type="figure" target="#fig_1">Figure 3</ref>):</p><formula xml:id="formula_16">∂G ∂w = 1 η (w − v) + λ w w = 0.</formula><p>Clearly, w and v must have the same direction and differ only in magnitude, that is, w = α v v . Sub- stituting this into the above equation, we get the solution</p><formula xml:id="formula_17">α = v − ηλ.</formula><p>Therefore the update is</p><formula xml:id="formula_18">w = α v v α = max(0, v − ηλ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">∞ and ∞,1 regularization</head><p>As above, since the ∞,1 norm on matrices (2) is separable into the ∞ norm of each row, we can treat each row separately; thus, we want to mini- mize If we pre-sort the |x j | in nonincreasing order, it's easy to see how to compute this: for ρ = 1, . . . , n, see if there is a value ξ ≤ x ρ such that decreasing all the x 1 , . . . , x ρ to ξ amounts to a to- tal decrease of ηλ. The largest ρ for which this is possible gives the correct solution.</p><formula xml:id="formula_19">G(w) = 1 2η w − v 2 + λ max j |x j | + const.</formula><p>But this situation seems similar to another op- timization problem, projection onto the 1 -ball, which <ref type="bibr" target="#b5">Duchi et al. (2008)</ref> solve in linear time without pre-sorting. In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other <ref type="bibr" target="#b4">(Duchi and Singer, 2009;</ref><ref type="bibr" target="#b0">Bach et al., 2012</ref>). Intuitively, the 1 projection of v is exactly what is cut out by the ∞ proximal operator, and vice versa <ref type="figure">(Fig- ure 4)</ref>.</p><p>Duchi et al.'s algorithm modified for the present problem is shown as Algorithm 1. It partitions the x j about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8). If so, it recursively searches the right side; if not, the left side. At the conclusion of the algorithm, ρ is set to the largest value that passes the test (line 13), and finally the new x j are computed (line 16) -the only difference from Duchi et al.'s algorithm.</p><p>This algorithm is asymptotically faster than that of <ref type="bibr" target="#b13">Quattoni et al. (2009)</ref> return i − 1</p><note type="other">. They reformulate ∞,1 regularization as a constrained optimization prob- lem (in which the ∞,1 norm is bounded by µ) and provide a solution in O(n log n) time. The method shown here is simpler and faster because it can work on each row separately. Algorithm 1 Linear-time algorithm for the proxi- mal operator of the ∞ norm. 1</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model using the open-source NPLM toolkit released by <ref type="bibr" target="#b17">Vaswani et al. (2013)</ref>, extending it to use the additional regularizers as described in this paper. <ref type="bibr">2</ref> We use a vocabulary size of 100k and word embeddings with 50 dimen- sions. We use two hidden layers of rectified linear units ( <ref type="bibr" target="#b10">Nair and Hinton, 2010</ref>).</p><p>We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5. After tok- enization, Europarl has 56M tokens and Gigaword AFP has 870M tokens. For both corpora, we hold out a validation set of 5,000 tokens. We train each model for 10 iterations over the training data.</p><p>Our experiments break down into three parts. First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings. Second, we take a closer look at how the model evolves through the training pro- cess. Finally, we explore the downstream impact of our method on a statistical phrase-based ma- chine translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluating perplexity and network size</head><p>We first look at the impact that the ∞,1 regular- izer has on the perplexity of our validation set. The main results are shown in <ref type="table">Table 1</ref>. For λ ≤ 0.01, the regularizer seems to have little impact: no hid- den units are pruned, and perplexity is also not af- fected. For λ = 1, on the other hand, most hidden units are pruned -apparently too many, since per- plexity is worse. But for λ = 0.1, we see that we are able to prune out many hidden units: up to half of the first layer, with little impact on perplexity. We found this to be consistent across all our exper- iments, varying n-gram size, initial hidden layer size, and vocabulary size. <ref type="table" target="#tab_2">Table 2</ref> shows the same information for 5-gram models trained on the larger Gigaword AFP cor- pus. These numbers look very similar to those on Europarl: again λ = 0.1 works best, and, counter to expectation, even the final number of units is similar. <ref type="table" target="#tab_3">Table 3</ref> shows the result of varying the vocabu- lary size: again λ = 0.1 works best, and, although it is not shown in the table, we also found that the final number of units did not depend strongly on the vocabulary size. <ref type="table" target="#tab_4">Table 4</ref> shows results using the 2,1 norm (Eu- roparl corpus, 5-grams, 100k vocabulary). Since this is a different regularizer, there isn't any rea- son to expect that λ behaves the same way, and indeed, a smaller value of λ seems to work best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">A closer look at training</head><p>We also studied the evolution of the network over the training process to gain some insights into how the method works. The first question we want to 2-gram <ref type="table" target="#tab_2">3-gram  5-gram  λ  layer 1 layer 2 ppl layer 1 layer 2 ppl layer 1 layer 2 ppl  0  1,000  50  103  1,000  50  66  1,000  50  55  0.001  1,000  50  104  1,000  50  66  1,000  50  54  0.01  1,000  50  104  1,000  50  63  1,000  50  55  0.1  499  47  105  652  49  66  784  50  55  1.0  50  24  111  128  32  76  144  29  68   Table 1</ref>: Comparison of ∞,1 regularization on 2-gram, 3-gram, and 5-gram neural language models. The network initially started with 1,000 units in the first hidden layer and 50 in the second. A regularization strength of λ = 0.1 consistently is able to prune units while maintaining perplexity, even though the final number of units varies considerably across models. The vocabulary size is 100k.      answer is whether the method is simply remov- ing units, or converging on an optimal number of units. <ref type="figure" target="#fig_3">Figure 5</ref> suggests that it is a little of both: if we start with too many units (900 or 1000), the method converges to the same number regardless of how many extra units there were initially. But if we start with a smaller number of units, the method still prunes away about 50 units.</p><p>Next, we look at the behavior over time of dif- ferent regularization strengths λ. We found that not only does λ = 1 prune out too many units, it does so at the very first iteration <ref type="figure" target="#fig_4">(Figure 6, above)</ref>, perhaps prematurely. By contrast, the λ = 0.1 run prunes out units gradually. By plotting these curves together with perplexity ( <ref type="figure" target="#fig_4">Figure 6, below)</ref>, we can see that the λ = 0.1 run is fitting the model and pruning it at the same time, which seems preferable to fitting without any pruning (λ = neural LM λ none Europarl Gigaword AFP 0 (none) 23. <ref type="bibr">2</ref> 24.7 (+1.5) 25.2 (+2.0) 0.1 24.6 (+1.4) 24.9 (+1.7) <ref type="table">Table 5</ref>: The improvements in translation accuracy due to the neural LM (shown in parentheses) are affected only slightly by ∞,1 regularization. For the Europarl LM, there is no statistically signifi- cant difference, and for the Gigaword AFP LM, a statistically significant but small decrease of −0.3.</p><p>0.01) or pruning first and then fitting (λ = 1).</p><p>We can also visualize the weight matrix itself over time <ref type="figure" target="#fig_5">(Figure 7</ref>), for λ = 0.1. It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluating on machine translation</head><p>We also looked at the impact of our method on statistical machine translation systems. We used the Moses toolkit ( <ref type="bibr" target="#b6">Koehn et al., 2007)</ref> to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext. We augmented this system with neu- ral LMs trained on the Europarl data and the Gi- gaword AFP data. Based on the results from the perplexity experiments, we looked at models both built with a λ = 0.1 regularizer, and without regu- larization (λ = 0). We built our system using the newscommentary dataset v8. We tuned our model using newstest13 and evaluated using newstest14. After standard cleaning and tokenization, there were 155k paral- lel sentences in the newscommentary dataset, and 3,000 sentences each for the tuning and test sets. <ref type="table">Table 5</ref> shows that the addition of a neural LM helps substantially over the baseline, with im- provements of up to 2 BLEU. Using the Europarl model, the BLEU scores obtained without and with regularization were not significantly differ- ent (p ≥ 0.05), consistent with the negligible per- plexity difference between these models. On the Gigaword AFP model, regularization did decrease the BLEU score by 0.3, consistent with the small perplexity increase of the regularized model. The decrease is statistically significant, but small com- pared with the overall benefit of adding a neu- ral LM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Researchers have been exploring the use of neu- ral networks for language modeling for a long time. <ref type="bibr" target="#b14">Schmidhuber and Heil (1996)</ref> proposed a character n-gram model using neural networks which they used for text compression. <ref type="bibr" target="#b18">Xu and Rudnicky (2000)</ref> proposed a word-based proba- bility model using a softmax output layer trained using cross-entropy, but only for bigrams. <ref type="bibr" target="#b2">Bengio et al. (2003)</ref> defined a probabilistic word n-gram model and demonstrated improvements over con- ventional smoothed language models. <ref type="bibr" target="#b9">Mnih and Teh (2012)</ref> sped up training of log-bilinear lan- guage models through the use of noise-contrastive estimation (NCE). <ref type="bibr" target="#b17">Vaswani et al. (2013)</ref> also used NCE to train the architecture of <ref type="bibr" target="#b2">Bengio et al. (2003)</ref>, and were able to integrate a large- vocabulary language model directly into a ma- chine translation decoder. <ref type="bibr" target="#b1">Baltescu et al. (2014)</ref> describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features. Beyond feed-forward neural network lan- guage models, researchers have explored using more complicated neural network architectures.</p><p>RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles <ref type="bibr" target="#b8">(Mikolov et al., 2011</ref>). <ref type="bibr" target="#b16">Sundermeyer et al. (2015)</ref> use the long-short term mem- ory (LSTM) neural architecture to show a per- plexity improvement over the RNNLM toolkit. In future work, we plan on exploring how our method could improve these more complicated neural models as well.</p><p>Automatically limiting the size of neural net- works is an old idea. The "Optimal Brain Dam- age" (OBD) technique ( <ref type="bibr" target="#b7">LeCun et al., 1989</ref>) com- putes a saliency based on the second derivative of the objective function with respect to each parame- ter. The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned. The pruning process is separate from the training pro- cess, whereas regularization performs training and pruning simultaneously. Regularization in neural networks is also an old idea; for example, Now- land and Hinton (1992) mention both 2 2 and 0 regularization. Our method develops on this idea by using a mixed norm to prune units, rather than parameters.</p><p>Srivastava et al. introduce a method called dropout in which units are directly deactivated at random during training ( <ref type="bibr" target="#b15">Srivastava et al., 2014)</ref>, which induces sparsity in the hidden unit activa- tions. However, at the end of training, all units are reactivated, as the goal of dropout is to re- duce overfitting, not to reduce network size. Thus, dropout and our method seem to be complemen- tary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a method for auto-sizing a neu- ral network during training by removing units us- ing a ∞,1 regularizer. This regularizer drives a unit's input weights as a group down to zero, al- lowing the unit to be pruned. We can thus prune units out of our network during training with min- imal impact to held-out perplexity or downstream performance of a machine translation system.</p><p>Our results showed empirically that the choice of a regularization coefficient of 0.1 was robust to initial configuration parameters of initial network size, vocabulary size, n-gram order, and training corpus. Furthermore, imposing a single regularizer on the objective function can tune all of the hidden layers of a network with one setting. This reduces the need to conduct expensive, multi-dimensional grid searches in order to determine optimal sizes. We have demonstrated the power and efficacy of this method on a feed-forward neural network for language modeling though experiments on per- plexity and machine translation. However, this method is general enough that it should be applica- ble to other domains, both inside natural language processing and outside. As neural models become more pervasive in natural language processing, the ability to auto-size networks for fast experimen- tation and quick exploration will become increas- ingly important.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The (unsquared) 2 norm and ∞ norm both have sharp tips at the origin that encourage sparsity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of the two possible cases for the 2 gradient update. Point v is drawn with a hollow dot, and point w is drawn with a solid dot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number of units in first hidden layer over time, with various starting sizes (λ = 0.1). If we start with too many units, we end up with the same number, although if we start with a smaller number of units, a few are still pruned away.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Above: Number of units in first hidden layer over time, for various regularization strengths λ. A regularization strength of ≤ 0.01 does not zero out any rows, while a strength of 1 zeros out rows right away. Below: Perplexity over time. The runs with λ ≤ 0.1 have very similar learning curves, whereas λ = 1 is worse from the beginning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Evolution of the first hidden layer weight matrix after 1, 5, and 10 iterations (with rows sorted by ∞ norm). A nonlinear color scale is used to show small values more clearly. The four vertical blocks correspond to the four context words. The light bar at the bottom is the rows that are close to zero, and the white bar is the rows that are exactly zero.</figDesc><graphic url="image-1.png" coords="8,74.27,62.81,149.67,112.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results from training a 5-gram neural LM 
on the AFP portion of the Gigaword dataset. As 
with the smaller Europarl corpus (Table 1), a reg-
ularization strength of λ = 0.1 is able to prune 
units while maintaining perplexity. 

vocabulary size 
λ 
10k 25k 50k 100k 
0 
47 
60 
54 
55 
0.001 47 
54 
54 
54 
0.01 
47 
58 
55 
55 
0.1 
48 
62 
55 
55 
1.0 
61 
64 
65 
68 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>A regularization strength of λ = 0.1 is 
best across different vocabulary sizes. 

λ 
layer 1 layer 2 perplexity 
0 
1,000 
50 
100 
0.0001 
1,000 
50 
54 
0.001 
1,000 
50 
55 
0.01 
616 
50 
57 
0.1 
199 
32 
65 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results using 2,1 regularization. 

</table></figure>

			<note place="foot" n="1"> In the notation p,q, the subscript p corresponds to the norm over each group of parameters, and q corresponds to the norm over the group norms. Contrary to more common usage, in this paper, the groups are rows, not columns.</note>

			<note place="foot" n="2"> These extensions have been contributed to the NPLM project.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Tomer Levinboim, Anto-nios Anastasopoulos, and Ashish Vaswani for their helpful discussions, as well as the reviewers for their assistance and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">OxLM: A neural language modelling framework for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient online and batch learning using forward backward splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2899" to="2934" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient projections onto the 1-ball for learning in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL, Interactive Poster and Demonstration Sessions</title>
		<meeting>ACL, Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RNNLMrecurrent neural network language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukar</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rectified linear units improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simplifying neural networks by soft weight-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="239" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient projection for l 1,∞ regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequential neural text compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="142" to="146" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From feedforward to recurrent LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Audio, Speech, and Language</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can artificial neural networks learn language models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Statistical Language Processing</title>
		<meeting>International Conference on Statistical Language essing</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
