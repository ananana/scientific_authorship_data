<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NaturalLI: Natural Logic Inference for Common Sense Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
							<email>angeli@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<address>
									<postCode>94305, 94305</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<address>
									<postCode>94305, 94305</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NaturalLI: Natural Logic Inference for Common Sense Reasoning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="534" to="545"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Common-sense reasoning is important for AI applications, both in NLP and many vision and robotics tasks. We propose NaturalLI: a Natural Logic inference system for inferring common sense facts-for instance, that cats have tails or tomatoes are round-from a very large database of known facts. In addition to being able to provide strictly valid derivations, the system is also able to produce derivations which are only likely valid, accompanied by an associated confidence. We both show that our system is able to capture strict Natural Logic inferences on the Fra-CaS test suite, and demonstrate its ability to predict common sense facts with 49% recall and 91% precision.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We approach the task of database completion: given a database of true facts, we would like to predict whether an unseen fact is true and should belong in the database. This is intuitively cast as an inference problem from a collection of candi- date premises to the truth of the query. For exam- ple, we would like to infer that no carnivores eat animals is false given a database containing the cat ate a mouse (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>These inferences are difficult to capture in a principled way while maintaining high recall, particularly for large scale open-domain tasks. Learned inference rules are difficult to general- ize to arbitrary relations, and standard IR methods easily miss small but semantically important lex- ical differences. Furthermore, many methods re- quire explicitly modeling either the database, the query, or both in a formal meaning representation (e.g., Freebase tuples).</p><p>Although projects like the Abstract Meaning Representation ( <ref type="bibr" target="#b1">Banarescu et al., 2013</ref>  The path to the boxed premise the cat ate a mouse disproves the query no carnivores eat animals, as it passes through the negation relation (). This path is one of many candidates taken; the premise is one of many known facts in the database. The edge labels denote Natural Logic inference steps.</p><p>headway in providing broad-coverage meaning representations, it remains appealing to use hu- man language as the vessel for inference. Fur- thermore, OpenIE and similar projects have been very successful at collecting databases of natural language snippets from an ever-increasing corpus of unstructured text. These factors motivate our use of Natural Logic -a proof system built on the syntax of human language -for broad coverage database completion.</p><p>Prior work on Natural Logic has focused on in- ferences from a single relevant premise, making use of only formally valid inferences. We improve upon computational Natural Logic in three ways: (i) our approach operates over a very large set of candidate premises simultaneously; (ii) we do not require explicit alignment between a premise and the query; and (iii) we allow imprecise inferences at an associated cost learned from data.</p><p>Our approach casts inference as a single uni- fied search problem from a query to any valid supporting premise. Each transition along the search denotes a (reverse) inference step in Natu- ral Logic, and incurs a cost reflecting the system's confidence in the validity of that step. This ap- proach offers two contributions over prior work in database completion: (i) it allows for unstructured text as the input database without any assump- tions about the schema or domain of the text, and (ii) it proposes Natural Logic for inference, rather than translating to a formal logic syntax. More- over, the entire pipeline is implemented in a single elegant search framework, which scales easily to large databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MacCartney's Natural Logic</head><p>Natural Logic aims to capture common logical in- ferences by appealing directly to the structure of language, as opposed to running deduction on an abstract logical form. The logic builds upon tra- ditional rather than first-order logic: to a first ap- proximation, Natural Logic can be seen as an en- hanced version of Aristotle's syllogistic system <ref type="bibr" target="#b39">(van Benthem, 2008)</ref>. A working understanding of the logic as syllogistic reasoning is sufficient for understanding the later contributions of the paper. While some inferences of first-order logic are not captured by Natural Logic, it nonetheless allows for a wide range of intuitive inferences in a com- putationally efficient and conceptually clean way.</p><p>We build upon the variant of the logic intro- duced by the NatLog system <ref type="bibr" target="#b21">(MacCartney and Manning, 2007;</ref><ref type="bibr" target="#b39">2008;</ref><ref type="bibr" target="#b24">2009)</ref>, based on earlier the- oretical work on Natural <ref type="bibr">Logic and Monotonicity Calculus (van Benthem, 1986;</ref><ref type="bibr" target="#b37">Valencia, 1991)</ref>. Later work formalizes many aspects of the logic <ref type="bibr" target="#b16">(Icard, 2012;</ref><ref type="bibr" target="#b9">Djalali, 2013)</ref>; we adopt the formal semantics of <ref type="bibr" target="#b15">Icard and Moss (2014)</ref>, along with much of their notation.</p><p>At a high level, Natural Logic proofs operate by mutating spans of text to ensure that the mutated sentence follows from the original -each step is much like a syllogistic inference. We construct a complete proof system in three parts: we define MacCartney's atomic relations between lexical en- tries (Section 2.1), the effect these lexical muta- tions have on the validity of the sentence (Sec- tion 2.2), and a practical approach for executing these proofs. We review MacCartney's alignment- based approach in Section 2.3, and show that we can generalize and simplify this system in Sec- tion 3.</p><formula xml:id="formula_0">D ϕ ≡ ψ (equivalence) D ϕ ψ (forward entail.) D ϕ ψ (reverse entail.) D ϕ ψ (negation) D ϕ ψ (alternation) D ϕ ψ (cover)</formula><p>Figure 2: The model-theoretic interpretation of the MacCartney relations. The figure shows the re- lation between the denotation of ϕ (dark) and ψ (light). The universe is denoted by D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lexical Relations</head><p>MacCartney and Manning (2007) introduce seven set-theoretic relations between the denotations of any two lexical items. The denotation of a lexical item is the set of objects in the domain of discourse D to which that lexical item refers. For instance, the denotation of cat would be the set of all cats. Two denotations can then be compared in terms of set algebra: if we define the set of cats to be ϕ and the set of animals to be ψ, we can state that ϕ ⊆ ψ.</p><p>The six informative relations are summarized in <ref type="figure">Figure 2</ref>; a seventh relation (#) corresponds to to the completely uninformative relation. For in- stance, the example search path in <ref type="figure" target="#fig_0">Figure 1</ref> makes use of the following relations:</p><p>No x y The x y cat carnivore animal ≡ a animal animal mouse</p><p>Denotations are not required to be in the space of predicates (e.g., cat, animal).</p><p>In the first example, the denotations of No and The are in the space of operators p → (p → t): functions from predicates p to truth values t. The relation becomes the conjunction of two claims: ∀x∀y ¬ (no x y ∧ the x y) and ∀x∀y (no x y ∨ the x y). This is analogous to the construction of the set-theoretic definition of in <ref type="figure">Figure 2</ref>: ϕ ∩ ψ = ∅ and ϕ ∪ ψ = D (see <ref type="bibr" target="#b15">Icard and Moss (2014)</ref>).</p><p>Examples of the last two relations ( and ) and the complete independence relation (#) include: cat dog animal nonhuman cat # friendly</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Monotonicity and Polarity</head><p>The previous section details the relation between lexical items; however, we still need a theory for how to "project" the relation induced by a lexical mutation as a relation between the two containing sentences. For example, cat animal, and some cat meows some animal meows, but no cat barks no animal barks. Despite differing by the same lexical relation, the first example describes a valid entailment, while the second does not.</p><p>We appeal to two important concepts: mono- tonicity as a property of arguments to natural lan- guage operators, and polarity as a property of lexi- cal items in a sentence. Much like monotone func- tions in calculus, an <ref type="bibr">[upwards]</ref> monotone operator has an output truth value which is non-decreasing (i.e., material implication) as the input "increases" (i.e., the subset relation). From the example above, some is upwards monotone in its first argument, and no is downwards monotone in its first argu- ment.</p><p>Polarity is a property of lexical items in a sen- tence determined by the operators acting on it. All lexical items have upward polarity by default; up- wards monotone operators preserve polarity, and downwards monotone operators reverse polarity. For example, mice in no cats eat mice has down- ward polarity, whereas mice in no cats don't eat mice has upward polarity (it is in the scope of two downward monotone operators). The relation be- tween two sentences differing by a single lexical relation is then given by the projection function ρ in <ref type="table">Table 1</ref>. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Proof By Alignment</head><p>MacCartney and Manning (2007) approach the in- ference task in the context of inferring whether a single relevant premise entails a query. Their ap- proach first generates an alignment between the premise and the query, and then classifies each aligned segment into one of the lexical relations in <ref type="figure">Figure 2</ref>. Inference reduces to projecting each r ≡ # ρ(r) ≡ # <ref type="table">Table 1</ref>: The projection function ρ, shown for downward polarity contexts only. The input r is the lexical relation between two words in a sen- tence; the projected relation ρ(r) is the relation between the two sentences differing only by that word. Note that ρ is the identity function in up- ward polarity contexts. of these relations according to the projection func- tion ρ <ref type="table">(Table 1)</ref> and iteratively joining two pro- jected relations together to get the final entailment relation. This join relation, denoted as , is given in <ref type="table" target="#tab_1">Table 2</ref>.</p><formula xml:id="formula_1">≡ # ≡ ≡ # # # # # # # ≡ # # # # # # # # # # # # # # #</formula><p>To illustrate, we can consider MacCartney's example inference from Stimpy is a cat to Stimpy is not a poodle.</p><p>An alignment of the two statements would provide three lexical mutations: r 1 := cat → dog, r 2 := · → not, and r 3 := dog → poodle. Each of these are then pro- jected with the projection function ρ, and are joined using the join relation:</p><formula xml:id="formula_2">r 0 ρ(r 1 ) ρ(r 2 ) ρ(r 3 ),</formula><p>where the initial relation r 0 is axiomatically ≡. In MacCartney's work this style of proof is presented as a table. The last column (s i ) is the relation be- tween the premise and the i th step in the proof, and is constructed inductively as s i := s i−1 ρ(r i ):</p><formula xml:id="formula_3">Mutation r i ρ(r i ) s i r 1 cat→dog r 2 · →not r 3 dog→poodle</formula><p>In our example, we would conclude that Stimpy is a cat Stimpy is not a poodle since s 3 is ; therefore the inference is valid. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inference as a Finite State Machine</head><p>We show that the tabular proof formulation from Section 2.3 can be viewed as a finite state machine, and present a novel observation that we can loss- lessly collapse this finite state machine into only three intuitive inference states. These observations allow us to formulate our search problem such that a search path corresponds to an input to (i.e., path through) this collapsed state machine. Taking notation from Section 2.3, we construct a finite state machine over states s ∈ {, , . . . }. A machine in state s i corresponds to relation s i holding between the initial premise and the de- rived fact so far. States therefore correspond to states of logical validity. The start state is ≡. Out- going transitions correspond to inference steps. Each transition is labeled with a projected relation ρ(r) ∈ {, , . . . }, and spans from a source state s to a target s according to the join table. That is,  <ref type="figure" target="#fig_1">Figure 3a</ref> shows the automaton, with trivial edges omitted for clarity.</p><p>Our second contribution is collapsing this au- tomaton into the three meaningful states we use as output: valid (ϕ ⇒ ψ), invalid (ϕ ⇒ ¬ψ), and unknown validity (ϕ ψ). We can cluster states in <ref type="figure" target="#fig_1">Figure 3a</ref> into these three categories. The rela- tions ≡ and correspond to valid inferences; and correspond to invalid inferences; , and # correspond to unknown validity. This cluster- ing mirrors that used by MacCartney for his tex- tual entailment experiments.</p><p>Collapsing the FSA into the form in <ref type="figure" target="#fig_1">Figure 3b</ref> becomes straightforward from observing the reg- ularities in <ref type="figure" target="#fig_1">Figure 3a</ref>. Nodes in the valid cluster transition to invalid nodes always and only on the relations and . Symmetrically, invalid nodes transition to valid nodes always and only on and . A similar pattern holds for the other transitions. A few observations deserve passing remark. First, even though the states and appear meaningful, in fact there is no "escaping" these states to either a valid or invalid inference. Sec- ond, the hierarchy over relations presented in Icard (2012) becomes apparent -in particular, always behaves as negation, whereas its two "weaker" versions ( and ) only behave as negation in cer- tain contexts. Lastly, with probabilistic inference, transitioning to the unknown state can be replaced with staying in the current state at a (potentially arbitrarily large) cost to the confidence of valid- ity. This allows us to make use of only two states: valid and invalid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>537</head><p>Natural Logic allows us to formalize our approach elegantly as a single search problem. Given a query, we search over the space of possible facts for a valid premise in our database. The nodes in our search problem correspond to candidate facts (Section 4.1); the edges are mutations of these facts (Section 4.2); the costs over these edges en- code the confidence that this edge maintains an informative inference (Section 4.5). This mirrors the automaton defined in Section 3, except impor- tantly we are constructing a reversed derivation, and are therefore "traversing" the FSA backwards.</p><p>This approach is efficient over a large database of 270 million entries without making use of ex- plicit queries over the database; nor does the approach make use of any sort of approximate matching against the database, beyond lemmatiz- ing individual lexical items. The motivation in prior work for approximate matches -to improve the recall of candidate premises -is captured ele- gantly by relaxing Natural Logic itself. We show that allowing invalid transitions with appropriate costs generalizes JC distance <ref type="bibr" target="#b18">(Jiang and Conrath, 1997</ref>) -a common thesaurus-based similarity met- ric (Section 4.3). Importantly, however, the entire inference pipeline is done within the framework of weighted lexical transitions in Natural Logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Nodes</head><p>The space of possible nodes in our search is the set of possible partial derivations. To a first ap- proximation, this is a pair (w, s) of a surface form w tagged with word sense and polarity, and an in- ference state s ∈ {valid, invalid} in our collapsed FSA <ref type="figure" target="#fig_1">(Figure 3b</ref>). For example, the search path in <ref type="figure" target="#fig_0">Figure 1</ref>  During search, we assume that the validity states s are reversible -if we know that the cat ate a mouse is true, we can infer that no carnivores eat animals is false. In addition, our search keeps track of some additional information:</p><p>Mutation Index Edges between sentences are most naturally defined to correspond to mutations of individual lexical items. We therefore maintain an index of the next item to mutate at each search state. Importantly, this enforces that each deriva- tion orders mutations left-to-right; this is compu- tationally efficient, at the expense of rare search errors. A similar observation is noted in <ref type="bibr" target="#b24">MacCartney (2009)</ref>, where prematurely collapsing to # oc- casionally misses inferences.</p><p>Polarity Mutating operators can change the po- larity on a span in the fact. Since we do not have the full parse tree at our disposal at search time, we track a small amount of metadata to guess the scope of the mutated operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transitions</head><p>We begin by introducing some terminology. A transition template is a broad class of transitions; for instance WordNet hypernymy. A transition (or transition instance) is a particular instantiation of a transition template. For example, the tran- sition from cat to feline. Lastly, an edge in the search space connects two nodes, which are sep- arated by a single transition instance. For exam- ple, an edge exists between some felines have tails and some cats have tails. Transition [instances] are stored statically in memory, whereas edges are constructed on demand.</p><p>Transition templates provide a means of defin- ing transitions and subsequently edges in our search space using existing lexical resources (e.g., WordNet, distributional similarity, etc.). We can then define a mapping from these templates to Natural Logic lexical relations. This allows us to map every edge in our search graph back to the Natural Logic relation it instantiates. The full table of transition templates is given in Ta- ble 3, along with the Natural Logic relation that instances of the template introduce. We include most relations in WordNet as transitions, and parametrize insertions and deletions by the part of speech of the token being inserted/deleted.</p><p>Once we have an edge defining a lexical mu- tation with an associated Natural Logic relation r, we can construct the corresponding end node (w , s ) such that w is the sentence with the lex- ical mutation applied, and s is the validity state obtained from the FSA in Section 3. For instance, if our edge begins at (w, s), and there exists a tran- sition in the FSA from s r − → s, then we define the end point of the edge to be (w , s ). To illustrate concretely, suppose our search state is: <ref type="table">Table 3</ref>: The edges allowed during inference. Entries with a dagger ( †) are parametrized by their part-of-speech tag, from the restricted list of {noun,adjective,verb,other}. The first column de- scribes the type of the transition. The set-theoretic relation introduced by each relation is given in the second column.</p><note type="other">(some felines have tails, valid) Transition Template Relation WordNet hypernym WordNet hyponym WordNet antonym † WordNet synonym/pertainym † ≡ Distributional nearest neighbor ≡ Delete word † Add word † Operator weaken Operator strengthen Operator negate Operator synonym ≡ Change word sense ≡</note><p>The transition template for WordNet hyper- nymy gives us a transition instance from feline to cat, corresponding to the Natural Logic infer- ence cat − → feline. Recall, we are constructing the inference in reverse, starting from the conse- quent (query). We then notice that the transition valid − → valid in the FSA ends in our current inference state (valid), and set our new inference state to be the start state of the FSA transition -in this case, we maintain validity.</p><p>Note that negation is somewhat subtle, as the transitions are not symmetric from valid to in- valid and visa versa, and we do not know our true inference state with respect to the premise yet. In practice, the search procedure treats all three of {, , } as negation, and re-scores complete derivations once their inference states are known.</p><p>It should be noted that the mapping from transi- tion templates to relation types is intentionally im- precise. For instance, clearly nearest neighbors do not preserve equivalence (≡); more subtly, while all cats like milk all cats hate milk, it is not the case that some cats like milk some cats hate milk. <ref type="bibr">2</ref> We mitigate this imprecision by introducing a cost for each transition, and learning the appro- priate value for this cost (see Section 5). The cost of an edge from fact (w, v) with surface form w and validity v to a new fact (w , v ), using a transi- tion instance t i of template t and mutating a word with polarity p, is given by f t i · θ t,v,p . We define this as: f t i : A value associated with every transition instance t i , intuitively corresponding to how "far" the endpoints of the transition are. θ t,v,p : A learned cost for taking a transition of template t, if the source of the edge is in a in- ference state of v and the word being mutated has polarity p.</p><p>The notation for f t i is chosen to evoke an anal- ogy to features. We set f t i to be 1 in most cases; the exceptions are the edges over the WordNet hy- pernym tree and the nearest neighbors edges. In the first case, taking the hypernymy relation from w to w to be ↑ w→w , we set:</p><formula xml:id="formula_4">f ↑ w→w = log p(w ) p(w) = log p(w ) − log p(w).</formula><p>The value f ↓ w→w is set analogously. We define p(w) to be the "probability" of a concept -that is, the normalized frequency of a word w or any of its hyponyms in the Google N-Grams corpus <ref type="bibr" target="#b5">(Brants and Franz, 2006</ref>). Intuitively, this ensures that relatively long paths through fine-grained sec- tions of WordNet are not unduly penalized. For instance, the path from cat to animal traverses six intermediate nodes, na¨ıvelyna¨ıvely yielding a prohibitive search depth of 6. However, many of these tran- sitions have low weight: for instance f ↑ cat→feline is only 0.37.</p><p>For nearest neighbors edges, we take Neu- ral Network embeddings learned in <ref type="bibr" target="#b14">Huang et al. (2012)</ref> corresponding to each vocabulary entry. We then define f N N w→w to be the arc cosine of the cosine similarity (i.e., the angle) between word vectors associated with lexical items w and w :</p><formula xml:id="formula_5">f N N w→w = arccos w · w ww .</formula><p>For instance, f N N cat→dog = 0.43. In practice, we explore the 100 nearest neighbors of each word.</p><p>We can express f t i as a feature vector by rep- resenting it as a vector with value f t i at the index corresponding to (t, v, p) -the transition template, the validity of the inference, and the polarity of the mutated word. Note that the size of this vector mirrors the number of cost parameters θ t,v,p , and is in general smaller than the number of transition instances.</p><p>A search path can then be parametrized by a sequence of feature vectors f 1 , f 2 , . . . , f n , which in turn can be collapsed into a single vector f = i f i . The cost of a path is defined as θ · f , where θ is the vector of θ t,v,p values. Both f and θ are constrained to be non-negative, or else the search problem is misspecified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalizing Similarities</head><p>An elegant property of our definitions of f t i is its ability to generalize JC distance. Let us assume we have lexical items w 1 and w 2 , with a least common subsumer lcs. The JC distance dist jc (w 1 , w 2 ) is:</p><formula xml:id="formula_6">dist jc (w 1 , w 2 ) = log p(lcs) 2 p(w 1 ) · p(w 2 ) .<label>(1)</label></formula><p>For simplicity, we simplify θ ↑,v,p and θ ↓,v,p as simply θ ↑ and θ ↓ . Without loss of generality, we also assume that a path in our search is only modi- fying a single lexical item w 1 , eventually reaching a mutated form w 2 .</p><p>We can factorize the cost of a path, θ · f , along the path from w 1 to w 2 through its lowest common subsumer (lcs), [w 1 , w <ref type="formula" target="#formula_6">(1)</ref> 1 , . . . , lcs, . . . , w </p><formula xml:id="formula_7">θ · φ = θ ↑ log p(w (1) 1 ) − log p(w 1 ) + . . . + θ ↓ log p(lcs) − log p(w (n) 1 ) + . . . = θ ↑ log p(lcs) p(w 1 ) + θ ↓ log p(lcs) p(w 2 ) = log p(lcs) θ ↑ +θ ↓ p(w 1 ) θ ↑ · p(w 2 ) θ ↓ .</formula><p>Note that setting both θ ↑ and θ ↓ to 1 exactly yields Formula (1) for JC distance. This, in addi- tion to the inclusion of nearest neighbors as tran- sitions, allows the search to capture the intuition that similar objects have similar properties (e.g., as used in <ref type="bibr" target="#b0">Angeli and Manning (2013)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Deletions in Inference</head><p>Although inserting lexical items in a derivation (deleting words from the reversed derivation) is trivial, the other direction is not. For brevity, we refer to a deletion in the derivation as an insertion, since from the perspective of search we are insert- ing lexical items.</p><p>Na¨ıvelyNa¨ıvely, at every node in our search we must consider every item in the vocabulary as a possi- ble insertion. We can limit the number of items we consider by storing the database as a trie. Since the search mutates the fact left-to-right (as per Section 4.1), we can consider children of a trie node as candidate insertions. To illustrate, given a search state with fact w 0 w 1 . . . w n and mutation index i, we would look up completions w i+1 for w 0 w 1 . . . w i in our trie of known facts.</p><p>Although this approach works well when i is relatively large, there are too many candidate in- sertions for small i. We special case the most ex- treme example for this, where i = 0 -that is, when we are inserting into the beginning of the fact. In this case, rather than taking all possible lexical items that start any fact, we take all items which are followed by the first word of our current fact. To illustrate, given a search state with fact w 0 w 1 . . . w n , we would propose candidate inser- tions w −1 such that w −1 w 0 w 1 . . . w k is a known fact for some w 1 . . . w k . More concretely, if we know that fluffy cats have tails, and are at a node corresponding to cats like boxes, we propose fluffy as a possible insertion: fluffy cats like boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Confidence Estimation</head><p>The last component in inference is translating a search path into a probability of truth. We notice from Section 4.2 that the cost of a path can be rep- resented as θ · f . We can normalize this value by negating every element of the cost vector θ and passing it through a sigmoid:</p><formula xml:id="formula_8">confidence = 1 1 + e −(−θ·f ) .</formula><p>Importantly, note that the cost vector must be non-negative for the search to be well-defined, and therefore the confidence value will be constrained to be between 0 and 1 2 . At this point, we have a confidence that the given path has not violated strict Natural Logic. However, to translate this value into a probability we need to incorporate whether the inference path is confidently valid, or confidently invalid. To il- lustrate, a fact with a low confidence should trans- late to a probability of 1 2 , rather than a probability of 0. We therefore define the probability of valid- ity as follows: We take v to be 1 if the query is in the valid state with respect to the premise, and −1 if the query is in the invalid state. For complete- ness, if no path is given we can set v = 0. The probability of validity becomes:</p><formula xml:id="formula_9">p(valid) = v 2 + 1 1 + e vθ·f .<label>(2)</label></formula><p>Note that in the case where v = −1, the above expression reduces to 1 2 − confidence; in the case where v = 0 it reduces to simply 1 2 . Furthermore, note that the probability of truth makes use of the same parameters as the cost in the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning Transition Costs</head><p>We describe our procedure for learning the transi- tion costs θ. Our training data D consists of query facts q and their associated gold truth values y. Equation <ref type="formula" target="#formula_9">(2)</ref> gives us a probability that a partic- ular inference is valid; we axiomatically consider a valid inference from a known premise to be justi- fication for the truth of the query. This is at the ex- pense of the (often incorrect) assumption that our database is clean and only contains true facts.</p><p>We optimize the likelihood of our gold annota- tions according to this probability, subject to the constraint that all elements in our cost vector θ be non-negative. We run the search algorithm de- scribed in Section 4 on every query q i ∈ D. This produces the highest confidence path x 1 , along with its inference state v i . We now have annotated tuples: ((x i , v i ), y i ) for every element in our train- ing set. Analogous to logistic regression, the log likelihood of our training data D, subject to costs θ, is:</p><formula xml:id="formula_10">l θ (D) = 0≤i&lt;|D| y i log v i 2 + 1 1 + e v i θ·f (x i ) + (1 − y i ) log −v i 2 + 1 1 + e −v i θ·f (x i ) ,</formula><p>where y i is 1 if the example is annotated true and 0 otherwise, and f (x i ) are the features extracted for path x i . The objective function is the negative log likelihood with an L 2 regularization term and a log barrier function to prohibit negative costs:</p><formula xml:id="formula_11">O(D) = −l θ (D) + 1 2σ 2 θ 2 2 − log(θ).</formula><p>We optimize this objective using conjugate gra- dient descent. Although the objective is non- convex, in practice we can find a good initializa- tion of weights to reduce the risk of arriving at lo- cal optima.</p><p>An elegant property of this formulation is that the weights we are optimizing correspond directly <ref type="table">Table 4</ref>: Results on the FraCaS textual entailment suite. N is this work; M07 refers to <ref type="bibr" target="#b21">MacCartney and Manning (2007)</ref>; M08 refers to <ref type="bibr" target="#b22">MacCartney and Manning (2008)</ref>. The relevant sections of the corpus intended to be handled by this system are sections 1, 5, and 6 (although not 2 and 9, which are also included in M08).</p><p>to the costs used during search. This creates a pos- itive feedback loop -as better weights are learned, the search algorithm is more likely to find con- fident paths, and more data is available to train from. We therefore run this learning step for mul- tiple epochs, re-running search after each epoch. The weights for the first epoch are initialized to an approximation of valid Natural Logic weights. Subsequent epochs initialize their weights to the output of the previous epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate our system on two tasks: the Fra- CaS test suite, used by <ref type="bibr" target="#b21">MacCartney and Manning (2007;</ref><ref type="bibr" target="#b39">2008)</ref>, evaluates the system's ability to cap- ture Natural Logic inferences even without the ex- plicit alignments of these previous systems. In addition, we evaluate the system's ability to pre- dict common-sense facts from a large corpus of OpenIE extractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">FraCaS Entailment Corpus</head><p>The FraCaS corpus <ref type="bibr" target="#b8">(Cooper et al., 1996</ref>) is a small corpus of entailment problems, aimed at provid- ing a comprehensive test of a system's handling of various entailment patterns. We process the cor- pus following <ref type="bibr" target="#b21">MacCartney and Manning (2007)</ref>. It should be noted that many of the sections of the corpus are not directly applicable to Natu- ral Logic inferences; <ref type="bibr" target="#b21">MacCartney and Manning (2007)</ref> identify three sections which are in the scope of their system, and consequently our sys- tem as well.</p><p>Results on the dataset are given in <ref type="table">Table 4</ref>.  Since the corpus is not a blind test set, the re- sults are presented less as a comparison of perfor- mance, but rather to validate the expressive power of our search-based approach against MacCart- ney's align-and-classify approach. For the exper- iments, costs were set to express valid Natural Logic inference as a hard constraint.</p><p>The results show that the system is able to cap- ture Natural Logic inferences with similar accu- racy to the state-of-the-art system of <ref type="bibr" target="#b22">MacCartney and Manning (2008)</ref>. Note that our system is com- paratively crippled in this framework along at least two dimensions: It cannot appeal to the premise when constructing the search, leading to the intro- duction of a class of search errors which are en- tirely absent from prior work. Second, the deriva- tion process itself does not have access to the full parse tree of the candidate fact.</p><p>Although precision is fairly high even on the non-applicable sections of FraCaS, recall is sig- nificantly lower than prior work. This is a direct consequence of not having alignments to appeal to. For instance, we can consider two inferences: Jack saw Jill is playing ? = ⇒ Jill is playing Jill saw Jack is playing</p><formula xml:id="formula_12">? = ⇒ Jill is playing</formula><p>It is clear from the parse of the sentence that the first is valid and the second is not; however, from the perspective of the search algorithm both make the same two edits: inserting Jack and saw. In order to err on the side of safety, we disallow deleting the verb saw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Common Sense Reasoning</head><p>We validate our system's ability to infer unseen common sense facts from a large database of such facts. Whereas evaluation on FraCaS shows that our search formulation captures applicable in- ferences as well as prior work, this evaluation presents a novel use-case for Natural Logic infer- ence.</p><p>For our database of facts, we run the Ol- lie OpenIE system <ref type="bibr" target="#b25">(Mausam et al., 2012</ref>) over Wikipedia, <ref type="bibr">3</ref> Simple Wikipedia, <ref type="bibr">4</ref> and a random 5% of CommonCrawl. Extractions with confidence below 0.25 or which contained pronouns were discarded. This yielded a total of 305 million unique extractions composed entirely of lexical items which mapped into our vocabulary (186 707 items). Each of these extracted triples (e 1 , r, e 2 ) was then flattened into a plain-text fact e 1 r e 2 and lemmatized. This yields 270 million unique lem- matized premises in our database. In general, each fact in the database could be arbitrary unstructured text; our use of Ollie extractions is motivated only by a desire to extract short, concise facts.</p><p>For our evaluation, we infer the top 689 most confident facts from the ConceptNet project <ref type="bibr" target="#b36">(Tandon et al., 2011</ref>). To avoid redundancy with Word- Net, we take facts from eight ConceptNet rela- tions: MemberOf, HasA, UsedFor, CapableOf, Causes, HasProperty, Desires, and CreatedBy. We then treat the surface text field of these facts as our candidate query. This yields queries like the following:</p><p>not all birds can fly noses are used to smell nobody wants to die music is used for pleasure For negative examples, we take the 689 ReVerb extractions <ref type="bibr" target="#b10">(Fader et al., 2011</ref>) judged as false by Mechanical Turk workers <ref type="bibr" target="#b0">(Angeli and Manning, 2013)</ref>. This provides a set of plausible but nonetheless incorrect examples, and ensures that our recall is not due to over-zealous search. Search costs are tuned from an additional set of 540 true ConceptNet and 540 false ReVerb extractions.</p><p>Results are shown in <ref type="table" target="#tab_4">Table 5</ref>. We compare against the baseline of looking up each fact verba- tim in the fact database. Note that both the query and the facts in the database are short snippets, al- ready lemmatized and lower-cased; therefore, it is not in principle unreasonable to expect a database of 270 million extractions to contain these facts. Nonetheless, only 12% of facts were found via a direct lookup. We show that NaturalLI (allowing lookups) improves this recall four-fold, at only an 9.4% drop in precision.</p><p>A large body of work is devoted to compiling open-domain knowledge bases. For instance, OpenIE systems ( <ref type="bibr" target="#b43">Yates et al., 2007;</ref><ref type="bibr" target="#b10">Fader et al., 2011</ref>) extract concise facts via surface or depen- dency patterns. In a similar vein, NELL <ref type="bibr" target="#b6">(Carlson et al., 2010;</ref><ref type="bibr" target="#b12">Gardner et al., 2013</ref>) continuously learns new high-precision facts from the internet.</p><p>Many NLP applications query large knowl- edge bases. Prominent examples include ques- tion answering <ref type="bibr" target="#b41">(Voorhees, 2001)</ref>, semantic pars- ing ( <ref type="bibr" target="#b44">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b45">Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b19">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b2">Berant and Liang, 2014)</ref>, and information extraction sys- tems ( <ref type="bibr" target="#b13">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b35">Surdeanu et al., 2012)</ref>. A goal of this work is to improve accuracy on these downstream tasks by providing a proba- bilistic knowledge base for likely true facts.</p><p>A natural alternative to the approach taken in this paper is to extend knowledge bases by in- ferring and adding new facts directly. For in- stance, <ref type="bibr" target="#b32">Snow et al. (2006)</ref> present an approach to enriching the WordNet taxonomy; <ref type="bibr" target="#b36">Tandon et al. (2011)</ref> extend ConceptNet with new facts; <ref type="bibr" target="#b34">Soderland et al. (2010)</ref> use ReVerb extractions to enrich a domain-specific ontology.  and  use Neural Tensor Networks to predict unseen relation triples in WordNet and Freebase, following a line of work by <ref type="bibr" target="#b4">Bordes et al. (2011)</ref> and <ref type="bibr" target="#b17">Jenatton et al. (2012)</ref>. <ref type="bibr" target="#b42">Yao et al. (2012)</ref> and <ref type="bibr" target="#b29">Riedel et al. (2013)</ref> present a related line of work, inferring new relations between Free- base entities via inference over both Freebase and OpenIE relations. In contrast, this work runs infer- ence over arbitrary text, without restricting itself to a particular set of relations, or even entities.</p><p>The goal of tackling common-sense reasoning is by no means novel in itself. Work by <ref type="bibr">Reiter and McCarthy (Reiter, 1980;</ref><ref type="bibr" target="#b26">McCarthy, 1980)</ref> at- tempts to reason about the truth of a consequent in the absence of strict logical entailment. Similarly, <ref type="bibr" target="#b27">Pearl (1989)</ref> presents a framework for assigning confidences to inferences which can be reason- ably assumed. Our approach differs from these at- tempts in part in its use of Natural Logic as the un- derlying inference engine, and more substantially in its attempt at creating a broad-coverage sys- tem. More recently, work by <ref type="bibr" target="#b31">Schubert (2002)</ref> and Van <ref type="bibr" target="#b40">Durme et al. (2009)</ref> approach common sense reasoning with episodic logic; we differ in our fo- cus on inferring truth from an arbitrary query, and in making use of longer inferences.</p><p>This work is similar in many ways to work on recognizing textual entailment -e.g., <ref type="bibr" target="#b30">Schoenmackers et al. (2010)</ref>, <ref type="bibr" target="#b3">Berant et al. (2011)</ref>. Work by <ref type="bibr" target="#b20">Lewis and Steedman (2013)</ref> is particularly rele- vant, as they likewise evaluate on the FraCaS suite (Section 1; 89% accuracy with gold trees). They approach entailment by constructing a CCG parse of the query, while mapping questions which are paraphrases of each other to the same logical form using distributional relation clustering. However, their system is unlikely to scale to either our large database of premises, or our breadth of relations. <ref type="bibr" target="#b11">Fader et al. (2014)</ref> propose a system for ques- tion answering based on a sequence of paraphrase rewrites followed by a fuzzy query to a structured knowledge base. This work can be thought of as an elegant framework for unifying this two-stage process, while explicitly tracking the "risk" taken with each paraphrase step. Furthermore, our sys- tem is able to explore mutations which are only valid in one direction, rather than the bidirectional entailment of paraphrases, and does not require a corpus of such paraphrases for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented NaturalLI, an inference system over unstructured text intended to infer common sense facts. We have shown that we can run infer- ence over a large set of premises while maintain- ing Natural Logic semantics, and that we can learn how to infer unseen common sense facts.</p><p>Future work will focus on enriching the class of inferences we can make with Natural Logic. For example, extending the approach to handle meronymy and relation entailments. Furthermore, we hope to learn richer lexicalized parameters, and use the syntactic structure of a fact during search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Natural Logic inference cast as search. The path to the boxed premise the cat ate a mouse disproves the query no carnivores eat animals, as it passes through the negation relation (). This path is one of many candidates taken; the premise is one of many known facts in the database. The edge labels denote Natural Logic inference steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Natural logic inference expressed as a finite state automaton. Omitted edges go to the unknown state (#), with the exception of omitted edges from ≡, which go to the state of the edge type. Green states (≡, ) denote valid inferences; red states ( , ) denote invalid inferences; blue states (, ) denote inferences of unknown validity. (b) The join table collapsed into the three meaningful states over truth values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Formally</head><label></label><figDesc>, for every relation r and nodes a 1 and a 2 in the same cluster, if we have transitions a 1 r − → b 1 and a 2 r − → b 2 then b 1 and b 2 are neces- sarily in the same cluster. As a concrete example, we can take r = and the two states in the in- valid cluster: a 1 = , a 2 = . Although − →≡ and − →→, both ≡ and are in the same cluster (valid). It is not trivial a priori that the join table should have this regularity, and it certainly simpli- fies the logic for inference tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) have made</head><label>have</label><figDesc></figDesc><table>No carnivores 
eat animals? 

The carnivores 
eat animals 

The cat 
eats animals 

The cat 
ate an animal 

The cat 
ate a mouse 

≡ 

No cats 
eat animals 

No cats 
eat mice 

. . . 

. . . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The join table as shown in Icard (2012). Entries in the table are the result of joining a row with a column.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy inferring common-sense facts 
on a balanced test set. Lookup queries the lem-
matized lower-case fact directly in the 270M fact 
database. NaturalLI Only disallows such lookups, 
and infers every query from only distinct premises 
in the database. NaturalLI + Lookup takes the 
union of the two systems. 

</table></figure>

			<note place="foot" n="1"> Note that this table optimistically assumes every operator is additive and multiplicative, as defined in Icard (2012).</note>

			<note place="foot" n="2"> The latter example is actually a consequence of the projection function in Table 1 being overly optimistic.</note>

			<note place="foot" n="3"> http://wikipedia.org/ (2013-07-03) 4 http://simple.wikipedia.org/ (2014-03-25) 542</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their thoughtful comments. We gratefully acknowl-edge the support of the Defense Advanced Re-search Projects Agency (DARPA) Deep Explo-ration and Filtering of Text (DEFT) Program un-der Air Force Research Laboratory (AFRL) con-tract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Philosophers are mortal: Inferring the truth of unseen facts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Linguistic Annotation Workshop</title>
		<meeting>Linguistic Annotation Workshop<address><addrLine>Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Web 1T 5gram version 1. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Franz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Estevam R Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3618</idno>
		<title level="m">Learning new facts from knowledge bases with neural tensor networks and semantic word vectors</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using the framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Eijck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Genabith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Jaspars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The FraCaS Consortium</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Synthetic logic. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Djalali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recent progress on monotonicity. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Moss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inclusion and exclusion in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studia Logica</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic similarity based on corpus statistics and lexical taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Research on Computational Linguistics</title>
		<meeting>the 10th International Conference on Research on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural logic for textual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Modeling semantic containment and exclusion in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>In Coling</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An extended model of natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth international conference on computational semantics</title>
		<meeting>the eighth international conference on computational semantics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Circumscription-a form of non-monotonic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Probabilistic semantics for nonmonotonic reasoning: A survey. Principles of Knowledge Representation and Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A logic for default reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="132" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning first-order horn clauses from web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Can we derive general world knowledge from texts? In HLT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenhart</forename><surname>Schubert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adapting open information extraction to domain-specific relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiinstance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deriving a web-scale common sense fact database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Gerard De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Studies on natural logic and categorial grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor Manuel Sánchez</forename><surname>Valencia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Essays in logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Benthem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A brief history of natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Benthem</surname></persName>
		</author>
		<idno>PP-2008-05</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deriving generalized knowledge from corpora using wordnet abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenhart K</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Question answering in TREC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic databases of universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">TextRunner: Open information extraction on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
