<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Neural Word Representations in Tensor-Based Compositional Settings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London School of Electronic Engineering and Computer Science Mile End Road</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
							<email>dimitri.kartsaklis@cs.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London School of Electronic Engineering and Computer Science Mile End Road</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London School of Electronic Engineering and Computer Science Mile End Road</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Neural Word Representations in Tensor-Based Compositional Settings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="708" to="719"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We provide a comparative study between neural word representations and traditional vector spaces based on co-occurrence counts, in a number of com-positional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scala-bility, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of composi-tional method is important; on the larger-scale tasks, they are outperformed by neu-ral word embeddings, which show robust, stable performance across the tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural word embeddings ( <ref type="bibr" target="#b2">Bengio et al., 2006;</ref><ref type="bibr" target="#b9">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b30">Mikolov et al., 2013a</ref>) have received much attention in the dis- tributional semantics community, and have shown state-of-the-art performance in many natural lan- guage processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level ( <ref type="bibr" target="#b1">Baroni et al., 2014</ref>), we are aware of only one work that attempts a comparison of the two approaches in compositional settings <ref type="bibr" target="#b4">(Blacoe and Lapata, 2012)</ref>, and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder.</p><p>The purpose of this paper is to provide a more complete picture regarding the potential of neu- ral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts. We are especially interested in investi- gating the performance of neural word vectors in compositional models involving general mathe- matical composition operators, rather than in the more task-or domain-specific deep-learning com- positional settings they have generally been used with so far (for example, by <ref type="bibr" target="#b40">Socher et al. (2012)</ref>, <ref type="bibr" target="#b19">Kalchbrenner and Blunsom (2013)</ref> and many oth- ers).</p><p>In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by <ref type="bibr" target="#b8">Coecke et al. (2010)</ref>. We test a range of implementations based on this framework, together with additive and multiplicative approaches <ref type="bibr" target="#b34">(Mitchell and Lapata, 2008</ref>), in a variety of different tasks. Specif- ically, we use the verb disambiguation task of <ref type="bibr" target="#b16">Grefenstette and Sadrzadeh (2011a)</ref> and the tran- sitive sentence similarity task of <ref type="bibr" target="#b22">Kartsaklis and Sadrzadeh (2014)</ref> as small-scale focused experi- ments on pre-defined sentence structures. Addi- tionally, we evaluate our vector spaces on para- phrase detection (using the Microsoft Research Paraphrase Corpus of <ref type="bibr" target="#b11">Dolan et al. (2005)</ref>) and di- alogue act tagging using the Switchboard Corpus (see e.g. ( <ref type="bibr" target="#b41">Stolcke et al., 2000)</ref>).</p><p>In all of the above tasks, we compare the neural word embeddings of <ref type="bibr" target="#b30">Mikolov et al. (2013a)</ref> with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The gen- eral picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches.</p><p>We proceed as follows: Section 2 provides a concise introduction to distributional word repre- sentations in natural language processing. Section 3 takes a closer look to the subject of composi- tionality in vector space models of meaning and describes the range of compositional operators ex- amined here. In Section <ref type="bibr">4</ref> we provide details about the vector spaces used in the experiments. Our ex- perimental work is described in detail in Section 5, and the results are discussed in Section 6. Finally, Section 7 provides conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Meaning representation</head><p>There are several approaches to the representation of word, phrase and sentence meaning. As nat- ural languages are highly creative and it is very rare to see the same sentence twice, any practical approach dealing with large text segments must be compositional, constructing the meaning of phrases and sentences from their constituent parts. The ideal method would therefore express not only the similarity in meaning between those con- stituent parts, but also between the results of their composition, and do this in ways which fit with linguistic structure and generalisations thereof.</p><p>Formal semantics Formal approaches to the semantics of natural language have long built upon the classical idea of compositionality - that the meaning of a sentence is a function of the meanings of its parts <ref type="bibr">(Frege, 1892)</ref>. In compositional type-logical approaches, predicate- argument structures representing phrases and sen- tences are built from their constituent parts by β- reduction within the lambda calculus framework <ref type="bibr" target="#b36">(Montague, 1970)</ref>: for example, given a represen- tation of John as john and sleeps as λx.sleep (x), the meaning of the sentence "John sleeps" can be constructed as λx.sleep (x)(john ) = sleep (john ). Given a suitable pairing between words and semantic representations of them, this method can produce structured sentential repre- sentations with broad coverage and good gener- alisability (see e.g. <ref type="bibr" target="#b6">(Bos, 2008)</ref>). The above logi- cal approach is extremely powerful because it can capture complex aspects of meaning such as quan- tifiers and their interaction (see e.g. ( <ref type="bibr" target="#b10">Copestake et al., 2005)</ref>), and enables inference using well stud- ied and developed logical methods (see e.g. <ref type="bibr" target="#b5">(Bos and Gabsdil, 2000)</ref>).</p><p>Distributional hypothesis However, such for- mal approaches are less able to express similar- ity in meaning. We would like to capture the intuition that while John and Mary are distinct, they are rather similar to each other (both of them are humans) and dissimilar to words such as dog, pavement or idea. The same applies at the phrase and sentence level: "dogs chase cats" is similar in meaning to "hounds pursue kittens", but less so to "cats chase dogs" (despite the lexical overlap).</p><p>Distributional methods provide a way to address this problem. By representing words and phrases as vectors or tensors in a (usually highly dimen- sional) vector space, one can express similarity in meaning via a suitable distance metric within that space (usually cosine distance); furthermore, composition can be modelled via suitable linear- algebraic operations.</p><p>Co-occurrence-based word representations One way to produce such vectorial representa- tions is to directly exploit Harris (1954)'s intuition that semantically similar words tend to appear in similar contexts. We can construct a vector space in which the dimensions correspond to contexts, usually taken to be words as well. The word vector components can then be calculated from the frequency with which a word has co-occurred with the corresponding contexts in a window of words, with a predefined length. <ref type="table">Table 1</ref> shows 5 3-dimensional vectors for the words Mary, John, girl, boy and idea. The words philosophy, book and school signify vector space dimensions. As the vector for John is closer to Mary than it is to idea in the vector space-a di- rect consequence of the fact that John's contexts are similar to Mary's and dissimilar to idea's-we can infer that John is semantically more similar to Mary than to idea.</p><p>Many variants of this approach exist: perfor- mance on word similarity tasks has been shown to be improved by replacing raw counts with weighted values (e.g. mutual information)-see ( <ref type="bibr" target="#b42">Turney et al., 2010)</ref> and below for discussion, and <ref type="bibr" target="#b24">(Kiela and Clark, 2014</ref>  <ref type="bibr" target="#b26">Leech et al., 1994)</ref>.</p><p>Neural word embeddings Deep learning tech- niques exploit the distributional hypothesis dif- ferently. Instead of relying on observed co- occurrence frequencies, a neural language model is trained to maximise some objective function re- lated to e.g. the probability of observing the sur- rounding words in some context ( <ref type="bibr" target="#b31">Mikolov et al., 2013b</ref>):</p><formula xml:id="formula_0">1 T T t=1 −c≤j≤c,j =0 log p(w t+j |w t )<label>(1)</label></formula><p>Optimizing the above function, for example, pro- duces vectors which maximise the conditional probability of observing words in a context around the target word w t , where c is the size of the training window, and w 1 w 2 , · · · w T a sequence of words forming a training instance. Therefore, the resulting vectors will capture the distributional in- tuition and can express degrees of lexical similar- ity. This method has an obvious advantage com- pared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity prob- lems, which is always an important issue for co- occurrence word spaces. Additionally, neural vec- tors have also proven successful in other tasks ( <ref type="bibr" target="#b32">Mikolov et al., 2013c</ref>), since they seem to en- code not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity <ref type="bibr" target="#b43">(Turney, 2006</ref>). For ex- ample, it is possible to extract the singular:plural relation (apple:apples, car:cars) using vector sub- traction:</p><p>− Both neural and co-occurrence-based ap- proaches have advantages over classical formal approaches in their ability to capture lexical se- mantics and degrees of similarity; their success at extending this to the sentence level and to more complex semantic phenomena, though, depends on their applicability within compositional mod- els, which is the subject of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Compositional models</head><p>Compositional distributional models represent meaning of a sequence of words by a vector, ob- tained by combining meaning vectors of the words within the sequence using some vector composi- tion operation. In a general classification of these models, one can distinguish between three broad cases: simplistic models which combine word vectors irrespective of their order or relation to one another, models which exploit linear word order, and models which use grammatical structure.</p><p>The first approach combines word vectors by vector addition or point-wise multiplication <ref type="bibr" target="#b34">(Mitchell and Lapata, 2008)</ref>-as this is indepen- dent of word order, it cannot capture the differ- ence between the two sentences "dogs chase cats" and "cats chase dogs". The second approach has generally been implemented using some form of deep learning, and captures word order, but not by necessarily caring about the grammatical structure of the sentence. Here, one works by recursively building and combining vectors for subsequences of words within the sentence using e.g. autoen- coders ( <ref type="bibr" target="#b40">Socher et al., 2012</ref>) or convolutional fil- ters ( <ref type="bibr" target="#b20">Kalchbrenner et al., 2014</ref>). We do not con- sider this approach in this paper. This is because, as mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be com- bined with results in linguistics and formal se- mantics to provide generalisable models that can canonically extend to complex semantic phenom- ena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been in- stantiated in various ways in the work of <ref type="bibr" target="#b0">Baroni and Zamparelli (2010)</ref>, <ref type="bibr" target="#b16">Grefenstette and Sadrzadeh (2011a)</ref>, and <ref type="bibr" target="#b23">Kartsaklis et al. (2012)</ref>.</p><p>General framework Formally, we can spec- ify the vector representation of a word sequence</p><formula xml:id="formula_1">w 1 w 2 · · · w n as the vector − → s = − → w 1 − → w 2 · · · − → w n ,</formula><p>where is a vector operator, such as addition +, point-wise multiplication , tensor product ⊗, or matrix multiplication ×.</p><p>In the simplest compositional models (the first approach described above), is + or , e.g. see <ref type="bibr" target="#b34">(Mitchell and Lapata, 2008)</ref>. Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vectors, known as tensors. Whereas a vector − → v is an ele- ment of an atomic vector space V , a tensor z is an element of a tensor space V ⊗ W ⊗ · · · ⊗ Z. The number of tensored spaces is referred to by the or- der of the space. Using a general duality theorem from multi-linear algebra <ref type="bibr" target="#b7">(Bourbaki, 1989)</ref>, it fol- lows that tensors are in one-one correspondence with multi-linear maps, that is we have:</p><formula xml:id="formula_2">z ∈ V ⊗W ⊗· · ·⊗Z ∼ = f z : V → W → · · · → Z</formula><p>In such a tensor-based formalism, meanings of nouns are vectors and meanings of predicates such as adjectives and verbs are tensors. Meaning of a string of words is obtained by applying the compo- sitions of multi-linear map duals of the tensors to the vectors. For the sake of demonstration, take the case of an intransitive sentence "Sbj Verb"; the meaning of the subject is a vector − → Sbj ∈ V and the meaning of the intransitive verb is a ten- sor Verb ∈ V ⊗ W . Meaning of the sentence is obtained by applying f V erb to − → Sbj, as follows:</p><formula xml:id="formula_3">− −−−− → Sbj Verb = f V erb ( − → Sbj)</formula><p>By tensor-map duality, the above becomes equivalent to the following, where composition has now become the familiar notion of matrix mul- tiplication, that is is ×:</p><formula xml:id="formula_4">Verb × − → Sbj</formula><p>In general and for words with tensors of order higher than two, becomes a generalisation of ×, referred to by tensor contraction, see e.g. <ref type="bibr" target="#b21">Kartsaklis and Sadrzadeh (2013)</ref>. Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations, e.g. see <ref type="bibr" target="#b16">Grefenstette and Sadrzadeh (2011a)</ref>, <ref type="bibr" target="#b22">Kartsaklis and Sadrzadeh (2014)</ref>. In such cases, be- comes a combination of a range of operations such as ×, ⊗, , and +.</p><p>Specific models In the current paper we will ex- periment with a variety of models. In <ref type="table" target="#tab_1">Table 2</ref>, we present these models in terms of their composi- tion operators and a reference to the main paper in which each model was introduced. For the sim- ple compositional models the sentence is a string of any number of words; for the grammar-based models, we consider simple transitive sentences "Sbj Verb Obj" and introduce the following abbre- viations for the concrete method used to build a tensor for the verb:</p><p>1. Verb is a verb matrix computed using the for- mula</p><formula xml:id="formula_5">i − − → Sbj i ⊗ − − → Obj i , where</formula><p>− − → Sbj i and − − → Obj i are the subjects and objects of the verb across the corpus. These models are referred to by rela- tional (Grefenstette and Sadrzadeh, 2011a); they are generalisations of predicate seman- tics of transitive verbs, from pairs of individ- uals to pairs of vectors. The models reduce the order 3 tensor of a transitive verb to an order 2 tensor (i.e. a matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Verb is a verb matrix computed using the for- mula</p><formula xml:id="formula_6">− − → Verb ⊗ − − → Verb, where − − →</formula><p>Verb is the distri- butional vector of the verb. These models are referred to by Kronecker, which is the term sometimes used to denote the outer prod- uct of tensors ( <ref type="bibr" target="#b17">Grefenstette and Sadrzadeh, 2011b</ref>). This models also reduces the order 3 tensor of a transitive verb to an order 2 ten- sor.</p><p>3. The models of the last five lines of the table use the so-called Frobenius operators from categorical compositional distributional se- mantics ( <ref type="bibr" target="#b23">Kartsaklis et al., 2012</ref>) to expand the relational matrices of verbs from order 2 to order 3. The expansion is obtained by ei- ther copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the di- mension of the object in that space, hence re- ferred to by Copy-Obj; furthermore, we can take addition, multiplication, or outer product of these, which are referred to by Frobenius- Add, Frobenius-Mult, and Frobenius-Outer ( <ref type="bibr" target="#b22">Kartsaklis and Sadrzadeh, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic word spaces</head><p>Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to ( <ref type="bibr" target="#b24">Kiela and Clark, 2014;</ref><ref type="bibr" target="#b37">Polajnar and Clark, 2014</ref>) for recent studies). We in- stantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head><p>Linear algebraic formula Reference  GS11 Our first word space is based on a typ- ical configuration that has been used in the past extensively for compositional distributional mod- els (see below for details), so it will serve as a useful baseline for the current work. In this vec- tor space, the co-occurrence counts are extracted from the British National Corpus (BNC) ( <ref type="bibr" target="#b26">Leech et al., 1994)</ref>. As basis words, we use the most fre- quent nouns, verbs, adjectives and adverbs (POS tags SUBST, VERB, ADJ and ADV in the BNC XML distribution 2 ). The vector space is lemma- tized, that is, it contains only "canonical" forms of words.</p><formula xml:id="formula_7">Addition w1w2 · · · wn − → w1 + − → w2 + · · · + − → wn Mitchell and Lapata (2008) Multiplication w1w2 · · · wn − → w1 − → w2 · · · − → wn</formula><p>In order to weight the raw co-occurrence counts, we use positive point-wise mutual information (PPMI). The component value for a target word t and a context word c is given by:</p><formula xml:id="formula_8">PPMI(t, c) = max 0, log p(c|t) p(c)</formula><p>where p(c|t) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of c overall.</p><p>Vector spaces based on point-wise mutual in- formation (or variants thereof) have been success- fully applied in various distributional and compo- sitional tasks; see e.g. <ref type="bibr" target="#b16">Grefenstette and Sadrzadeh (2011a)</ref>, <ref type="bibr" target="#b34">Mitchell and Lapata (2008)</ref>,  for details. PPMI has been shown to achieve state-of-the-art results (  and is suggested by the review of <ref type="bibr" target="#b24">Kiela and Clark (2014)</ref>. Our use here of the BNC as a corpus and the window length of 5 is based on previ- ous use and better performance of these param- eters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette KS14 In this variation, we train a vector space from the ukWaC corpus <ref type="bibr">3 (Ferraresi et al., 2008)</ref>, originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). The vector space is again lemmatized. As context we consider a 5-word window from either side of the target word, while as our weighting scheme we use local mutual information (i.e. point-wise mu- tual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using sin- gular value decomposition (SVD).</p><p>In general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus <ref type="bibr" target="#b25">(Landauer and Dumais, 1997;</ref><ref type="bibr" target="#b39">Schütze, 1997)</ref>. SVD has been shown to perform well on a variety of tasks similar to ours <ref type="bibr" target="#b0">(Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b22">Kartsaklis and Sadrzadeh, 2014</ref>).</p><p>Neural word embeddings (NWE) For our neu- ral setting, we used the skip-gram model of <ref type="bibr" target="#b31">Mikolov et al. (2013b)</ref> trained with negative sam- pling. The specific implementation that was tested in our experiments was a 300-dimensional vec- tor space learned from the Google News corpus and provided by the word2vec 4 toolkit. Fur- thermore, the gensim library ( ˇ Rehůřek and So- jka, 2010) was used for accessing the vectors. On the contrary with the previously described co-occurrence vector spaces, this version is not lem- matized.</p><p>The negative sampling method improves the ob- jective function of Equation 1 by introducing neg- ative examples to the training algorithm. Assume that the probability of a specific (c, t) pair of words (where t is a target word and c another word in the same context with t), coming from the training data, is denoted as p(D = 1|c, t). The objective function is then expressed as follows:</p><formula xml:id="formula_9">(c,t)∈D p(D = 1|c, t)<label>(2)</label></formula><p>That is, the goal is to set the model parameters in a way that maximizes the probability of all obser- vations coming from the training data. Assume now that D is a set of randomly selected incorrect (c , t ) pairs that do not occur in D, then Equation 2 above can be recasted in the following way:</p><formula xml:id="formula_10">(c,t)∈D p(D = 1|c, t) (c ,t )∈D p(D = 0|c , t )<label>(3)</label></formula><p>In other words, the model tries to distinguish a tar- get word t from random draws that come from a noise distribution. In the implementation we used for our experiments, c is always selected from a 5-word window around t. More details about the negative sampling approach can be found in ( <ref type="bibr" target="#b31">Mikolov et al., 2013b)</ref>; the note of  also provides an intuitive explanation of the underlying setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experiments explore the use of the vector spaces above, together with the compositional op- erators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; para- phrasing; and dialogue act tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Disambiguation</head><p>We use the transitive verb disambiguation dataset described in <ref type="bibr" target="#b16">Grefenstette and Sadrzadeh (2011a)</ref>  <ref type="bibr">5</ref> . This dataset consists of ambiguous transitive verbs together with their arguments, landmark verbs that identify one of the verb senses, and human judgements that specify how similar is the disam- biguated sense of the verb in the given context to one of the landmarks. This is similar to the in- transitive dataset described in <ref type="bibr" target="#b34">(Mitchell and Lapata, 2008)</ref>. Consider the sentence "system meets specification"; here, meets is the ambiguous tran- sitive verb, and system and specification are its ar- guments in this context. Possible landmarks for meet are satisfy and visit; for this sentence, the human judgements show that the disambiguated meaning of the verb is more similar to the land- mark satisfy and less similar to visit.</p><p>The task is to estimate the similarity of the sense of a verb in a context with a given landmark. To get our similarity measures, we compose the verb with its arguments using one of our compositional models; we do the same for the landmark and then compute the cosine similarity of the two vectors. We evaluate the performance by averaging the hu- man judgements for the same verb, argument and landmark entries, and calculating the Spearman's correlation between the average values and the co- sine scores. As a baseline, we compare this with the correlation produced by using only the verb vector, without composing it with its arguments. <ref type="table">Table 3</ref> shows the results of the experiment. NWE copy-object composition yields the best cor- relation with the human judgements, and top per- formance across all vector spaces and models with a Spearman ρ of 0.456. For the KS14 space, the best result comes from  <ref type="table">Table 3</ref>: Spearman ρ correlations of models with human judgements for the word sense disam- biguation task. The best result (NWE Copy ob- ject) outperforms the nearest co-occurrence-based competitor (KS14 Frobenius outer) with a statisti- cally significant difference (p &lt; 0.05, t-test). while the best operator for the GS11 space is point-wise multiplication (0.348).</p><p>For simple point-wise composition, only mul- tiplicative GS11 and additive NWE improve over their corresponding verb-only baselines (but both perform worse than the KS14 baseline). With tensor-based composition in co-occurrence based spaces, copy subject yields lower results than the corresponding baselines. Other composition methods, except Kronecker for KS14, improve over the verb-only baselines. Finally we should note that, despite the small training corpus, the GS11 vector space performs comparatively well: for instance, Kronecker model improves the pre- viously reported score of 0.28 (Grefenstette and Sadrzadeh, 2011b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentence similarity</head><p>In this experiment we use the transitive sen- tence similarity dataset described in <ref type="bibr" target="#b22">Kartsaklis and Sadrzadeh (2014)</ref>. The dataset consists of transi- tive sentence pairs and a human similarity judge- ment <ref type="bibr">6</ref> . The task is to estimate a similarity measure between two sentences. As in the disambiguation task, we first compose word vectors to obtain sen- tence vectors, then compute cosine similarity of them. We average the human judgements for iden- tical sentence pairs to compute a correlation with cosine scores. <ref type="table">Table 4</ref> shows the results. Again, the best performing vector space is KS14, but this time with addition: the Spearman ρ correlation score with averaged human judgements is 0.732. Addi- tion was the means for the other vector spaces to achieve top performance as well: GS11 and NWE got 0.682 and 0.689 respectively.</p><p>None of the models in tensor-based composi- tion outperformed addition. KS14 performs worse with tensor-based methods here than in the other vector spaces. However, GS11 and NWE, except copy subject for both of them and Frobenius multi- plication for NWE, improved over their verb-only baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Paraphrasing</head><p>In this experiment we evaluate our vector spaces on a mainstream paraphrase detection task. <ref type="bibr">6</ref> The textual content of this dataset is the same as that of <ref type="bibr" target="#b21">(Kartsaklis and Sadrzadeh, 2013)</ref>, the difference is that the dataset of <ref type="bibr" target="#b22">(Kartsaklis and Sadrzadeh, 2014</ref>) has updated hu- man judgements whereas the previous dataset used the orig- inal annotations of the intransitive dataset of <ref type="bibr" target="#b35">(Mitchell and Lapata, 2010</ref>).  <ref type="table">Table 4</ref>: Results for sentence similarity. There is no statistically significant difference between KS14 addition and NWE addition (the second best result).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GS11 KS14 NWE</head><p>Specifically, we get classification results on the Microsoft Research Paraphrase Corpus paraphrase corpus ( <ref type="bibr" target="#b11">Dolan et al., 2005</ref>) working in the follow- ing way: we construct vectors for the sentences of each pair; if the cosine similarity between the two sentence vectors exceeds a certain threshold, the pair is classified as a paraphrase, otherwise as not a paraphrase. For this experiment and that of Section 5.4 below, we investigate only the addi- tion and point-wise multiplication compositional models, since at their current stage of development tensor-based models can only efficiently handle sentences of fixed structure. Nevertheless, the simple point-wise compositional models still al- low for a direct comparison of the vector spaces, which is the main goal of this paper.</p><p>For each vector space and model, a number of different thresholds were tested on the first 2000 pairs of the training set, which we used as a de- velopment set; in each case, the best-performed threshold was selected for a single run of our "classifier" on the test set (1726 pairs). Addition- ally, we evaluate the NWE model with a lemma- tized version of the corpus, so that the experimen- tal setup is maximally similar for all vector spaces. The results are shown in the first part of <ref type="table">Table 5</ref>.</p><p>Additive NWE gives the highest performance, with both lemmatized and un-lemmatized versions outperforming the GS11 and KS14 spaces. In the un-lemmatized case, the accuracy of our sim- ple "classifier" (0.73) is close to state-of-the-art range. The state-of-the art result (0.77 accuracy  <ref type="table">Table 5</ref>: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results significantly outperform corresponding nearest competitors (for accuracy): p &lt; 0.05, χ 2 test. and 0.84 F-score 7 ) by the time of this writing has been obtained using 8 machine translation metrics and three constituent classifiers ( <ref type="bibr" target="#b28">Madnani et al., 2012</ref>). The multiplicative model gives lower results than the additive model across all vector spaces. The KS14 vector space shows the steadiest per- formance, with a drop in accuracy of only 0.04 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experi- enced drops by more than 0.20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dialogue act tagging</head><p>As our last experiment, we evaluate the word spaces on a dialogue act tagging task ( <ref type="bibr" target="#b41">Stolcke et al., 2000</ref>) over the Switchboard corpus <ref type="bibr" target="#b14">(Godfrey et al., 1992)</ref>. Switchboard is a collection of ap- proximately 2500 dialogs over a telephone line by 500 speakers from the U.S. on predefined topics. <ref type="bibr">8</ref> The experiment pipeline follows <ref type="bibr" target="#b33">(Milajevs and Purver, 2014</ref>). The input utterances are prepro- cessed so that the parts of interrupted utterances are concatenated ( <ref type="bibr" target="#b44">Webb et al., 2005</ref>). Disfluency markers and commas are removed from the utter- ance raw texts. For GS11 and KS14 the utterance tokens are POS-tagged and lemmatized; for NWE, we test the vectors in both a lemmatized and an un-lemmatized version of the corpus. <ref type="bibr">9</ref> We split the training and testing utterances as suggested by <ref type="bibr" target="#b41">Stolcke et al. (2000)</ref>. Utterance vectors are then obtained as in the previous experiments; they are reduced to 50 dimensions using SVD and a k- nearest-neighbour classifier is trained on these re- duced utterance vectors (the 5 closest neighbours by Euclidean distance are retrieved to make a clas- 7 F-scores use the standard definition F = 2(precision * recall )/(precision + recall ).</p><p>8 The dataset and a Python interface to it are available at http://compprag.christopherpotts.net/ swda.html <ref type="bibr">9</ref> We use WordNetLemmatizer of the NLTK library <ref type="bibr" target="#b3">(Bird, 2006</ref>). sification decision). The results are shown in the second part of <ref type="table">Table 5</ref>.</p><p>Un-lemmatized NWE addition gave the best ac- curacy (0.63) and F-score (0.60) (averaged over tag classes), i.e. similar results to <ref type="bibr" target="#b33">(Milajevs and Purver, 2014</ref>)-although note that the dimension- ality of our NWE vectors is 10 times lower than theirs. Multiplicative NWE outperformed the cor- responding model in <ref type="bibr" target="#b33">(Milajevs and Purver, 2014)</ref>. In general, addition consistently outperforms mul- tiplication for all the models. Lemmatization dramatically lowers tagging accuracy: the lem- matized GS11, KS14 and NWE models perform much worse than un-lemmatized NWE, suggest- ing that morphological features are important for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Previous comparisons of co-occurrence-based and neural word vector representations vary widely in their conclusions. While <ref type="bibr" target="#b1">Baroni et al. (2014)</ref> conclude that "context-predicting models obtain a thorough and resounding victory against their count-based counterparts", this seems to contra- dict, at least at the first consideration, the more conservative conclusion of  that "analogy recovery is not restricted to neural word embeddings [. . . ] a similar amount of relational similarities can be recovered from traditional dis- tributional word representations" and the findings of Blacoe and Lapata (2012) that "shallow ap- proaches are as good as more computationally in- tensive alternatives" on phrase similarity and para- phrase detection tasks.</p><p>It seems clear that neural word embeddings have an advantage when used in tasks for which they have been trained; our main questions here are whether they outperform co-occurrence based alternatives across the board; and which ap- proach lends itself better to composition using general mathematical operators. To partially an-swer this question, we can compare model be- haviour against the baselines in isolation.</p><p>For the disambiguation and sentence similarity tasks the baseline is the similarity between verbs only, ignoring the context-see above. For the paraphrase task, we take the global vector-based similarity reported in ( <ref type="bibr" target="#b29">Mihalcea et al., 2006</ref>): 0.65 accuracy and 0.75 F-score. For the dialogue act tagging task the baseline is the accuracy of the bag-of-unigrams model in <ref type="bibr" target="#b33">(Milajevs and Purver, 2014</ref>): 0.60.</p><p>Sections 5.1 and 5.2 show that although the best choice of vector representation might vary, for small-scale tasks all methods give fairly compet- itive results. The choice of compositional oper- ator seems to be more important and more task- specific: while a tensor-based operation (Frobe- nius copy-object) performs best for verb disam- biguation, the best result for sentence similarity is achieved by a simple additive model, with all other compositional methods behaving worse than the verb-only baseline in the KS14 case. GS11 and NWE, on the other hand, outperform their base- lines with a number of compositional methods, al- though both of them achieve lower performance than KS14 overall.</p><p>Based on only small-scale experiment results, one could conclude that there is little significant difference between the two ways of obtaining vec- tors. GS11 and NWE show similar behaviour in comparison to their baselines, while it is possible to tune a co-occurrence based vector space (KS14) and obtain the best result. Large scale tasks reveal another pattern: the GS11 vector space, which be- haves stably on the small scale, drags behind the KS14 and NWE spaces in the paraphrase detec- tion task. In addition, NWE consistently yields best results. Finally, only the NWE space was able to provide adequate results on the dialogue act tag- ging task. <ref type="table">Table 6</ref> summarizes model performance with regard to baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we compared the performance of two co-occurrence-based semantic spaces with vectors learned by a neural network in compositional set- tings. We carried out two small-scale tasks (word sense disambiguation and sentence similarity) and two large-scale tasks (paraphrase detection and di- alogue act tagging).  <ref type="table">Table 6</ref>: Summary of vector space performance against baselines. General improvement (cases where more than a half of the models perform bet- ter) and decrease with regard to a corresponding baseline is respectively marked by + and −. A bold value means that the model gave the best re- sult in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GS11 KS14 NWE</head><p>On small-scale tasks, where the sentence struc- tures are predefined and relatively constrained, NWE gives better or similar results to count-based vectors. Tensor-based composition does not al- ways outperform simple compositional operators, but for most of the cases gives results within the same range.</p><p>On large-scale tasks, neural vectors are more successful than the co-occurrence based alterna- tives. However, this study does not reveal whether this is because of their neural nature, or just be- cause they are trained on a larger amount of data.</p><p>The question of whether neural vectors outper- form co-occurrence vectors therefore requires fur- ther detailed comparison to be entirely resolved; our experiments suggest that this is indeed the case in large-scale tasks, but the difference in size and nature of the original corpora may be a confound- ing factor. In any case, it is clear that the neural vectors of word2vec package perform steadily off-the-shelf across a large variety of tasks. The size of the vector space (3 million words) and the available code-base that simplifies the access to the vectors, makes this set a good and safe choice for experiments in the future. Of course, even bet- ter performances can be achieved by training neu- ral language models specifically for a given task (see e.g. <ref type="bibr" target="#b20">Kalchbrenner et al. (2014)</ref>).</p><p>The choice of compositional operator (tensor- based or a simple point-wise operation) depends strongly on the task and dataset: tensor-based composition performed best with the verb dis- ambiguation task, where the verb senses depend strongly on the arguments of the verb. However, it seems to depend less on the nature of the vectors itself: in the disambiguation task, tensor-based composition proved best for both co-occurrence- based and neural vectors; in the sentence similar- ity task, where point-wise operators proved best, this was again true across vector spaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>car − − − → cars Perhaps even more importantly, semantic relation- ships are preserved in a very intuitive way: − − → king − −−→ man ≈ − −− → queen − −−−−→ woman allowing the formation of analogy queries similar to − − → king − −−→ man + −−−−→ woman = ?, obtaining − −− → queen as the result. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Sbj)) Kartsaklis and Sadrzadeh (2014)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>http://www.natcorp.ox.ac.uk/ and Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Compositional methods.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Levy et al. (2014) improved Mikolov et al. (2013c)&apos;s method of retrieving relational similarities by changing the underlying objective function.</note>

			<note place="foot" n="3"> http://wacky.sslmit.unibo.it/ 4 https://code.google.com/p/word2vec/</note>

			<note place="foot" n="5"> This and the sentence similarity dataset are available at http://www.cs.ox.ac.uk/activities/ compdistmeaning/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the three anonymous reviewers for their fruitful comments.</p><p>Sup-port by EPSRC grant EP/F042728/1 is grate-fully acknowledged by Milajevs, Kartsaklis and Sadrzadeh. Purver is partly supported by Con-CreTe: the project ConCreTe acknowledges the fi-nancial support of the Future and Emerging Tech-nologies (FET) programme within the Seventh Framework Programme for Research of the Eu-ropean Commission, under FET grant number 611733.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Interactive presentation sessions</title>
		<meeting>the COLING/ACL on Interactive presentation sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">First-order inference and the interpretation of questions and answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Gabsdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Gotelog</title>
		<meeting>Gotelog</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wide-coverage semantic analysis with boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings, Research in Computational Semantics</title>
		<editor>Johan Bos and Rodolfo Delmonte</editor>
		<imprint>
			<publisher>College Publications</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
	<note>Semantics in Text Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Commutative Algebra: Chapters 1-7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bourbaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Srpinger Verlag</publisher>
			<pubPlace>Berlin/New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<idno>abs/1003.4394</idno>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Minimal recursion semantics: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan A</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="281" to="332" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Microsoft research paraphrase corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-05" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introducing and evaluating ukWaC, a very large web-derived corpus of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google</title>
		<meeting>the 4th Web as Corpus Workshop (WAC-4) Can we beat Google</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gottlob Frege. 1892. On sense and reference</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="563" to="584" />
			<pubPlace>Ludlow</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
<note type="report_type">ICASSP-92</note>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<title level="m">word2vec Explained: deriving Mikolov et al.&apos;s negativesampling word-embedding method</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Experimenting with transitive verbs in a DisCoCat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<idno>Word. 717</idno>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prior disambiguation of word tensors for constructing sentence vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNL)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNL)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1590" to="1601" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A study of entanglement in a categorical framework of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL)</title>
		<meeting>the 11th Workshop on Quantum Physics and Logic (QPL)<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A unified sentence space for categorical distributional-compositional semantics: Theory and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India, December</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012: Posters</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A systematic study of semantic vector space model parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<meeting>the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Solution to Plato&apos;s Problem: The Latent Semantic Analysis Theory of Acquision, Induction, and Representation of Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Claws4: the tagging of the british national corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th conference on Computational linguistics</title>
		<meeting>the 15th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="622" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Corpus-based and knowledge-based measures of text semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="775" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Investigating the contribution of distributional semantic information for dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<meeting>the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Universal grammar. Theoria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Montague</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="373" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving distributional semantic vectors through context selection and normalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ambiguity resolution in natural language learning. csli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="12" to="36" />
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Similarity of semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="416" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dialogue act classification based on intra-utterance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Workshop on Spoken Language Understanding</title>
		<meeting>the AAAI Workshop on Spoken Language Understanding</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
