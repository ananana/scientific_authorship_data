<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing and Hybridizing Count-based and Neural Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizing and Hybridizing Count-based and Neural Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1163" to="1172"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language model-ing exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs) are statistical models that, given a sentence w I 1 := w 1 , . . . , w I , calculate its probability P (w I 1 ). LMs are widely used in applica- tions such as machine translation and speech recog- nition, and because of their broad applicability they have also been widely studied in the literature. The most traditional and broadly used language model- ing paradigm is that of count-based LMs, usually smoothed n-grams <ref type="bibr" target="#b36">(Witten and Bell, 1991;</ref><ref type="bibr" target="#b7">Chen and Goodman, 1996)</ref>. Recently, there has been a fo- cus on LMs based on neural networks ( <ref type="bibr" target="#b24">Nakamura et al., 1990;</ref><ref type="bibr" target="#b4">Bengio et al., 2006;</ref><ref type="bibr" target="#b22">Mikolov et al., 2010)</ref>, which have shown impressive improvements in performance over count-based LMs. On the other hand, these neural LMs also come at the cost of in- creased computational complexity at both training and test time, and even the largest reported neural LMs ( <ref type="bibr" target="#b8">Chen et al., 2015;</ref><ref type="bibr" target="#b35">Williams et al., 2015</ref>) are trained on a fraction of the data of their count-based counterparts ( <ref type="bibr" target="#b5">Brants et al., 2007)</ref>.</p><p>In this paper we focus on a class of LMs, which we will call mixture of distributions LMs (MODLMs; §2). Specifically, we define MODLMs as all LMs that take the following form, calculat- ing the probabilities of the next word in a sentence w i given preceding context c according to a mix- ture of several component probability distributions P k (w i |c):</p><formula xml:id="formula_0">P (w i |c) = K k=1 λ k (c)P k (w i |c).<label>(1)</label></formula><p>Here, λ k (c) is a function that defines the mixture weights, with the constraint that K k=1 λ k (c) = 1 for all c. This form is not new in itself, and widely used both in the calculation of smoothing coeffi- cients for n-gram LMs ( <ref type="bibr" target="#b7">Chen and Goodman, 1996)</ref>, and interpolation of LMs of various varieties <ref type="bibr" target="#b16">(Jelinek and Mercer, 1980)</ref>. The main contribution of this paper is to demon- strate that depending on our definition of c, λ k (c), and P k (w i |c), Eq. 1 can be used to describe not only n-gram models, but also feed-forward ( <ref type="bibr" target="#b24">Nakamura et al., 1990;</ref><ref type="bibr" target="#b4">Bengio et al., 2006;</ref><ref type="bibr" target="#b29">Schwenk, 2007</ref>) and recurrent ( <ref type="bibr" target="#b22">Mikolov et al., 2010;</ref><ref type="bibr" target="#b32">Sundermeyer et al., 2012</ref>) neural network LMs ( §3). This observation is useful theoretically, as it provides a single mathe- matical framework that encompasses several widely used classes of LMs. It is also useful practically, in that this new view of these traditional models allows us to create new models that combine the desirable features of n-gram and neural models, such as: neurally interpolated n-gram LMs ( §4.1), which learn the interpolation weights of n-gram models using neural networks, and neural/n-gram hybrid LMs ( §4.2), which add a count-based n-gram component to neural mod- els, allowing for flexibility to add large-scale external data sources to neural LMs.</p><p>We discuss learning methods for these models ( §5) including a novel method of randomly dropping out more easy-to-learn distributions to prevent the pa- rameters from falling into sub-optimal local minima. Experiments on language modeling benchmarks ( §6) find that these models outperform baselines in terms of performance and convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mixture of Distributions LMs</head><p>As mentioned above, MODLMs are LMs that take the form of Eq. 1. This can be re-framed as the fol- lowing matrix-vector multiplication:</p><formula xml:id="formula_1">p c = D c λ c ,</formula><p>where p c is a vector with length equal to vocabulary size, in which the jth element p c,j corresponds to P (w i = j|c), λ c is a size K vector that contains the mixture weights for the distributions, and D c is a J- by-K matrix, where element d c,j,k is equivalent to the probability P k (w i = j|c). <ref type="bibr">2</ref> An example of this formulation is shown in <ref type="figure">Fig. 1</ref>. Note that all columns in D represent probability distributions, and thus must sum to one over the J words in the vocabulary, and that all λ must sum to 1 over the K distributions. Under this condition, the vector p will represent a well-formed probability distribution as well. This conveniently allows us to <ref type="figure">Figure 1</ref>: MODLMs as linear equations calculate the probability of a single word w i = j by calculating the product of the jth row of D c and λ</p><formula xml:id="formula_2">Probabilities p Coefficients λ                         p 1 = d 1,1 d 1,2 · · · d 1,K λ 1 p 2 d 2,1 d 2,2 · · · d 2,K λ 2 . . . . . . . . . . . . . . . . . . p J d J,1 d J,2 · · · d J,K λ K Distribution matrix D</formula><formula xml:id="formula_3">c P k (w i = j|c) = d c,j λ c .</formula><p>In the sequel we show how this formulation can be used to describe several existing LMs ( §3) as well as several novel model structures that are more power- ful and general than these existing models ( §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Existing LMs as Linear Mixtures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">n-gram LMs as Mixtures of Distributions</head><p>First, we discuss how count-based interpolated n- gram LMs fit within the MODLM framework.</p><p>Maximum likelihood estimation: n-gram mod- els predict the next word based on the previous N -1 words. In other words, we set c = w i−1 i−N +1 and calculate P (w i |w i−1 i−N +1 ). The maximum-likelihood (ML) estimate for this probability is</p><formula xml:id="formula_4">P M L (w i |w i−1 i−N +1 ) = c(w i i−N +1 )/c(w i−1 i−N +1 ),</formula><p>where c(·) counts frequency in the training corpus. Interpolation: Because ML estimation as- signs zero probability to word sequences where c(w i i−N +1 ) = 0, n-gram models often interpolate the ML distributions for sequences of length 1 to N . The simplest form is static interpolation</p><formula xml:id="formula_5">P (w i |w i−1 i−n+1 ) = N n=1 λ S,n P M L (w i |w i−1 i−n+1 ). (2)</formula><p>λ S is a vector where λ S,n represents the weight put on the distribution P M L (w i |w i−1 i−n+1 ). This can be expressed as linear equations <ref type="figure" target="#fig_0">(Fig. 2a)</ref> by set- ting the nth column of D to the ML distribution P M L (w i |w i−1 i−n+1 ), and λ(c) equal to λ S .</p><p>Probabilities p Static interpolation can be improved by calcu- lating λ(c) dynamically, using heuristics based on the frequency counts of the context <ref type="bibr" target="#b11">(Good, 1953;</ref><ref type="bibr" target="#b17">Katz, 1987;</ref><ref type="bibr" target="#b36">Witten and Bell, 1991)</ref>. These meth- ods define a context-sensitive fallback probability α(w i−1 i−n+1 ) for order n models, and recursively cal- culate the probability of the higher order models from the lower order models:</p><formula xml:id="formula_6">Heuristic interp. coefficients λ                         p 1 = d 1,1 d 1,2 · · · d 1,N λ 1 p 2 d 2,1 d 2,2 · · · d 2,N λ 2 . . . . . . . . . . . . . . . . . . p J d J,1 d J,2 · · · d J,N λ N Count-based probabilities P C (w i = j|w i−1 i−n+1 ) (a) Interpolated n-grams as MODLMs Probabilities p Result of softmax(NN(c))                         p 1 = 1 0 · · · 0 λ 1 p 2 0 1 · · · 0 λ 2 . . . . . . . . . . . . . . . . . . p J 0 0 · · · 1 λ J J-</formula><formula xml:id="formula_7">P (w i |w i−1 i−n+1 ) = α(w i−1 i−n+1 )P (w i |w i−1 i−n+2 )+ (1 − α(w i−1 i−n+1 ))P M L (w i |w i−1 i−n+1 ). (3)</formula><p>To express this as a linear mixture, we con- vert α(w i−1 i−n+1 ) into the appropriate value for λ n (w i−1 i−N +1 ). Specifically, the probability assigned to each P M L (w i |w i−1 i−n+1 ) is set to the product of the fallbacks α for all higher orders and the probability of not falling back (1 − α) at the current level:</p><formula xml:id="formula_8">λ n (w i−1 i−N +1 ) = (1−α(w i−1 i−n+1 )) N ˜ n=n+1 α(w i−1 i−˜ n+1 ).</formula><p>Discounting: The widely used technique of dis- counting ( <ref type="bibr" target="#b26">Ney et al., 1994</ref>) defines a fixed discount d and subtracts it from the count of each word before calculating probabilities:</p><formula xml:id="formula_9">P D (w i |w i−1 i−n+1 ) = (c(w i i−n+1 ) − d)/c(w i−1 i−n+1 )</formula><p>. Discounted LMs then assign the remaining probabil- ity mass after discounting as the fallback probability</p><formula xml:id="formula_10">β D (w i−1 i−n+1 ) =1 − J j=1 P D (w i = j|w i−1 i−n+1 ), P (w i |w i−1 i−n+1 ) =β D (w i−1 i−n+1 )P (w i |w i−1 i−n+2 )+ P D (w i |w i−1 i−n+1 ).<label>(4)</label></formula><p>In this case, P D (·) does not add to one, and thus vi- olates the conditions for MODLMs stated in §2, but it is easy to turn discounted LMs into interpolated LMs by normalizing the discounted distribution:</p><formula xml:id="formula_11">P N D (w i |w i−1 i−n+1 ) = P D (w i |w i−1 i−n+1 ) J j=1 P D (w i = j|w i−1 i−n+1 )</formula><p>, which allows us to replace β(·) for α(·) and P N D (·) for P M L (·) in Eq. 3, and proceed as normal.</p><p>Kneser-Ney (KN; <ref type="bibr" target="#b19">Kneser and Ney (1995)</ref>) and Modified <ref type="bibr">KN (Chen and Goodman, 1996</ref>) smooth- ing further improve discounted LMs by adjusting the counts of lower-order distributions to more closely match their expectations as fallbacks for higher or- der distributions. Modified KN is currently the de- facto standard in n-gram LMs despite occasional improvements <ref type="bibr" target="#b34">(Teh, 2006;</ref><ref type="bibr" target="#b10">Durrett and Klein, 2011)</ref>, and we will express it as P KN (·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural LMs as Mixtures of Distributions</head><p>In this section we demonstrate how neural network LMs can also be viewed as an instantiation of the MODLM framework.</p><p>Feed-forward neural network LMs: Feed- forward LMs ( <ref type="bibr" target="#b4">Bengio et al., 2006;</ref><ref type="bibr" target="#b29">Schwenk, 2007)</ref> are LMs that, like n-grams, calculate the prob- ability of the next word based on the previous words. Given context w i−1 i−N +1 , these words are converted into real-valued word representation vec- tors r i−1 i−N +1 , which are concatenated into an over- all representation vector q = ⊕(r i−1 i−N +1 ), where ⊕(·) is the vector concatenation function. q is then run through a series of affine transforms and non- linearities defined as function NN(q) to obtain a vector h. For example, for a one-layer neural net- </p><formula xml:id="formula_12">Probabilities p Result of softmax(NN(c))                         p 1 = d 1,1 d 1,2 · · · d 1,N λ 1 p 2 d 1,2 d 2,2 · · · d 2,N λ 2 . . . . . . . . . . . . . . . . . . p J d J,1 d J,2 · · · d J,N λ N Count-based probabilities P C (w i = j|w i−1 i−n+1 ) (a) Neurally interpolated n-gram LMs Probabilities p Result of softmax(NN(c))                         p 1 = d 1,1 · · · d 1,N 1 · · · 0 λ 1 p 2 d 2,1 · · · d 2,N 0 · · · 0 λ 2 . . . . . . . . . . . . . . . . . . . . . . . . p J d J,1 · · · d J,N 0 · · · 1 λ J+N Count-based</formula><p>where W q and b q are weight matrix and bias vec- tor parameters respectively. Finally, the probabil- ity vector p is calculated using the softmax function p = softmax(hW s + b s ), similarly parameterized. As these models are directly predicting p with no concept of mixture weights λ, they cannot be inter- preted as MODLMs as-is. However, we can per- form a trick shown in <ref type="figure" target="#fig_0">Fig. 2b</ref>, not calculating p di- rectly, but instead calculating mixture weights λ = softmax(hW s + b s ), and defining the MODLM's distribution matrix D as a J-by-J identity matrix. This is equivalent to defining a linear mixture of J Kronecker δ j distributions, the jth of which assigns a probability of 1 to word j and zero to everything else, and estimating the mixture weights with a neu- ral network. While it may not be clear why it is use- ful to define neural LMs in this somewhat round- about way, we describe in §4 how this opens up pos- sibilities for novel expansions to standard models.</p><p>Recurrent neural network LMs: LMs using recurrent neural networks (RNNs) ( <ref type="bibr" target="#b22">Mikolov et al., 2010)</ref> consider not the previous few words, but also maintain a hidden state summarizing the sentence up until this point by re-defining the net in Eq. 5 as</p><formula xml:id="formula_14">RNN(q i ) := tanh(q i W q + h i−1 W h + b q ),</formula><p>where q i is the current input vector and h i−1 is the hidden vector at the previous time step. This allows for consideration of long-distance dependencies be- yond the scope of standard n-grams, and LMs using RNNs or long short-term memory (LSTM) networks <ref type="bibr" target="#b32">(Sundermeyer et al., 2012</ref>) have posted large im- provements over standard n-grams and feed-forward models. Like feed-forward LMs, LMs using RNNs can be expressed as MODLMs by predicting λ in- stead of predicting p directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Novel Applications of MODLMs</head><p>This section describes how we can use this frame- work of MODLMs to design new varieties of LMs that combine the advantages of both n-gram and neural network LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neurally Interpolated n-gram Models</head><p>The first novel instantiation of MODLMs that we propose is neurally interpolated n-gram models, shown in <ref type="figure">Fig. 3a</ref>. In these models, we set D to be the same matrix used in n-gram LMs, but calculate λ(c) using a neural network model. As λ(c) is learned from data, this framework has the potential to allow us to learn more intelligent interpolation functions than the heuristics described in §3.1. In addition, because the neural network only has to calculate a softmax over N distributions instead of J vocabu- lary words, training and test efficiency of these mod- els can be expected to be much greater than that of standard neural network LMs.</p><p>Within this framework, there are several design decisions. First, how we decide D: do we use the maximum likelihood estimate P M L or KN estimated distributions P KN ? Second, what do we provide as input to the neural network to calculate the mixture weights? To provide the neural net with the same information used by interpolation heuristics used in traditional LMs, we first calculate three features for each of the N contexts w i−1 i−n+1 : a binary feature in- dicating whether the context has been observed in the training corpus (c(w i−1 i−n+1 ) &gt; 0), the log fre- quency of the context counts (log(c(w i−1 i−n+1 )) or zero for unobserved contexts), and the log frequency of the number of unique words following the context (log(u(w i−1 i−n+1 )) or likewise zero). When using dis- counted distributions, we also use the log of the sum of the discounted counts as a feature. We can also optionally use the word representation vector q used in neural LMs, allowing for richer representation of the input, but this may or may not be necessary in the face of the already informative count-based features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural/n-gram Hybrid Models</head><p>Our second novel model enabled by MODLMs is neural/n-gram hybrid models, shown in <ref type="figure">Fig. 3b</ref>. These models are similar to neurally interpolated n-grams, but D is augmented with J additional columns representing the Kronecker δ j distributions used in the standard neural LMs. In this construc- tion, λ is still a stochastic vector, but its contents are both the mixture coefficients for the count-based models and direct predictions of the probabilities of words. Thus, the learned LM can use count-based models when they are deemed accurate, and deviate from them when deemed necessary.</p><p>This model is attractive conceptually for several reasons. First, it has access to all information used by both neural and n-gram LMs, and should be able to perform as well or better than both models. Sec- ond, the efficiently calculated n-gram counts are likely sufficient to capture many phenomena nec- essary for language modeling, allowing the neural component to focus on learning only the phenom- ena that are not well modeled by n-grams, requiring fewer parameters and less training time. Third, it is possible to train n-grams from much larger amounts of data, and use these massive models to bootstrap learning of neural nets on smaller datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning Mixtures of Distributions</head><p>While the MODLM formulations of standard heuris- tic n-gram LMs do not require learning, the remain- ing models are parameterized. This section dis- cusses the details of learning these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning MODLMs</head><p>The first step in learning parameters is defining our training objective. Like most previous work on LMs ( <ref type="bibr" target="#b4">Bengio et al., 2006</ref>), we use a negative log- likelihood loss summed over words w i in every sen- tence w in corpus W</p><formula xml:id="formula_15">L(W) = − w∈W w i ∈w log P (w i |c),</formula><p>where c represents all words preceding w i in w that are used in the probability calculation. As noted in Eq. 2, P (w i = j|c) can be calculated efficiently from the distribution matrix D c and mixture func- tion output λ c .</p><p>Given that we can calculate the log likelihood, the remaining parts of training are similar to training for standard neural network LMs. As usual, we per- form forward propagation to calculate the probabili- ties of all the words in the sentence, back-propagate the gradients through the computation graph, and perform some variant of stochastic gradient descent (SGD) to update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Block Dropout for Hybrid Models</head><p>While the training method described in the previ- ous section is similar to that of other neural network models, we make one important modification to the training process specifically tailored to the hybrid models of §4.2. This is motivated by our observation (detailed in §6.3) that the hybrid models, despite being strictly more expressive than the corresponding neural net- work LMs, were falling into poor local minima with higher training error than neural network LMs. This is because at the very beginning of training, the count-based elements of the distribution matrix in <ref type="figure">Fig. 3b</ref> are already good approximations of the tar- get distribution, while the weights of the single-word δ j distributions are not yet able to provide accurate probabilities. Thus, the model learns to set the mix- ture proportions of the δ elements to near zero and rely mainly on the count-based n-gram distributions.</p><p>To encourage the model to use the δ mixture com- ponents, we adopt a method called block dropout ( <ref type="bibr" target="#b0">Ammar et al., 2016)</ref>. In contrast to standard dropout ( <ref type="bibr" target="#b31">Srivastava et al., 2014</ref>), which drops out single nodes or connections, block dropout ran- domly drops out entire subsets of network nodes. In our case, we want to prevent the network from over- using the count-based n-gram distributions, so for a randomly selected portion of the training examples (here, 50%) we disable all n-gram distributions and force the model to rely on only the δ distributions. To do so, we zero out all elements in λ(c) that cor- respond to n-gram distributions, and re-normalize over the rest of the elements so they sum to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Network and Training Details</head><p>Finally, we note design details that were determined based on preliminary experiments.</p><p>Network structures: We used both feed-forward networks with tanh non-linearities and LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>) networks. Most experiments used single-layer 200-node net- works, and 400-node networks were used for ex- periments with larger training data. Word repre- sentations were the same size as the hidden layer. Larger and multi-layer networks did not yield im- provements.</p><p>Training: We used ADAM ( <ref type="bibr" target="#b18">Kingma and Ba, 2015</ref>) with a learning rate of 0.001, and minibatch sizes of 512 words. This led to faster convergence than standard SGD, and more stable optimization than other update rules. Models were evaluated ev- ery 500k-3M words, and the model with the best de- velopment likelihood was used. In addition to the block dropout of §5.2, we used standard dropout with a rate of 0.5 for both feed-forward ( <ref type="bibr" target="#b31">Srivastava et al., 2014</ref>) and LSTM ( <ref type="bibr" target="#b27">Pham et al., 2014</ref>) nets in the neural LMs and neural/n-gram hybrids, but not in the neurally interpolated n-grams, where it re- sulted in slightly worse perplexities.</p><p>Features: If parameters are learned on the data used to train count-based models, they will heav- ily over-fit and learn to trust the count-based distri- butions too much. To prevent this, we performed 10-fold cross validation, calculating count-based el- ements of D for each fold with counts trained on the other 9/10. In addition, the count-based contextual features in §4.1 were normalized by subtracting the training set mean, which improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>In this section, we perform experiments to eval- uate the neurally interpolated n-grams ( §6.2) and neural/n-gram hybrids ( §6.3), the ability of our mod- els to take advantage of information from large data sets ( §6.4), and the relative performance compared PTB Sent Word ASP <ref type="table" target="#tab_0">Sent Word  train  42k 890k train 100k 2.1M  valid 3.4k  70k valid 1.8k  45k  test</ref> 3.8k 79k test 1.8k 46k   <ref type="bibr">4</ref> (details in Tab. 1).</p><p>The PTB corpus uses the standard vocabulary of 10k words, and for the ASPEC corpus we use a vocabu- lary of the 20k most frequent words. Our implemen- tation is included as supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results for Neurally Interpolated n-grams</head><p>First, we investigate the utility of neurally interpo- lated n-grams. In all cases, we use a history of N = 5 and test several different settings for the models: Estimation type: λ(c) is calculated with heuris- tics (HEUR) or by the proposed method using feed- forward (FF), or LSTM nets.</p><p>Distributions: We compare P M L (·) and P KN (·). For heuristics, we use Witten-Bell for ML and the appropriate discounted probabilities for KN.</p><p>Input features: As input features for the neural network, we either use only the count-based features (C) or count-based features together with the word representation for the single previous word <ref type="bibr">(CR)</ref>.</p><p>From the results shown in Tab. 2, we can first see that when comparing models using the same set of input distributions, the neurally interpolated model outperforms corresponding heuristic methods. We can also see that LSTMs have a slight advantage over FF nets, and models using word representa- tions have a slight advantage over those that use only the count-based features. Overall, the best model achieves a relative perplexity reduction of 4- 5% over KN models. Interestingly, even when using simple ML distributions, the best neurally interpo- lated n-gram model nearly matches the heuristic KN method, demonstrating that the proposed model can automatically learn interpolation functions that are nearly as effective as carefully designed heuristics. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results for Neural/n-gram Hybrids</head><p>In experiments with hybrid models, we test a neural/n-gram hybrid LM using LSTM networks with both Kronecker δ and KN smoothed 5-gram distributions, trained either with or without block dropout. As our main baseline, we compare to LSTMs with only δ distributions, which have re- ported competitive numbers on the PTB data set ( <ref type="bibr">Zaremba et al., 2014</ref>). <ref type="bibr">6</ref> We also report results for heuristically smoothed KN 5-gram models, and the best neurally interpolated n-grams from the previous section for reference.</p><p>The results, shown in Tab. 3, demonstrate that similarly to previous research, LSTM LMs (2) achieve a large improvement in perplexity over n- gram models, and that the proposed neural/n-gram hybrid method (5) further reduces perplexity by 10- 11% relative over this strong baseline.</p><p>Comparing models without (4) and with (5) the proposed block dropout, we can see that this method contributes significantly to these gains. To examine this more closely, we show the test perplexity for the <ref type="bibr">5</ref> Neurally interpolated n-grams are also more efficient than standard neural LMs, as mentioned in §4.1. While a standard LSTM LM calculated 1.4kw/s on the PTB data, the neurally in- terpolated models using LSTMs and FF nets calculated 11kw/s and 58kw/s respectively, only slightly inferior to 140kw/s of heuristic KN. <ref type="bibr">6</ref> Note that unlike this work, we opt to condition only on in- sentence context, not inter-sentential dependencies, as training through gradient calculations over sentences is more straight- forward and because examining the effect of cross-boundary information is not central to the proposed method. Thus our baseline numbers are not directly comparable (i.e. have higher perplexity) to previous reported results on this data, but we still feel that the comparison is appropriate.  <ref type="table">Table 3</ref>: PTB/ASPEC perplexities for traditional KN (1) and LSTM LMs (2), neurally interpolated n- grams <ref type="formula">(3)</ref>, and neural/n-gram hybrid models without (4) and with (5) block dropout.  <ref type="formula" target="#formula_0">(1)</ref> standard n-grams, (2) standard LSTMs, (3) neurally interpolated n-grams, and (4) neural/n-gram hybrids on lower frequency words.</p><p>three models using δ distributions in <ref type="figure">Fig. 5</ref>, and the amount of the probability mass in λ(c) assigned to the non-δ distributions in the hybrid models. From this, we can see that the model with block dropout quickly converges to a better result than the LSTM LM, but the model without converges to a worse result, assigning too much probability mass to the dense count-based distributions, demonstrating the learning problems mentioned in §5.2.</p><p>It is also of interest to examine exactly why the proposed model is doing better than the more stan- dard methods. One reason can be found in the be- havior with regards to low-frequency words. In <ref type="figure" target="#fig_2">Fig.  4</ref>, we show perplexities for words that appear n times or less in the training corpus, for n = 10, n = 100, n = 1000 and n = ∞ (all words). From the results, we can first see that if we com- pare the baselines, LSTM language models achieve better perplexities overall but n-gram language mod- els tend to perform better on low-frequency words, corroborating the observations of <ref type="bibr" target="#b8">Chen et al. (2015</ref>  <ref type="figure">Figure 5</ref>: Perplexity and dense distribution ratio of the baseline LSTM LM (1), and the hybrid method without (2) and with (3) block dropout.</p><p>The neurally interpolated n-gram models consis- tently outperform standard KN-smoothed n-grams, demonstrating their superiority within this model class. In contrast, the neural/n-gram hybrid mod- els tend to follow a pattern more similar to that of LSTM language models, similarly with consistently higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results for Larger Data Sets</head><p>To examine the ability of the hybrid models to use counts trained over larger amounts of data, we per- form experiments using two larger data sets: WSJ: The PTB uses data from the 1989 Wall Street Journal, so we add the remaining years be- tween 1987 and 1994 (1.81M sents., 38.6M words).</p><p>GW: News data from the English Gigaword 5th Edition (LDC2011T07, 59M sents., 1.76G words).</p><p>We incorporate this data either by training net pa- rameters over the whole large data, or by separately training count-based n-grams on each of PTB, WSJ, and GW, and learning net parameters on only PTB data. The former has the advantage of training the net on much larger data. The latter has two main ad- vantages: 1) when the smaller data is of a particular domain the mixture weights can be learned to match this in-domain data; 2) distributions can be trained on data such as Google n-grams (LDC2006T13), which contain n-gram counts but not full sentences.</p><p>In the results of <ref type="figure">Fig. 6</ref>, we can first see that the neural/n-gram hybrids significantly outperform the traditional neural LMs in the scenario with larger data as well. Comparing the two methods for in- corporating larger data, we can see that the results are mixed depending on the type and size of the data  <ref type="figure">Figure 6</ref>: Models trained on PTB (1,2), PTB+WSJ (3,4,5) or PTB+WSJ+GW (6,7,8) using standard neural LMs <ref type="figure">(1,3,6</ref>), neural/n-gram hybrids trained all data (2,4,7), or hybrids trained on PTB with ad- ditional n-gram distributions (5,8).</p><formula xml:id="formula_16">(1) d/p (2) KN+d/p (3) d/w (4) KN+d/w (5) KN+d/p +wLM (6) d/g (7) KN+d/g (8) KN+d/p +gLM</formula><p>being used. For the WSJ data, training on all data slightly outperforms the method of adding distribu- tions, but when the GW data is added this trend re- verses. This can be explained by the fact that the GW data differs from the PTB test data, and thus the effect of choosing domain-specific interpolation coefficients was more prominent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with Static Interpolation</head><p>Finally, because the proposed neural/n-gram hybrid models combine the advantages of neural and n- gram models, we compare with the more standard method of training models independently and com- bining them with static interpolation weights tuned on the validation set using the EM algorithm. <ref type="table">Tab. 4</ref> shows perplexities for combinations of a standard neural model (or δ distributions) trained on PTB, and count based distributions trained on PTB, WSJ, and GW are added one-by-one using the standard static and proposed LSTM interpolation methods. From the results, we can see that when only PTB data is used, the methods have similar results, but with the more diverse data sets the proposed method edges out its static counterpart. <ref type="bibr">7</ref> Interp δ+PTB +WSJ +GW Lin.</p><p>95.1 70.5 65.8 LSTM 95.3 68.3 63.5 <ref type="table">Table 4</ref>: PTB perplexity for interpolation between neural (δ) LMs and count-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>A number of alternative methods focus on interpo- lating LMs of multiple varieties such as in-domain and out-of-domain LMs ( <ref type="bibr" target="#b6">Bulyko et al., 2003;</ref><ref type="bibr" target="#b2">Bacchiani et al., 2006;</ref><ref type="bibr" target="#b12">Gülçehre et al., 2015)</ref>. Perhaps most relevant is Hsu (2007)'s work on learning to interpolate multiple LMs using log-linear models. This differs from our work in that it learns functions to estimate the fallback probabilities α n (c) in Eq. 3 instead of λ(c), and does not cover interpolation of n-gram components, non-linearities, or the connec- tion with neural network LMs. Also conceptually similar is work on adaptation of n-gram LMs, which start with n-gram probabilities <ref type="bibr" target="#b9">(Della Pietra et al., 1992;</ref><ref type="bibr" target="#b20">Kneser and Steinbiss, 1993;</ref><ref type="bibr" target="#b28">Rosenfeld, 1996;</ref><ref type="bibr" target="#b15">Iyer and Ostendorf, 1999</ref>) and adapt them based on the distribution of the current document, albeit in a linear model. There has also been work incorpo- rating binary n-gram features into neural language models, which allows for more direct learning of n- gram weights <ref type="bibr" target="#b23">(Mikolov et al., 2011</ref>), but does not af- ford many of the advantages of the proposed model such as the incorporation of count-based probability estimates. Finally, recent works have compared n- gram and neural models, finding that neural models often perform better in perplexity, but n-grams have their own advantages such as effectiveness in extrin- sic tasks ( <ref type="bibr" target="#b3">Baltescu and Blunsom, 2015)</ref> and better modeling of rare words <ref type="bibr" target="#b8">(Chen et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper, we proposed a framework for lan- guage modeling that generalizes both neural net- work and count-based n-gram LMs. This allowed us to learn more effective interpolation functions for count-based n-grams, and to create neural LMs that incorporate information from count-based models.</p><p>As the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such as neural machine translation  or dialog response generation ( <ref type="bibr" target="#b30">Sordoni et al., 2015)</ref>. In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interpretations of existing models as mixtures of distributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: Two new expansions to n-gram and neural LMs made possible in the MODLM framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Perplexities of (1) standard n-grams, (2) standard LSTMs, (3) neurally interpolated n-grams, and (4) neural/n-gram hybrids on lower frequency words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Data sizes for the PTB and ASPEC corpora.</head><label>1</label><figDesc></figDesc><table>Dst./Ft. 
HEUR 
FF 
LSTM 
ML/C 
220.5/265.9 146.6/164.5 144.4/162.7 
ML/CR 
-
145.7/163.9 142.6/158.4 
KN/C 
140.8/156.5 138.9/152.5 136.8/151.1 
KN/CR 
-
136.9/153.0 135.2/149.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>PTB/ASPEC perplexities for traditional 
heuristic (HEUR) and proposed neural net (FF or 
LSTM) interpolation methods using ML or KN dis-
tributions, and count (C) or count+word representa-
tion (CR) features. 

to post-facto static interpolation of already-trained 
models ( §6.5). For the main experiments, we evalu-
ate on two corpora: the Penn Treebank (PTB) data 
set prepared by Mikolov et al. (2010), 3 and the first 
100k sentences in the English side of the ASPEC 
corpus (Nakazawa et al., 2015) </table></figure>

			<note place="foot" n="1"> Work was performed while GN was at the Nara Institute of Science and Technology and CD was at Carnegie Mellon University. Code and data to reproduce experiments is available at http://github.com/neubig/modlm</note>

			<note place="foot" n="2"> We omit the subscript c when appropriate.</note>

			<note place="foot" n="3"> http://rnnlm.org/simple-examples.tgz 4 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/</note>

			<note place="foot" n="7"> In addition to better perplexities, neural/n-gram hybrids are trained in a single pass instead of performing post-facto interpolation, which may give advantages when training for other objectives (Auli and Gao, 2014; Li et al., 2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Kevin Duh, Austin Matthews, Shinji Watanabe, and anonymous reviewers for valuable comments on earlier drafts. This work was sup-ported in part by JSPS KAKENHI Grant Number 16H05873, and the Program for Advancing Strate-gic International Networks to Accelerate the Circu-lation of Talented Researchers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">One parser, many languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1602.01595</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Decoder integration and expected bleu training for recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Map adaptation of stochastic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="68" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pragmatic neural language modelling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="820" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large language models in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Getting more mileage from web text sources for conversational speech language modeling using classdependent mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bulyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT</title>
		<meeting>HLT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="7" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<title level="m">Strategies for Training Large Vocabulary Neural Language Models</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive language modeling using minimum discriminant estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Della</forename><surname>Stephen Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="103" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical investigation of discounting in cross-domain language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The population frequencies of species and the estimation of population parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="237" to="264" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized linear interpolation of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="136" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling long distance dependence in language: Topic mixtures versus dynamic cache models. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rukmini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpolated estimation of markov source parameters from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on pattern recognition in practice</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the dynamic adaptation of stochastic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Steinbiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="586" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno>abs/1510.03055</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. InterSpeech</title>
		<meeting>InterSpeech</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`nock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernock`cernock`y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural network approach to word category prediction for English texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuteru</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Kawabata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overview of the 2nd Workshop on Asian Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideya</forename><surname>Mino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WAT</title>
		<meeting>WAT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On structuring probabilistic dependences in stochastic language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Vu Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICFHR</title>
		<meeting>ICFHR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to adaptive statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="228" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. InterSpeech</title>
		<meeting>InterSpeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Bayesian interpretation of interpolated Kneser-Ney</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>School of Computing, National Univ. of Singapore</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scaling recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjani</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mrva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">C</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1085" to="1094" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
