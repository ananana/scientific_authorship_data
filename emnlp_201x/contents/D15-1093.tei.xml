<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Timeline Difference for Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
							<email>fukumoto@yamanashi.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate Faculty of Interdisciplinary Research</orgName>
								<orgName type="laboratory">Graduate Faculty of Interdisciplinary Research Univ. of Yamanashi</orgName>
								<orgName type="institution">Univ. of Yamanashi</orgName>
								<address>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimi</forename><surname>Suzuki</surname></persName>
							<email>ysuzuki@yamanashi.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate Faculty of Interdisciplinary Research</orgName>
								<orgName type="laboratory">Graduate Faculty of Interdisciplinary Research Univ. of Yamanashi</orgName>
								<orgName type="institution">Univ. of Yamanashi</orgName>
								<address>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Timeline Difference for Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper addresses text categorization problem that training data may derive from a different time period from the test data. We present a learning framework which extends a boosting technique to learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-the-art biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text categorization supports and improves several tasks such as creating digital libraries, informa- tion retrieval, and even helping users to interact with search engines ( <ref type="bibr" target="#b18">Mourao et al., 2008)</ref>. A growing number of machine learning (ML) tech- niques have been applied to the text categorization task ( <ref type="bibr" target="#b26">Xue et al., 2008;</ref><ref type="bibr" target="#b9">Gopal and Yang, 2010)</ref>. Each document is represented using a vector of features/terms <ref type="bibr" target="#b27">(Yang and Pedersen, 1997;</ref><ref type="bibr" target="#b10">Hassan et al., 2007)</ref>. Then, the documents with category label are used to train classifiers. Once category models are trained, each test document is classi- fied by using these models. A basic assumption in the categorization task is that the distributions of terms between training and test documents are identical. When the assumption does not hold, the classification accuracy is worse. However, it is often the case that the term distribution in the training data is different from that of the test data when the training data may drive from a different time period from the test data. Manual annotation of tagged new data is very expensive and time- consuming. The methodology for accurate clas- sification of the new test data by making the max- imum use of tagged old data is needed in learning techniques.</p><p>In this paper, we present a method for text cat- egorization that minimizes the impact of tempo- ral effects. Our approach extends a boosting tech- nique to learn accurate model for timeline adap- tation. We used two types of labeled training data: One is the same creation time period with the test data. Another is different creation time period with the test data. We call the former same- period training, and the latter diff-period train- ing data. For the same-period training data, the learner shows the same behavior as the boosting. In contrast, for diff-period training data, once they are wrongly predicted by the learned model, these data would be useless to classify test data. We decreased the weights of these data by applying Gaussian function in order to weaken their im- pacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The analysis of temporal aspects is a practical problem as well as the process of large-scale heterogeneous data since the World-Wide Web (WWW) is widely used by various sorts of peo- ple. It is widely studied in many text process- ing tasks. One attempt is concept or topic drift dealing with temporal effects <ref type="bibr" target="#b14">(Klinkenberg and Joachims, 2000;</ref><ref type="bibr" target="#b13">Kleinberg, 2002;</ref><ref type="bibr" target="#b15">Lazarescu et al., 2004;</ref><ref type="bibr" target="#b6">Folino et al., 2007;</ref><ref type="bibr" target="#b23">Song et al., 2014</ref>). Wang et al. developed the continuous time dy- namic topic model (cDTM) ( ). He et al. proposed a method to find bursts, peri- ods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates <ref type="bibr" target="#b11">(He and Parker, 2010)</ref>. They used Moving Aver- age Convergence/Divergence (MACD) histogram which was used in technical stock market analysis <ref type="bibr" target="#b19">(Murphy, 1999</ref>) to detect bursts.</p><p>Another attempt is domain adaptation. The goal of this attempt is to develop learning algorithms that can be easily ported from one domain to an- other <ref type="bibr" target="#b3">(Daumé III, 2007;</ref><ref type="bibr">Sparinnapakorn and Ku-bat, 2007;</ref><ref type="bibr" target="#b8">Glorot et al., 2011;</ref><ref type="bibr" target="#b22">Siao and Guo, 2013)</ref>. Domain adaptation is particularly inter- esting in Natural Language Processing (NLP) be- cause it is often the case that we have a collec- tion of labeled data in one domain but truly de- sire a model that can work well for another do- main. Lots of studies addressed domain adapta- tion in NLP tasks such as part-of-speech tagging <ref type="bibr" target="#b22">(Siao and Guo, 2013)</ref>, named-entity <ref type="bibr" target="#b3">(Daumé III, 2007)</ref>, and sentiment classification <ref type="bibr" target="#b8">(Glorot et al., 2011</ref>) are presented. One approach to domain adaptation is to use transfer learning. The transfer learning is a learning technique that retains and ap- plies the knowledge learned in one or more tasks to efficiently develop an effective hypothesis for a new task. The earliest discussion is done by ML community in a NIPS-95 workshop 1 , and more re- cently, transfer learning techniques have been suc- cessfully applied in many applications. Blitzer et al. proposed a method for sentiment classi- fication using structural correspondence learning that makes use of the unlabeled data from the tar- get domain to extract some relevant features that may reduce the difference between the domains ( <ref type="bibr" target="#b0">Blitzer et al., 2006</ref>). Several authors have at- tempted to learn classifiers across domains us- ing transfer learning in the text classification task ( <ref type="bibr" target="#b20">Raina et al., 2006;</ref><ref type="bibr" target="#b2">Dai et al., 2007;</ref><ref type="bibr" target="#b24">Sparinnapakorn and Kubat, 2007)</ref>. Raina et al. proposed a transfer learning algorithm that constructs an in- formative Bayesian prior for a given text classi- fication task ( <ref type="bibr" target="#b20">Raina et al., 2006</ref>). They reported that a 20 to 40% test error reduction over a com- monly used prior in the binary text classification task. <ref type="bibr">Dai et al. presented</ref> a method called TrAd- aBoost which extends boosting-based learning al- gorithms <ref type="bibr" target="#b2">(Dai et al., 2007)</ref>. Their experimental results show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new one. All of these approaches aimed at utilizing a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. However, the temporal effects are not explicitly incorporated into their models.</p><p>To our knowledge, there have been only a few previous work on temporal-based text categoriza- tion. Mourao et al. investigated the impact of temporal evolution of document collections on the document classification ( <ref type="bibr" target="#b18">Mourao et al., 2008)</ref>. <ref type="bibr">Salles et al. presented</ref> an approach to classify doc- uments in scenarios where the method uses infor- mation about both the past and the future, and this information may change over time ( <ref type="bibr" target="#b21">Salles et al., 2010)</ref>. They address the drawbacks of which in- stances to select by approximating the Temporal Weighting Function (TWF) using a mixture of two Gaussians. However, their method needs tagged training data across full temporal range of training documents to construct TWF.</p><p>There are three novel aspects in our method. Firstly, we propose a method for text categoriza- tion that minimizes the impact of temporal effects in a learning technique. Secondly, from manual annotation of data perspective, the method allows users to annotate only a limited number of newly training data. Finally, from the perspective of ro- bustness, the method is automated, and can be applied easily to a new domain, or different lan- guages, given sufficient old labeled documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Timeline Difference</head><p>Our learning model, Timeline Adaptation by Boosting (TABoost) is based on AdaBoost (Fre- und and <ref type="bibr" target="#b7">Schapire, 1997)</ref>. AdaBoost aims to boost the accuracy of a weak learner by adjusting the weights of training instances and learn a classi- fier accordingly. The TABoost uses two types of training data, same-period and diff-period train- ing data. The assumption is that the quantity of the same-period data is limited, while diff-period training data is abundant. The TABoost aims at utilizing the diff-period training data to make up the deficit of a small amount of the same-period to construct a high-quality classification model for the test data. Similar to the TrAdaBoost presented by <ref type="bibr" target="#b2">(Dai et al., 2007)</ref>, TABoost is the same behav- ior as boosting for the same-period training data. In contrast, once diff-period training instances are wrongly predicted, we assume that these instances do not contribute to the accurate test data classifi- cation, and the weights of these instances decrease in order to weaken their impacts. The difference between TrAdaBoost and TABoost is a weight- ing manner, i.e. TABoost is a continuous time- line model, and it weights these instances by ap- plying Gaussian function in order to weaken their impacts. TABoost is illustrated in <ref type="figure">Figure 1</ref>.</p><p>The training data set T r is partitioned into two labeled sets T r dp , and T r sp . T r dp in <ref type="figure">Figure 1</ref> Input { The diff-period data T r dp , the same- period data T r sp , and the maximum number of iterations N . }</p><formula xml:id="formula_0">Output { h f (x) = N t=1 α t h t (x i ). } Initialization { w 1 = (w 1 1 , · · · , w 1 n+m ). } TABoost { For t = 1,· · ·,N 1. Set P t = w t / ( n+m k=1 w t i ). 2.</formula><p>Train a weak learner on the combined training set T r dp and T r sp with the distribution P t , and create weak hy- pothesis h t : X → {−1, +1} 3. Calculate the error of h t on the com- bined training set T r dp and T r sp :</p><formula xml:id="formula_1">t = n+m i=1 w t i ·|ht(x i )−c(x i )| n+m i=1 w t i . 4. Chose α t = 1 2 In( 1−t t ) 5.</formula><p>Update the new weight vector: <ref type="figure">Figure 1</ref>: Flow of the algorithm shows the diff-period training data that T r dp = {(x dp i , c(x dp i ))}, where x dp i ∈ X dp (i = 1, · · ·, n), and X dp refers to the diff-period instance space. Similarly, T r sp represents the same-period train- ing data that T r sp = {(x sp i , c(x sp i ))}, where x sp i ∈ X sp (i = 1, · · ·, m), and X sp refers to the same- period instance space. n and m are the number of documents in T r dp and T r sp , respectively. c(x i ) returns a label for the input instance x i . The com- bined training set T r = {(x i ,c(x i ))} is given by:</p><formula xml:id="formula_2">w t+1 i = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ w t i exp F (δ)αt|ht(x i )−c(x i )|, 1 ≤ i ≤ n w t i exp −αt|ht(x i )−c(x i )|, n + 1 ≤ i ≤ n + m }</formula><formula xml:id="formula_3">x i = x dp i i = 1, · · · , n x sp i i = n + 1, · · · , n + m</formula><p>In each iteration round shown in <ref type="figure">Figure 1</ref>, if a diff-period training instance is wrongly predicted, the instance may be useless to classify test data correctly. We decrease its training weight to re- duce the effect. To do this, we assume a standard lognormal distribution <ref type="bibr" target="#b1">(Crow, 1988)</ref></p><formula xml:id="formula_4">, i.e. F (δ) = 1 √ 2π exp(− δ 2</formula><p>2 ). δ in F (δ) represents time dif- ference between diff-period training and test data. For instance, if the training data is 1999, and test data is 2000, δ equals to 1. Similarly, if the train- ing data is 2000, and test data is 1999, δ is −1. The greater the time difference value, the smaller the training weight. We fit the model according to the temporal range of the data. As shown in <ref type="figure">Figure 1</ref>, we decrease its training weight w t i to re- duce its effect through multiplying its weight by</p><formula xml:id="formula_5">exp F (δ)αt|ht(x i )−c(x i )| .</formula><p>We used the Support Vector Machines (SVM) as a learner. We represented each training and test document as a vector, each dimension of a vec- tor is a noun word appeared in the document, and each element of the dimension is a term frequency. We applied the algorithm shown in <ref type="figure">Figure 1</ref>. Af- ter several iterations, a learner model is created by linearly combining weak learners, and a test doc- ument is classified by using a learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated our TABoost by using the Mainichi Japanese newspaper documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We choose the Mainichi Japanese newspaper corpus from 1991 to 2012. The corpus con- sists of 2,883,623 documents organized into 16 categories. We selected 8 categories, "Inter- national(Int)", "Economy(Eco)", "Home", "Cul- ture", "Reading", "Arts", "Sports", and "Local news(Local)", each of which has sufficient num- ber of documents. All documents were tagged by using a morphological analyzer Chasen <ref type="bibr" target="#b17">(Matsumoto et al., 2000</ref>) and selected noun words. The total number of documents assigned to these cate- gories are 787,518. For each category within each year, we divided documents into three folds: 2% of documents are used as the same-period training data, 50% of documents are the diff-period train- ing data, and the remains are used to test our clas- sification method. 2 We used LIBLINEAR <ref type="bibr" target="#b5">(Fan et al., 2008</ref>) as a basic learner in the experiments. We compared our method, TABoost with four baselines: (1) TABoost with the same-period training data only (TAB s), (2) SVM, (3) TrAdaBoost ( <ref type="bibr" target="#b2">Dai et al., 2007)</ref>, and (4) biased-SVM ( <ref type="bibr" target="#b16">Liu et al., 2003)</ref> by SVM-light <ref type="bibr" target="#b12">(Joachims, 1998)</ref>. TAB s is the same behavior as boosting. TrAdaBoost (TrAdaB) is presented by <ref type="bibr" target="#b2">(Dai et al., 2007)</ref>. Biased-SVM (b-SVM) is known as the state-of-the-art SVMs method, and often used for comparison <ref type="bibr" target="#b4">(Elkan and Noto, 2008)</ref>. Similar to SVM, for biased-SVM, we used the first two folds as a training data, and classified test documents directly, i.e. we used closed data. We empirically selected values of two parameters, "c" (trade-off between training er- ror and margin) and "j", i.e. cost (cost-factor, by which training errors on positive instances) that optimized result obtained by classification of test documents. Similar to ( <ref type="bibr" target="#b16">Liu et al., 2003)</ref>, "c" is searched in steps of 0.02 from 0.01 to 0.61. "j" is searched in steps of 5 from 1 to 200. As a re- sult, we set c and j to 0.01 and 10, respectively. To make comparisons fair, all five methods including our method are based on linear kernel. Throughout the experiments, the number of iterations is set to 100. We used error rate as an evaluation measure <ref type="bibr" target="#b2">(Dai et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Categorization results for 8 categories (48% of the test documents, i.e. 378,008 documents) are shown in <ref type="table" target="#tab_0">Table 1</ref>. Each value in <ref type="table" target="#tab_0">Table 1</ref> shows macro-averaged error rate across 22 years. "M- Avg" refers to macro-averaged error rate across categories. The results obtained by biased-SVM show minimum error rate obtained by varying the parameters, "c" and "j".</p><p>As can be seen clearly from Temporal Distance "SVM" "TrAdaB" "bSVM" "TAB"</p><p>Figure 2: Performance against temporal distance performance obtained by TAB was the best among the five methods. The macro average error rates with TrAdaB and TAB were lower to those ob- tained by b-SVM, although b-SVM in <ref type="table" target="#tab_0">Table 1</ref> was the result obtained by using the closed data. In contrast, SVM did not work well. This demon- strates that once the training data drive from a dif- ferent time period from the test data, the distri- butions of terms between training and test docu- ments are not identical. The results obtained by TAB s were worse than those obtained by TrAd- aBoost, b-SVM, and TAB. This shows that (i) the same-period training data we used is not sufficient to train a model alone, and (ii) TAB demonstrates a good transfer ability. <ref type="figure">Figure 2</ref> illustrates error rate against the tempo- ral difference between diff-period training and test data. Both training and test data are the documents from 1991 to 2012. For instance, "10" of the x- axis in <ref type="figure">Figure 2</ref> indicates that the test documents are created 10 years later than the training docu- ments. We can see from <ref type="figure">Figure 2</ref> that the result obtained by TAB was the best in all of the tempo- ral distances. There are no significant differences among three methods, bSVM, TrAdaB, and TAB when the test and training data are the same time period. The performance of these methods includ- ing SVM drops when the creation time of the test data differs greatly from the diff-period training data. However, the performance of TAB was still better to those obtained by other methods. This demonstrates that the algorithm with continuous timeline model works well for categorization. <ref type="figure">Figure 3</ref> shows the error rate against the number of iterations. Each curve shows averaged error rate under time period between same-period and diff- Number of Iterations "diff_1" "diff_5" "diff_10" "diff_15" "diff_21" <ref type="figure">Figure 3</ref>: Iteration curves by temporal distance period training data. For example, "diff 10" indi- cates that the difference time period between same and diff training data is ± 10 years. We can see from <ref type="figure">Figure 3</ref> that all curves except for "diff 20" drop rapidly and converge around 10 iterations. "diff 20" converges around 20 iterations. This was the same behaviour as TrAdaBoost, i.e. TrAd- aBoost converges around 20 iterations. The fast convergence is not particularly surprising because we used a small number of same-period (2%) and a large number of diff-period (50%) training data. It is necessary to examine how the ratio between same-period and diff-period training data affects overall performance for further quantitative eval- uation, although a main contribution of TAB is in situations using by both of a small amount of la- beled new data which is not sufficient to train a model alone, and a large amount of old data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a method for text catego- rizaiton that minimizes the impact of temporal ef- fects. The results using Japanese Mainichi News- paper corpus show that it works well for cate- gorization, especially when the creation time of the test data differs greatly from the training data. There are a number of interesting directions for future work. The rate of convergence of TAB (O( Inn/N )) is slow which can also be found in <ref type="bibr" target="#b2">(Dai et al., 2007)</ref>. Here, n is the number of train- ing data, and N is the number of iterations. In the future, we will try to extend the framework to address this issue. We used Japanese newspaper documents in the experiments. For quantitative evaluation, we need to apply our mehtod to other data such as ACM-DL and a large, heterogeneous collection of web content in addition to the experi- ment to examine the performance agasint the ratio between same-period and diff-period training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : The error rates across categories</head><label>1</label><figDesc></figDesc><table>Cat 
TAB s SVM TrAdaB b-SVM 
TAB 
Int 
0.409 0.467 
0.326 
0.253 0.329 
Eco 
0.368 0.429 
0.243 
0.228 0.208 
Home 
0.475 0.649 
0.312 
0.460 0.172 
Culture 
0.468 0.848 
0.440 
0.559 0.196 
Reading 
0.358 0.520 
0.298 
0.357 0.337 
Arts 
0.402 0.684 
0.330 
0.588 0.331 
Sports 
0.226 0.212 
0.107 
0.075 0.123 
Local 
0.586 0.305 
0.400 
0.156 0.303 
M-Avg 
0.411 0.514 
0.307 
0.334 0.257 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 , the overall</head><label>1</label><figDesc></figDesc><table>0.15 

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

0.5 

0.55 

0.6 

0.65 

0.7 

-25 
-20 
-15 
-10 
-5 
0 
5 
10 
15 
20 
25 

Error Rate 

</table></figure>

			<note place="foot" n="1"> http://socrates.acadiau.ca/courses/comp/dsilver/ NIPS95 LTL/transfer.workshop.1995.html.</note>

			<note place="foot" n="2"> When the creation time period of the training data is the same as the test data, we used only the same-period training data.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Structural Correspondence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Log-normal Distributions: Theory and Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Crow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Dekker</publisher>
			<pubPlace>NewYork</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boosting for Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th International Conference on Machine Learning</title>
		<meeting>of the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 45th Annual Meeting of the Association of computational Linguistics</title>
		<meeting>of the 45th Annual Meeting of the Association of computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Classifiers from Only Positive and Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the KDD&apos;08</title>
		<meeting>of the KDD&apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Adaptive Distributed Ensemble Approach to Mine Concept-drifting Data Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Folino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pizzuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Spezzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th IEEE International Conference on Tools with Artificial Intelligence</title>
		<meeting>of the 19th IEEE International Conference on Tools with Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A DecisionTheoretic Generalization of On-Line Learning and an Application to Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th International Conference on Machine Learning</title>
		<meeting>of the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilabel Classification with Meta-level Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RandomWalk Term Weighting for Improved Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nanea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Semantic Computing</title>
		<meeting>of the IEEE International Conference on Semantic Computing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topic Dynamics: An Alternative Model of Bursts in Streams of Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th ACM SIGKDD Conference on Knowledge discovery and Data Mining</title>
		<meeting>of the 16th ACM SIGKDD Conference on Knowledge discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SVM Light Support Vector Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dept. of Computer Science Cornell University</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bursty and Hierarchical Structure in Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting Concept Drift with Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klinkenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International Conference on Machine Learning</title>
		<meeting>of the 17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Using Multiple Windows to Track Concept Drift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Lazarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="59" />
		</imprint>
	</monogr>
	<note>Intelligent Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building Text Classifiers using Positive and Unlabeled Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICDM&apos;03</title>
		<meeting>of the ICDM&apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Japanese Morphological Analysis System Chasen Version 2.2.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naist Technical Report</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding Temporal Aspects in Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mourao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Couto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st ACM International Conference on Web Search and Data Mining</title>
		<meeting>of the 1st ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Technical Analysis of the Financial Markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constructing Informative Priors using Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd International Conference on Machine Learning</title>
		<meeting>of the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="713" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporally-aware Algorithms for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Pappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 30th International Conference on Machine Learning</title>
		<meeting>of the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing topic evolution in bioinformatics: Investigation of dynamics of the field with conference data in dblp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="397" to="428" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining Subclassifiers in Text Categorization: A DST-based Solution and a Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sparinnapakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous Time Dynamic Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>of the 24th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Topicbridged PLSA for Cross-Domain Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Comparative Study on Feature Selection in Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th International Conference on Machine Learning</title>
		<meeting>of the 14th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
