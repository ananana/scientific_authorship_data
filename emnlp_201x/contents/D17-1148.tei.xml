<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Dahlmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Inc. Kasernenstr</orgName>
								<address>
									<addrLine>25</addrLine>
									<postCode>52064</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Inc. Kasernenstr</orgName>
								<address>
									<addrLine>25</addrLine>
									<postCode>52064</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Petrushkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Inc. Kasernenstr</orgName>
								<address>
									<addrLine>25</addrLine>
									<postCode>52064</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">eBay Inc. Kasernenstr</orgName>
								<address>
									<addrLine>25</addrLine>
									<postCode>52064</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1411" to="1420"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German→English news domain and English→Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong NMT baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation has become state-of- the-art in recent years, reaching higher transla- tion quality than statistical phrase-based machine translation (PBMT) on many tasks. Human anal- ysis ( <ref type="bibr" target="#b2">Bentivogli et al., 2016)</ref> showed that NMT makes significantly fewer reordering errors, and also is able to select correct word forms more of- ten than PBMT in the case of morphologically rich target languages. Overall, the fluency of the MT output improves when NMT is used, and the number of lexical choice errors is also reduced. However, state-of-the-art NMT approaches based on an encoder-decoder architecture with an atten- tion mechanism as introduced by ( ) exhibit weaknesses that sometimes lead to MT errors which a phrase-based MT system does not make. In particular, PBMT usually can better translate rare words (e.g. singletons), as well as memorize and use phrasal translations. NMT has problems translating rare words because of limi- tations on the vocabulary size, as well as the fact that word embeddings are used to represent both source and target words. A rare word's embedding can not be trained reliably.</p><p>Another handicap of NMT is a general diffi- culty of fixing errors made by a neural MT sys- tem. Since NMT does not explicitly use or save word-to-word or phrase-to-phrase mappings, and its search is a target word beam search with al- most no constraints, it is difficult to fix errors by an NMT system. It is important to quickly fix cer- tain errors in real-life applications of MT systems to avoid negative user feedback or other (e.g. le- gal) consequences. An error identified in the out- put of a PBMT system can be fixed by tracing which phrase pair was used that resulted in the error, and down-weighting or even removing the phrase pair. Also, in PBMT it is easy to add an "override" translation.</p><p>In this work, we combine the strengths of NMT and PBMT approaches by introducing a novel hy- brid search algorithm. In this algorithm, the stan- dard NMT beam search is extended with phrase translation hypotheses from a statistical phrase ta- ble. The decision on when to use what phrasal translations is taken based on the attention mech- anism of the NMT model, which provides a soft coverage of the source sentence words. All par- tial phrasal translations are scored with the NMT decoder and can be continued with a word-based NMT translation candidate or another phrasal translation candidate.</p><p>The proposed search algorithm uses a log-linear model in which the NMT translation score is com- bined with standard phrase translation scores, in- cluding a target n-gram language model (LM) score. Thus, a LM trained on additional monolin- gual data can be used. The decisions on the word order in the produced target translation are taken based only on the states of the NMT decoder.</p><p>This paper is structured as follows. We review related work in Section 1.1. The baseline NMT model we use is described in Section 2, where we also recap the log-linear model combination used in PBMT. Section 3 presents the details of the pro- posed hybrid search. Experimental results are pre- sented in Section 4, followed by conclusions and outlook in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>In the line of research closely related to our ap- proach, neural models are used as additional fea- tures in vanilla phrase-based systems. Examples include the work of <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>), <ref type="bibr">(JunczysDowmunt et al., 2016)</ref>, etc. Such approaches have certain limitations: first, the search space of the model is still restricted by what can be produced using a phrase table extracted from parallel data based on word alignments. Second, the organiza- tion of the search, in which only a limited target word history (e.g. 4 last target words) is avail- able for each partial hypothesis, makes it diffi- cult to integrate recurrent neural network LMs and translation models which take all previously gen- erated target words into account. That is why, for instance, the attention-based NMT models were usually applied only in rescoring ( <ref type="bibr">Peter et al., 2016)</ref>.</p><p>In ( <ref type="bibr">Stahlberg et al., 2017</ref>), a two-step transla- tion process is used, where in the first step a SMT translation lattice is generated, and in the second step the NMT decoder combines NMT scores with the Bayes-risk of the translations according to the lattice. In contrast, we explicitly use phrasal trans- lations and language model scores in an integrated search.</p><p>In <ref type="bibr" target="#b0">(Arthur et al., 2016</ref>), a statistical word lex- icon is used to influence NMT hypotheses, also based on the attention mechanism. ( <ref type="bibr">Gülçehre et al., 2015</ref>) combine target n-gram LM scores with NMT scores to find the best translation. ( <ref type="bibr" target="#b8">He et al., 2016</ref>) also use a target LM, but add fur- ther SMT features such as word penalty and word lexica to the NMT beam search. To the best of our knowledge, no previous work extends the beam search with phrasal translation hypotheses of PBMT, like we propose in this paper.</p><p>In ( <ref type="bibr">Tang et al., 2016)</ref>, the NMT decoder is modified to switch between using externally de- fined phrases and standard NMT word hypothe- ses. However, only one target phrase per source phrase is considered, and the reported improve- ments are significant only when manually selected phrase pairs (mostly for rare named entities) are used.</p><p>Somewhat related to our work is the concept of coverage-based NMT ( <ref type="bibr">Tu et al., 2016)</ref>, where the model architecture is changed to explicitly ac- count for source coverage. In our work, we use a standard NMT architecture, but track coverage with accumulated attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural MT</head><p>Neural MT proposed by ( ) maximizes the conditional log-likelihood of the target sentence E : e 1 , . . . , e I given the source sentence F : f 1 , . . . , f J :</p><formula xml:id="formula_0">H D = − 1 N N n=1 log p θ (E n |F n )</formula><p>where (E n , F n ) refers to the n-th training sen- tence pair in a dataset D, and N denotes the total number of sentence pairs in the training corpus. When using the encoder-decoder architecture by ), the conditional probability can be written as:</p><formula xml:id="formula_1">p(e 1 · · · e I |f 1 · · · f J ) = I i=1 p(e i |e i−1 · · · e 1 , c) with p(e i |e i−1 · · · e 1 , c) = g(s i , e i−1 , c),</formula><p>where I is the length of the target sentence and J is the length of source sentence, c is a fixed-length vec- tor to encode the source sentence, s i is a hidden state of RNN at time step i, and g(·) is a non- linear function to approximate the word probabil- ity. When the attention mechanism is used, the vector c in each sentence is replaced by a time- variant representation c i that is a weighted sum- mary over a sequence of annotations (h 1 , . . . , h J ), and h j contains information about the whole input sentence, but with a strong focus on the parts sur- rounding the j-th word ( ). Then, the context vector can be defined as:</p><formula xml:id="formula_2">c i = J j α ij h j where α ij = exp(r ij ) J j=1 exp(r ij )</formula><p>.</p><p>Therefore, α ij is normalized over all source po- sitions j. Also, r ij = a(s i−1 , h j ) is the atten- tion model used to calculate the log-likelihood of aligning the i-th target word to the j-th source word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phrase-based MT</head><p>The log-linear model, as introduced in <ref type="bibr">(Och and Ney, 2002</ref>), allows decomposing the translation probability P r(e I 1 |f J 1 ) by using an arbitrary num- ber of features h m (f J 1 , e I 1 ). Each feature is multi- plied by a corresponding scaling factor λ m :</p><formula xml:id="formula_3">P r(e I 1 |f J 1 ) = exp M m=1 λ m h m (f J 1 , e I 1 ) ˜ e ˜ I 1 exp M m=1 λ m h m (f J 1 , ˜ e ˜ I 1 )</formula><p>.</p><p>The standard PBMT approach uses a log-linear model in which bidirectional phrasal and lexical scores, language model scores, distortion scores, word penalties and phrase penalties are combined as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hybrid Approach</head><p>In this section we describe our proposed hybrid NMT approach. The algorithm allows translations to be generated partially by phrases 1 and partially by words. Section 3.1 describes the models we use to score hypotheses. The search algorithm is presented in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Log-linear Combination</head><p>We use a log-linear model combination to intro- duce SMT models into the NMT search. Since translations can be partially generated by phrases, we introduce the phrase segmentation s K 1 as a hid- den variable into the models similarly to <ref type="bibr">(Zens and Ney, 2008)</ref>, where K is the number of phrases used in the translation. Note that, unlike stan- dard PBMT, s K 1 does not need to cover the whole source sentence, as parts of the translation can be generated by words. Using the maximum approx- imation, the search criterion then isêˆI</p><formula xml:id="formula_4">isˆisê isêˆ isêˆI 1 = arg max I,e I 1 max s K 1 M m=1 λ m h m (f J 1 , e I 1 , s K 1 )</formula><p>.</p><p>(1) Let˜fLet˜ Let˜f k , ˜ e k be the chosen phrase pairs in the seg- mentation s K 1 for k = 1, . . . , K. In our experi- ments with the proposed hybrid search, we use the following features:</p><p>1. The NMT feature h NMT .</p><p>1 As in SMT, phrases can consist of only a single token.</p><p>2. The word penalty feature h WP counts the number of target words. This feature can help control the length of translations. 3. The source word coverage feature h SWC counts the number of source words translated by phrases:</p><formula xml:id="formula_5">h SWC (f J 1 , e I 1 , s K 1 ) = K k=1 | ˜ f k |.</formula><p>The purpose of this feature is to control the usage of phrases. 4. The phrase penalty feature h PP counts the number of phrases used. Together with the word penalty and the source word coverage feature, the phrase penalty can control the length of chosen phrases. 5. The n-gram language model feature h LM . 6. The bidirectional phrase features h Phr and h iPhr . Note that these features are only ap- plied for those parts of the translation that are generated by phrases. The other parts get a phrase score of zero. The scaling factors λ m are tuned with minimum error rate training (MERT) <ref type="bibr">(Och, 2003)</ref> on n-best lists of the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search</head><p>The algorithm is based on the beam search for NMT, which generates translations one word per time step in a left-to-right fashion. We modify this search to allow hypothesizing phrases in addition to normal word hypotheses. The phrases are sug- gested based on the neural attention, starting from the source position with the maximal current atten- tion. We only suggest phrases if a source position is focused. We check that suggested phrases do not overlap with already translated source words by keeping track of the sum of attention in pre- vious time steps for each source position. Thus, the problem of global reordering is left entirely to the NMT model and we follow the attention when hypothesizing phrases.</p><p>Hypotheses are scored by NMT and SMT mod- els. The beam is divided into two parts of fixed size: the word beam and the phrase beam. The phrase beam is used to score target phrases which were hypothesized from an entry in a previous word beam. In order to score a target phrase con- sisting of k words with the NMT model, we use k time steps, allowing us to keep the efficiency of batched NMT scoring. Once a target phrase has been fully scored (and if the hypothesis has not been pruned), the hypothesis is returned to the word beam. Both beams are generated and pruned independently in each time step.</p><p>The algorithm has some hyper-parameters that need to be set manually. First, we have the beam size N p for phrase hypotheses and the beam size N w for word hypotheses. Second, τ focus is the minimum attention that needs to be on a source position to consider it for extending with a phrase translation candidate whose source phrase starts on that position. Third, τ cov is the minimum sum of attention of a source position over previous time steps at which it is considered to be covered. We do not hypothesize phrases that overlap with cov- ered positions.</p><p>In the following, we describe the search in de- tail. Let f J 1 be the source sentence. Before search, we run the standard phrase matching algorithm on the source sentence to retrieve the translation op- tions E(j, j ) for source positions 1 ≤ j &lt; j ≤ J from a given SMT phrase table. With each hypoth- esis h, we associate the following items:</p><p>• C(h, j) is the sum of the NMT attention to source position j involved in generating the target words of h. This can be considered as a soft coverage vector for h.</p><p>• Q(h) is the partial log-linear score of h ac- cording to Equation 1.</p><p>• E(h) is the n-gram target word history of h.</p><p>• If h is a phrase hypothesis with target phrase˜e phrase˜ phrase˜e, of which k words already have been scored by NMT, then P (h) := (˜ e, k) is the phrase state. Also, each hypothesis is associated with its cor- responding NMT hidden state. We initialize the beam to consist of an empty word hypothesis. Each step of the beam search proceeds as follows:</p><p>1. Let B = [B w , B p ] be the previous beam with word/phrase hypotheses, respectively. First, we generate the attention vector α h,j and the distribution over target wordsˆpwordsˆ wordsˆp h (e) for each hypothesis h ∈ B and word e in the NMT target vocabulary V T using the NMT model in batched scoring 2 .</p><formula xml:id="formula_6">2. Initialize new beam [B w , B p ] = [∅, ∅].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generate new word hypotheses: find the</head><p>maximal N w pairs (h, e) with h ∈ B w and e ∈ V T according to the score Q(h) + λ NMT · logˆplogˆ logˆp h (e). For the top pairs h = (h, e), set</p><formula xml:id="formula_7">Q(h ) = Q(h) + λ N M T · logˆplogˆ logˆp h (e) + λ LM · log p LM (e|E(h)) + λ W P</formula><p>and insert h into B w . Update the soft cover- age C(h , j) = C(h, j)+α h,j for 1 ≤ j ≤ J. 4. Generate new phrase hypotheses: for each previous word hypothesis h ∈ B w , convert the soft attention C(h, ·) into a binary cover- age set C, such that j ∈ C iff. C(h, j) &gt; τ cov . Identify the current NMT focus asˆj</p><formula xml:id="formula_8">asˆasˆj = arg max 1≤j≤J, α h,j &gt;τ focus α h,j .</formula><p>If there is no such j with α h,j &gt; τ focus , no phrase hypotheses are generated from h in this step. Otherwise, for each source phrase length l with C ∩{ˆj∩{ˆ∩{ˆj, ˆ j +1, . . . , ˆ j +l −1} = ∅ and each target phrase˜ephrase˜ phrase˜e ∈ E( ˆ j, ˆ j + l), create a new hypothesis h = (h, ˜ e 1 ) with the score</p><formula xml:id="formula_9">Q(h ) = Q(h) + λ NMT · logˆplogˆ logˆp h (e 1 ) + λ LM · log p LM (˜ e|E(h)) + |˜e||˜e| · λ WP + λ PP + l · λ SWC .<label>(2)</label></formula><p>Note that, in this step, the full target phrase is scored using the language model, while only the first target word is scored using NMT. Ini- tialize the phrase state of h : P (h ) = (˜ e, 1). As in step 3, update the soft coverage. If |˜e||˜e| = 1, insert h into B w , otherwise insert into B p . 5. Advance previous phrase hypotheses: for each h ∈ B p , with phrase state P (h) = (˜ e, k), score the (k + 1)-th target word of˜eof˜ of˜e using NMT, setting h = (h, ˜ e k+1 ) and</p><formula xml:id="formula_10">Q(h ) = Q(h) + λ NMT · logˆplogˆ logˆp h (˜ e k+1 ).</formula><p>As in step 3, update the soft coverage. Set the new phrase state as P (h ) = (˜ e, k + 1 If phrase scores from a phrase table are to be in- cluded in the search, Equation 2 needs to be modi- fied by adding λ Phr log p( ˜ f |˜e|˜e) and λ iPhr log p(˜ e|˜fe|˜ e|˜f ). As in the pure NMT beam search, this proce- dure is repeated until either the last word of all hypotheses in a step is the sentence end token, or 2 · J many beam steps have been performed. Fi- nally, the best translation is chosen as the one in B f with the highest score.</p><p>Note that the same target sequence can be gen- erated with different phrasal segmentations. Dur- ing search, if two hypotheses have the same full target history in a beam, we recombine them and discard the hypothesis with the lower score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform experiments comparing the transla- tion quality of our hybrid approach to phrase- based and pure end-to-end NMT baselines. We present results on two tasks: an in- house English→Russian e-commerce task (trans- lation of real product/item descriptions from an e-commerce site), and the WMT 2016 German→English task (news domain). The cor- pus statistics are shown in <ref type="table">Table 1.</ref> For the English→Russian task, the parallel training data consists of an in-domain part (ca. 5.5M running words) of product/item titles and de- scriptions and other e-commerce content. The rest is out-of-domain data (UN, subtitles, TAUS data collections, etc.) sampled to have significant n- gram overlap with the in-domain description data. Item descriptions are provided by private sellers and, like any user-generated content, may con- tain ungrammatical sentences, spelling errors, and other noise. Product descriptions usually originate from product catalogs and are more "clean", but on the other hand, are difficult to translate because of rare domain-specific terminology. Both types of text contain itemizations, measurement units, and other structures which are usually not found in normal sentences. We tune the system on a devel- opment set that is a mix of product and item de- scriptions, and evaluate on separate product/item description test sets. For development and test sets, two reference translations are used. For the phrase-based baselines, we use an in- house phrase-decoder ( <ref type="bibr">Matusov and Köprü, 2010)</ref> which is similar to the Moses decoder ( <ref type="bibr">Koehn et al., 2007</ref>). We use standard SMT features, including word-level and phrase-level translation probabilities, the distortion model, 5-gram LMs, and a 7-gram joint translation and reordering model reimplemented based on the work of ( <ref type="bibr" target="#b7">Guta et al., 2015</ref>). The language model for the e- commerce task is trained on additional mono- lingual Russian item description data containing 28.2M words. For the WMT task, we use the En- glish News Crawl data containing 3.8B words for additional language model data. The tuning is per- formed using MERT <ref type="bibr">(Och, 2003)</ref> to increase the BLEU score on the development set. To stabilize the optimization on the English→Russian task, we detach Russian morphological suffixes from the word stems both in hypotheses and references us- ing a context-independent "poor man's" morpho- logical analysis. We prefix each suffix with a spe- cial symbol and treat them as separate tokens.</p><p>We have implemented our NMT model in   <ref type="bibr" target="#b5">Gal and Ghahramani (2016)</ref>. We set the dropout probabil- ity for input and recurrent connections of the RNN to 0.2 and word embedding dropout probability to 0.1. On the English→Russian task, the model is then fine-tuned on in-domain data for 10 epochs. The vocabulary is limited using byte pair encoding (BPE) ( <ref type="bibr">Sennrich et al., 2016b</ref>) with 40K splits sep- arately for each language. To speed up training we use approximate loss as described in <ref type="bibr" target="#b9">(Jean et al., 2015</ref>). For pure NMT experiments, we employ length normalization ( <ref type="bibr" target="#b8">Wu et al., 2016)</ref>, as other- wise short translations would be favored.</p><p>For the hybrid approach, we use the same trained end-to-end model as in the NMT base- line. We use all the phrase-based model features plus the NMT score and run MERT as described in Section 3.1. Language models are trained on the level of BPE tokens. We consider at most 100 translation options for each source phrase. If not specified otherwise, we use a beam size of 96 for phrase hypotheses and a beam size of 32 for word hypotheses, resulting in a combined beam size of 128. Furthermore, we set the focus threshold τ focus = 0.3 and the coverage threshold τ cov = 0.7 by default. We also perform experiments where these hyper-parameters are varied.</p><p>3 http://tensorflow.org</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">E-commerce English→Russian</head><p>The results on the e-commerce English→Russian task are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>NMT vs. phrase-based SMT The pure NMT system exhibits large improve- ments over the phrase-based baseline <ref type="bibr">4</ref> . These im- provements are also significantly larger than when we use the NMT model to rescore PBMT 1000- best lists. NMT results are not improved when the beam size is increased from 12 to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid search vs. pure NMT search</head><p>For the hybrid approach, we train a phrase-table on the in-domain data and split the source and tar- get phrases with BPE afterwards for compatibility with the NMT vocabulary. With the hybrid ap- proach, when using a LM trained only on the target side of bilingual data, we get an improvement of 0.3% BLEU on item descriptions and 1.4% BLEU on product descriptions over the pure NMT sys- tem. When we use the LM trained on extra mono- lingual data, we get total improvements of 1.0% BLEU and 2.3% BLEU with the hybrid approach. In contrast, when we add this language model and a word penalty on top of the pure NMT system and tune scaling factors with MERT, we get small improvements (last row of <ref type="table" target="#tab_2">Table 2</ref>) only on prod- uct descriptions. This shows that the hybrid ap- proach can exploit the LM better than a purely word-based NMT approach. We have also per- formed experiments utilizing the additional mono- lingual data for synthetic training data for NMT as in ( <ref type="bibr">Sennrich et al., 2016a</ref>), but did not get im- provements.</p><p>To analyze the improvements of the hybrid sys- tem, we perform experiments in which we either  <ref type="table">Table 3</ref>: Translation results of the hybrid approach on the e-commerce English→Russian task with different SMT model combinations. The first row shows results with all models enabled. In the following rows, we either remove or limit exactly one model compared to the full system.</p><note type="other">Maximal source phrase length 1 26.7 56.4 29.1 51.6 Minimal source phrase length 2 27.0 55.9 30.0 51.1</note><p>disable or limit some of the SMT models. The results are shown in <ref type="table">Table 3</ref>. Without the lan- guage model, the hybrid approach has almost no improvements over the NMT baseline. This in- dicates that the language model is crucial in se- lecting appropriate phrase candidates. Similarly, when we disable the source word coverage feature, the translation quality is degraded, suggesting that this feature helps choose between phrase hypothe- ses and word hypotheses during the search. Next, we do not use phrase-level scores. Here, we ob- serve only a small degradation of translation qual- ity. Finally, we limit the source length of phrases used in the search, allowing only one-word source phrases in one experiment and only source phrases with two or more words in another experiment. In both cases, the translation quality decreases. Thus, both one-word phrases and longer phrases are nec- essary to obtain the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tuning the beam size</head><p>Next, we study the effect of different beam sizes on translation quality. The results are shown in <ref type="table">Table 4</ref>. Note that we retune the system for each choice. With a total beam size of 128, we get the best results by using a phrase beam size of 96 and a word beam size of 32. When we use a phrase beam size of 116 or 64 instead, the translation quality worsens. In another experiment, we de- crease the total beam size to 64. The translation quality degrades only slightly, which means that we can still expect MT quality improvements with hybrid search even if we optimize the system for speed. To further test this, we reduce the beam sizes to N w = 12 and N p = 4 after tuning with N w = 32 and N p = 96. We get BLEU scores of 27.1% on item descriptions and 30.1% on product descriptions, losing 0.3% and 0.7% BLEU respec- tively compared to the full beam size.  <ref type="table">Table 4</ref>: Effect of the beam size (word beam size N w + phrase beam size N p ) for the hybrid ap- proach on the e-commerce English→Russian task.</p><p>Tuning the attention focus/coverage thresholds <ref type="table" target="#tab_6">Table 5</ref> shows results with different values for the coverage threshold τ cov . Again, we retune the sys- tem for each choice. Setting the coverage thresh- old to 1.0 or even disabling the coverage check (by setting τ cov = ∞) has little effect on the transla- tion scores on this task. This can be explained by the fact that translation from English to Russian is mostly monotonic. We also tried varying the fo- cus threshold τ focus between 0.0 and 0.3 but did not notice any significant effect on this task.   Further human analysis by a native Russian speaker of the pure NMT vs. hybrid search trans- lations shows that hybrid search is often able to correct the following known NMT handicaps:</p><p>• incorrect translation of rare words (among other reasons, due to incorrect sub-word unit translation in which rare words are aggres- sively segmented).</p><p>• repetition of same or similar words as a result of multiple attention to the same source word, as well as untranslated words that received no attention.</p><p>• incorrect or partially correct word-by-word translation when a phrasal (non-literal) trans- lation should be used instead.</p><p>In all of these cases, the usage of phrasal trans- lations is able to better enforce the coverage, and this, in turn, leads to improved lexical choice. The fact that not many long phrase pairs are selected indicates, in our opinion, that the search and mod- eling problem in NMT is far from being solved: with the right, diverse model scores, the proposed hybrid search is able to select and extend better hy- potheses with words, most of which already had a high NMT probability. Yet they are not always selected in the pure NMT beam search, among other reasons, due to competition from words erro- neously placed near them in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">WMT 2016 German→English</head><p>The results on the WMT German→English task are shown in <ref type="table" target="#tab_7">Table 6</ref>. The initial phrase-based baseline uses the 5-gram language model esti- mated on the target side of bilingual data. By adding the News Crawl LM data, we gain 2.5% and 2.3% BLEU on the test sets, but PBMT still is behind NMT.</p><p>For the hybrid approach, we use a beam size of 64 and a maximal number of beam steps of 1.5 · J (instead of 2 · J) to speed up experiments. We use separate word penalty features, one for word-based hypotheses and one for phrase-based hypotheses to allow for more control of transla- tion lengths. With the hybrid approach, using the 5-gram language model estimated on the target side of bilingual data, and phrase scores, we get small improvements in BLEU over the NMT base- line. However, the TER increases. We experiment with different thresholds, setting τ focus = 0.1 and τ cov = 1.0. With this hybrid system, we get im- provements of 1.0% and 1.1% BLEU over pure NMT. Finally, we add the News Crawl LM data on top. This significantly improves the results by 1.7% and 2.0% BLEU. In total, we gain 2.7% and 3.1% BLEU over pure NMT. These results re- inforce the fact that, similar to PBMT, language model quality is important for the proposed hybrid search. In contrast, we have also tried applying only the LM (including News Crawl data) with a word penalty on top of NMT, but did not get con- sistent improvements. <ref type="figure" target="#fig_1">Figure 1</ref> shows an example for the phrase pairs chosen by the hybrid system on top of the NMT attention. The hybrid approach correctly trans- lates the German idiom "nach und nach" as "grad- ually", while the pure NMT system incorrectly translates it word-by-word as "after and after". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a novel hybrid search that extends NMT with phrase-based models. The NMT beam search was modified to insert phrasal translations based on the current and accumulated attention weights of the NMT decoder RNN. The NMT model score was used in a log-linear model with standard phrase-based scores as well as an n-gram language model. We described the algo- rithm in detail, in which we keep separate beams for NMT word hypotheses and hypotheses with an incomplete phrasal translation, as well as in- troduce parameters which control the source sen- tence coverage. Numerous experiments on two large vocabulary translation tasks showed that the hybrid search improves BLEU scores significantly as compared to a strong NMT baseline that already outperforms phrase-based SMT by a large margin.</p><p>In the future, we plan to focus on integration of phrasal components into NMT training, including better coverage constraints, as well as methods for context-dependent translation override within our hybrid search algorithm. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>The</head><label></label><figDesc>German→English system is trained on par- allel corpora provided for the constrained WMT 2017 evaluation (Europarl, Common Crawl, and others). We use the WMT 2015 evaluation data as development set, and the evaluation is performed on two sets from the WMT evaluations in 2014 and 2016. Only a single human reference transla- tion is provided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example alignment from the hybrid search, with the source sentence on the bottom and the translation on the left. The blue rectangles signify phrase pairs on top of the NMT attention. The pure NMT translation is "the system is tested after and after testing and improved by testing programs."</figDesc><graphic url="image-1.png" coords="9,84.53,62.81,190.49,175.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Overview of translation results on the e-commerce English→Russian task. 

Python using the TensorFlow 3 deep learning li-
brary. We use the embedding size of 620, RNN 
size of 1000 and GRU cells. The model is trained 
with maximum likelihood loss for 15 epochs us-
ing Adam optimizer (Kingma and Ba, 2014) on 
complete data in batches of 100 sentences. The 
learning rate is initialized to 0.0002, decaying by 
0.9 each epoch. For regularization we use L2 loss 
with weight 10 −7 and dropout following </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Effect of the threshold parameters 
on the hybrid approach on the e-commerce 
English→Russian task. 

Analysis 

To understand the behavior of the hybrid search, 
we count the number of source words that are </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Overview of translation results on the WMT German→English task. 

translated by phrases in the product descriptions 
test set. Of the 9320 source words, 7109 (76.3%) 
are covered by phrase hypotheses. 78.3% of the 
source phrases are unigrams, 19.5% are bigrams 
and 2.2% are trigrams or longer. Among the many 
one-word phrases used, almost all (99.2%) are also 
within the top 3 predictions of word-based NMT, 
and 90.3% are equal to the top NMT prediction. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>7th International Joint Conference on Natural Lan- guage Processing of the Asian Federation of Natural Language Processing. pages 1-10.</head><label></label><figDesc></figDesc><table>Marcin Junczys-Dowmunt, Tomasz Dwojak, and Rico 
Sennrich. 2016. The AMU-UEDIN submission to 
the WMT16 news translation task: Attention-based 
NMT models as feature functions in phrase-based 
SMT. In Proceedings of the First Conference on 
Machine Translation, WMT 2016, colocated with 
ACL. pages 319-325. 

Diederik P. Kingma and Jimmy Ba. 2014. Adam: 
A method for stochastic optimization. 
CoRR 
abs/1412.6980. 

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, et al. 2007. Moses: Open source 
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on 
interactive poster and demonstration sessions. As-
sociation for Computational Linguistics, pages 177-
180. 

Evgeny Matusov and Selçuk Köprü. 2010. AppTek's 
APT Machine Translation System for IWSLT 2010. 
In Marcello Federico, Ian Lane, Michael Paul, and 
François Yvon, editors, International Workshop on 
Spoken Language Translation, IWSLT. pages 29-36. 

Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Compu-
tational Linguistics. pages 160-167. 

Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the 
40th Annual Meeting of the Association for Compu-
tational Linguistics. pages 295-302. 

Jan-Thorsten Peter, Andreas Guta, Nick Rossenbach, 
Miguel Graa, and Hermann Ney. 2016. The rwth 
aachen machine translation system for iwslt 2016. 
In International Workshop on Spoken Language 
Translation, IWSLT. 

Rico Sennrich, Barry Haddow, and Alexandra Birch. 
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the 
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL. 

Rico Sennrich, Barry Haddow, and Alexandra Birch. 
2016b. Neural machine translation of rare words 
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational 
Linguistics, ACL. 

Felix Stahlberg,Adrì a de Gispert, Eva Hasler, and 
Bill Byrne. 2017. Neural machine translation by 
minimising the Bayes-risk with respect to syntactic 
translation lattices. In Proceedings of the 15th Con-
ference of the European Chapter of the Association 
for Computational Linguistics. Valencia, Spain. </table></figure>

			<note place="foot" n="2"> If a target word e is not in VT , setˆpsetˆ setˆp h (e) = ˆ p h (UNK) where UNK is a special token denoting unknowns. Note that this almost never happens when using a word segmentation like BPE (Sennrich et al., 2016b).</note>

			<note place="foot" n="4"> The significance of these improvements was also confirmed by an in-house human evaluation with 3 judges.</note>

			<note place="foot">Richard Zens and Hermann Ney. 2008. Improvements in dynamic programming beam search for phrasebased statistical machine translation. In International Workshop on Spoken Language Translation, IWSLT. pages 198-205.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Tamer Alkhouli and Jan-Thorsten Peter for helpful discussions. We thank the anonymous reviewers for their suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1557" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural versus phrasebased machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1503.03535</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison between count and neural network models based on joint translation and reordering sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Guta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1401" to="1411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with SMT features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
