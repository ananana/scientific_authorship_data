<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speeding Up Neural Machine Translation Decoding by Cube Pruning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Intelligent Information Processing Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences (ICT/CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<email>liang.huang.sh@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Research, Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Intelligent Information Processing Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences (ICT/CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Intelligent Information Processing Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences (ICT/CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Intelligent Information Processing Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences (ICT/CAS</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speeding Up Neural Machine Translation Decoding by Cube Pruning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4284" to="4294"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4284</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although neural machine translation has achieved promising results, it suffers from slow translation speed. The direct consequence is that a trade-off has to be made between translation quality and speed, thus its performance can not come into full play. We apply cube pruning, a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3× on GPUs and 3.5× on CPUs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) has shown promising results and drawn more attention re- cently ( <ref type="bibr">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b5">Cho et al., 2014b;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr">Gehring et al., 2017a,b;</ref><ref type="bibr" target="#b19">Vaswani et al., 2017)</ref>. A widely used ar- chitecture is the attention-based encoder-decoder framework ( <ref type="bibr" target="#b5">Cho et al., 2014b</ref>; <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) which assumes there is a common seman- tic space between the source and target language pairs. The encoder encodes the source sentence to a representation in the common space with the recurrent neural network (RNN) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>) and the decoder decodes this representation to generate the target sentence word by word. To generate a target word, a probabil- ity distribution over the target vocabulary is drawn based on the attention over the entire source se- quence and the target information rolled by an- other RNN. At the training time, the decoder is forced to generate the ground truth sentence, while at inference, it needs to employ the beam search algorithm to search through a constrained space due to the huge search space.</p><p>Even with beam search, NMT still suffers from slow translation speed, especially when it works not on GPUs, but on CPUs, which are more com- mon practice. The first reason for the inefficiency is that the generation of each target word requires extensive computation to go through all the source words to calculate the attention. Worse still, due to the recurrence of RNNs, target words can only be generated sequentially rather than in parallel. The second reason is that large vocabulary on tar- get side is employed to avoid unknown words (UNKs), which leads to a large number of nor- malization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decod- ing speed by reducing the size of vocabulary ( <ref type="bibr">Mi et al., 2016a</ref>) or/and the number of parameters, which can not realize the full potential of NMT.</p><p>In this paper, we borrow ideas from phrase- based and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding <ref type="bibr" target="#b3">(Chiang, 2007;</ref><ref type="bibr" target="#b14">Huang and Chiang, 2007)</ref>. Informally, cube pruning "coarsens" the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimen- sions of which are target words in the target vocab- ulary, part translations retained in the beam search and different combinations of similar target hid- den states, respectively. The clustering operation can directly decrease the number of target hidden states in the following calculations, together with cube pruning, resulting in less RNN expansion op- erations to generate the next hidden state (related to the first reason) and less softmax operations over the target vocabulary (related to the second reason). The experiment results show that, when receiving the same or even better translation qual- ity, our method can speed up the decoding speed by 3.3× on GPUs and 3.5× on CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The proposed strategy can be adapted to optimize the beam search algorithm in the decoder of vari- ous NMT models. Without loss of generality, we take the attention-based NMT ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> as an example to introduce our method. In this section, we first introduce the attention-based NMT model and then the cube pruning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Attention-based NMT Model</head><p>The attention-based NMT model follows the encoder-decoder framework with an extra atten- tion module. In the following parts, we will intro- duce each of the three components. Assume the source sequence and the observed translation are</p><formula xml:id="formula_0">x = {x 1 , · · · , x |x| } and y = {y * 1 , · · · , y * |y| }. Encoder</formula><p>The encoder uses a bidirectional GRU to obtain two sequences of hidden states. The fi- nal hidden state of each source word is got by con- catenating the corresponding pair of hidden states in those sequences. Note that e x i is employed to represent the embedding vector of the word x i .</p><formula xml:id="formula_1">− → h i = −−−→ GRU e x i , − → h i−1 (1) ← − h i = ←−−− GRU e x i , ← − h i+1<label>(2)</label></formula><formula xml:id="formula_2">h i = − → h i ; ← − h i (3)</formula><p>Attention The attention module is designed to extract source information (called context vector) which is highly related to the generation of the next target word. At the j-th step, to get the con- text vector, the relevance between the target word y * j and the i-th source word is firstly evaluated as</p><formula xml:id="formula_3">r ij = v T a tanh (W a s j−1 + U a h i )<label>(4)</label></formula><p>Then, the relevance is normalized over the source sequence, and all source hidden states are added weightedly to produce the context vector.</p><formula xml:id="formula_4">α ij = exp (r ij ) |x| i =1 exp r i j ; c j = |x| i=1 α ij h i (5)</formula><p>Decoder The decoder also employs a GRU to unroll the target information. The details are de- scribed in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>. At the j-th de- coding step, the target hidden state s j is given by</p><formula xml:id="formula_5">s j = f e y * j−1 , s j−1 , c j<label>(6)</label></formula><p>The probability distribution D j over all the words in the target vocabulary is predicted conditioned on the previous ground truth words, the context vector c j and the unrolled target information s j .</p><formula xml:id="formula_6">t j = g e y * j−1 , c j , s j (7) o j = W o t j (8) D j = softmax (o j )<label>(9)</label></formula><p>where g stands for a linear transformation, W o is used to map t j to o j so that each target word has one corresponding dimension in o j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cube Pruning</head><p>The cube pruning algorithm, proposed by Chiang (2007) based on the k-best parsing algorithm of <ref type="bibr" target="#b13">Huang and Chiang (2005)</ref>, is actually an accel- erated extension based on the naive beam search algorithm. Beam search, a heuristic dynamic pro- gramming searching algorithm, explores a graph by expanding the most promising nodes in a lim- ited set and searches approximate optimal results from candidates. For the sequence-to-sequence learning task, given a pre-trained model, the beam search algorithm finds a sequence that ap- proximately maximizes the conditional probabil- ity <ref type="bibr" target="#b10">(Graves, 2012;</ref><ref type="bibr" target="#b1">Boulanger-Lewandowski et al., 2013)</ref>. Both Sutskever et al. (2014) and <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> employed the beam search algorithm into the NMT decoding to produce translations with relatively larger conditional probability with respect to the optimized model parameters. Re- markably, <ref type="bibr" target="#b14">Huang and Chiang (2007)</ref> successfully applied the cube pruning algorithm to the decod- ing of SMT. They found that the beam search al- gorithm in SMT can be extended, and they uti- lized the cube pruning and some variants to op- timize the search process in the decoding phase of phrase-based ( <ref type="bibr" target="#b15">Och and Ney, 2004</ref>) and syntax- based <ref type="bibr" target="#b2">(Chiang, 2005;</ref><ref type="bibr" target="#b7">Galley et al., 2006</ref>) systems,  Eq. <ref type="formula" target="#formula_5">(6)</ref>:</p><formula xml:id="formula_7">s j = f (e y * j−1 , s j−1 , c j ) 551.07 75.73% 1370.92 19.42%</formula><p>Eq. <ref type="formula">(7)</ref>: which decreased a mass of translation candidates and achieved a significant speed improvement by reducing the size of complicated search space, thereby making it possible to actualize the thought of improving the translation performance through increasing the beam size.</p><formula xml:id="formula_8">t j = g(e y * j−1 , c j , s j ) 88</formula><p>In the traditional SMT decoding, the cube prun- ing algorithm aims to prune a great number of partial translation hypotheses without computing and storing them. For each decoding step, those hypotheses with the same translation rule are grouped together, then the cube pruning algorithm is conducted over the hypotheses. We illustrate the detailed process in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NMT Decoder with Cube Pruning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definitions</head><p>We define the related storage unit tuple of the i-th candidate word in the j-th beam as</p><formula xml:id="formula_9">n i j = (c i j , s i j , y i j , bp i j )</formula><p>, where c i j is the negative log- likelihood (NLL) accumulation in the j-th beam, s i j is the decoder hidden state in the j-th beam, y i j is the index of the j-th target word in large vocab- ulary and bp i j is the backtracking pointer for the j-th decoding step. Note that, for each source sen- tence, we begin with calculating its encoded rep- resentation and the first hidden state s 0 0 in decoder, then searching from the initial tuple (0.0, s 0 0 , 0, 0) existing in the first beam <ref type="bibr">1</ref> .</p><p>It is a fact that Equation <ref type="formula" target="#formula_6">(9)</ref> produces the prob- ability distribution of the predicted target words over the target vocabulary V . <ref type="bibr" target="#b5">Cho et al. (2014b)</ref> indicated that whenever a target word is generated, the softmax function over V computes probabil- ities for all words in V , so the calculation is ex- pensive when the target vocabulary is large. As such, <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> (and many others) only used the top-30k frequent words as target vocabulary, and replaced others with UNK. How- ever, the final normalization operation still brought high computation complexity for forward calcula- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Time Cost in Decoding</head><p>We conducted an experiment to explore how long each calculation unit in the decoder would take. We decoded the MT03 test dataset by using naive beam search with beam size of 10 and recorded the time consumed in the computation of Equation (6), <ref type="formula">(7)</ref>, <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_6">(9)</ref>, respectively. The statistical results in <ref type="table">Table 1</ref> show that the recurrent calcula- tion unit consumed the most time on GPUs, while the softmax computation also took lots of time. On CPUs, the most expensive computational time cost was caused by the softmax operation over the entire target vocabulary 2 . In order to avoid the time-consuming normalization operation in test- ing, we introduced self-normalization (denoted as SN) into the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-normalization</head><p>Self-normalization <ref type="bibr" target="#b6">(Devlin et al., 2014</ref>) was de- signed to make the model scores which are pro- duced by the output layer be approximated by the probability distribution over the target vocab- ulary without normalization operation. According to Equation <ref type="formula" target="#formula_6">(9)</ref>, for an observed target sentence y = {y * 1 , · · · , y * |y| }, the Cross-Entropy (CE) loss could be written as</p><formula xml:id="formula_10">L θ = − |y| j=1 log D j [y * j ] = − |y| j=1 log exp o j [y * j ] y ∈V exp (o j [y ]) = |y| j=1 log y ∈V exp o j [y ] − o j [y * j ]<label>(10)</label></formula><p>where o j is the model score generated by Equa- tion (8) at the j-th step, we marked the softmax normalizer y ∈V exp (o j [y ]) as Z. Following the work of <ref type="bibr" target="#b6">Devlin et al. (2014)</ref>, we modified the CE loss function into</p><formula xml:id="formula_11">L θ = − |y| j=1 log D j [y * j ] − α(log Z − 0) 2 = − |y| j=1 log D j [y * j ] − α log 2 Z<label>(11)</label></formula><p>The objective function, shown in Equation (11), is optimized to make sure log Z is approximated to 0, equally, make Z close to 1 once it converges. We chose the value of α empirically. Because the softmax normalizer Z is converged to 1 in infer- ence, we just need to ignore Z and predict the tar- get word distribution at the j-th step only with o j :</p><formula xml:id="formula_12">D j = o j<label>(12)</label></formula><p>3.4 Cube Pruning <ref type="table">Table 1</ref> clearly shows that the equations in the NMT forward calculation take lots of time. Here, according to the idea behind the cube pruning algorithm, we tried to reduce the time of time- consuming calculations, e.g., Equation <ref type="formula" target="#formula_5">(6)</ref>, and further decrease the search space by introducing the cube pruning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Integrating into NMT Decoder</head><p>Extended from the naive beam search in the NMT decoder, cube pruning, treated as a pruning al- gorithm, attempts to reduce the search space and computation complexity by merging some simi- lar items in a beam to accelerate the naive beam search, keeping the 1-best searching result almost unchanged or even better by increasing the beam size. Thus, it is a fast and effective algorithm to generate candidates. Assume that T restores the set of the finished translations. For each step in naive beam search process, beamsize−|T| times forward calcula- tions are required to acquire beamsize−|T| prob- ability distributions corresponding to each item in the previous beam ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. while for each step in cube pruning, in terms of some constraints, we merge all similar items in the pre- vious beam into one equivalence class (called a sub-cube). The constraint we used here is that items being merged in the previous beam should have the same target words. Then, for the sub- cube, only one forward calculation is required to obtain the approximate predictions by using the loose hidden state. Elements in the sub-cube are sorted by previous accumulated NLL along the columns (the first dimension of beam size) and by the approximate predictions along the rows (the second dimension of vocabulary size). Af- ter merging, one beam may contain several sub- cubes (the third dimension), we start to search from item in the upper left corner of each sub- cube, which is the best one in the sub-cube, and continue to spread out until enough candidates are found. Once a item is selected, the exact hidden state will be used to calculate its exact NLL.</p><p>Through all above steps, the frequency of for- ward computations decreases. We give an exam- ple to dive into the details in <ref type="figure">Figure 2</ref>.</p><p>Assume that the beam size is 4. Given the 10 th   <ref type="figure">Figure 2</ref>, (6.1, 433) constructs the sub-cube C1; (6.5, 35), (7.0, 35) and (7.3, 35) are put to- gether to compose another sub-cube C2. Items in part (a) are ranked in ascending order along both row and column dimension according to the ac- cumulated NLL. For each sub-cube, we use the first state vector in each sub-cube as the approx- imate one to produce the next probability distribu- tion and the next state. At beginning, each upper- left corner element in each sub-cube is pushed into a minimum heap, after popping minimum element from the heap, we calculate and restore the exact NLL of the element, then push the right and lower ones alongside the minimum element into heap. At this rate, the searching continues just like the "diffusion" in the sub-cube until 4 elements are popped, which are ranked in terms of their exact NLLs to construct the 11 th beam. Note that once an element is popped, we calculate its exact NLL. From the step (e) in <ref type="figure">Figure 2</ref>, we can see that 4</p><note type="other">0.8 7.3 7.8 ··· 7.3 ··· ··· ··· 9.8 3.3 1.3 7.7 8.2 7.0 6.5 C2 0.1 6.2 ··· ··· 4.5 2.5 8.6 6.1 C1 V_ 67 4 (k or ea n) V_ 83 57 (k or ea ns ) V_ 57 4 (k or ea ) V_ 29 (fr om ) V_ 88 0 (li vi ng ) V_ 8 (in ) 0.8 8.1 7.3 7.8 ··· 7.3 ··· ··· ··· 9.8 3.3 1.3 7.7 8.2 7.0 6.5 C2 0.1 6.2 ··· ··· 4.5 2.5 8.6 6.1 C1 V_ 67 4 (k or ea n</note><p>elements have been popped from C1 and C2, and then ranked in terms of their exact NLLs to build the 11 th beam. We refer above algorithm as the naive cube pruning algorithm (called NCP)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Accelerated Cube Pruning</head><p>In each step of the cube pruning algorithm, after merging the items in the previous beam, some sim- ilar candidates are grouped together into one or more sub-cube(s). We also try to predict the ap- proximate distribution for each sub-cube only ac- cording to the top-1 state vector (the first row in the sub-cube in <ref type="figure">Figure 2)</ref>, and select next candi- dates after ranking. The predicted probability dis- tribution will be very similar to that of the naive beam search. Besides, Each sub-cube only re- quires one forward calculation. Thus, it could save more search space and further reduce the computa- tion complexity for the decoder. Unlike the naive cube pruning algorithm, accelerated cube pruning pops each item, then still use the approximate NLL instead of the exact one. We denote this kind of accelerated cube pruning algorithm as ACP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We verified the effectiveness of proposed cube pruning algorithm on the Chinese-to-English (Zh- En) translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>The Chinese-English training dataset consists of 1.25M sentence pairs 3 . We used the NIST 2002 (MT02) dataset as the validation set with 878 sen- tences, and the NIST 2003 (MT03) dataset as the test dataset, which contains 919 sentences.</p><p>The lengths of the sentences on both sides were limited up to 50 tokens, then actually 1.11M sen- tence pairs were left with 25.0M Chinese words and 27.0M English words. We extracted 30k most frequent words as the source and target vocabular- ies for both sides.</p><p>In all the experiments, case-insensitive 4-gram BLEU ( <ref type="bibr" target="#b16">Papineni et al., 2002</ref>) was employed for the automatic evaluation, we used the script mteval-v11b.pl 4 to calculate the BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System</head><p>The system is an improved version of attention- based NMT system named RNNsearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> where the decoder employs a con- ditional GRU layer with attention, consisting of two GRUs and an attention module for each step <ref type="bibr">5</ref> . Specifically, Equation <ref type="formula" target="#formula_5">(6)</ref> is replaced with the fol- lowing two equations:</p><formula xml:id="formula_13">˜ s j = GRU 1 (e y * j−1 , s j−1 )<label>(13)</label></formula><formula xml:id="formula_14">s j = GRU 2 (c j , ˜ s j )<label>(14)</label></formula><p>Besides, for the calculation of relevance in Equa- tion (4), s j−1 is replaced with˜swith˜ with˜s j−1 . The other components of the system keep the same as RNNsearch. Also, we re-implemented the beam search algorithm as the naive decoding method, and naive searching on the GPU and CPU server were conducted as two baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details</head><p>Specially, we employed a little different settings from <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>: Word embedding sizes on both sides were set to 512, all hidden sizes in the GRUs of both encoder and decoder were also set to 512. All parameter matrices, including bias matrices, were initialized with the uniform distribution over [−0.1, 0.1]. Parameters were up- dated by using mini-batch Stochastic Gradient De- scent (SGD) with batch size of 80 and the learning rate was adjusted by AdaDelta (Zeiler, 2012) with decay constant ρ=0.95 and denominator constant =1e-6. The gradients of all variables whose L2- norm are larger than a pre-defined threshold 1.0 were normalized to the threshold to avoid gradi- ent explosion ( <ref type="bibr" target="#b17">Pascanu et al., 2013)</ref>. Dropout was applied to the output layer with dropout rate of 0.5. We exploited length normalization ( <ref type="bibr" target="#b4">Cho et al., 2014a</ref>) strategy on candidate translations in beam search decoding. The model whose BLEU score was the high- est on the validation set was used to do testing. Maximal epoch number was set to 20. Training was conducted on a single Tesla K80 GPU, it took about 2 days to train a single NMT model on the Zh-En training data. For self-normalization, we empirically set α as 0.5 in Equation <ref type="formula" target="#formula_11">(11)</ref>  <ref type="bibr">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Search Strategies</head><p>We conducted experiments to decode the MT03 test dataset on the GPU and CPU server respec- tively, then compared search quality and efficiency among following six search strategies under differ- ent beam sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison of Average Merging Rate</head><p>We first give the definition of the Average Merging Rate (denoted as AMR). Given a test dataset, we counted the total word-level candidates (noted as N w ) and the total sub-cubes (noted as N c ) during the whole decoding process, then the AMR can be simply computed as</p><formula xml:id="formula_15">m = N w /N c<label>(15)</label></formula><p>The MT03 test dataset was utilized to com- pare the trends of the AMR values under all <ref type="figure">Figure 3</ref>: AMR comparison on the MT03 test dataset. Decoding the MT03 test dataset on a sin- gle GeForce GTX TITAN X GPU server under the different searching settings. y-axis represents the AMR on the test dataset in the whole searching process and x-axis indicates beam size. Unsurpris- ingly, we got exactly the same results on the CPU server, not shown here.</p><p>six methods. We used the pre-trained model to translate the test dataset on a single GeForce GTX TITAN X GPU server. Beam size varies from 1 to 40, values are included in the set {1, 2, 3, 4, 8, 10, 15, 18, 20, 30, 40}.</p><p>For each beam size, six different searching settings were ap- plied to translate the test dataset respectively. The curves of the AMRs during the decoding on the MT03 test dataset under the proposed methods are shown in <ref type="figure">Figure 3</ref>. Note that the AMR values of NBS are always 1 whether there is SN or not.</p><p>Comparing the curves in the <ref type="figure">Figure 3</ref>, we could observe that the naive beam search does not con- duct any merging operation in the whole searching process, while the average merging rate in the cube pruning almost grows as the beam size increases. Comparing the red curves to the blue ones, we can conclude that, in any case of beam size, the AMR of the accelerated cube pruning surpasses the ba- sic cube pruning by a large margin. Besides, self- normalization could produces the higher average merging rate comparing to the counterpart without self-normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison on the GPU Server</head><p>Intuitively, as the value of the AMR increases, the search space will be reduced and computation ef- ficiency improves. We compare the two proposed searching strategies and the naive beam search in two conditions (with self-normalization and with- out self-normalization). <ref type="figure">Figure 4</ref> demonstrates the results of comparison between the proposed searching methods and the naive beam search baseline in terms of search quality and search effi- ciency under different beam sizes.</p><p>By fixing the beam size and the dataset, we compared the changing trend of BLEU scores for the three distinct searching strategies under two conditions. Without self-normalization, <ref type="figure">Figure 4a</ref> shows the significant improvement of the search speed, however the BLEU score drops about 0.5 points. We then equipped the search algorithm with self-normalization. <ref type="figure">Figure 4b</ref> shows that the accelerated cube pruning search algorithm only spend about one-third of the time of the naive beam search to achieve the best BLEU score with beam size 30. Concretely, when the beam size is set to be 30, ACP+SN is 3.3 times faster than the baseline on the MT03 test dataset, and both per- formances are almost the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Comparison on the CPU Server</head><p>Similar to the experiments conducted on GPUs, we also translated the whole MT03 test dataset on the CPU server by using all six search strate- gies under different beam sizes. The trends of the BLEU scores over those strategies are shown in <ref type="figure">Figure 5</ref>.</p><p>The proposed search methods gain the similar superiority on CPUs to that on GPUs, and the decoding speed is obviously slower than that on GPUs. From the <ref type="figure">Figure 5a</ref>, we can also clearly see that, compared with the NBS-SN, NCP-SN only speeds up the decoder a little, ACP-SN pro- duces much more acceleration. However, when we did not introduce self-normalization, the pro- posed search methods will also result in a loss of about 0.5 BLEU score. The self-normalization made the ACP strategy faster than the baseline by about 3.5×, in which condition the NBS+SN got the best BLEU score 38.05 with beam size 30 while the ACP+SN achieved the highest score 38.12 with beam size 30. The results could be ob- served in <ref type="figure">Figure 5b</ref>. Because our method is on the algorithmic level and platform-independent, it is reasonable that the proposed method can not only perform well on GPUs, but also accelerate the de- coding significantly on CPUs. Thus, the acceler- ated cube pruning with self-normalization could improve the search quality and efficiency stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Decoding Time</head><p>In this section, we only focus on the consuming time of translating the entire MT03 test dataset. Under the two conditions, we calculated the times spent on translating the entire test dataset for dif- ferent beam sizes, then draw the curves in <ref type="figure">Figure  6</ref> and 7. From the <ref type="figure">Figure 6a</ref> and 6b, we could observe that accelerated cube pruning algorithm speeds up the decoding by about 3.8× on GPUs when the beam size is set to 40. <ref type="figure">Figure 7a</ref> and 7b show that the accelerated cube pruning algo- rithm speeds up the decoding by about 4.2× on CPU server with the beam size 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Recently, lots of works devoted to improve the ef- ficiency of the NMT decoder. Some researchers employed the way of decreasing the target vocabu- lary size. <ref type="bibr">Jean et al. (2015)</ref> improved the decoding efficiency even with the model using a very large target vocabulary but selecting only a small sub- set of the whole target vocabulary. Based on the work of <ref type="bibr">Jean et al. (2015)</ref>, <ref type="bibr">Mi et al. (2016b)</ref> intro- duced sentence-level and batch-level vocabularies as a very small subset of the full output vocabu- lary, then predicted target words only on this small vocabulary, in this way, they only lost 0.1 BLEU points, but reduced target vocabulary substantially. Some other researchers tried to raise the effi- ciency of decoding from other perspectives. <ref type="bibr" target="#b20">Wu et al. (2016)</ref> introduced a coverage penalty α and length normalization β into beam search decoder to prune hypotheses and sped up the search pro- cess by 30%∼40% when running on CPUs. <ref type="bibr" target="#b12">Hu et al. (2015)</ref> used a priority queue to choose the best hypothesis for the next search step, which drastically reduced search space.</p><p>Inspired by the works of <ref type="bibr">Mi et al. (2016b)</ref> and <ref type="bibr" target="#b14">Huang and Chiang (2007)</ref>, we consider prun- ing hypothesis in NMT decoding by using cube pruning algorithm, but unlike traditional SMT de- coding where dynamic programming was used to merge equivalent states (e.g., if we use phrase- based decoding with trigram language model, we can merge states with same source-side coverage vector and same previous two target words). How- ever, this is not appropriate for current NMT de- coding, since the embedding of the previous target word is used as one input of the calculation unit of each step in the decoding process, we could group equivalence classes containing the same previous target word together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We extended cube pruning algorithm into the de- coder of the attention-based NMT. For each step in beam search, we grouped similar candidates in previous beam into one or more equivalence class(es), and bad hypotheses were pruned out. We started searching from the upper-left corner in each equivalence class and spread out until enough candidates were generated. Evaluations show that, compared with naive beam search, our method could improve the search quality and efficiency to a large extent, accelerating the NMT decoder by 3.3× and 3.5× on GPUs and CPUs, respectively. Also, the translation precision could be the same or even better in both situations. Besides, self- normalization is verified to be helpful to accelerate cube pruning even further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cube pruning in SMT decoding. (a): the values in the grid denote the negative log-likelihood cost of the terminal combinations on both dimensions, and each dimension denotes a translation candidate in this example; (b)-(d): the process of popping the best candidate of top three items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>NBS-SN: Naive Beam Search without SN NBS+SN: Naive Beam Search with SN NCP-SN: Cube Pruning without SN NCP+SN: Cube Pruning with SN ACP-SN: Accelerated Cube Pruning without SN ACP+SN: Accelerated Cube Pruning with SN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>Figure 4: Comparison among the decoding results of the MT03 test dataset on the single GeForce GTX TITAN X GPU server under the three different searching settings. y-axis represents the BLEU score of translations, x-axis indicates that how long it will take for translating one word on average.</figDesc><graphic url="image-6.png" coords="8,75.62,271.93,217.70,130.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>Figure 6: Comparison among the decoding results of the MT03 test dataset on the single GeForce GTX TITAN X GPU server under the three different searching settings. y-axis represents the BLEU score of translations, x-axis indicates that how long it will take for translating one word on average.</figDesc><graphic url="image-10.png" coords="9,75.62,281.90,217.70,130.62" type="bitmap" /></figure>

			<note place="foot" n="1"> The initial target word index y 0 0 equals to 0, which actually corresponds to the Beginning Of Sentence (BOS) token in target vocabulary.</note>

			<note place="foot" n="2"> Note that, identical to Bahdanau et al. (2015), we only used 30k as the vocabulary size.</note>

			<note place="foot" n="3"> These sentence pairs are mainly extracted from LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/mosesdecoder/blob/ master/scripts/generic/mteval-v11b.pl 5 https://github.com/nyu-dl/dl4mt-tutorial/blob/ master/docs/cgru.pdf</note>

			<note place="foot" n="6"> Following Devlin et al. (2014), we had tried 0.01, 0.1, 0.5 1.0 and 10.0 for the value of α, we found that 0.5 produced the best result.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the three anonymous reviewers for their comments, Kai Zhao and Haitao Mi for ugges-tions. This work is supported in part by NSF IIS-1817231 &amp; IIS-1656051, and is also supported in part by National Natural Science Foundation of <ref type="bibr">China (No. 61472428 &amp; No. 61662077</ref>).</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio chord recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISMIR</title>
		<imprint>
			<biblScope unit="page" from="335" to="340" />
			<date type="published" when="2013" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved beam search with constrained softmax for nmt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit XV</title>
		<meeting>MT Summit XV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">297</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Better kbest parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology, Parsing &apos;05</title>
		<meeting>the Ninth International Workshop on Parsing Technology, Parsing &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The alignment template approach to statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="449" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, Georgia, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
