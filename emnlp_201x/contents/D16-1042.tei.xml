<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Tree Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Brychcín</surname></persName>
							<email>brychcin@kiv.zcu.cz nlp.kiv.zcu.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Applied Sciences</orgName>
								<orgName type="laboratory">NTIS -New Technologies for the Information Society</orgName>
								<orgName type="institution">University of West Bohemia</orgName>
								<address>
									<addrLine>Technická 8</addrLine>
									<postCode>306 14</postCode>
									<settlement>Plzeň</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Tree Language Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="436" to="446"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles. The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The first one is based on Gibbs sampling. It is fast, but does not guarantee to find the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to find the most probable tree. We provide comparison of both algorithms. We combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language modeling is one of the core disciplines in natural language processing (NLP). Automatic speech recognition, machine translation, optical character recognition, and other tasks strongly de- pend on the language model (LM). An improve- ment in language modeling often leads to better performance of the whole task. The goal of lan- guage modeling is to determine the joint probabil- ity of a sentence. Currently, the dominant approach is n-gram language modeling, which decomposes the joint probability into the product of conditional probabilities by using the chain rule. In traditional n-gram LMs the words are represented as distinct symbols. This leads to an enormous number of word combinations.</p><p>In the last years many researchers have tried to capture words contextual meaning and incorporate it into the LMs. Word sequences that have never been seen before receive high probability when they are made of words that are semantically similar to words forming sentences seen in training data. This ability can increase the LM performance because it reduces the data sparsity problem. In NLP a very common paradigm for word meaning representation is the use of the Distributional hypothesis. It sug- gests that two words are expected to be semanti- cally similar if they occur in similar contexts (they are similarly distributed in the text) <ref type="bibr" target="#b10">(Harris, 1954)</ref>. Models based on this assumption are denoted as dis- tributional semantic models (DSMs).</p><p>Recently, semantically motivated LMs have be- gun to surpass the ordinary n-gram LMs. The most commonly used architectures are neural network LMs ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b20">Mikolov et al., 2010;</ref><ref type="bibr" target="#b21">Mikolov et al., 2011</ref>) and class-based LMs. Class- based LMs are more related to this work thus we investigate them deeper. <ref type="bibr" target="#b3">Brown et al. (1992)</ref> introduced class-based LMs of English. Their unsupervised algorithm searches classes consisting of words that are most probable in the given context (one word window in both di- rections). However, the computational complex- ity of this algorithm is very high. This approach was later extended by <ref type="bibr" target="#b18">(Martin et al., 1998;</ref><ref type="bibr">Whit-taker and Woodland, 2003)</ref> to improve the complex- ity and to work with wider context. <ref type="bibr" target="#b9">Deschacht et al. (2012)</ref> used the same idea and introduced La- tent Words Language Model (LWLM), where word classes are latent variables in a graphical model. They apply Gibbs sampling or the expectation max- imization algorithm to discover the word classes that are most probable in the context of surround- ing word classes. A similar approach was pre- sented in <ref type="bibr" target="#b4">(Brychcín and Konopík, 2014;</ref><ref type="bibr" target="#b5">Brychcín and Konopík, 2015)</ref>, where the word clusters de- rived from various semantic spaces were used to im- prove LMs.</p><p>In above mentioned approaches, the meaning of a word is inferred from the surrounding words inde- pendently of their relation. An alternative approach is to derive contexts based on the syntactic relations the word participates in. Such syntactic contexts are automatically produced by dependency parse-trees. Resulting word representations are usually less top- ical and exhibit more functional similarity (they are more syntactically oriented) as shown in <ref type="bibr" target="#b23">(Padó and Lapata, 2007;</ref><ref type="bibr" target="#b15">Levy and Goldberg, 2014)</ref>.</p><p>Dependency-based methods for syntactic parsing have become increasingly popular in NLP in the last years <ref type="bibr">(Kübler et al., 2009)</ref>. <ref type="bibr" target="#b24">Popel and Mareček (2010)</ref> showed that these methods are promising direction of improving LMs. Recently, unsuper- vised algorithms for dependency parsing appeared in <ref type="bibr" target="#b11">(Headden III et al., 2009;</ref><ref type="bibr" target="#b7">Cohen et al., 2009;</ref><ref type="bibr" target="#b25">Spitkovsky et al., 2010;</ref><ref type="bibr" target="#b26">Spitkovsky et al., 2011;</ref><ref type="bibr" target="#b17">Mareček and Straka, 2013)</ref> offering new possibili- ties even for poorly-resourced languages.</p><p>In this work we introduce a new DSM that uses tree-based context to create word roles. The word role contains the words that are similarly distributed over similar tree-based contexts. The word role encodes the semantic and syntactic properties of a word. We do not rely on parse trees as a prior knowl- edge, but we jointly learn the tree structures and word roles. Our model is a soft clustering, i.e. one word may be present in several roles. Thus it is the- oretically able to capture the word polysemy. The learned structure is used as a LM, where each word role is conditioned on its parent role. We present the unsupervised algorithm that discovers the tree struc- tures only from the distribution of words in a training corpus (i.e. no labeled data or external sources of in- formation are needed). In our work we were inspired by class-based <ref type="bibr">LMs (Deschacht et al., 2012)</ref>, unsu- pervised dependency parsing <ref type="bibr" target="#b17">(Mareček and Straka, 2013)</ref>, and tree-based DSMs ( <ref type="bibr" target="#b15">Levy and Goldberg, 2014)</ref>. This paper is organized as follows. We start with the definition of our model (Section 2). The pro- cess of learning the hidden sentence structures is ex- plained in Section 3. We introduce two algorithms for searching the most probable tree for a given sen- tence (Section 4). The experimental results on En- glish and Czech corpora are presented in Section 6. We conclude in Section 7 and offer some directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Latent Tree Language Model</head><p>In this section we describe Latent Tree Language Model (LTLM). LTLM is a generative statistical model that discovers the tree structures hidden in the text corpus.</p><p>Let L be a word vocabulary with total of |L| dis- tinct words. Assume we have a training corpus w divided into S sentences. The goal of LTLM or other LMs is to estimate the probability of a text P (w). Let N s denote the number of words in the s-th sentence. The s-th sentence is a sequence of words w s = {w s,i } Ns i=0 , where w s,i ∈ L is a word at position i in this sentence and w s,0 = &lt; s &gt; is an artificial symbol that is added at the beginning of each sentence.</p><p>Each sentence s is associated with the dependency graph G s . We define the dependency graph as a labeled directed graph, where nodes correspond to the words in the sentence and there is a label for each node that we call role. Formally, it is a triple</p><formula xml:id="formula_0">G s = (V s , E s , r s ) consisting of:</formula><p>• The set of nodes V s = {0, 1, ..., N s }. Each token w s,i is associated with node i ∈ V s .</p><p>• The set of edges E s ⊆ V s × V s .</p><p>• The sequence of roles r s = {r s,i } Ns i=0 , where 1 ≤ r s,i ≤ K for i ∈ V s . K is the number of roles.</p><p>The artificial word w s,0 = &lt; s &gt; at the beginning of the sentence has always role 1 (r s,0 = 1). Anal- ogously to w, the sequence of all r s is denoted as r and sequence of all G s as G. Edge e ∈ E s is an ordered pair of nodes (i, j). We say that i is the head or the parent and j is the dependent or the child. We use the notation i → j for such edge. The directed path from node i to node j is denoted as i * → j.</p><p>We place a few constraints on the graph G s .</p><p>• The graph G s is a tree. It means it is the acyclic graph (if i → j then not j * → i), where each node has one parent (if i → j then not k → j for every k = i).</p><p>• The graph G s is projective (there are no cross edges). For each edge (i, j) and for each k be- tween i and j (i.e. i &lt; k &lt; j or i &gt; k &gt; j) there must exist the directed path i * → k.</p><p>• The graph G s is always rooted in the node 0.</p><p>We denote these graphs as the projective depen- dency trees. Example of such a tree is on <ref type="figure" target="#fig_0">Figure 1</ref>. For the tree G s we define a function</p><formula xml:id="formula_1">h s (j) = i, when (i, j) ∈ E s (1)</formula><p>that returns the parent for each node except the root. We use graph G s as a representation of the Bayesian network with random variables E s and r s . The roles r s,i represent the node labels and the edges express the dependences between the roles. The conditional probability of the role at position i given its parent role is denoted as P (r s,i |r s,hs(i) ). The conditional probability of the word at position i in the sentence given its role r s,i is denoted as P (w s,i |r s,i ).</p><p>We model the distribution over words in the sen- tence s as the mixture</p><formula xml:id="formula_2">P (w s ) = P (w s |r s,0 ) = Ns i=1 K k=1 P (w s,i |r s,i = k)P (r s,i = k|r s,hs(i) ). (2)</formula><p>The root role is kept fixed for each sentence (r s,0 = 1) so P (w s ) = P (w s |r s,0 ).</p><p>We look at the roles as mixtures over child roles and simultaneously as mixtures over words. We can represent dependency between roles with a set of K multinomial distributions θ over K roles, such that</p><formula xml:id="formula_3">P (r s,i |r s,hs(i) = k) = θ (k)</formula><p>r s,i . Simultaneously, de- pendency of words on their roles can be represented as a set of K multinomial distributions φ over |L| words, such that P (w s,i |r</p><formula xml:id="formula_4">s,i = k) = φ (k)</formula><p>w s,i . To make predictions about new sentences, we need to assume a prior distribution on the parameters θ (k) and φ (k) .</p><p>We place a Dirichlet prior D with the vector of</p><formula xml:id="formula_5">K hyper-parameters α on a multinomial distribu- tion θ (k) ∼ D(α) and with the vector of |L| hyper- parameters β on a multinomial distribution φ (k) ∼ D(β).</formula><p>In general, D is not restricted to be Dirichlet distribution. It could be any distribution over dis- crete children, such as logistic normal. In this paper, we focus only on Dirichlet as a conjugate prior to the multinomial distribution and derive the learning algorithm under this assumption.</p><p>The choice of the child role depends only on its parent role, i.e. child roles with the same parent are mutually independent. This property is especially important for the learning algorithm (Section 3) and also for searching the most probable trees (Section 4). We do not place any assumption on the length of the sentence N s or on how many children the parent node is expected to have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parameter Estimation</head><p>In this section we present the learning algorithm for LTLM. The goal is to estimate θ and φ in a way that maximizes the predictive ability of the model (generates the corpus with maximal joint probability P (w)).</p><p>Let χ k (i,j) be an operation that changes the tree</p><formula xml:id="formula_6">G s to G s χ k (i,j) : G s → G s ,<label>(3)</label></formula><p>such that the newly created tree G (V s , E s , r s ) consists of:</p><formula xml:id="formula_7">• V s = V s . • E s = (E s \ {(h s (i), i)}) ∪ {(j, i)}.</formula><p>• r s,a = r s,a for a = i k for a = i , where 0 ≤ a ≤ N s .</p><p>It means that we change the role of the selected node i so that r s,i = k and simultaneously we change the parent of this node to be j. We call this operation a partial change.</p><p>The newly created graph G must satisfy all con- ditions presented in Section 2, i.e. it is a projec- tive dependency tree rooted in the node 0. Thus not all partial changes χ k (i,j) are possible to perform on graph G s .</p><p>Clearly, for the sentence s there is at most Ns(1+Ns) 2 parent changes 1 . To estimate the parameters of LTLM we apply Gibbs sampling and gradually sample χ k (i,j) for trees G s . For doing so we need to determine the posterior predictive distribution 2</p><formula xml:id="formula_8">G s ∼ P (χ k (i,j) (G s )|w, G),<label>(4)</label></formula><p>from which we will sample partial changes to update the trees. In the equation, G denote the sequence of all trees for given sentences w and G s is a result of one sampling. In the following text we derive this equation under assumptions from Section 2.</p><p>The posterior predictive distribution of Dirichlet multinomial has the form of additive smoothing that is well known in the context of language modeling. The hyper-parameters of Dirichlet prior determine how much is the predictive distribution smoothed. Thus the predictive distribution for the word-in-role distribution can be expressed as</p><formula xml:id="formula_9">P (w s,i |r s,i , w \s,i , r \s,i ) = n (w s,i |r s,i ) \s,i + β n (•|r s,i ) \s,i + |L| β ,<label>(5)</label></formula><p>1 The most parent changes are possible for the special case of the tree, where each node i has parent i − 1. Thus for each node i we can change its parent to any node j &lt; i and keep the projectivity of the tree. That is Ns(1+Ns) 2 possibilities. <ref type="bibr">2</ref> The posterior predictive distribution is the distribution of an unobserved variable conditioned by the observed data, i.e. P (Xn+1|X1, ..., Xn), where Xi are i.i.d. (independent and identically distributed random variables). where n (w s,i |r s,i ) \s,i is the number of times the role r s,i has been assigned to the word w s,i , exclud- ing the position i in the s-th sentence. The sym- bol • represents any word in the vocabulary so that</p><formula xml:id="formula_10">n (•|r s,i ) \s,i = l∈L n (l|r s,i )</formula><p>\s,i . We use the symmetric Dirichlet distribution for the word-in-role probabili- ties as it could be difficult to estimate the vector of hyper-parameters β for large word vocabulary. In the above mentioned equation, β is a scalar.</p><p>The predictive distribution for the role-by-role distribution is</p><formula xml:id="formula_11">P r s,i |r s,hs(i) , r \s,i = n (r s,i |r s,hs(i) ) \s,i + α r s,i n (•|r s,hs(i) ) \s,i + K k=1 α k .<label>(6)</label></formula><p>Analogously to the previous equation,</p><formula xml:id="formula_12">n (r s,i |r s,hs(i) ) \s,i</formula><p>denote the number of times the role r s,i has the parent role r s,hs(i) , excluding the position i in the s-th sentence. The symbol • represents any possible role to make the probability distribution summing up to 1. We assume an asymmetric Dirichlet distribution.</p><p>We can use predictive distributions of above men- tioned Dirichlet multinomials to express the joint probability that the role at position i is k (r s,i = k) with parent at position j conditioned on current val- ues of all variables, except those in position i in the sentence s P (r s,i = k, j|w, r \s,i ) ∝ P (w s,i |r s,i = k, w \s,i , r \s,i ) × P (r s,i = k|r s,j , r \s,i ) × a:hs(a)=i P (r s,a |r s,i = k, r \s,i ).</p><p>The choice of the node i role affects the word that is produced by this role and also all the child roles of the node i. Simultaneously, the role of the node i depends on its parent j role. Formula 7 is derived from the joint probability of a sentence s and a tree G s , where all probabilities which do not depend on the choice of the role at position i are removed and equality is replaced by proportionality (∝). We express the final predictive distribution for sampling partial changes χ k (i,j) as</p><formula xml:id="formula_14">P (χ k (i,j) (G s )|w, G) ∝ P (r s,i = k, j|w, r \s,i ) P (r s,i , h s (i)|w, r \s,i )<label>(8)</label></formula><p>that is essentially the fraction between the joint probability of r s,i and its parent after the partial change and before the partial change (conditioned on all other variables). This fraction can be in- terpreted as the necessity to perform this partial change.</p><p>We investigate two strategies of sampling partial changes:</p><p>• Per sentence: We sample a single partial change according to Equation 8 for each sen- tence in the training corpus. It means during one pass through the corpus (one iteration) we perform S partial changes.</p><p>• Per position: We sample a partial change for each position in each sentence. We perform in total N = S s=1 N s partial changes during one pass. Note that the denominator in Equation 8 is constant for this strategy and can be removed.</p><p>We compare both training strategies in Section 6. After enough training iterations, we can estimate the conditional probabilities φ </p><formula xml:id="formula_15">φ (k) l ≈ n (w s,i =l|r s,i =k) + β n (•|r s,i =k) + |L| β<label>(9)</label></formula><formula xml:id="formula_16">θ (p) k ≈ n (r s,i =k|r s,hs(i) =p) + α k n (•|r s,hs(i) =p) + K m=1 α m .<label>(10)</label></formula><p>These equations are similar to equations 5 and 6, but here the counts n do not exclude any position in a corpus.</p><p>Note that in the Gibbs sampling equation, we assume that the Dirichlet parameters α and β are given. We use a fixed point iteration technique de- scribed in <ref type="bibr" target="#b22">(Minka, 2003)</ref> to estimate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head><p>In this section we present two approaches for search- ing the most probable tree for a given sentence as- suming we have already estimated the parameters θ and φ.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Non-deterministic Inference</head><p>We use the same sampling technique as for estimat- ing parameters (Equation 8), i.e. we iteratively sam- ple the partial changes χ k (i,j) . However, we use equa- tions 9 and 10 for predictive distributions of Dirich- let multinomials instead of 5 and 6. In fact, these equations correspond to the predictive distributions over the newly added word w s,i with the role r s,i into the corpus, conditioned on w and r. This sampling technique rarely finds the best solution, but often it is very near.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deterministic Inference</head><p>Here we present the deterministic algorithm that guarantees to find the most probable tree for a given sentence. We were inspired by Cocke-Younger- Kasami (CYK) algorithm <ref type="bibr" target="#b14">(Lange and Leiß, 2009)</ref>. Let T n s,a,c denote the subtree of G s (subgraph of G s that is also a tree) containing subsequence of nodes {a, a + 1, ..., c}. The superscript n de- notes the number of children the root of this sub- tree has. We denote the joint probability of a sub- tree from position a to position c with the cor- responding words conditioned by the root role k as P n ({w s,i } c i=a , T n s,a,c |k). Our goal is to find the tree G s = T 1+ s,0,Ns that maximizes probability P (w s , G s ) = P 1+ ({w s,i } Ns i=0 , T 1+ s,0,Ns |0). Similarly to CYK algorithm, our approach fol-lows bottom-up direction and goes through all pos- sible subsequences for a sentence (sequence of words). At the beginning, the probabilities for sub- sequences of length 1 (i.e. single words) are calcu- lated as P 1+ ({w s,a }, T 1+ s,a,a |k) = P (w s,a |r s,a = k). Once it has considered subsequences of length 1, it goes on to subsequences of length 2, and so on.</p><p>Thanks to mutual independence of roles under the same parent, we can find the most probable subtree with the root role k and with at least two root chil- dren according to</p><formula xml:id="formula_17">P 2+ ({w s,i } c i=a , T 2+ s,a,c |k) = max b:a&lt;b&lt;c [P 1+ ({w s,i } b i=a , T 1+ s,a,b |k)× P 1+ ({w s,i } c i=b+1 , T 1+ s,b+1,c |k)].</formula><p>(11) It means we merge two neighboring subtrees with the same root role k. This is the reason why the new subtree has at least two root children. This formula is visualized on <ref type="figure" target="#fig_3">Figure 2a</ref>. Unfortunately, this does not cover all subtree cases. We find the most proba- ble tree with only root child as follows To find the most probable subtree no matter how many children the root has, we need to take the maximum from both mentioned equations P 1+ = max(P 2+ , P 1 ).</p><p>The algorithm has complexity O(N 3 s K 2 ), i.e. it has cubic dependence on the length of the sentence N s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Side-dependent LTLM</head><p>Until now, we presented LTLM in its simplified ver- sion. In role-by-role probabilities (role conditioned on its parent role) we did not distinguish whether the role is on the left side or the right side of the parent. However, this position keeps important information about the syntax of words (and their roles).</p><p>We assume separate multinomial distributions ˙ θ for roles that are on the left and¨θand¨ and¨θ for roles on the right. Each of them has its own Dirichlet prior with hyper-parameters ˙ α and¨αand¨ and¨α, respectively. The pro- cess of estimating LTLM parameters is almost the same. The only difference is that we need to rede- fine the predictive distribution for the role-by-role distribution (Equation 6) to include only counts of roles on the appropriate side. Also, every time the role-by-role probability is used we need to distin- guish sides:</p><formula xml:id="formula_18">P (r s,i |r s,hs(i) ) = ˙ θ (r s,hs(i) ) r s,i for i &lt; h s (i)) ¨ θ (r s,hs(i) ) r s,i for i &gt; h s (i)) .<label>(13)</label></formula><p>In the following text we always assume the side- dependent LTLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results and Discussion</head><p>In this section we present experiments with LTLM on two languages, English (EN) and Czech (CS).</p><p>As a training corpus we use CzEng 1.0 (Bojar et al., 2012) of the sentence-parallel Czech-English corpus. We choose this corpus because it contains multiple domains, it is of reasonable length, and it is parallel so we can easily provide comparison be- tween both languages. The corpus is divided into 100 similarly-sized sections. We use parts 0-97 for training, the part 98 as a development set, and the last part 99 for testing.</p><p>We have removed all sentences longer than 30 words. The reason was that the complexity of the learning phase and the process of searching most probable trees depends on the length of sentences. It has led to removing approximately a quarter of all sentences. The corpus is available in a tokenized form so the only preprocessing step we use is lower- casing. We keep the vocabulary of 100,000 most fre- quent words in the corpus for both languages. The less frequent words were replaced by the symbol &lt;unk&gt;. Statistics for the final corpora are shown in <ref type="table">Table 1</ref>.</p><p>We measure the quality of LTLM by perplexity that is the standard measure used for LMs. Perplex- ity is a measure of uncertainty. The lower perplexity means the better predictive ability of the LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpora</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentences</head><p>Tokens OOV <ref type="table">rate  EN train  11,530,604 138,034,779  1.30%  EN develop.  117,735  1,407,210  1.28%  EN test  117,360  1,405,106  1.33%  CS train  11,832,388 133,022,572  3.98%  CS develop.  120,754  1,353,015  4.00%  CS test  120,573  1,357,717</ref> 4.03% <ref type="table">Table 1</ref>: Corpora statistics. OOV rate denotes the out-of-vocabulary rate. During the process of parameter estimation we measure the perplexity of joint probability of sen- tences and their trees defined as PPX(P (w, G)) = N 1 P (w,G) , where N is the number of all words in the training data w.</p><p>As we describe in Section 3, there are two ap- proaches for the parameter estimation of LTLM. During our experiments, we found that the per- position strategy of training has the ability to con- verge faster, but to a worse solution compared to the per-sentence strategy which converges slower, but to a better solution.</p><p>We train LTLM by 500 iterations of the per- position sampling followed by another 500 iterations of the per-sentence sampling. This proves to be effi-</p><note type="other">Model EN CS 2-gram MKN 165.9 272.0 3-gram MKN 67.7 99.3 4-gram MKN 46.2 73.5 300n RNNLM 51.2 69.4 4-gram LWLM 52.7 81.5 PoS STLM 455.7 747.3 1000r STLM 113.7 211.0 1000r det. LTLM 54.2 111.1 4-gram MKN + 300n RNNLM 36.8 (-20.4%) 49.5 (-32.7%) 4-gram MKN + 4-gram LWLM 41.5 (-10.2%) 62.4 (-15.1%)</note><p>4-gram MKN + PoS STLM 42.9 (-7.1%) 63.3 (-13.9%) 4-gram MKN + 1000r STLM 33.6 (-27.3%) 50.1 (-31.8%) 4-gram MKN + 1000r det. LTLM 24.9 (-43.1%) 37.2 (-49.4%) <ref type="table">Table 2</ref>: Perplexity results on the test data. The numbers in brackets are the relative improvements compared with standalone 4-gram MKN LM. cient in both aspects, the reasonable speed of con- vergence and the satisfactory predictive ability of the model. The learning curves are showed on <ref type="figure" target="#fig_5">Fig- ure 3</ref>. We present the models with 10, 20, 50, 100, 200, 500, and 1000 roles. The higher role cardinal- ity models were not possible to create because of the very high computational requirements. Similarly to the training of LTLM, the non-deterministic in- ference uses 100 iterations of per-position sampling followed by 100 iterations of per-sentence sampling.</p><p>In the following experiments we measure how well LTLM generalizes the learned patterns, i.e. how well it works on the previously unseen data. Again, we measure the perplexity, but of prob- ability P (w) for mutual comparison with differ- ent LMs that are based on different architectures (PPX(P (w)) = N 1 P (w) ). To show the strengths of LTLM we compare it with several state-of-the-art LMs. We experi- ment with Modified Kneser-Ney (MKN) interpola- tion <ref type="bibr" target="#b6">(Chen and Goodman, 1998)</ref>   <ref type="table">Model\roles  10  20  50 100 200 500 1000  10  20  50 100 200 500</ref>   <ref type="table">Table 3</ref>: Perplexity results on the test data for LTLMs and STLMs with different number of roles. Deter- ministic inference is denoted as det. and non-deterministic inference as non-det.</p><p>MST parser <ref type="bibr" target="#b19">(McDonald et al., 2005</ref>). We use the same architecture as for LTLM and experiment with two approaches to represent the roles. Firstly, the roles are given by the part-of-speech tag (denoted as PoS STLM). No training is required, all information come from CzEng corpus. Secondly, we learn the roles using the same algorithm as for LTLM. The only difference is that the trees are kept unchanged. Note that both deterministic and non-deterministic inference perform almost the same in this model so we do not distinguish between them.</p><p>We combine baseline 4-gram MKN model with other models via linear combination (in the tables denoted by the symbol +) that is simple but very ef- ficient technique to combine LMs. Final probability is then expressed as</p><formula xml:id="formula_19">P (w) = S s=1 Ns i=1 λP LM1 + (λ − 1) P LM2 .<label>(14)</label></formula><p>In the case of MKN the probability P MKN is the probability of a word w s,i conditioned by 3 previous words with MKN smoothing. For LTLM or STLM this probability is defined as</p><formula xml:id="formula_20">P LTLM (w s,i |r s,hs(i) ) = K k=1 P (w s,i |r s,i = k)P (r s,i = k|r s,hs(i) ). (15)</formula><p>We use the expectation maximization algorithm ( <ref type="bibr" target="#b8">Dempster et al., 1977)</ref> for the maximum likelihood estimate of λ parameter on the development part of the corpus. The influence of the number of roles on the perplexity is shown in <ref type="table">Table 3</ref>  From the tables we can see several important findings. Standalone LTLM performs worse than MKN on both languages, however their combi- nation leads to dramatic improvements compared with other LMs. Best results are achieved by 4- gram MKN interpolated with 1000 roles LTLM and the deterministic inference. The perplexity was improved by approximately 46% on English and 49% on Czech compared with standalone MKN. The deterministic inference outperformed the non- deterministic one in all cases. LTLM also signifi-  <ref type="table">,  but  not  everyone  sees  it  .  it  's  one  ,  but  was  he  saw  him  .  that  is  thing  ;  course  it  i  made  it  !  let  was  life  - though  not  she  found  her  ...  there  knows  name  - or  this  they  took  them  '  something  really  father  ...  perhaps  that  that  gave  his  what  nothing  says  mother  :  and  the  it  told  me  "  everything  comes  way  maybe  now  who  felt  a  how  here  does  wife  (  although  had  you  thought  out  why  someone  gets  place  ?  yet  &lt;unk&gt;  someone  knew  that  - god  has  idea  naught  except  all  which  heard  himself  -  Table 4</ref>: Ten most probable word substitutions on each position in the sentence "Everything has beauty, but not everyone sees it." produced by 1000 roles LTLM with the deterministic inference.</p><p>cantly outperformed STLM where the syntactic de- pendency trees were provided as a prior knowledge. The joint learning of syntax and semantics of a sen- tence proved to be more suitable for predicting the words.</p><p>An in-depth analysis of semantic and syntactic properties of LTLM is beyond the scope of this pa- per. For better insight into the behavior of LTLM, we show the most probable word substitutions for one selected sentence (see <ref type="table">Table 4</ref>). We can see that the original words are often on the front po- sitions. Also it seems that LTLM is more syntac- tically oriented, which confirms claims from ( <ref type="bibr" target="#b15">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b23">Padó and Lapata, 2007)</ref>, but to draw such conclusions a deeper analysis is required. The properties of the model strongly depends on the number of distinct roles. We experimented with maximally 1000 roles. To catch the meaning of var- ious words in natural language, more roles may be needed. However, with our current implementation, it was intractable to train LTLM with more roles in a reasonable time. Training 1000 roles LTLM took up to two weeks on a powerful computational unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper we introduced the Latent Tree Lan- guage Model. Our model discovers the latent tree structures hidden in natural text and uses them to predict the words in a sentence. Our experiments with English and Czech corpora showed dramatic improvements in the predictive ability compared with standalone Modified Kneser-Ney LM. Our Java implementation is available for research purposes at https://github.com/brychcin/LTLM.</p><p>It was beyond the scope of this paper to explic- itly test the semantic and syntactic properties of the model. As the main direction for future work we plan to investigate these properties for example by comparison with human-assigned judgments. Also, we want to test our model in different NLP tasks (e.g. speech recognition, machine translation, etc.).</p><p>We think that the role-by-role distribution should depend on the distance between the parent and the child, but our preliminary experiments were not met with success. We plan to elaborate on this assump- tion. Another idea we want to explore is to use different distributions as a prior to multinomials. For example, <ref type="bibr" target="#b1">Blei and Lafferty (2006)</ref> showed that the logistic-normal distribution works well for topic modeling because it captures the correlations be- tween topics. The same idea might work for roles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of LTLM for the sentence "Everything has beauty, but not everyone sees it."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>a) The root has two or more children. (b) The root has only one child.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Searching the most probable subtrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>P</head><label></label><figDesc>1 ({w s,i } c i=a , T 1 s,a,c |k) = max b,m:a≤b≤c,1≤m≤K [P (w s,b |r s,b = m) × P (r s,b = m|k)× P 1+ ({w s,i } b−1 i=a , T 1+ s,a,b−1 |m)× P 1+ ({w s,i } c i=b+1 , T 1+ s,b+1,c |m)]. (12) This formula is visualized on Figure 2b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves of LTLM for both English and Czech. The points in the graphs represent the perplexities in every 100th iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, with Recurrent Neural Network LM (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011) 3 , and with LWLM (De- schacht et al., 2012) 4 . We have also created syntac- tic dependency tree based LM (denoted as STLM). Syntactic dependency trees for both languages are provided within CzEng corpus and are based on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model weights optimized on development data when interpolated with 4-gram MKN LM.</figDesc></figure>

			<note place="foot" n="3"> Implementation is available at http://rnnlm.org/. Size of the hidden layer was set to 300 in our experiments. It was computationally intractable to use more neurons. 4 Implementation is available at http://liir.cs. kuleuven.be/software.php.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This publication was supported by the project LO1506 of the Czech Ministry of Education, Youth and Sports. Computational resources were provided by the CESNET LM2015042 and the CERIT Sci-entific Cloud LM2015085, provided under the pro-gramme "Projects of Large Research, Development, and Innovations Infrastructures". Lastly, we would like to thank the anonymous reviewers for their in-sightful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The joy of parallelism with czeng 1.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Zdeněkzdeněkˇzdeněkžabokrtsk´zdeněkžabokrtsk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Majliš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Maršík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey, may</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classbased n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic spaces for improving language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Brychcín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloslav</forename><surname>Konopík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="192" to="209" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent semantics in language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Brychcín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloslav</forename><surname>Konopík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="108" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Group, Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Logistic normal priors for unsupervised probabilistic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The latent words language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Deschacht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">De</forename><surname>Belder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="384" to="409" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<title level="m">Dependency parsing. Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">To cnf or not to cnf? an efficient yet presentable version of the cyk algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Leiß</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Informatica Didactica, 8</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Algorithms for bigram and trigram word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Liermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="37" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Prague Congress Center</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Estimating a dirichlet distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dependencybased construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perplexity of n-gram and dependency language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Text, Speech and Dialogue, TSD&apos;10</title>
		<meeting>the 13th International Conference on Text, Speech and Dialogue, TSD&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing without gold part-of-speech tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language modelling for russian and english using words and classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="104" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
