<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsu</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group RWTH Aachen University Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="862" to="868"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>862</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised learning of cross-lingual word embedding offers elegant matching of words across languages, but has fundamental limitations in translating sentences. In this paper , we propose simple yet effective methods to improve word-byword translation of cross-lingual embeddings, using only monolingual corpora but without any back-translation. We integrate a language model for context-aware search, and use a novel denoising autoencoder to handle reordering. Our system surpasses state-of-the-art unsupervised neural translation systems without costly iterative training. We also analyze the effect of vocabulary size and denoising type on the translation performance , which provides better understanding of learning the cross-lingual word embedding and its usage in translation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building a machine translation (MT) system re- quires lots of bilingual data. Neural MT mod- els ( <ref type="bibr" target="#b3">Bahdanau et al., 2015)</ref>, which become the current standard, are even more difficult to train without huge bilingual supervision <ref type="bibr" target="#b12">(Koehn and Knowles, 2017)</ref>. However, bilingual resources are still limited to some of the selected language pairs-mostly from or to English.</p><p>A workaround for zero-resource language pairs is translating via an intermediate (pivot) language. To do so, we need to collect parallel data and train MT models for source-to-pivot and pivot-to-target individually; it takes a double effort and the de- coding is twice as slow.</p><p>Unsupervised learning is another alternative, where we can train an MT system with only mono- lingual corpora. Decipherment methods ( <ref type="bibr" target="#b17">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b15">Nuhn et al., 2013)</ref> are the first work in this direction, but they often suffer from a huge latent hypothesis space ( <ref type="bibr" target="#b10">Kim et al., 2017)</ref>. Recent work by <ref type="bibr" target="#b2">Artetxe et al. (2018)</ref> and  train sequence-to-sequence MT models of both translation directions together in an unsupervised way. They do back-translation <ref type="bibr" target="#b18">(Sennrich et al., 2016a</ref>) back and forth for every itera- tion or batch, which needs an immensely long time and careful tuning of hyperparameters for massive monolingual data.</p><p>Here we suggest rather simple methods to build an unsupervised MT system quickly, based on word translation using cross-lingual word embed- dings. The contributions of this paper are:</p><p>• We formulate a straightforward way to com- bine a language model with cross-lingual word similarities, effectively considering context in lexical choices.</p><p>• We develop a postprocessing method for word-by-word translation outputs using a de- noising autoencoder, handling local reorder- ing and multi-aligned words.</p><p>• We analyze the effect of different artificial noises for the denoising model and propose a novel noise type.</p><p>• We verify that cross-lingual embedding on subword units performs poorly in translation.</p><p>• We empirically show that cross-lingual map- ping can be learned using a small vocabulary without losing the translation performance.</p><p>The proposed models can be efficiently trained with off-the-shelf softwares with little or no changes in the implementation, using only mono- lingual data. The provided analyses help for bet- ter learning of cross-lingual word embeddings for translation purpose. Altogether, our unsupervised MT system outperforms the sequence-to-sequence neural models even without training signals from the opposite translation direction, i.e. via back- translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cross-lingual Word Embedding</head><p>As a basic step for unsupervised MT, we learn a word translation model from monolingual corpora of each language. In this work, we exploit cross- lingual word embedding for word-by-word trans- lation, which is state-of-the-art in terms of type translation quality ( <ref type="bibr" target="#b1">Artetxe et al., 2017;</ref><ref type="bibr" target="#b5">Conneau et al., 2018)</ref>.</p><p>Cross-lingual word embedding is a continu- ous representation of words whose vector space is shared across multiple languages. This en- ables distance calculation between word embed- dings across languages, which is actually finding translation candidates.</p><p>We train cross-lingual word embedding in a fully unsupervised manner:</p><p>1. Learn monolingual source and target embed- dings independently. For this, we run skip- gram algorithm augmented with character n- gram ( <ref type="bibr" target="#b4">Bojanowski et al., 2017</ref>).</p><p>2. Find a linear mapping from source embed- ding space to target embedding space by adversarial training ( <ref type="bibr" target="#b5">Conneau et al., 2018</ref>). We do not pre-train the discriminator with a seed dictionary, and consider only the top V cross-train words of each language as input to the discriminator.</p><p>Once we have the cross-lingual mapping, we can transform the embedding of a given source word and find a target word with the closest em- bedding, i.e. nearest neighbor search. Here, we apply cross-domain similarity local scaling <ref type="bibr" target="#b5">(Conneau et al., 2018)</ref> to penalize the word similarities in dense areas of the embedding distribution.</p><p>We further refine the mapping obtained from Step 2 as follows (Artetxe et al., 2017):</p><p>3. Build a synthetic dictionary by finding mu- tual nearest neighbors for both translation di- rections in vocabularies of V cross-train words.</p><p>4. Run a Procrustes problem solver with the dic- tionary from Step 3 to re-train the mapping ( <ref type="bibr" target="#b20">Smith et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Repeat</head><p>Step 3 and 4 for a fixed number of iterations to update the mapping further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence Translation</head><p>In translating sentences, cross-lingual word em- bedding has several drawbacks. We describe each of them and our corresponding solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context-aware Beam Search</head><p>The word translation using nearest neighbor search does not consider context around the cur- rent word. In many cases, the correct translation is not the nearest target word but other close words with morphological variations or synonyms, de- pending on the context. The reasons are in two-fold: 1) Word embed- ding is trained to place semantically related words nearby, even though they have opposite meanings. 2) A hubness problem of high-dimensional em- bedding space hinders a correct search, where lots of different words happen to be close to each other <ref type="bibr" target="#b16">(Radovanovi´cRadovanovi´c et al., 2010)</ref>.</p><p>In this paper, we integrate context information into word-by-word translation by combining a lan- guage model (LM) with cross-lingual word em- bedding. Let f be a source word in the current position and e a possible target word. Given a his- tory h of target words before e, the score of e to be the translation of f would be:</p><formula xml:id="formula_0">L(e; f, h) = λ emb log q(f, e) + λ LM log p(e|h)</formula><p>Here, q(f, e) is a lexical score defined as:</p><formula xml:id="formula_1">q(f, e) = d(f, e) + 1 2 where d(f, e) ∈ [−1, 1]</formula><p>is a cosine similarity be- tween f and e. It is transformed to the range [0, 1] to make it similar in scale with the LM probability.</p><p>In our experiments, we found that this simple lin- ear scaling is better than sigmoid or softmax func- tions in the final translation performance. Accumulating the scores per position, we per- form a beam search to allow only reasonable trans- lation hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Denoising</head><p>Even when we have correctly translated words for each position, the output is still far from an ac- ceptable translation. We adopt sequence denois- ing autoencoder ( <ref type="bibr" target="#b9">Hill et al., 2016)</ref> to improve the translation output of Section 3.1. The main idea is to train a sequence-to-sequence neural network model that takes a noisy sentence as input and pro- duces a (denoised) clean sentence as output, both of which are of the same (target) language. The model was originally proposed to learn sentence embeddings, but here we use it directly to actually remove noise in a sentence.</p><p>Training label sequences for the denoising net- work would be target monolingual sentences, but we do not have their noisy versions at hand. Given a clean target sentence, the noisy input should be ideally word-by-word translation of the corre- sponding source sentence. However, such bilin- gual sentence alignment is not available in our un- supervised setup.</p><p>Instead, we inject artificial noise into a clean sentence to simulate the noise of word-by-word translation. We design different noise types after the following aspects of word-by-word translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Insertion</head><p>Word-by-word translation always outputs a target word for every position. However, there are a plenty of cases that multiple source words should be translated to a single target word, or that some source words are rather not translated to any word to make a fluent output. For example, a German sentence "Ich höre zu." would be translated to "I'm listening to." by a word-by-word transla- tor, but "I'm listening." is more natural in English <ref type="figure" target="#fig_0">(Figure 1</ref>). We pretend to have extra target words which might be translation of redundant source words, by inserting random target words to a clean sentence:</p><p>1. For each position i, sample a probability p i ∼ Uniform(0, 1).</p><p>2. If p i &lt; p ins , sample a word e from the most frequent V ins target words and insert it before position i.</p><p>We limit the inserted words by V ins because tar- get insertion occurs mostly with common words, e.g. prepositions or articles, as the example above. We insert words only before-not after-a posi- tion, since an extra word after the ending word (usually a punctuation) is not probable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Deletion</head><p>Similarly, word-by-word translation cannot handle the contrary case: when a source word should be translated into more than one target words, or a target word should be generated from no source words for fluency. For example, a German word "im" must be "in the" in English, but word transla- tion generates only one of the two English words. Another example is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. To simulate such situations, we drop some words randomly from a clean target sentence (Hill et al., 2016):</p><p>1. For each position i, sample a probability p i ∼ Uniform(0, 1).</p><p>2. If p i &lt; p del , drop the word in the position i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Reordering</head><p>Also, translations generated word-by-word are not in an order of the target language. In our beam search, LM only assists in choosing the right word in context but does not modify the word order. A common reordering problem of German→English is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.  2. Add δ i to index i and sort the incremented indices i + δ i in an increasing order.</p><p>3. Rearrange the words to be in the new po- sitions, to which their original indices have moved by Step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>de-en en-de fr-en en-fr System BLEU [%] BLEU [%] BLEU [%] BLEU [%]</head><p>Word-by-Word 11 This is a generalized version of swapping two neighboring words ( <ref type="bibr" target="#b9">Hill et al., 2016</ref>). Reordering is highly dependent of each language, but we found that this noise is generally close to word- by-word translation outputs.</p><p>Insertion, deletion, and reordering noises were ap- plied to each mini-batch with different random seeds, allowing the model to see various noisy ver- sions of the same clean sentence over the epochs.</p><p>Note that the deletion and permutation noises are integrated in the neural MT training of <ref type="bibr" target="#b2">Artetxe et al. (2018)</ref> and  as additional training objectives. Whereas we optimize an inde- pendent model solely for denoising without archi- tecture change. It allows us to easily train a larger network with a larger data. Insertion noise is of our original design, which we found to be the most effective (Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We applied the proposed methods on WMT 2016 German↔English task and WMT 2014 French↔English task. For German/English, we trained word embeddings with 100M sentences sampled from News Crawl 2014-2017 monolin- gual corpora. For French, we used News Crawl 2007-2014 (around 42M sentences). The data was lowercased and filtered to have a maximum sen- tence length 100. German compound words were splitted beforehand. Numbers were replaced with category labels and recovered back after decoding by looking at the source sentence. Also, frequent casing was applied to the translation output.</p><p>fasttext <ref type="bibr" target="#b4">(Bojanowski et al., 2017</ref>) was used to learn monolingual embeddings for only the words with minimum count 10. MUSE ( <ref type="bibr" target="#b5">Conneau et al., 2018</ref>) was used for cross-lingual mappings with V cross-train = 100k and 10 refinement iterations (Step 3-5 in Section 2). Other parameters follow the values in <ref type="bibr" target="#b5">Conneau et al. (2018)</ref>. With the same data, we trained 5-gram count-based LMs using KenLM (Heafield, 2011) with its default setting.</p><p>Denoising autoencoders were trained using Sockeye (Hieber et al., 2017) on News Crawl 2016 for German/English and News Crawl 2014 for French. We considered only top 50k frequent words for each language and mapped other words to &lt;unk&gt;. The unknowns in the denoised output were replaced with missing words from the noisy input by a simple line search.</p><p>We used 6-layer Transformer encoder/decoder ( <ref type="bibr" target="#b21">Vaswani et al., 2017</ref>) for denoisers, with embed- ding/hidden layer size 512, feedforward sublayer size 2048 and 8 attention heads.</p><p>As a validation set for the denoiser training, we used newstest2015 (German ↔ English) or newstest2013 (French ↔ English), where the input/output sides both have the same clean target sentences, encouraging a denoiser to keep at least clean part of word-by-word translations. Here, the noisy input showed a slight degradation of per- formance; the model seemed to overfit to specific noises in the small validation set.</p><p>Optimization of the denoising models was done with Adam ( <ref type="bibr" target="#b11">Kingma and Ba, 2015)</ref>: initial learn- ing rate 0.0001, checkpoint frequency 4000, no learning rate warmup, multiplying 0.7 to the learn- ing rate when the perplexity on the validation set did not improve for 3 checkpoints. We stopped the training if it was not improved for 8 checkpoints. <ref type="table">Table 1</ref> shows the results. LM improves word- by-word baselines consistently in all four tasks, giving at least +3% BLEU. When our denoising model is applied on top of it, we have additional gain around +3% BLEU. Note that our meth- ods do not involve any decoding steps to gener- ate pseudo-parallel training data, but still perform better than unsupervised MT systems that rely on repetitive back-translations ( <ref type="bibr" target="#b2">Artetxe et al., 2018;</ref>) by up to +3.9% BLEU. The total training time of our method is only 1-2 days with a single GPU.  To examine the effect of each noise type in de- noising autoencoder, we tuned each parameter of the noise and combined them incrementally (Ta- ble 2). Firstly, for permutations, a significant im- provement is achieved from d per = 3, since a local reordering usually involves a sequence of 3 to 4 words. With d per &gt; 5, it shuffles too many con- secutive words together, yielding no further im- provement. This noise cannot handle long-range reordering, which is usually a swap of words that are far from each other, keeping the words in the middle as they are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study: Denoising</head><p>Secondly, we applied the deletion noise with different values of p del . 0.1 gives +0.8% BLEU, but we immediately see a degradation with a larger value; it is hard to observe one-to-many transla- tions more than once in each sentence pair.</p><p>Finally, we optimized V ins for the insertion noise, fixing p ins = 0.1. Increasing V ins is gener- ally not beneficial, since it provides too much vari- ations in the inserted word; it might not be related to its neighboring words. Overall, we observe the best result (+1.5% BLEU) with V ins = 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study: Vocabulary</head><p>We also examined how the translation perfor- mance varies with different vocabularies of cross- lingual word embedding in  than word embeddings, especially with smaller vocabulary size. For small BPE tokens (1-3 char- acters), the context they meet during the embed- ding training is much more various than a com- plete word, and a direct translation of such small token to a BPE token of another language would be very ambiguous.</p><p>For word level embeddings, we compared dif- ferent vocabulary sizes used for training the cross-lingual mapping (the second step in Section 2). Surprisingly, cross-lingual word embedding learned only on top 20k words is comparable to that of 200k words in the translation quality. We also increased the search vocabulary to more than 200k but the performance only degrades. This means that word-by-word translation with cross- lingual embedding depends highly on the frequent word mappings, and learning the mapping be- tween rare words does not have a positive effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a simple pipeline to greatly improve sentence translation based on cross-lingual word embedding. We achieved context-aware lexical choices using beam search with LM, and solved insertion/deletion/reordering problems using denoising autoencoder. Our novel insertion noise shows a promising performance even combined with other noise types. Our meth- ods do not need back-translation steps but still out- performs costly unsupervised neural MT systems. In addition, we proved that for general translation purpose, an effective cross-lingual mapping can be learned using only a small set of frequent words, not on subword units.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of denoising an insertion noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of denoising a deletion noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of denoising the reordering noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>From</head><label></label><figDesc>a clean target sentence, we corrupt its word order by random permutations. We limit the maximum distance between an original position and its new position like Lample et al. (2018): 1. For each position i, sample an integer δ i from [0, d per ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Translation results with different values of de-
noising parameters for German→English. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>The first three 
rows show that BPE embeddings performs worse 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Translation results with different vocabularies 
for German→English. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has received funding from the Euro-pean Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme, grant agreement No. 694537 (SEQ-CLAS). The GPU computing cluster was partially funded by Deutsche Forschungsgemeinschaft (DFG) under grant INST 222/1168-1 FUGG. The work reflects only the authors' views and neither ERC nor DFG is responsible for any use that may be made of the information it contains.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual word embeddings for low-resource language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Makarucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="937" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations (ICLR 2018)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR 2015)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR 2018</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL-HLT 2008)</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 6th Workshop on Statistical Machine Translation (WMT 2011)</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sockeye: A toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno>arXiv Preprint. 1712.05690</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised training for large vocabulary translation using sparse lexicon and word classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="650" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR 2015)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2017 1st Workshop on Neural Machine Translation (NMT 2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2003)</title>
		<meeting><address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR 2018</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beam search for solving substitution ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51th Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1569" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the existence of obstinate results in vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Radovanovi´cradovanovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjana</forename><surname>Ivanovi´civanovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2010)</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association of Computational Linguistics (ACL 2016)</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association of Computational Linguistics (ACL 2016)</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
