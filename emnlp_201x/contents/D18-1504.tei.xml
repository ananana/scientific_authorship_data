<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Learning for Targeted Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><surname>Key</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Learning for Targeted Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4737" to="4742"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4737</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Targeted sentiment analysis (TSA) aims at extracting targets and classifying their sentiment classes. Previous works only exploit word em-beddings as features and do not explore more potentials of neural networks when jointly learning the two tasks. In this paper, we carefully design the hierarchical multi-layer bidi-rectional gated recurrent units (HMBi-GRU) model to learn abstract features for both tasks, and we propose a HMBi-GRU based joint model which allows the target label of word to have influence on its sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HMBi-GRU in learning abstract features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes ( <ref type="bibr" target="#b5">Hu and Liu, 2004;</ref><ref type="bibr" target="#b6">Jin et al., 2009;</ref><ref type="bibr" target="#b9">Li et al., 2010;</ref><ref type="bibr" target="#b20">Yang and Cardie, 2013)</ref>. For example, given a sentence "ESPN poll says Michael Jordan is the greatest basketball athlete", the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respec- tively.</p><p>Targeted sentiment analysis can be seen as two tasks: target extraction and sentiment classifica- tion. Some researchers have tackled two tasks separately, e.g., target extraction ( <ref type="bibr" target="#b10">Liu et al., 2013;</ref><ref type="bibr" target="#b18">Wang et al., 2016a;</ref><ref type="bibr" target="#b22">Yin et al., 2016</ref>) and senti- ment classification ( <ref type="bibr" target="#b17">Tang et al., 2016;</ref><ref type="bibr" target="#b19">Wang et al., 2016b;</ref><ref type="bibr" target="#b16">Ruder et al., 2016)</ref>. Recently, some re- searches have attempted to conduct the two tasks jointly and generally see them as sequence label- ing problems, where the B/I/O labels indicate tar- get boundaries and the Positive/Neutral/Negative labels denote sentiment classes <ref type="bibr" target="#b8">(Klinger and Cimiano, 2013;</ref><ref type="bibr" target="#b20">Yang and Cardie, 2013)</ref>. <ref type="bibr" target="#b12">Mitchell et al. (2013)</ref> explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three mod- els: pipeline, joint and collapsed, according to dif- ferent labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, <ref type="bibr" target="#b23">Zhang et al. (2015)</ref> introduce word embedding representations into the CRF framework and find that it is bene- ficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or col- lapsed methods.</p><p>With the success of deep learning techniques, neural networks have demonstrated their capabil- ity of sequence labeling <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b13">Pei et al., 2014;</ref><ref type="bibr" target="#b1">Chen et al., 2015</ref>). However, <ref type="bibr" target="#b23">Zhang et al. (2015)</ref> only use word embeddings to enrich features without taking full advantages of neural networks' potential in automatically capturing im- portant sequence labeling features like long dis- tance dependencies and character-level features.</p><p>To make better use of neural networks to explore appropriate character-level features and high-level semantic features for the two tasks, we design a hierarchical multi-layer bidirec- tional gated recurrent units networks (HMBi- GRU) which uses a multi-layer Bi-GRU to auto- matically learn character features (e.g. capital- ization, noun suffix, etc) on letter sequence and model long distance dependencies between words on the concatenation of word embedding and its character features. The learned character features can also address out-of-vocabulary word prob- lems.</p><p>In above example, the target label and senti- ment label for Michael Jordon are "B-Person, I- Person" and "B-Positive, I-Positive", we can see that the boundary information (B, I) of target la- bel and sentiment label is consistent. From the view of human, we should first predict the target label and give corresponding sentiment label af- terwards. Therefore, we introduce target label in- formation into predicting sentiment label. In this way, our model can know about the target bound- ary information when predicting the sentiment la- bel. Meanwhile, we also introduce transition ma- trix <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>) to model the depen- dencies between labels.</p><p>We conduct experiments on two datasets, and the performances show that our models outper- form other baselines. This verifies the effective- ness of neural networks in TSA. In the experi- ments, we find that the target label information is important for predicting sentiment label. We also analyze the performance of multi-layer Bi-GRU and hierarchical architecture in learning character features and dependencies between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We will detailedly introduce our model in this section, and our model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Supposing that a sentence is composed of n words [w 1 , w 2 , ..., w n ]. For each word w i con- sists of l i characters [c 1 , c 2 , ..., c l i ] and l i is the length of w i . We embed all words and charac- ters into low-dimensional real-value vectors which can be learned by language model ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b11">Mikolov et al., 2013</ref>). We represent sen- tence as a matrix of word embeddings W = [E 1 , E 2 , ..., E n ] ∈ R n×dw . Similarly, word w i is denoted as a matrix of character embeddings C i ∈ R l i ×dc , and d w and d c are the size of word embedding and character embedding respectively.</p><p>First, we design a hierarchical two-layer archi- tecture where each layer includes a multi-layer bidirectional Gated Recurrent Units (MBi-GRU). GRU is good at modeling a sequence with the ben- efits of avoiding the gradient vanishing and ex- ploding problems. For a MBi-GRU, supposing that it has M layers of Bi-GRU, the hidden state on layer m ∈ {1, 2, ..., m} at time t ∈ {1, 2, ..., n} is recursively computed by:</p><formula xml:id="formula_0">h m t = BiGRU(h m−1 t , h m t−1 ).<label>(1)</label></formula><p>where the superscript of h denotes the correspond- ing layer of a MBi-GRU, and h 0 means the origi- nal inputs. BiGRU is bidirectional GRU which is defined as:</p><formula xml:id="formula_1">BiGRU(x t , h t−1 ) = − → h t ⊕ ← − h t ; (2) − → h t = GRU(x t , − − → h t−1 ); (3) ← − h t = GRU(x t , ← − − h t−1 ).<label>(4)</label></formula><p>where x t is inputs which can be word embeddings or the hidden states of other BiGRU. ⊕ indicates the operation of concatenating two vectors. With the matrix of character embeddings C i as inputs, we utilize a MBi-GRU to learn character- level abstract features for word w i based on its character embeddings. Through MBi-GRU, we can obtain the hidden states</p><formula xml:id="formula_2">[h M 1 , h M 2 , ..., h M l i</formula><p>] on which a max-pooling operation is applied to out- put the character-level features r i ∈ R 2dc for word w i . The character features of all words in a sentence form a new matrix C ∈ R n×2dc . Next, We concatenate C with the matrix of word embeddings W and denote the concatenation as F ∈ R n×(dw+2dc) . With F as input, We uti- lize another MBi-GRU to learn the hidden states</p><formula xml:id="formula_3">H = [h M 1 , h M 2 , ..., h M n ]</formula><p>as the final representa- tions of the sentence. Therefore, the hierarchical two-layer MBi-GRU architecture can learn high- level abstract features with consideration of both character-level and word-level information.</p><p>After learning the final representations for sen- tence, we first project the features: tf i = h M i of each word into target label space by:</p><formula xml:id="formula_4">y i t = f (tf i · W t p + b t p )<label>(5)</label></formula><p>where W t p and b t p are weight matrix and bias. As we know, the boundary of a target should be the same as that of its sentiment in sequence label. As the example in Section 1, the target la- bel and sentiment label of Michael Jordan are "B- Person, I-Person" and "B-Positive, I-Positive" re- spectively. To learn this kind of consistency, we introduce the target label information into predict- ing sentiment label by:</p><formula xml:id="formula_5">y i s = f (sf i · W s p + b s p )<label>(6)</label></formula><p>where sf i = h M i ⊕ y i t , W t s and b t s are weight ma- trix and bias respectively. This makes our model know the target label information when predicting their sentiment.</p><p>For sequence labeling, there usually exist de- pendencies between labels. Take the target label- ing task for example, label I will never follow label</p><formula xml:id="formula_6">M i c h a e l J o r d a n … … MBi-GRU MBi-GRU … … MBi-GRU I-Person B-Person B-Positive I-Positive r i E i y i t y i s y i+1 t y i+1 s E i+1 r i+1 A s i,i+1</formula><p>A t i,i+1  </p><formula xml:id="formula_7">s(y t , x, θ t ) = n i=1 (A t i−1,i + y i t );<label>(7)</label></formula><p>where A t is label transition matrix for target label- ing. θ t = θ ∪ {A t i,j }, and θ denotes parameters of HMBi-GRUs.</p><p>Next, we normalize the target label scores over all possible labeling paths of target (i.e., Y t ) by a softmax function:</p><formula xml:id="formula_8">p t (y t |x) = e s(yt,x,θt) ˆ yt∈Yt e s( ˆ yt,x,θt) ;<label>(8)</label></formula><p>We can also use Eq. 7 and Eq. 8 to get the normal- ized sentiment label scores p s (y s |x). To train our model, we define the loss function by:</p><formula xml:id="formula_9">loss = − log(p t (y t |x)) − log(p s (y s |x)).<label>(9)</label></formula><p>Finally, we obtain targets label sequence y * t and their sentiment label sequence y * s which have max- imal score y * t = arg maxˆy∈Ytmaxˆ maxˆy∈Yt (s(x, ˆ y, θ t )) y * s = arg maxˆy∈Ysmaxˆ maxˆy∈Ys (s(x, ˆ y, θ s )). y * t and y * s can be com- puted by Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>To validate the effectiveness of our model, we con- duct experiments on two datasets, consisting of  <ref type="bibr">3</ref> for English words and Spanish words re- spectively. The character embeddings are initial- ized by Xavier ( <ref type="bibr" target="#b4">Glorot and Bengio, 2010)</ref> and their dimension is 50. In our model, all unknown words, weight matrices and biases are initialized by Xavier <ref type="bibr" target="#b4">Glorot and Bengio (2010)</ref>. The dimen- sions of the character-level and word-level hidden states in MBi-GRU are set to 300 and 600 respec- tively. The layer number of multi-layer bidirec- tional GRU is set to 2. To avoid overfitting, we adopt dropout on embeddings, sf i and tf i , and the dropout rate is set to 0.5. The word embed- dings and character embeddings will be tuned dur- ing training. Finally, we utilize Adam ( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>) to optimize all parameters of our model.</p><note type="other">Datasets #Sent #Target #Pos #Neg #Neu English 2350 3288 707 275 2306 Spanish 5145 6658 1555 1007 4096</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>To investigate the performance of our joint model, we compare it with several baselines as follows:</p><p>•  inputs and multi-label CRF which contains two separate output clique potentials and two sepa- rate edge clique potentials for target extraction and sentiment classification respectively. There also exist links between target labels and sentiment la- bels for each word ( <ref type="bibr" target="#b23">Zhang et al., 2015</ref>).</p><p>• Neural uses word embeddings transformed with non-linear function as inputs, and others are the same as Discrete model ( <ref type="bibr" target="#b23">Zhang et al., 2015</ref>).</p><p>• Integrated integrates both discrete features and word embeddings into the same CRF frame- work and other settings are the same as Dis- crete ( <ref type="bibr" target="#b23">Zhang et al., 2015</ref>).</p><p>• Bi-GRU only uses word embeddings as in- puts, and Bi-GRU is employed to learn represen- tations for sentence.</p><p>• MBi-GRU also uses word embeddings as in- puts, but MBi-GRU is utilized to model sentence.</p><p>• HBi-GRU first uses Bi-GRU to learn charac- ter level features for each word. Then, character level features and word embeddings are concate- nated as inputs for another Bi-GRU to learn final representations for sentence.</p><p>• No-Target uses HMBi-GRU to learn repre- sentations for sentence, but h M i (depicted in Sec- tion 2) are used to predict target label and senti- ment label separately. No-Target doesn't let target label information to affect sentiment label. This is the biggest difference between No-Target and ours.</p><p>It is noticed that all of Bi-GRU, MBi-GRU and HBi-GRU use transition matrix to model the de- pendencies between labels and introduce target la- bel information into predicting sentiment label. <ref type="table" target="#tab_3">Table 2</ref> displays the performance comparison of our models with the baselines. We can see that Discrete gets the worst results on English dataset, and Neural gets the worst results on Spanish dataset. The Integrate greatly improves the perfor- mances on both datasets because discrete features and word embeddings can complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>Bi-GRU greatly improves the performance compared with Discrete and Neural but gets worse performance than Integrate. This verifies the ef- fectiveness of neural networks in TAS. However, simple neural networks are not enough to acquire better results. MBi-GRU learns high-level features via multi-layer bidirectional GRU and achieves comparable results compared with Integrate.</p><p>Nevertheless, Bi-GRU and MBi-GRU do not make full use of character-level features. HBi- GRU incorporates character-level features by Bi- GRU on letter sequence of word. We can see that HBi-GRU improves about 1.85% and 1.16% in TSA on both datasets compared with Integrate. The performance of HBi-GRU demonstrates the importance of character-level features in TSA, and the hierarchical architecture is good at leaning multi-level (character-level, word-level) features.</p><p>Our model improves 3.20%, 2.59% in TSA and 2.39%, 0.27% in DT on both datasets compared with the existing best system: Integrate. Com- pared with No-Target, our model introduces tar- get label information into predicting sentiment la- bel and improves about 0.66%, 1.44% in TSA and 0.59%, 0.91% in DT on both datasets. The im- provements demonstrate that target label informa- tion plays important roles in predicting sentiment label. It is noticed that the results of our model in DT are also improved compared with No-Target. The reason may be that the gradients from sen- timent loss have positive effects on detecting tar- gets.</p><p>In a word, our model achieves state-of-the-art in DT and TSA on both datasets. Character-level fea- tures play great roles in DT and TSA, and HMBi- GRU is good at learning multi-level features. It is useful to learn boundary consistence by introduc- ing target label information into predicting senti- ment label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Study</head><p>Here, we use a tweet from English Dataset as a case study, and the tweet is "Congratulations to our Champ Roger Federer .. . We can see that No-Target wrongly regard Champ as the be- ginning position and ignore Federer. The rea- sons are that the first letter of Champ is capital- ized, which may mislead No-Target and there is no correlation between target and sentiment label. In our model, we incorporate target label infor- mation into predicting sentiment label. Therefore, our model tends to force target and sentiment label to have same boundary information.</p><p>This case study shows that the target label in- formation plays important roles in predicting sen- timent label because they share the same boundary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Early works on target sentiment analysis were based on subjects and features. For example, <ref type="bibr" target="#b21">Yi et al. (2003)</ref> extracted all references to the given subject and determined the sentiment of each ref- erence. <ref type="bibr" target="#b5">Hu and Liu (2004)</ref> first proposed several techniques to mine the product features that cus- tomers have expressed their opinions and deter- mined their sentiment, and <ref type="bibr" target="#b15">Popescu and Etzioni (2007)</ref> utilized unsupervised methods to identify opinions with respect to features and determine the polarity of opinions. <ref type="bibr" target="#b6">Jin et al. (2009)</ref> proposed a novel lexicalized HMMs model to mine cus- tomer reviews of a product and extract highly spe- cific product related entities which reviewers ex- pressed their opinion, and they also identified the sentiment of opinion entities. The works of <ref type="bibr" target="#b20">(Yang and Cardie, 2013)</ref> and ( <ref type="bibr" target="#b9">Li et al., 2010</ref>) are similar to <ref type="bibr" target="#b6">(Jin et al., 2009</ref>). However, these works only take pre-defined features into account and can not find new features. To automatically extract targets and predict their sentiment, <ref type="bibr" target="#b12">Mitchell et al. (2013)</ref> first proposed a conditional random fields (CRF) framework to jointly detect entities and identify their sentiment. Based on the work of ( <ref type="bibr" target="#b12">Mitchell et al., 2013)</ref>, <ref type="bibr" target="#b23">Zhang et al. (2015)</ref> explored the ef- fect of word embeddings and automatic feature combinations by extending a CRF baseline using neural networks.</p><p>We propose a neural networks based joint model which extracts targets and their sentiments simultaneously. Our model takes full advan- tages of neural networks' potential in capturing se- quence labeling features such as long distance de- pendencies and character-level features. Further- more, Our model allows the target label to have positive effects on their sentiment label because target label shares boundary information with sen- timent label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a HMBi-GRU based joint model for targeted sentiment analysis. Our model will simultaneously extract targets and pre- dict their sentiment. Furthermore, our model in- troduces target information into predicting corre- sponding sentiment label. Experiments show that the well-designed neural networks can greatly im- prove the result for targeted sentiment analysis, and target label information plays great roles in predicting sentiment label.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall architecture of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.". We apply No- Target and our model on the tweet. No-Target and our model get the same target labels: [O,O,O,O,B- Person,I-Person,...], and we can see that both models correctly extract the target: Roger Fed- erer, and this results show the effectiveness of both models in detecting targets. Our model successfully obtains the correct sentiment labels: [O,O,O,O,B-Positive,I-Positive,...]. However, No- Target predicts a wrong sentiment label sequence: [O,O,O,B-Positive,I-Positive,O,...]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Statistics of Datasets.</head><label>1</label><figDesc></figDesc><table>English tweets and Spanish tweets, which are con-
structed by Mitchell et al. (2013) 1 . Table 2 de-
picts the statistics of data, which contains sentence 
number, target number and the number of positive 
target, negative target and neutral target. To evalu-
ate the system performance, we adopt Precision, 
Recall and F-measure. In our experiments, we 
evaluate the performance of detecting targets (DT) 
and targeted sentiment analysis (TSA) which a tar-
get is taken as correct only when the boundary and 
the sentiment are both correctly recognized. We 
also adopt Precision, Recall and F-measure used 
in Zhang et al. (2015) to evaluate our model. The 
reason why we don't compare with Mitchell et al. 
(2013) is that they only evaluate the beginning of 
targets along with the sentiment expressed towards 
it. 
In our experiments, we use embeddings from 
Pennington et al. (2014) 2 and Cieliebak et al. 
(2017) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>34.06 43.30 43.09 24.67 31.35 71.08 47.56 56.96 46.36 31.02 37.15 Neural 54.45 42.12 47.17 37.55 28.95 32.45 65.05 47.79 55.07 40.28 29.58 34.09 Integrate 61.47 49.28 54.59 44.62 35.84 39.67 71.32 61.11 65.74 46.67 39.99 43.02</figDesc><table>Discrete uses traditional discrete features as Model 

English 
Spanish 
DT 
TSA 
DT 
TSA 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 
Discrete 
59.55 Bi-GRU 
58.13 43.46 49.62 45.76 32.29 37.73 65.24 53.02 58.45 46.33 37.50 41.45 
MBi-GRU 58.27 49.01 53.24 45.80 35.21 39.81 66.14 60.07 62.95 45.61 40.04 42.64 
HBi-GRU 57.24 53.88 55.41 44.94 38.60 41.52 68.24 61.81 64.82 46.53 42.21 44.18 
No-Target 61.24 52.44 56.39 45.90 39.21 42.21 66.72 63.57 65.10 45.06 43.31 44.17 
OURS 
60.12 53.68 56.98 46.52 39.99 42.87 68.64 63.66 66.01 48.09 43.44 45.61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Performance comparison of our models with the baselines.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.m-mitchell.com/code/index. html 2 https://nlp.stanford.edu/projects/ glove/ 3 https://spinningbytes.com/resources/ embeddings/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers for their insight-ful suggestions. Our work is supported by Na-tional Natural Science Foundation of China un-der Grant No.61433015 and the National Key Re-search and Development Program of China under Grant No.2017YFB1002101. The corresponding author of this paper is Houfeng Wang.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A twitter corpus and benchmark resources for german sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Cieliebak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Deriu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Uzdilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SocialNLP</title>
		<imprint>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel lexicalized hmm-based learning framework for web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hay</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional inter-dependencies of subjective expressions and targets and their value for a joint model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="848" to="854" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure-aware review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Ju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="653" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Opinion target extraction using partially-supervised word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2134" to="2140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqui</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ENMLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1643" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orena</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing and text mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A hierarchical model of reviews for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John G</forename><surname>Breslin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02745</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06679</idno>
		<title level="m">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Nasukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Niblack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2003. ICDM 2003. Third IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaimeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="612" to="621" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
