<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoxuan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY)</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2768" to="2773"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent years have witnessed the proliferation of Massive Open Online Courses (MOOCs). With massive learners being offered MOOCs, there is a demand that the forum contents within MOOCs need to be classified in order to facilitate both learners and instructors. Therefore we investigate a significant application, which is to associate forum threads to subtitles of video clips. This task can be regarded as a document ranking problem, and the key is how to learn a distinguish-able text representation from word sequences and learners&apos; behavior sequences. In this paper, we propose a novel cascade model, which can capture both the latent semantics and latent similarity by mod-eling MOOC data. Experimental results on two real-world datasets demonstrate that our textual representation outperforms state-of-the-art unsupervised counterparts for the application.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of Massive Open On- line Courses (MOOCs), more and more learners participate in MOOCs ( <ref type="bibr">Anderson et al., 2014</ref>). Due to the lack of effective management, most of the discussion forums within MOOCs are over- loaded and in chaos ( <ref type="bibr" target="#b6">Huang et al., 2014</ref>). There- fore, a key problem is how to manage the forum contents.</p><p>To manage the forum contents, threads of fo- rums can be regarded as documents and be classi- fied to groups. There are several straightforward methods, such as defining sub-forums accord- ing to weeks and asking learners to tag threads. However their effectiveness is limited <ref type="bibr" target="#b15">(Rossi and Gnawali, 2014)</ref>, because learners have few incen- tives to tag threads. Recently, machine learning solutions have been proposed, e.g., content-related thread identification ( <ref type="bibr" target="#b18">Wise et al., 2016)</ref>, confusion classification ( <ref type="bibr">Agrawal et al., 2015</ref>) and sentiment classification ( <ref type="bibr" target="#b14">Ramesh et al., 2015</ref>). However they are developed for specific research problems and cannot be applied to our problem. Moreover, they require labeling data which needs domain experts to label data for different courses.</p><p>We observe that the video clips of a MOOC would have many well-formed subtitles composed by instructors. Moreover, within MOOC settings, the course contents can be broken down to knowl- edge points, and each video clip just corresponds to a knowledge point. Consequently, we pro- pose to fulfill the application, which is to asso- ciate threads to subtitles of video clips, i.e., thread- subtitle matching. By this way, the relevant videos to the threads can be recommended to learners, and the chaotic threads in discussion forums can also be well grouped.</p><p>However, it is challenging to identify the rele- vant video clips for threads without labeling data. To address this issue, we regard it as a document ranking problem based on the calculation of sim- ilarity between documents. The key problem of this task is to learn a textual representation, which can cluster similar documents and meanwhile dis- tinguish irrelevant ones.</p><p>Intuitively, Bag-of-words model (BOW) can be utilized to calculate the similarity between threads and subtitles ( <ref type="bibr" target="#b16">Salton and Buckley, 1988)</ref>. However, BOW cannot effectively capture se- mantics of words and documents. In addition, recently-studied semantic word embeddings, e.g., <ref type="bibr">Word2Vec (Mikolov et al., 2013)</ref>, can capture the semantics. Para2Vec ( <ref type="bibr" target="#b10">Le and Mikolov, 2014)</ref> can capture the similarity to some degree, but not ex- plicitly model the latent similarity of documents.</p><p>Since the latent similarity is crucial to determine whether a document can be associated to the right target, in our task, the document representation is expected to preserve both the latent semantics and similarity.</p><p>In this paper, we leverage two kinds of se- quential information: 1) word sequence of sub- titles and forum contents, and 2) clickstream log of learning behaviors. Specifically, different from conventional representation learning tasks, e.g. Word2Vec and Para2Vec, we consider the click- stream data, which reflects the relationship be- tween thread and video's subtitle. For instance, if a user watches a video and then clicks a thread in fo- rums, the video would be relevant to the thread. In order to learn representations from the two types of data, we propose a novel cascade model.</p><p>Our basic idea is to jointly model three com- ponents: 1) word-word coherence, 2) document- document coherence, and 3) word-document co- herence. The three components are cascaded for learning the low-dimensional word embeddings. Then the learned embeddings are used to calculate similarities between threads and subtitles.</p><p>To summarize, our contributions include:</p><p>• We study an application-oriented research problem, which is how to capture the latent similarity when learning text representation.</p><p>• We propose a novel cascade model to learn the document representation from heteroge- neous sequential data: 1) word sequence and 2) learners' clickstream.</p><p>• We collect two real-world MOOC datasets and conduct thorough experiments. The re- sults demonstrate that our proposed model outperforms the state-of-the-art unsupervised counterparts on the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>MOOC data has attracted extensive research at- tention and many interesting research problems have been studied. For example, dropout predict- ing ( <ref type="bibr" target="#b13">Qiu et al., 2016)</ref>, sentiment analysis of learn- ing gains ( <ref type="bibr" target="#b14">Ramesh et al., 2015)</ref>, instructor inter- vention ( <ref type="bibr" target="#b2">Chaturvedi et al., 2014</ref>) and answer rec- ommendation (Jenders et al., 2016), etc. Partic- ularly, (Agrawal et al., 2015) considers a similar task as ours, which is to recommend video clips to threads. But its solution is designed for the spe- cific task and needs labeling data. Our solution is an unsupervised learning method and the learned embeddings have other applications, e.g. thread retrieval.</p><p>How to represent text is a fundamental research problem in the field of information retrieval. Existing approaches can be generally classified into unsupervised methods and supervised meth- ods ( <ref type="bibr" target="#b17">Tang et al., 2015)</ref>. Although supervised em- beddings can obtain good performance in specific tasks, such as using deep neural network <ref type="bibr" target="#b11">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b9">Kim, 2014)</ref>, they need human efforts to get labels. Unsupervised word embeddings usu- ally leverage various levels of textual information. For example, Word2Vec learns word embeddings based on word coherence. Para2Vec utilizes word and document coherence to learn their embed- dings. Particularly, Hierarchical Document Vector (HDV) ( <ref type="bibr" target="#b3">Djuric et al., 2015</ref>) leverages both stream- ing documents and their contents to achieve better representation, which is similar to our proposed model. However, HDV regards the documents as the context of words, which cannot learn the la- tent similarity, since it fails to explicitly reflect the relationship between document and word. In or- der to model the heterogeneous MOOC data, we develop a cascade representation model. To our knowledge, ( <ref type="bibr" target="#b8">Jiang et al., 2017</ref>) also proposes an unsupervised learning model (called NOSE) for the task of thread-subtitle matching within MOOC settings. However, NOSE needs to build a het- erogeneous textual network beforehand and may suffer from heterogeneous issue, which our model can avoid.</p><p>Recently, representation learning has been ap- plied to many tasks, such as network embed- ding ( <ref type="bibr" target="#b5">Grover and Leskovec, 2016)</ref> and location embedding . In this paper, we focus on learning representation of words and doc- uments in MOOCs.</p><p>jumps from videos to threads may look for fur- ther relevant information from forums when s/he is watching a video, or s/he wants to review the relevant videos when s/he reads a thread.</p><p>However, learning from the log of clickstream data merely guarantees that similar documents are close enough in the embedding space, while dif- ferent documents cannot be scattered. To address this issue, we attempt to strengthen the relation- ship between words and their affiliated documents. Thus, words within the same documents would be gathered and otherwise scattered in the embedding space. Consequently, the latent similarity can be embodied by word embeddings.</p><p>Based on the aforementioned idea, we can model the data by three components: 1) latent se- mantics at word level, 2) latent similarity at docu- ment level, and 3) latent similarity between words and documents. To integrate all the three kinds of information into a uniform learning framework, we propose a novel cascade model, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. L 1 , L 2 and L 3 correspond to the log- likelihood of three components respectively. For- mally, we aim at minimizing the log-likelihood function:</p><formula xml:id="formula_0">L = L 1 + L 2 + L 3<label>(1)</label></formula><p>Note that L 3 not only learns the latent similarity, but also builds a connection between words and documents. In this way, our learned word embed- dings can be adopted to our task without learning classifiers by labeling data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word-level Latent Semantics</head><p>As to the part of L 1 , corresponding to the red/bottom part of <ref type="figure" target="#fig_0">Fig. 1</ref>, we leverage the Word2Vec model to learn the semantics of words. In this paper, we take the Continuous bag-of- words (CBOW) architecture. The objective func- tion is to minimize the log-likelihood:</p><formula xml:id="formula_1">L 1 = − T t=1 log P(w t |w t−cw : w t+cw ) (2)</formula><p>where c w is the context window length used in word sequence, and w t−cw : w t+cw is the sub- sequence (w t−cw , . . . , w t+cw ) excluding w t itself. The probability P(w t |w t−cw : w t+cw ) is defined by the softmax function</p><formula xml:id="formula_2">exp(¯ v T vw t ) W w=1 exp(¯ v T vw)</formula><p>, where v wt is the vector representation of word w t , and ¯ v is averaged vector representation of the sub- sequence. Two methods can be employed to calcu- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document-level Latent Similarity</head><p>Similar to L 1 , we adopt the CBOW architecture for calculating L 2 , as shown by the green/top part of <ref type="figure" target="#fig_0">Fig. 1</ref>. The objective function is to minimize the log-likelihood:</p><formula xml:id="formula_3">L 2 = − M m=1 log P(d m |d m−c d : d m+c d )<label>(3)</label></formula><p>where M is the number of documents, c d is the context window length used in click- streams, and</p><formula xml:id="formula_4">d m−c d : d m+c d is the sub-sequence (d m−c d , . . . , d m+c d ) excluding d m itself. The probability P(d m |d m−c d : d m+c d )</formula><p>is also the soft- max function. Methods of hierarchical softmax and negative sampling can be employed to approx- imate the log-likelihood function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document-Word Latent Similarity</head><p>To learn the latent similarity, we make use of the relationship between words and documents, and then similar documents can be clustered, while different documents are scattered. Therefore, we propose the third component, L 3 , shown in the middle part of <ref type="figure" target="#fig_0">Fig. 1</ref>. Different from L 1 and L 2 , we employ negative sampling of documents to calculate its approximation, because there are numerous threads in MOOC forums. Given a pair (w t , d m ), representing that word w t appears in document d m , L 3 is denoted as:</p><formula xml:id="formula_5">L 3 = wt − log σ(v T dm v wt ) + C c=1 log σ(v T dc v wt )<label>(4)</label></formula><p>where σ(x) is the sigmoid function and C is the number of sampled negative documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>We adopt stochastic gradient descent (SGD) to minimize L. As to the components of L 1 and L 2 , we exploit the training methods proposed in ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) to the two kinds of se- quences, i.e., words and documents, respectively. For training L 3 , given the pair (w t , d m ), we calcu- late the gradients:</p><formula xml:id="formula_6">∂L 3 ∂v d j = σ(v T d j v wt ) − (j = m) v wt ,<label>(5)</label></formula><formula xml:id="formula_7">∂L 3 ∂v wt = σ(v T d j v wt ) − (j = m) v d j ,<label>(6)</label></formula><p>where d j represents both the positive and nega- tive samples, as d j ∈ {d i } ∪ {d c ∼ P n (w)|c = 1, . . . , C}. P n (w) is the noise distribution and we set it as unigram distribution raised to 3/4th power, which is the same as Word2Vec. (x) is an indi- cator function defined as:</p><formula xml:id="formula_8">(x) = 1 if x is true, 0 otherwise.<label>(7)</label></formula><p>The time complexity of updating L is O(T log T + M log M + T C) when using hierarchical softmax method for L 1 and L 2 , or O((2T + M )C) when using negative sampling method. Based on the complexity analysis, our cascade model is efficient enough and can be applied to MOOC datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Data Sets We collect the sequential data of two MOOCs from Coursera 1 and China University MOOC 2 respectively. The former is an interdis- cipline course called People and Network, and the second is called Introduction to MOOC. From both courses, we collect subtitles of video clips, forum contents and learners' log of clickstream. <ref type="table">Table 1</ref> shows the statistical information of the two MOOCs. For evaluation, we invite the teaching assistants (TAs) of respective courses to label test samples in advance. Note that our model is unsupervised. Therefore, labeled data (thread-subtitle matching pairs) are only used for evaluation, and we do not utilize dev dataset.</p><p>Experimental Setting We compare our embed- dings with unsupervised rivals and the labels are only used for evaluation. To ensure fair compar- ison, we represent documents with their averaged word embeddings. Note that in the training phase, we represent each thread/subtitle with a vector, in order to make the words within a document clus- tered and close to each other. We evaluate the fol- lowing methods.</p><p>• Bag-of-words(BOW): the classical text rep- resentation.</p><p>• Word2Vec: word embeddings which lever- ages word-level coherence and we adopt the CBOW architecture.</p><p>• Para2Vec: paragraph embeddings which con- siders document-level context information. We also adopt CBOW framework.</p><p>• Hierarchical Document Vector(HDV): the latest word embeddings with a hierarchical architecture for modeling streaming docu- ments and their contents.</p><p>• Cascade Document Representation (CDR): our proposed model which captures both the latent semantics and latent similarity.</p><p>We use the hype-parameters recommended by previous literatures. For all the evaluated base- lines, we use the same parameter setting. Thus it is fair to make comparison. The window size set in all baselines is 5 by default. The number of negative samples is empirically set as 5. The size of hidden layer is set as 100 for all the methods. We utilize the Precision@K (denoted by P@K) as metric. If the retrieved top-K subtitles hit at least one ground-truth label, we regard it as true; oth- erwise, it is false. In our experiments, we run 10 times and report the average result for each case. <ref type="table" target="#tab_0">People and Network  10,807  60  219  1,206 121,142 31,096  Introduction to MOOC  3,949  19  557  7,177 480,495 45,642   Table 1</ref>: Statistics of two MOOC datasets.  Result Firstly we use all the data to learn word embeddings by models. Then the learned word vectors are utilized to calculate the similarity be- tween threads and subtitles, and rank the subti- tles. <ref type="table" target="#tab_0">Table 2</ref> reports the results of thread-subtitle matching. We can notice that there are some anomalies in P@3 and P@5 results. It may be for the reason of dataset. In the first MOOC (peo- ple and network), video subtitles contain relatively less words, and therefore it is hard to get effec- tive representations. Overall, the proposed mod- els can achieve better performance than baselines, and we highlight the Precision@1 results. Com- pared to HDV which also considers the streaming documents, our model is better at every task. This indicates our model can effectively capture the la- tent similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Course Name #active users #video clips #threads #posts #words #clicks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We investigate the effect of number of dimen- sions, i.e., the size of the neural network's hid- den layer. <ref type="figure" target="#fig_2">From Fig.2</ref>, we find that CDR can achieve better performance than baselines with various numbers of dimensions. In addition, the optimal results can be obtained when the dimen- sion is set as 100 or 200 in both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an approach to solve a significant problem: how to learn distinguish- able representations from word sequences in doc- uments and clickstreams of learners. To model the heterogeneous data, we develop a cascade model which can jointly learn the latent semantics and latent similarity without labeling data. We con- duct experiments on two real datasets, and the re- sults demonstrate the effectiveness of our model. Moreover, our model is not limited to MOOC data. For instance, we can adopt the proposed al- gorithm to streaming documents, e.g. webpage click streams, since our method can model the document-document sequences. We leave this as the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of proposed model which is cascaded by three parts: L 1 , L 2 and L 3 .</figDesc><graphic url="image-1.png" coords="3,308.41,62.81,216.01,254.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: P@1 of different dimensions.</figDesc><graphic url="image-2.png" coords="5,310.00,289.56,107.71,104.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Result of thread-subtitle matching. 

</table></figure>

			<note place="foot" n="3"> Cascade Model Based on our observation, we utilize two kinds of sequential information: 1) word sequences of subtitles and threads, and 2) clickstream of learning behaviors. In this paper, we regard the subtitles or threads consistently as documents. Particularly, we discover that the log of learners&apos; clickstream, i.e., the click records of watching videos, reading threads and posting threads in a chronological order, can reflect the document-level latent semantics. An intuitive explanation is that a learner who</note>

			<note place="foot" n="1"> https://www.coursera.org, which is an educational technology company that offers MOOCs worldwide. 2 http://www.icourse163.org, which is a leading MOOCs platform in China.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Engaging with massive online courses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="687" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting instructors intervention in mooc forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical neural language models for joint representation of streaming documents and their content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Poi2vec: Geographical latent representation for predicting future visitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeow Meng</forename><surname>Chee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="102" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Superposter behavior in mooc forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpita</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sanders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">L@S</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Which answer is best?: Predicting accepted answers in mooc forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Krestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Naumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="679" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised embedding for latent similarity by modeling heterogeneous mooc data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoxuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="683" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock´ynock´y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling and predicting learning behavior in moocs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><forename type="middle">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised models of aspect-sentiment for online course discussion forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arti</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&amp;IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language independent analysis and classification of discussion threads in coursera mooc forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omprakash</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gnawali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IRI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="654" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Termweighting approaches in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bringing order to chaos in mooc discussion forums with content-related thread identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Friend Wise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovita</forename><surname>Vytasek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LAK</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
