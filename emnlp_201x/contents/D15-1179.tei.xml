<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gerow</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computation Institute</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
							<email>altosaar@princeton.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Evans</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computation Institute</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Sociology</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Jean</forename><surname>So</surname></persName>
							<email>richardjeanso@uchicago.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Department of English Language and Literature</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models. This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections. Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to fit large numbers of topics. To efficiently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015). The models are first evaluated on synthetic data, generated to simulate various collection-level asymmetries. We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Comparing large text collections is a critical task for the curation and analysis of human cultural history. Achievements of research and schol- arship are most accessible through textual arti- facts, which are increasingly available in digital archives. Text-based research, often undertaken by humanists, historians, lexicographers, and cor- pus linguists, explores patterns of words in docu- ments across time-periods and distinct collections of text. Here, we introduce two new topic models designed to compare large collections, Correlated LDA (C-LDA) and Correlated HDP (C-HDP), which are sensitive to document-topic asymme- try (where collections have different topic distribu- tions) and topic-word asymmetry (where a single topic has different word distributions in each col- lection). These models seek to address termino- logical questions, such as how a topic on physics is articulated distinctively in scientific compared to humanistic research. Accommodating poten- tial collection-level asymmetries is particularly important when researchers seek to analyze col- lections with little prior knowledge about shared or collection-specific topic structure. Our mod- els extend existing cross-collection approaches to accommodate these asymmetries and implement an efficient parallel sampling algorithm enabling users to examine the long tail of topics in particu- larly large collections.</p><p>Using topic models for comparative text min- ing was introduced by <ref type="bibr" target="#b27">Zhai et al. (2004)</ref>, who de- veloped the ccMix model which extended pLSA <ref type="bibr" target="#b8">(Hofmann, 1999)</ref>. Later work by <ref type="bibr" target="#b18">Paul and Girju (2009)</ref> developed ccLDA, which adopted the hier- archical Bayes framework of Latent Dirichlet Al- location or LDA ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>). These mod- els account for topic-word asymmetry by assum- ing variation in the vocabularies of topics is due to collection-level differences. Nevertheless, they require the same topics to be present in each col- lection. These models are useful for comparing collections under specific assumptions, but cannot accommodate collection-topic asymmetry (which arises in collections that do not share every topic or that have different numbers of topics). In situa- tions where collections do not share all topics, the results often include junk, mixed, or sparse top- ics, making them difficult to interpret <ref type="bibr" target="#b18">(Paul and Girju, 2009)</ref>. Such asymmetries make it difficult to use models like ccLDA and ccMix when little is known about collections in advance. This mo- tivates our efforts to model variation in the long tail of topic distributions, where correlations are more likely to appear when collections are weakly related.</p><p>C-LDA and C-HDP extend ccLDA <ref type="bibr" target="#b18">(Paul and Girju, 2009</ref>) to accommodate collection-topic level asymmetries, particularly by allowing non- common topics to appear in each collection. This added flexibility allows our models to discover topic correlations across arbitrary collections with different numbers of topics, even when there are few (or unknown) numbers of common topics. To demonstrate the effectiveness of our models, we evaluate them on synthetic data and show that they outperform related models such as ccLDA and dif- ferential topic models <ref type="bibr" target="#b5">(Chen et al., 2014</ref>). We then fit C-LDA to two large collections of humanities and sciences documents from JSTOR. Such histor- ical analyses of text would be intractable without an efficient sampler. An optimized sampler is re- quired in such situations because common topics in weakly-correlated collections are usually found in the tail of the document-topic distribution of a sufficiently large set of topics. To make this fea- sible on large datasets such as JSTOR, we employ a parallelized Metropolis-Hastings ( <ref type="bibr" target="#b9">Kronmal and Peterson Jr, 1979)</ref> and alias-table sampling frame- work, adapted from LightLDA ( <ref type="bibr" target="#b26">Yuan et al., 2015)</ref>. These optimizations, which achieve O(1) amor- tized sampling time per token, allow our models to be fit to large corpora with up to thousands of topics in a matter of hours -an order of magni- tude speed-up from ccLDA.</p><p>After reviewing work related to topic modeling across collections, section 3 describes C-LDA and C-HDP, and then details their technical relation- ship to existing models. Section 5 introduces the synthetic data and part of the JSTOR corpus used in our evaluations. We then compare our models' performances to other models in terms of held- out perplexity and a measure of distinguishabil- ity. The final results section exemplifies the use of C-LDA in a qualitative analysis of humanities and sciences research. We conclude with a brief discussion of the strengths of C-LDA and C-HDP, and outline directions for future work and applica- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our models seek to enable users to compare large collections that may only be weakly correlated and that may contain different numbers of topics. While topic models could be fit to separate collec- tions to make post-hoc comparisons <ref type="bibr" target="#b6">(Denny et al., 2014;</ref><ref type="bibr" target="#b24">Yang et al., 2011</ref>), our goal is to account for both document-topic asymmetry and topic-word asymmetry "in-model". In short, we seek to model the correlation between arbitrary collections. Pri- oritizing in-model solutions for document-topic asymmetry has been explored elsewhere, such as in hierarchical Dirichlet processes (HDP), which use an additional level to account for collection variations in document-topic distributions <ref type="bibr" target="#b20">(Teh et al., 2006</ref>).</p><p>One method designed to model topic-word asymmetry is ccMix ( <ref type="bibr" target="#b27">Zhai et al., 2004</ref>), which models the generative probability of a word in topic z from collection c as a mixture of shared and collection-specific distributions θ z :</p><formula xml:id="formula_0">p(w) = λ c p(w|θ z ) + (1 − λ c ) p(w|θ z,c )</formula><p>where θ z,c is collection-specific and λ c controls the mixing between shared and collection-specific topics. ccLDA extends ccMix to the LDA frame- work and adds a beta prior over λ c that reduces sensitivity to input parameters ( <ref type="bibr" target="#b18">Paul and Girju, 2009)</ref>. Another approach, differential topic mod- els ( <ref type="bibr" target="#b5">Chen et al., 2014)</ref>, is based on hierarchical Bayesian models over topic-word distributions. This method uses the transformed Pitman-Yor pro- cess (TPYP) to model topic-word distributions in each collection, with shared common base mea- sures. As (Paul and Girju, 2009) note, ccLDA cannot accommodate a topic if it is not com- mon across collections -an assumption made by ccMix, ccLDA and the TPYP. In a situation where a topic is found in only one collection, it would either dominate the shared topic portion (resulting in a noisy, collection-specific portion), or it would appear as a mixed topic, revealing two sets of un- related words <ref type="bibr" target="#b17">(Newman et al., 2010b</ref>). C-LDA ameliorates this situation by allowing the number of common and non-common topics to be speci- fied separately and by efficiently sampling the tail of the document-topic distribution, allowing users to examine less prominent regions of the topic space. C-HDP also grants collections document- topic independence using a hierarchical structure to model the differences between collections.</p><p>Due to increased demand for scalable topic model implementations, there has been a prolif- eration of optimized methods for efficient infer- ence, such as SparseLDA ( <ref type="bibr" target="#b25">Yao et al., 2009</ref>) and AliasLDA ( <ref type="bibr" target="#b11">Li et al., 2014</ref>). AliasLDA achieves O(K d ) complexity by using the Metropolis- Hastings-Walker algorithm and an alias table to sample topic-word distributions in O(1) time. Al- though this strategy introduces temporal staleness in the updates of sufficient statistics, the lag is overcome by more iterations, and converges sig- nificantly faster. A similar technique by <ref type="bibr" target="#b26">Yuan et al. (2015)</ref>, LightLDA, employs cycle-based Metropo- lis Hastings mixing with alias tables for both document-topic and topic-word distributions. De- spite introducing lag in the sufficient statistics, this method achieves O(1) amortized sampling complexity and results in even faster convergence than AliasLDA. In addition to being fully paral- lelized, C-LDA adopts this sampling framework to make comparing large collections more tractable for large numbers of topics. Our models' efficient sampling methods allow users to fit large num- bers of topics to big datasets where variation might not be observed in sub-sampled datasets or models with fewer topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Correlated LDA</head><p>In ccLDA (and ccMix), each topic has shared and collection-specific components for each col- lection. C-LDA extends ccLDA to make it more robust with respect to topic asymmetries between collections ( <ref type="figure">Figure 1a)</ref>. The crucial extension is that by allowing each collection to define a set of non-common topics in addition to common top- ics, the model removes an assumption imposed by ccLDA and other inter-collection models, namely that collections have the same number of topics. As a result, C-LDA is suitable for collections with- out a large proportion of common topics, and can also reduce noise (discussed in Section 2). To achieve this, C-LDA assumes document d in col- lection c has a multinomial document-topic dis- tribution θ with an asymmetric Dirichlet prior for K c topics, where the first K ∅ are common across collections. It is also possible to introduce a tree structure into the model that uses a bino- mial distribution to decide whether a word was drawn from common or non-common topics. This yields collection-specific background topics by us- ing a binomial distribution instead of a multino- mial. However, we prefer the simpler, non-tree version because background topics are unneces- sary when using an asymmetric α prior ( <ref type="bibr" target="#b21">Wallach et al., 2009a</ref>).</p><p>The generative process for C-LDA is as follows:</p><formula xml:id="formula_1">1. Sample a distribution φ k (shared component) from Dir(β) and a distribution σ k from Beta(δ1, δ2) for each common topic k ∈ {1, . . . , K ∅ }; 2. For each collection c, sample a distribution φ c k (collection- specific component) from Dir(β) for each common topic k ∈ {1, . . . , K ∅ } and non-common topic k ∈ {K ∅ + 1, . . . , Kc};</formula><p>3. For each document d in c, sample a distribution θ from Dir(αc);</p><p>4. For each word wi in d:</p><formula xml:id="formula_2">(a) Sample a topic zi ∈ {1, . . . , Kc} from Multi(θ); (b) If zi ≤ K ∅ , sample yi from Binomial(σz i ); (c) Sample wi from Multi(φ ξ z i ), where ξ = null , zi ≤ K ∅ and yi = 0; c , otherwise.</formula><p>Note that to capture common topics, K ∅ should be set such that ∃ c where K c = K ∅ . Other- wise, words sampled as a non-common topic will not have information about non-common topics in other collections. Then a "common-topic word" is found among non-common topics in all col- lections (a local minima) and it will take a long time to stabilize as a common topic. To avoid this, when determining the number of topics for sampling, the number of non-common topics for the collection with the smallest number of total topics should be zero. After inference, to distin- guish common and non-common topics in this col- lection, we model σ independently by assuming collections have the same mixing ratio for com- mon topics. With this reasonable assumption and an asymmetric α, common topics become sparse enough that some σ distributions reduce nearly to 0, distinguishing them as non-common topics. Al- though this may seem counterintuitive, it does not negatively affect results.</p><p>Three kinds of collection-level imbalance can confound inter-collection topic models: 1) in the numbers of topics between collections, 2) in the numbers of documents between collections, and 3) in the document-topic distributions. Each of</p><formula xml:id="formula_3">α θ z w y φ σ δ β W D C K ∅ K ∅ + Kc Gc G0 H γ G d α0 α1 z w y φ σ δ β W D C K(∞) K(∞)</formula><p>Figure 1: Graphical models of C-LDA (a; left) and C-HDP (b; right). these can cause topics in different collections to have significantly different numbers of words as- signed to the same topic. In this way, a topic can be dominated by the collection comprising most of its words. C-LDA addresses imbalances in the document-topic distributions between collections by estimating α. For imbalance in the number of topics and documents, C-LDA mimics document over-sampling in the Gibbs sampler using a differ- ent unit-value in the word count table for each col- lection. Specifically, a unit η c is chosen for each collection such that the average equivalent num- ber of assigned words per-topic ( d∈c η c N d /K c , where N d is the length of document d) is equal. This process both increases the topic quality (in terms of collection balance) in the resulting held- out perplexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Correlated HDP</head><p>To alleviate C-LDA's requirement that ∃ c such that K c = K ∅ , we introduce a variant of the model, the correlated hierarchical Dirichlet pro- cess (C-HDP), that uses a 3-level hierarchical Dirichlet process ( <ref type="bibr" target="#b20">Teh et al., 2006</ref>). The gener- ative process for C-HDP is the same as C-LDA shown above, except that here we assume a word's topic, z, is generated by a hierarchical Dirichlet process:</p><formula xml:id="formula_4">G 0 |γ, H ∼ DP(γ, H) G c |α 0 , G 0 ∼ DP(α 0 , G 0 ) G d |α 1 , G c ∼ DP(α 1 , G c ) z|G d ∼ G d</formula><p>where G 0 is a base measure for each collection- level Dirichlet process, and G c are base measures of document-level Dirichlet processes in each col- lection ( <ref type="figure">Figure 1b)</ref>. Thus, documents from the same collection will have similar topic distribu- tions compared to those from other collections, and collections are allowed to have distinct sets of topics due to the use of HDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Posterior Inference in C-LDA</head><p>C-LDA can be trained using collapsed Gibbs sam- pling with φ, θ, and σ integrated out. Given the status assignments of other words, the sampling distribution for word w i is given by:</p><formula xml:id="formula_5">p(y i , z i |w, y −i , z −i , δ, α, β) ∝ (N (d, z i ) + α c,z i ) q d ×      N (y i , z i ) + δ yi N (z i ) + k δ k × N (w i , y i , z i , ζ) + β N (y i , z i , ζ) + V β z i ≤ K ∅ N (w i , z i , c) + β N (z i , c) + V β z i &gt; K ∅ qw<label>(1)</label></formula><p>where </p><formula xml:id="formula_6">ζ = * yi = 0 c yi = 1 , N (· · · ) is</formula><note type="other">Algorithm 1 Sampling in C-LDA repeat for all documents {d} in parallel do for words {w} in d do z ← CYCLEMH(p, qw, q d , z) sample y given z Atomic update sufficient statics Estimate α until convergence procedure CYCLEMH(p, qw, q d , z) for i = 1 to N do if i is even then proposal q ← qw else proposal q ← q</note><formula xml:id="formula_7">d sample z ∼ ALIASTABLE(q) if RandUnif(1) &lt; min(1, p(z )q(z) p(z)q(z ) ) then z ← z return z</formula><p>tion when exact sampling is difficult. In a compli- mentary way, Walker's alias method (2004) allows one to effectively sample from a discrete distribu- tion by using an alias table, constructed in O(K) time, from which we can sample in O(1). Thus, reusing the sampler K times as the proposal distri- bution for Metropolis-Hastings yields O(1) amor- tized sampling time per-token.</p><p>Notice that in Eq. 1, the sampling distribution is the product of a single document-dependent term q d and a single word-dependent term q w . After burn-in, both terms will be sparse (without the smoothing factor). It is therefore reasonable to use q d and q w as cycle proposals ( <ref type="bibr" target="#b26">Yuan et al., 2015)</ref>, alternating them in each Metropolis-Hastings step. Our experiments show that the primary drawback of this method -stale sufficient statistics -does not empirically affect convergence. Our imple- mentation uses proposal distributions q w and q d , with y marginalized out. After the Metropolis- Hastings steps, y is sampled to update z, to reduce the size of the alias tables, yielding even faster convergence.</p><p>Lastly, the use of an asymmetric α allows C- LDA to discover correlations between less dom- inant topics across collections ( <ref type="bibr" target="#b21">Wallach et al., 2009a</ref>). We use Minka's fixed-point method, with a gamma hyper-prior to optimize α c for each col- lection separately <ref type="bibr" target="#b23">(Wallach, 2008)</ref>. All other hy- perparameters were fixed during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Posterior Inference in C-HDP</head><p>C-HDP uses the block sampling algorithm de- scribed in <ref type="bibr" target="#b4">(Chen et al., 2011</ref>), which is based on the Chinese restaurant process metaphor. Here, rather than tracking all assignments (as the sam- plers given in <ref type="figure">(Teh et al., 2006)</ref>), table indicators are used to track only the start of new tables, which allows us to adopt the same sampling framework as C-LDA. In the Chinese restaurant process, each Dirichlet process in the hierarchical structure is represented as a restaurant with an infinite num- ber of tables, each serving the same dish. New customers can either join a table with existing cus- tomers, or start a new table. If a new table is cho- sen, a proxy customer will be sent to the parent restaurant to determine the dish served to that ta- ble.</p><p>In the block sampler, indicators are used to de- note a customer creating a table (or tables) up to level u (0 as the root, 1 for collection level, and 2 for the document level), and u = ∅ indicates no table has been created. For example, when a cus- tomer creates a table at the collection level, and the proxy customer in the collection level creates a table at the root level, u is 0. With this metaphor, let n lz be the number of customers (including their proxies) served dish z at restaurant l, and let t lz be the number of tables serving dish z at restaurant l (l = 0 for root, l = c for collection level or l = d for document level), with N 0 = z n 0z and N c = z n cz . By the chain rule, the conditional probability of the state assignments for w i , given all others, is</p><formula xml:id="formula_8">p(yi, zi, ui|w, y −i , z−i, u−i, . . .) ∝ N (y, z) + δy N (z) + k δ k × N (w, y, z, ζ) + β N (y, z, ζ) + V β ×                    γα 0 γ+N 0 u = 0 α 0 γ+N 0 S ncz +1 tcz +1 S ncz tcz S n dz +1 t dz +1 S n dz t dz n 2 0z (tcz +1)(t dz +1) (n 0z +1)(ncz +1)(n dz +1) u = 1 S ncz +1 tcz S ncz tcz S n dz +1 t dz +1 S n dz t dz (t dz +1)(ncz −tcz +1) (ncz +1)(n dz +1) u = 2 α 0 +N 1 α 1 S n dz +1 t dz S n dz t dz n dz −t dz +1 n dz +1 u = ∅</formula><p>Here, S n t is the Stirling number, the ratios of which can be efficiently precomputed <ref type="bibr" target="#b2">(Buntine and Hutter, 2010</ref>). The concentration parameters γ, α 0 , and α 1 can be sampled using the auxiliary variable method ( <ref type="bibr" target="#b20">Teh et al., 2006</ref>).</p><p>Note that because conditional probability has the same separability as C-LDA (to give term q w and q d ), the same sampling framework can be used with two alterations: 1) when a new topic is created or removed at the root, collection, or document level, the related alias tables must be reset, which makes the sampling slightly slower  <ref type="figure">Figure 2</ref>: Held-out perplexity of C-LDA, C-HDP, ccLDA and TPYP fit to synthetic data, where K 1 = K 2 = K (a; left) and data with an asymmetric number of topics (b; right). than O(1), and 2) while the document alias table samples z and u simultaneously, after sampling z from the word alias table u must be sampled using t lc /n lz <ref type="bibr" target="#b4">(Chen et al., 2011</ref>). Parallelizing C-HDP requires an additional empirical method of merg- ing new topics between threads ( <ref type="bibr" target="#b15">Newman et al., 2009)</ref>, which is outside of the scope of this work. Our implementation of both models, C-LDA and C-HDP, are open-sourced online 1 .</p><formula xml:id="formula_9">K = K 1 ccLDA K = K 2 TPYP K = K 1 TPYP K = K 2 C-LDA C-HDP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Comparison</head><p>We use perplexity on held-out documents to eval- uate the performance of C-LDA and C-HDP. In all experiments, the gamma prior for α in C-LDA was set to <ref type="bibr">(1,</ref><ref type="bibr">1)</ref>, and (5, 0.1), (5, 0.1), (0.1, 0.1) for γ, α 0 , α 1 respectively in C-HDP. In the hold-out procedure, 20% of documents were randomly se- lected as test data. LDA, C-LDA and ccLDA were run for 1,000 iterations and C-HDP and the TII- variant of TPYP for 1,500 iterations (unless oth- erwise noted), all of which converged to a state where change in perplexity was less than 1% for ten consecutive iterations.</p><p>Perplexity was calculated from the marginal likelihood of a held-out document p(w|Φ, α), es- timated using the "left-to-right" method ( <ref type="bibr" target="#b22">Wallach et al., 2009b)</ref>. Because it is difficult to vali- date real-world data that exhibits different kinds of asymmetry, we use synthetic data generated specifically for our evaluation tasks <ref type="bibr" target="#b0">(AlSumait et al., 2009;</ref><ref type="bibr" target="#b22">Wallach et al., 2009b;</ref><ref type="bibr" target="#b10">Kucukelbir and Blei, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Topic Correlation</head><p>C-LDA is unique in the amount of freedom it al- lows when setting the number of topics for col- 1 https://github.com/iceboal/correlated-lda lections. To assess the models' performances with various topic correlations in a fair setting, we gen- erated two collections of synthetic data by fol- lowing the generative process (varying the num- ber of topics) and measured the models' perplex- ities against the ground truth parameters. In each experiment, two collections were generated, each with 1,000 documents containing 50 words each, over a vocabulary of 3,000. β and δ were fixed at 0.01 and 1.0 respectively, and α was asymmetri- cally defined as</p><formula xml:id="formula_10">1/(i + √ K c ) for i ∈ [0, K c − 1].</formula><p>Completely shared topics The assumptions im- posed by ccLDA and TPYP effectively make them a special case of our model where</p><formula xml:id="formula_11">K ∅ = K 1 = K 2 = .</formula><p>. .. To compare results, data was gener- ated such that all numbers of topics were equal to K ∈ <ref type="bibr">[10,</ref><ref type="bibr">90]</ref>. Additionally, all models were con- figured to use this ground truth parameter when training. Not surprisingly, ccLDA, C-LDA, and C- HDP have almost the same perplexity with respect to K because their structure is the same when all topics are shared <ref type="figure">(Figure 2a</ref>).</p><p>Asymmetric numbers of topics To explore the effect of asymmetry in the number of topics, data was generated such that one collection had K 1 ∈ [20, 60] topics while a second had a fixed K 2 = 40 topics. The number of shared topics was set to K ∅ = 20. The parameters for C-LDA and C-HDP (initial values) were set to ground truths, and, to retain a fair comparison, versions of ccLDA and TPYP were fit with both K = K 1 and K = K 2 . We find that ccLDA performs nearly as well as C-LDA and C-HDP when there is more symme- try between collection, namely when K 1 ≈ K 2 <ref type="figure">(Figure 2b)</ref>. TPYP, on the other hand, performs well with more topics (2 × max(K 1 , K 2 ) where the ground truth is K 1 &amp; K 2 ). In contrast, C-LDA and C-HDP perform more consistently than other models across varying degrees of asymmetry.</p><p>Partially-shared topics When collections have the same number of topics, C-LDA, C-HDP and ccLDA exhibit adequate flexibility, resulting in similar perplexities. When collections have in- creasingly few common topics, however, common and non-common topics from ccLDA are con- siderably less distinguishable than those from C- LDA. To evaluate the models' abilities in such sit- uations, data was generated for two collections having K 1 = K 2 = 50 topics, but with the shared number of topics K ∅ ∈ <ref type="bibr">[5,</ref><ref type="bibr">45]</ref>. We also set δ (0) = δ (1) = 5, and for comparison to ccLDA we used K = 50.</p><p>To measure this distinguishability, we examine the inferred σ. Recall that σ indicates what per- centage of a common topic is shared. When a topic is actually non-common, the value of σ should be small. We sort σ k for k ∈ <ref type="bibr">[1, K]</ref> in reverse and use</p><formula xml:id="formula_12">¯ σ common = 1 K ∅ K ∅ k=1 σ k ¯ σ non-common = 1 K−K ∅ K k=K ∅ +1 σ k (2)</formula><p>as measures of how well common and non- common topics were learned 2 . ¯ σ common is the av- erage of the K ∅ largest σ values, and ¯ σ non-common is the average of the rest. When δ (0) = δ (1) in the synthetic data, σ in the common portion should be 0.5, whereas it should be 0 in the non-common part. <ref type="figure" target="#fig_3">Figure 3</ref> shows that C-LDA better distin- guishes between common and non-common top- ics, especially when K ∅ is small. This allows non- common topics to be separated from the results by examining the value of σ. C-HDP has similar per- formance but larger σ values. In ccLDA, all topics are shared between collections which means that common and non-common topics are mixed. As expected, ccLDA performs similarly when all top- ics are common across collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semantic Coherence</head><p>Semantic coherence is a corpus-based metric of the quality of a topic, defined as the average pair- wise similarity of the top n words ( <ref type="bibr" target="#b16">Newman et al., 2010a;</ref><ref type="bibr" target="#b14">Mimno et al., 2011)</ref>. A PMI-based form of coherence, which has been found to be the best  proxy human judgements of topic quality, is de- fined for a topic k as:</p><formula xml:id="formula_13">1.0 ¯ σ ccLDA C-LDA C-HDP ideal</formula><formula xml:id="formula_14">C(k) = 2 n(n − 1) n (w i ,w j )∈k i&lt;j log D(w i , w j ) + 1 D(w i )D(w j )</formula><p>where D(·) computes the document co- occurrence. To accommodate coherence with common topics in C-LDA that have shared and collection-specific components we define mutual coherence, MC(k), as</p><formula xml:id="formula_15">MC(k) = 1 n 2 n w i ∈shared, w j ∈collection-specific log D(w i , w j ) + 1 D(w i )D(w j )</formula><p>so that for each collection, C(k) (2n words) is equal to C(k, shared) + C(k, collection-specific) + MC(k). <ref type="table">Table 1</ref> shows the semantic coherence of topics fit with ccLDA and C-LDA. We used a 10% sample of JSTOR due to the limited speed of ccLDA, using 50 (common) topics for ccLDA / C- LDA, and 250 non-common humanities topics for C-LDA. Although these settings are different for the models, the science topics are still comparable because they both have 50 topics. We found that C-LDA provides improved coherence in nearly all situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Inference Efficiency</head><p>To compare the model efficiency, we timed runs on a sample of 5,036 documents from JSTOR (in- troduced in the next section) with a 20% held- out and set K = K 1 = K 2 = 200 run on a commodity computer with four cores and 16GB of memory. <ref type="figure" target="#fig_5">Figure 4a</ref> shows the perplexity over    <ref type="table">Table 1</ref>: Average semantic coherence of the 50 common topics from JSTOR (top) and the average of the 10 best common topics judged by the mean value of different types of coherence (bottom).</p><formula xml:id="formula_16">LDA ccLDA TPYP C-LDA K2 = K1 + 0 C-LDA K2 = K1 + 30 C-LDA K2 = K1 + 60 C-HDP</formula><p>time and iterations. The inference algorithm intro- duces some staleness, which yields slower conver- gence in the first 200 iterations. This effect, how- ever, is outweighed in both C-LDA and C-HDP by the increased sampling speed. With 8 threads, C- LDA not only converges faster, but yields lower perplexity, likely due to threads introducing addi- tional stochasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance on JSTOR</head><p>To compare our models against slower models, we sampled 2,465 documents from JSTOR, withhold- ing 20% as testing set. We fit a model with 100 common and 50 non-common initial topics us- ing C-HDP, which produced 272 root topics after 2,000 iterations.The perplexity scores are roughly the same when C-LDA uses the same average number of topics per collection <ref type="figure" target="#fig_5">(Figure 4b</ref>), ex- cept when numbers of topics are very asymmet- ric. Our model begins to outperform ccLDA after 80 topics. C-HDP did not, however, out-perform C-LDA despite the original HDP outperforming LDA. This could be do to the fact that the hier- archical structure of C-HDP is considerably differ- ent than the typical 2-level HDP. Held-out perplex- ity on real data provides a quantitative evaluation of our models' performance in a real-world set- ting. However, the goal of our models is to enable a deeper analysis of large, weakly-related corpora, which we next discuss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head><p>Our models are designed to enable researchers to compare collections of text in a way that is scal- able and sensitive to collection-level asymmetries.</p><p>To demonstrate that C-LDA can fill this role, we fit a model to the entire JSTOR sciences and hu- manities collections with 100 science topics and 1000 humanities topics (to reveal the less popu- lar science-related topics in the humanities), and β = 0.01, δ = 1.0. JSTOR includes books and journal publications in over 9 million documents across nearly 3 thousand journals. We used the journal Science to represent a collection of scien- tific research and 76 humanist journals to repre- sent humanities research 3 . Words were lemma- tized, and the most and least frequent words dis- carded. The final humanities collection contained 149,734 documents and the sciences collection had 160,680 documents, with a combined vocabu- lary of 21,513 unique words. Together, these col- lections typify a real-world situation where there is likely some, but not overwhelming correlation.</p><p>The results indicate that the sciences and hu- manities share several topics. Both exhibit an in- terest in a "non-human" theme (common topic #2; <ref type="table">Table 2</ref>). This topic is quite similar in both collec- tions (pig and monkey for science documents; bird and gorilla for humanities documents), while their shared component forms a cohesive topic (animal,  <ref type="table">Topic 21  Topic 23  shared  science  humanities  shared  science  humanities  shared  science humanities  animal  pig  beast  economic  cost  rural  particle  energy  universe  specie  fly  creature  government  industry  local  physic  electron  quantum  dog  monkey  nonhuman  economy  company community  physicist  ray  physic  wild  guinea  natural  trade  price  village  energy  ion  technical  wolf  primate  humanity  major  market  region  experiment  atom  scientific  monkey  worm  bird  growth  product  urban  event  particle  relativity  horse  dog  living  capital  income  country  measurement  mass  physical  sheep  cat  gorilla  industry  industrial  area  atom  neutron  mechanic  lion  mammal  brute  institution  business  regional  interaction  proton  law  cat  cattle  ape  support  private  population  atomic  nucleus  reality   Table 2</ref>: Three topics from the JSTOR collections with their top words in shared and specific components.</p><p>Complete results available at http://j.mp/jstor-html.</p><p>specie, and monkey). This kind of correlation is also evident in topic #23, about physics. While the science documents clearly represent research in particle physics, it is interesting to find the topic is also represented by humanist research focused on cultural representations of science. This reflects a growing interest in science and technology studies that has gained recent traction in the humanities. Despite their differences, both collections engage with a similar theme, seen in the shared compo- nent with words like particle, energy and atom.</p><p>The results also indicate that while sciences and humanities documents can share themes, they of- ten diverge in how they are discussed. For exam- ple, common topic #21 could be identified as eco- nomic or capitalist, but in the collection-specific components, the two disciplines differ in their ar- ticulatation. Science uses terms like price and market, indicating an acceptance of free-market capitalism (especially as it affects the practice of science), while the humanities, which has long been critical of free-market capitalism, uses terms like rural and community, highlighting cultural facets of modern economics. These results pro- vide evidence about how ideas move between the sciences and humanities -a phenomenon that constitutes a growing area of research for histori- ans <ref type="bibr" target="#b7">(Galison, 2003;</ref><ref type="bibr" target="#b3">Canales, 2015)</ref>. C-LDA pro- vides empirical, measurable, and reproducible ev- idence of the shared research between these disci- plines, as well as how concepts are articulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our models provide a robust way to explore large and potentially weakly-related text collec- tions without imposing assumptions about the data. Like ccLDA and TPYP, our models ac- count for topic-word variation at the collection level. The models accommodate asymmetry in the numbers of topics (set in C-LDA, fit in C- HDP) and provide an efficient inference method which allows them to fit data with large values for K, which can help find correlations in less prevalent topics. Our primary contribution is our models' ability to accommodate asymmetries be- tween arbitrary collections. JSTOR, the world's largest digital collection of humanities research, was an ideal application setting given the size, asymmetry, and comprehensiveness of the human- ities collection. As we show, humanities and science research exhibit asymmetries with regard to vocabulary and topic structure -asymmetries that would be systematically overlooked using ex- isting models. By characterizing common top- ics as mixtures of shared and collection-specific components, we can capture a kind of topic-level homophily, where similar themes are articulated in different ways due to word-, document-, and collection-level variation. Future work on these models could explore methods to fit non-common topics for both collections. In general, C-LDA and C-HDP can be used whenever documents are sam- pled from ostensibly different populations, where the nature of the difference is unknown.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the number of status assignments for (· · · ), not including w i . Inference in C-LDA employs two optimiza- tions: a parallelized sampler and an efficient sam- pling algorithm (Algorithm 1). We use the paral- lel schema in (Smola and Narayanamurthy, 2010; Lu et al., 2013) which applies atomic updates to the sufficient statistics to avoid race conditions. The key idea behind the optimized sampler is the combination of alias tables and the Metropolis- Hastings method (MH), adapted from (Yuan et al., 2015; Li et al., 2014). Metropolis-Hastings is a Markov chain Monte Carlo method that uses a pro- posal distribution to approximate the true distribu-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distinguishability (Eq. 2) of topics fit with C-LDA,C-HDP and ccLDA. Blues lines denote ¯ σ common and red denote ¯ σ non-common .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Using JSTOR: perplexity vs. runtime and iterations (a; left) and perplexity vs. K (b; right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Coherence 
Mutual Coherence 
shared component 
collection-specific 
shared &amp; collection-specific 
all documents science humanities science humanities science 
humanities 
C-LDA 
-8.83 
-7.73 
-8.04 
-8.38 
-8.14 
-8.54 
-8.37 
ccLDA 
-9.04 
-8.22 
-8.27 
-8.38 
-8.15 
-8.69 
-8.40 
C-LDA 
-7.22 
-3.68 
-6.11 
-8.25 
-8.09 
-7.75 
-7.97 
ccLDA 
-8.11 
-5.68 
-7.12 
-8.24 
-7.88 
-8.22 
-7.95 

</table></figure>

			<note place="foot" n="2"> TPYP is not comparable using this metric, but its hierarchical structure will cause topics to mix naturally.</note>

			<note place="foot" n="3"> The list is available at http://j.mp/humanities-txt.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to David Blei for advice on applications of the model. This work contains analysis of pri-vate, or otherwise restricted data, made available to James Evans and Eamon Duede by ITHAKA (JSTOR), the opinions of whom are not repre-sented in this paper. Jaan Altosaar acknowledges support from the Natural Sciences and Engineer-ing Research Council of Canada. This work was supported by a grant from the Templeton Foun-dation to the Metaknowledge Research Network and by grant #1158803 from the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topic significance ranking of LDA generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loulwah</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barbará</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gentle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlotta</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Bayesian view of the Poisson-Dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1007.0296</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Physicist and the Philosopher: Einstein, Bergson, and the Debate that Changed our Understanding of Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimena</forename><surname>Canales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sampling table configurations for the hierarchical Poisson-Dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="296" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<title level="m">Differential topic models. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling email network content and structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Desmarais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 72nd Annual Midwest Political Science Association Conference, 2014; the Northeast Political Methodology Meeting, 2014; the 7th Annual Political Networks Conference, the Society for Political Methodology 31st Annual Summer Meeting</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Poincar&apos;s Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Galison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Norton, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the alias method for generating random variables from a discrete distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kronmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur V Peterson</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="214" to="218" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.0292</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Profile predictive inference. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing the sampling complexity of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Aaron Q Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander J</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerating topic model training on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Technologies and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast generation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Marsaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><forename type="middle">Wan</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed algorithms for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1801" to="1828" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluating topic models for digital libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Annual Joint Conference on Digital Libraries, JCDL &apos;10</title>
		<meeting>the 10th Annual Joint Conference on Digital Libraries, JCDL &apos;10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-cultural analysis of blogs and forums with mixed-collection topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1408" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An architecture for parallel topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hanna M Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Structured topic models for language. Unpublished doctoral dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanna M Wallach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topic modeling on historical newspapers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Torget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</title>
		<meeting>the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lightlda: Big topic models on modest computer clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Po</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1351" to="1361" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A cross-collection mixture model for comparative text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atulya</forename><surname>Velivelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="743" to="748" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
