<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding Bin Gao Microsoft</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">San Diego</orgName>
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<orgName type="institution" key="instit4">Yidian Inc</orgName>
								<orgName type="institution" key="instit5">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">San Diego</orgName>
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<orgName type="institution" key="instit4">Yidian Inc</orgName>
								<orgName type="institution" key="instit5">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjieren</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">San Diego</orgName>
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<orgName type="institution" key="instit4">Yidian Inc</orgName>
								<orgName type="institution" key="instit5">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
							<email>jiang.bian.prc@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">San Diego</orgName>
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<orgName type="institution" key="instit4">Yidian Inc</orgName>
								<orgName type="institution" key="instit5">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">San Diego</orgName>
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<orgName type="institution" key="instit4">Yidian Inc</orgName>
								<orgName type="institution" key="instit5">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding Bin Gao Microsoft</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="541" to="550"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tests, which measure human&apos;s verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by the deep learning technologies for text data. We found that the task was quite challenging, and simply applying existing technologies like word embedding could not achieve a good performance, due to the multiple senses of words and the complex relations among words. To tackle these challenges, we propose a novel framework to automatically solve the verbal IQ questions by leveraging improved word embedding by jointly considering the multi-sense nature of words and the relational information among words. Experimental results have shown that the proposed framework can not only outper-form existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Intelligence Quotient (IQ) test <ref type="bibr" target="#b24">(Stern, 1914)</ref> is a test of intelligence designed to formally study the success of an individual in adapting to a spe- cific situation under certain conditions. Common IQ tests measure various types of abilities such as verbal, mathematical, logical, and reasoning skills. These tests have been widely used in the study of psychology, education, and career development. In the community of artificial intelligence, agents have been invented to fulfill many interesting and chal- lenging tasks like face recognition, speech recogni- tion, handwriting recognition, and question answer- ing. However, as far as we know, there are very lim- ited studies of developing an agent to solve IQ tests, which in some sense is more challenging, since even common human beings do not always succeed on such tests. Considering that IQ test scores have been widely considered as a measure of intelligence, we think it is worth further investigating whether we can develop an agent that can solve IQ test questions.</p><p>The commonly used IQ tests contain several types of questions like verbal, mathematical, logical, and picture questions, among which a large proportion (near 40%) are verbal questions <ref type="bibr" target="#b3">(Carter, 2005</ref>). The recent progress on deep learning for natural lan- guage processing (NLP), such as word embedding technologies, has advanced the ability of machines (or AI agents) to understand the meaning of words and the relations among words. This inspires us to solve the verbal questions in IQ tests by lever- aging the word embedding technologies. However, our attempts show that a straightforward applica- tion of word embedding does not result in satisfac- tory performances. This is actually understandable. Standard word embedding technologies learn one embedding vector for each word based on the co- occurrence information in a text corpus. However, verbal comprehension questions in IQ tests usually consider the multiple senses of a word (and often fo- cus on the rare senses), and the complex relations among (polysemous) words. This has clearly ex- ceeded the capability of standard word embedding technologies.</p><p>To tackle the aforementioned challenges, we pro- pose a novel framework that consists of three com- ponents.</p><p>First, we build a classifier to recognize the spe- cific type (e.g., analogy, classification, synonym, and antonym) of verbal questions. For different types of questions, different kinds of relationships need to be considered and the solvers could have different forms. Therefore, with an effective ques- tion type classifier, we may solve the questions in a divide-and-conquer manner.</p><p>Second, we obtain distributed representations of words and relations by leveraging a novel word em- bedding method that considers the multi-sense na- ture of words and the relational knowledge among words (or their senses) contained in dictionaries. In particular, for each polysemous word, we retrieve its number of senses from a dictionary, and con- duct clustering on all its context windows in the corpus. Then we attach the example sentences for every sense in the dictionary to the clusters, such that we can tag the polysemous word in each con- text window with a specific word sense. On top of this, instead of learning one embedding vector for each word, we learn one vector for each pair of word-sense. Furthermore, in addition to learning the embedding vectors for words, we also learn the embedding vectors for relations (e.g., synonym and antonym) at the same time, by incorporating rela- tional knowledge into the objective function of the word embedding learning algorithm. That is, the learning of word-sense representations and relation representations interacts with each other, such that the relational knowledge obtained from dictionaries is effectively incorporated.</p><p>Third, for each type of question, we propose a specific solver based on the obtained distributed word-sense representations and relation represen- tations. For example, for analogy questions, we find the answer by minimizing the distance between word-sense pairs in the question and the word-sense pairs in the candidate answers.</p><p>We have conducted experiments using a com- bined IQ test set to test the performance of our pro- posed framework. The experimental results show that our method can outperform several baseline methods for verbal comprehension questions on IQ tests. We further deliver the questions in the test set to human beings through Amazon Mechanical Turk <ref type="bibr">1</ref> . The average performance of the human be- ings is even a little lower than that of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Verbal Questions in IQ Test</head><p>In common IQ tests, a large proportion of ques- tions are verbal comprehension questions, which play an important role in deciding the final IQ scores. For example, in Wechsler Adult Intelligence Scale <ref type="bibr" target="#b29">(Wechsler, 2008)</ref>, which is among the most fa- mous IQ test systems, the full-scale IQ is calculated from two IQ scores: Verbal IQ and Performance IQ, and around 40% of questions in a typical test are ver- bal comprehension questions. Verbal questions can test not only the verbal ability (e.g., understanding polysemy of a word), but also the reasoning abil- ity and induction ability of an individual. Accord- ing to previous studies <ref type="bibr" target="#b3">(Carter, 2005)</ref>, verbal ques- tions mainly have the types elaborated in <ref type="table" target="#tab_0">Table 1</ref>, in which the correct answers are highlighted in bold font.</p><p>Analogy-I questions usually take the form "A is to B as C is to ?". One needs to choose a word D from a given list of candidate words to form an analogical relation between pair (A, B) and pair <ref type="bibr">(C, D)</ref>. Such questions test the ability of identifying an implicit relation from word pair (A, B) and ap- ply it to compose word pair <ref type="bibr">(C, D)</ref>. Note that the Analogy-I questions are also used as a major eval- uation task in the word2vec models ( <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref>. Analogy-II questions require two words to be identified from two given lists in order to form an analogical relation like "A is to ? as C is to ?". Such questions are a bit more difficult than the Analogy- I questions since the analogical relation cannot be observed directly from the questions, but need to be searched for in the word pair combinations from the candidate answers. Classification questions require one to identify the word that is different (or dissim- ilar) from others in a given word list. Such ques- tions are also known as odd-one-out, which have been studied in <ref type="bibr" target="#b20">(Pintér et al., 2012</ref>). Classification questions test the ability to summarize the majority Type Example Analogy-I Isotherm is to temperature as isobar is to? (i) atmosphere, (ii) wind, (iii) pressure, (iv) latitude, (v) current. Analogy-II Identify two words (one from each set of brackets) that form a connection (analogy) when paired with the words in capitals: CHAPTER (book, verse, read), ACT (stage, audience, play). Classification Which is the odd one out? (i) calm, (ii) quiet, (iii) relaxed, (iv) serene, (v) unruffled. Synonym Which word is closest to IRRATIONAL? (i)intransigent, (ii) irredeemable, (iii) unsafe, (iv) lost, (v) nonsensical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Antonym</head><p>Which word is most opposite to MUSICAL? (i) discordant, (ii) loud, (iii) lyrical, (iv) verbal, (v) euphonious. sense of the words and identify the outlier. Synonym questions require one to pick one word out of a list of words such that it has the closest meaning to a given word. Synonym questions test the ability of identi- fying all senses of the candidate words and selecting the correct sense that can form a synonymous rela- tion to the given word. Antonym questions require one to pick one word out of a list of words such that it has the opposite meaning to a given word. Antonym questions test the ability of identifying all senses of the candidate words and selecting the cor- rect sense that can form an antonymous relation to the given word. <ref type="bibr" target="#b27">(Turney, 2008;</ref><ref type="bibr" target="#b28">Turney, 2011</ref>) stud- ied the analogy, synonym and antonym problem us- ing a supervised classification approach. Although there are some efforts to solve math- ematical, logical, and picture questions in IQ test <ref type="bibr" target="#b21">(Sanghi and Dowe, 2003;</ref><ref type="bibr" target="#b25">Strannegard et al., 2012;</ref><ref type="bibr" target="#b14">Kushmany et al., 2014;</ref><ref type="bibr" target="#b22">Seo et al., 2014;</ref><ref type="bibr" target="#b11">Hosseini et al., 2014;</ref><ref type="bibr" target="#b32">Weston et al., 2015)</ref>, there have been very few efforts to develop automatic methods to solve verbal questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning for Text Mining</head><p>Building distributed word representations ( <ref type="bibr" target="#b0">Bengio et al., 2003</ref>), a.k.a. word embeddings, has attracted increasing attention in the area of machine learn- ing. Different from conventional one-hot represen- tations of studies or distributional word representa- tions based on co-occurrence matrix between words such as LSA ( <ref type="bibr" target="#b8">Dumais et al., 1988</ref>) and LDA ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref>, distributed word representations are usually low-dimensional dense vectors trained with neural networks by maximizing the likelihood of a text corpus. Recently, a series of works applied deep learning techniques to learn high-quality word rep- resentations <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013;</ref><ref type="bibr" target="#b19">Pennington et al., 2014</ref>).</p><p>Nevertheless, since the above works learn word representations mainly based on the word co- occurrence information, it is quite difficult to obtain high quality embeddings for those words with very little context information; on the other hand, a large amount of noisy or biased context could give rise to ineffective word embeddings. Therefore, it is neces- sary to introduce extra knowledge into the learning process to regularize the quality of word embedding. Some efforts have paid attention to learn word em- bedding in order to address knowledge base comple- tion and enhancement <ref type="bibr" target="#b2">(Bordes et al., 2011;</ref><ref type="bibr" target="#b30">Weston et al., 2013a)</ref>, and some other efforts have tried to leverage knowledge to enhance word representations ( <ref type="bibr" target="#b15">Luong et al., 2013;</ref><ref type="bibr" target="#b31">Weston et al., 2013b;</ref><ref type="bibr" target="#b9">Fried and Duh, 2014;</ref><ref type="bibr" target="#b6">Celikyilmaz et al., 2015)</ref>. Moreover, all the above models assume that one word has only one embedding no matter whether the word is polysemous or not, which might cause some confusion for the polysemous words. To solve the problem, there are several efforts like <ref type="bibr" target="#b12">(Huang et al., 2012;</ref><ref type="bibr" target="#b26">Tian et al., 2014;</ref><ref type="bibr" target="#b17">Neelakantan et al., 2014</ref>). However, these models do not leverage any extra knowledge (e.g., relational knowledge) to en- hance word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Solving Verbal Questions</head><p>In this section, we introduce our proposed frame- work to solve the verbal questions, which consists of the following three components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification of Question Types</head><p>The first component of the framework is a question classifier, which identifies different types of verbal questions. Since different types of questions have their unique ways of expression, the classification task is relatively easy, and we therefore take a simple approach to fulfill the task. Specifically, we regard each verbal question as a short document and use the TF·IDF features to build its representation. Then we train an SVM classifier with linear kernel on a portion of labeled question data, and apply it to other questions. The question labels include Analogy-I, Analogy-II, Classification, Synonym, and Antonym. We use the one-vs-rest training strategy to obtain a linear SVM classifier for each question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding of Word-Senses and Relations</head><p>The second component of our framework leverages deep learning technologies to learn distributed rep- resentations for words (i.e. word embedding). Note that in the context of verbal question answering, we have some specific requirements on this learning process. Verbal questions in IQ tests usually con- sider the multiple senses of a word (and focus on the rare senses), and the complex relations among (polysemous) words, such as synonym and antonym relation. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of the multi- sense of words and the relations among word senses. We can see that irrational has three senses. Its first sense has an antonym relation with the second sense of rational, while its second sense has a synonym relation with nonsensical and an antonym relation with the first sense of rational.</p><p>The above challenge has exceeded the capability of standard word embedding technologies. To ad- dress this problem, we propose a novel approach that considers the multi-sense nature of words and integrate the relational knowledge among words (or their senses) into the learning process. In particu- lar, our approach consists of two steps. The first step aims at labeling a word in the text corpus with its specific sense, and the second step employs both the labeled text corpus and the relational knowledge contained in dictionaries to simultaneously learn embeddings for both word-sense pairs and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Multi-Sense Identification</head><p>First, we learn a single-sense word embedding by using the skip-gram method in word2vec ( <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref>.</p><p>Second, we gather the context windows of all oc- currences of a word used in the skip-gram model, and represent each context by a weighted average of the pre-learned embedding vectors of the con- text words. We use TF·IDF to define the weight- ing function, where we regard each context win- dow of the word as a short document to calcu- late the document frequency. Specifically, for a word w 0 , each of its context window can be de- noted by (w −N , · · · , w 0 , · · · , w N ). Then we repre- sent the window by calculating the weighted average of the pre-learned embedding vectors of the context words as ξ = 1 2N N i=−N,i =0 g w i v w i ,where g w i is the TF·IDF score of w i , and v w i is the pre-learned embedding vector of w i . After that, for each word, we use spherical k-means to cluster all its context representations, where cluster number k is set as the number of senses of this word in the online dictio- nary.</p><p>Third, we match each cluster to the correspond- ing sense in the dictionary. On one hand, we repre- sent each cluster by the average embedding vector of all those context windows included in the clus- ter. For example, suppose word w 0 has k senses and thus it has k clusters of context windows, we de- note the average embedding vectors for these clus- ters as ¯ ξ 1 , · · · , ¯ ξ k . On the other hand, since the on- line dictionary uses some descriptions and example sentences to interpret each word sense, we can rep- resent each word sense by the average embedding of those words including its description words and the words in the corresponding example sentences. Here, we assume the representation vectors (based on the online dictionary) for the k senses of w 0 are ζ 1 , · · · , ζ k . After that, we consecutively match each cluster to its closest word sense in terms of the dis- tance computed in the word embedding space:</p><formula xml:id="formula_0">( ¯ ξ i , ζ j ) = argmin i,j=1,··· ,k d( ¯ ξi, ζj),<label>(1)</label></formula><p>where d(·, ·) calculates the Euclidean distance and ( ¯ ξ i , ζ j ) is the first matched pair of window cluster and word sense. Here, we simply take a greedy strat- egy. That is, we remove ¯ ξ i and ζ j from the cluster vector set and the sense vector set, and recursively run (1) to find the next matched pair till all the pairs are found. Finally, each word occurrence in the cor- pus is relabeled by its associated word sense, which will be used to learn the embeddings for word-sense pairs in the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Co-Learning Word-Sense Pair Representations and Relation Representations</head><p>After relabeling the text corpus, different occur- rences of a polysemous word may correspond to its different senses, or more accurately word-sense pairs. We then learn the embeddings for word-  sense pairs and relations (obtained from dictionar- ies, such as synonym and antonym) simultaneously, by integrating relational knowledge into the objec- tive function of the word embedding learning model like skip-gram. We propose to use a function E r as described below to capture the relational knowledge. Specifically, the existing relational knowledge extracted from dictionaries, such as synonym, antonym, etc., can be naturally represented in the form of a triplet (head, relation, tail) (denoted by (h i , r, t j ) ∈ S, where S is the set of relational knowledge), which consists of two word-sense pairs (i.e. word h with its i-th sense and word t with its j-th sense), h, t ∈ W (W is the set of words) and a relationship r ∈ R (R is the set of relationships). To learn the relation representations, we make an as- sumption that relationships between words can be interpreted as translation operations and they can be represented by vectors. The principle in this model is that if the relationship (h i , r, t j ) exists, the repre- sentation of the word-sense pair t j should be close to that of h i plus the representation vector of the re- lationship r, i.e. h i + r; otherwise, h i + r should be far away from t j . Note that this model learns word-sense pair representations and relation repre- sentations in a unified continuous embedding space.</p><p>According to the above principle, we define E r as a margin-based regularization function over the set of relational knowledge S, Er = (h i ,r,t j )∈S Here [X] + = max(X, 0), γ &gt; 0 is a margin hyper- parameter, and d(·, ·) is the Euclidean distance be- tween two words in the embedding space.The set of corrupted triplets S (h,r,t) is defined as S (h i ,r,t j ) = {(h , r, t)} {(h, r, t )},which is constructed from S by replacing either the head word-sense pair or the tail word-sense pair by another randomly selected word with its randomly selected sense.</p><p>To avoid the trivial solution that simply increases the norms of representation vectors, we use an ad- ditional soft norm constraint on the relation repre- sentations as r i = 2σ(x i ) − 1, where σ(·) is the sigmoid function σ(x i ) = 1/(1 + e −x i ), r i is the i-th dimension of relation vector r, and x i is a latent variable, which guarantees that every dimension of the relation representation vector is within the range <ref type="figure" target="#fig_0">(−1, 1)</ref>.</p><p>By combining the skip-gram objective function and the regularization function derived from rela- tional knowledge, we get the combined objective J r = αE r − L that incorporates relational knowl- edge into the word-sense pair embedding calcula- tion process, where α is the combination coefficient. Our goal is to minimize J r , which can be optimized using back propagation neural networks. <ref type="figure" target="#fig_2">Figure 2</ref> shows the structure of the proposed model.By using this model, we can obtain the distributed representa- tions for both word-sense pairs and relations simul- taneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Solvers for Each Type of Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Analogy-I</head><p>For the Analogy-I questions like "A is to B as C is to ?", we answer them by optimizing: </p><formula xml:id="formula_1">D = argmax i b ,ia,ic,i d ; D ∈T cos(v (B,i b ) − v (A,ia) + v (C,ic) , v (D ,i d ) )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Analogy-II</head><p>As the form of the Analogy-II questions is like "A is to ? as C is to ?" with two lists of candidate answers, we can apply an optimization method as below to select the best (B, D) pair,</p><formula xml:id="formula_2">argmax i b ,ia,ic,i d ; B ∈T 1 ,D ∈T 2 cos(v (B ,i b ) − v (A,ia) + v (C,ic) , v (D ,i d ) ),<label>(3)</label></formula><p>where T 1 , T 2 are two lists of candidate words. Thus we get the answers B and D that can form an ana- logical relation between word pair (A, B) and word pair (C, D) under a certain specific word sense com- bination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Classification</head><p>For the Classification questions, we leverage the property that words with similar co-occurrence in- formation are distributed close to each other in the embedding space. The candidate word that is not similar to others does not have similar co- occurrence information to other words in the train- ing corpus, and thus this word should be far away from other words in the word embedding space. Therefore we first calculate a group of mean vec- tors m iw 1 ,··· ,iw N of all the candidate words with any possible word senses as below m iw 1 ,··· ,iw N =</p><formula xml:id="formula_3">1 N w j ∈T v (w j ,iw j ) ,</formula><p>where T is the set of candidate words, N is the capacity of T , w j is a word in T ; i w j (j = 1, · · · , N ; i w j = 1, · · · , k w j ) is the index for the word senses of w j , and k w j (j = 1, · · · , N ) is the number of word senses of w j . Therefore, the number of the mean vectors is M = N j=1 k w j . As both N and k w j are very small, the computation cost is acceptable. Then, we choose the word with such a sense that its closest sense to the correspond- ing mean vector is the largest among the candidate words as the answer, i.e.,</p><formula xml:id="formula_4">w = argmax w j ∈T min iw j ;l=1,··· ,M d(v (w j ,iw j ) , m l ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Synonym</head><p>For the Synonym questions, we empirically ex- plored two solvers. For the first solver, we also leverage the property that words with similar co- occurrence information are located closely in the word embedding space. Therefore, given the ques- tion word w q and the candidate words w i , we can find the answer by solving:</p><formula xml:id="formula_5">w = argmin iw q ,iw j ;w j ∈T d(v (w j ,iw j ) , v (wq ,iw q ) ),<label>(5)</label></formula><p>where T is the set of candidate words. The sec- ond solver is based on the minimization objective of the translation distance between entities in the re- lational knowledge model (2). Specifically, we cal- culate the offset vector between the embedding of question word w q and each word w j in the candi- date list. Then, we set the answer w as the candidate word with which the offset is the closest to the rep- resentation vector of the synonym relation r s , i.e.,</p><formula xml:id="formula_6">w = argmin iw q ,iw j ;w j ∈T |v (w j ,iw j ) − v (wq ,iw q ) | − rs .<label>(6)</label></formula><p>In practice, we found the second solver performs better (the results are listed in Section 4). For our baseline embedding model skip-gram, since it does not assume the relation representations explicitly, we use the first solver for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Antonym</head><p>Similar to solving the Synonym questions, we ex- plored two solvers for Antonym questions as well. That is, the first solver <ref type="formula" target="#formula_7">(7)</ref> is based on the small offset distance between semantically close words whereas the second solver (8) leverages the trans- lation distance between two words' offset and the embedding vector of the antonym relation. The first solver is based on the fact that since an antonym and its original word have similar co-occurrence infor- mation from which the embedding vectors are de- rived, the embedding vectors of both words with antonym relation will still lie closely in the embed- ding space.</p><formula xml:id="formula_7">w = argmin iw q ,iw j ;w j ∈T d(v (w j ,iw j ) , v (wq ,iw q ) ),<label>(7)</label></formula><formula xml:id="formula_8">w = argmin iw q ,iw j ;w j ∈T |v (w j ,iw j ) − v (wq ,iw q ) | − ra ,<label>(8)</label></formula><p>Here T is the set of candidate words and r a is the representation vector of the antonym relation. Again we found that the second solver performs better. Similarly, for skip-gram, the first solver is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments to examine whether our proposed framework can achieve satisfying results on verbal comprehension questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training Set for Word Embedding</head><p>We trained word embeddings on a publicly avail- able text corpus named wiki2014 2 , which is a large text snapshot from Wikipedia. After being pre- processed by removing all the html meta-data and replacing the digit numbers by English words, the final training corpus contains more than 3.4 billion word tokens, and the number of unique words, i.e. the vocabulary size, is about 2 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">IQ Test Set</head><p>According to our study, there is no online dataset specifically released for verbal comprehension ques- tions, although there are many online IQ tests for users to play with. In addition, most of the on- line tests only calculate the final IQ scores but do not provide the correct answers. Therefore, we only use the online questions to train the verbal question classifier described in Section 3.1. Specifically, we manually collected and labeled 30 verbal questions from the online IQ test Websites 3 for each of the five types (i.e. Analogy-I, Analogy-II, Classifica- tion, Synonym, and Antonym) and trained an one-vs-rest SVM classifier for each type. The total accu- racy on the training set itself is 95.0%. The classifier was then applied in the test set below.</p><p>We collected a set of verbal comprehension ques- tions associated with correct answers from pub- lished IQ test books, such as <ref type="bibr" target="#b3">(Carter, 2005;</ref><ref type="bibr" target="#b4">Carter, 2007;</ref><ref type="bibr" target="#b18">Pape, 1993;</ref><ref type="bibr" target="#b13">Ken Russell, 2002</ref>), and we used this collection as the test set to evaluate the ef- fectiveness of our new framework. In total, this test set contains 232 questions with the correspond- ing answers. <ref type="bibr">4</ref> The number of each question type (i.e., Analogy-I, Analogy-II, Classification, Syn- onym, Antonym) are respectively 50, 29, 53, 51, 49.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>In our experiments, we compare our new relation knowledge powered model to several baselines.</p><p>Random Guess Model (RG). Random guess is the most straightforward way for an agent to solve questions. In our experiments, we used a random guess agent which would select an answer randomly regardless of what the question was. To measure the performance of random guess, we ran each task for 5 times and calculated the average accuracy.</p><p>Human Performance (HP). Since IQ tests are designed to evaluate human intelligence, it is quite natural to leverage human performance as a base- line. To collect human answers on the test ques- tions, we delivered them to human beings through Amazon Mechanical Turk (AMT), a crowd-sourcing Internet marketplace that allows people to partici- pate in Human Intelligence Tasks. In our study, we published five AMT jobs, one job corresponding to one specific question type. The jobs were deliv- ered to 200 people. To control the quality of the collected results, we used several strategies: (i) we imposed high restrictions on the workers by requir- ing all the workers to be native English speakers in North America and to be AMT Masters (who have demonstrated high accuracy on previous tasks on AMT marketplace); (ii) we recruited a large number of workers in order to guarantee the statistical confi- dence in their performances; (iii) we tracked their age distribution and education background, which are very similar to those of the overall population in the U.S.</p><p>Latent Dirichlet Allocation Model (LDA). This baseline model leveraged one of the most common classical distributional word representations, i.e. La- tent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>). In particular, we trained word representations using LDA on wiki2014 with the topic number 1000.</p><p>Skip-Gram Model (SG). In this baseline, we applied the word embedding trained by skip- gram ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) (denoted by SG-1) on wiki2014. In particular, we set the window size as 5, the embedding dimension as 500, the negative sam- pling count as 3, and the epoch number as 3. In ad- dition, we also employed a pre-trained word embed- ding by Google 5 with the dimension of 300 (denoted by SG-2).</p><p>Glove.</p><p>Another powerful word embedding model ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>). Glove configura- tions are the same as those in running SG-1.</p><p>Multi-Sense Model (MS). In this baseline, we applied the multi-sense word embedding models proposed in <ref type="bibr" target="#b12">(Huang et al., 2012;</ref><ref type="bibr" target="#b26">Tian et al., 2014;</ref><ref type="bibr" target="#b17">Neelakantan et al., 2014</ref>) (denoted by MS-1, MS-2 and MS-3 respectively). For MS-1, we directly used the published multi-sense word embedding vectors by the authors 6 , in which they set 10 senses for the top 5% most frequent words. For MS-2 and MS- 3, we get the embedding vectors by usingf the re- leased codes from the authors using the same con- figurations as MS-1.</p><p>Relation Knowledge Powered Model (RK). This is our proposed method in Section 3. In par- ticular, when learning the embedding on wiki2014, we set the window size as 5, the embedding dimen- sion as 500, the negative sampling count as 3, and the epoch number as 3. We adopted the online Long- man Dictionary as the dictionary used in multi-sense clustering. We used a public relation knowledge set, WordRep ( ), for relation training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Accuracy of Question Classifier</head><p>We applied the question classifier trained in Sec- tion 4.1.2 on the test set, and got the total accuracy 5 https://code.google.com/p/word2vec/ 6 http://ai.stanford.edu/ ˜ ehhuang/ 93.1%. For RG and HP, the question classifier was not needed. For other methods, the wrongly classi- fied questions were also sent to the corresponding wrong solver to find an answer. If the solver re- turned an empty result (which was usually caused by invalid input format, e.g., an Analogy-II question was wrongly input to the Classification solver), we would randomly select an answer. <ref type="table" target="#tab_2">Table 2</ref> demonstrates the accuracy of answering verbal questions by using all the approaches men- tioned in Section 4.2. The numbers for all the mod- els are mean values from five repeated runs. From this table, we observe: (i) RK can achieve the best overall accuracy than all the other methods. In par- ticular, RK can raise the overall accuracy by about 4.63% over HP 7 . (ii) RK is empirically superior to the skip-gram models SG-1/SG-2 and Glove. Ac- cording to our understanding, the improvement of RK over SG-1/SG-2/Glove comes from two aspects: multi-sense and relational knowledge. Note that the performance difference between MS-1/MS-2/MS-3 and SG-1/SG-2/Glove is not significant, showing that simply changing single-sense word embedding to multi-sense word embedding does not bring too much benefit. One reason is that the rare word- senses do not have enough training data (contextual information) to produce high-quality word embed- ding. By further introducing the relational knowl- edge among word-senses, the training for rare word- senses will be linked to the training of their related word-senses. As a result, the embedding quality of the rare word-senses will be improved. (iii) RK is empirically superior than the two multi-sense algo- rithms MS-1, MS-2 and MS-3, demonstrating the effectiveness brought by adopting fewer model pa- rameters and using an online dictionary in building the multi-sense embedding model. These results are quite impressive, indicating the potential of using machines to comprehend human knowledge and even achieve a comparable level of human intelligence.   To sum up, the experimental results have demon- strated the effectiveness of the proposed RK model compared with several baseline methods. Although the test set is not large, the generalization of RK to other test sets should not be a concern due to the un- supervised nature of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Overall Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Accuracy on Different Question Types</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We investigated how to automatically solve verbal comprehension questions in IQ Tests by using word embedding techniques. In particular, we proposed a three-step framework: (i) to recognize the spe- cific type of a verbal comprehension question by a classifier, (ii) to leverage a novel deep learning model to co-learn the representations of both word- sense pairs and relations among words (or their senses), (iii) to design dedicated solvers, based on the obtained word-sense pair representations and re- lation representations, for addressing each type of questions. Experimental results have demonstrated that this novel framework can achieve better per- formance than existing methods for solving verbal comprehension questions and even exceed the aver- age performance of the Amazon Mechanical Turk workers involved in the experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example on the multi-sense of words and the relations between word senses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The structure of the proposed model. where T contains all the candidate answers, cos means cosine similarity, and i b , i a , i c , i d are the indexes for the word senses of B, A, C, D respectively. Finally D is selected as the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Types of verbal questions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 reports the accuracy of answering various types of verbal questions by each method. From the</head><label>2</label><figDesc></figDesc><table>Analogy-I Analogy-II Classification Synonym Antonym Total 
RG 
24.60 
11.72 
20.75 
19.27 
23.13 
20.51 
LDA 
28.00 
13.79 
39.62 
27.45 
30.61 
29.31 
HP 
45.87 
34.37 
47.23 
50.38 
53.30 
46.23 
SG 
SG-1 
38.00 
24.14 
37.74 
45.10 
40.82 
38.36 
SG-2 
38.00 
20.69 
39.62 
47.06 
44.90 
39.66 
Glove 
45.09 
24.14 
32.08 
47.06 
40.82 
39.03 
MS 
MS-1 
36.36 
19.05 
41.30 
50.00 
36.59 
38.67 
MS-2 
40.00 
20.69 
41.51 
49.02 
40.82 
40.09 
MS-3 
17.65 
20.69 
47.17 
47.06 
30.61 
36.73 
RK 
48.00 
34.48 
52.83 
60.78 
51.02 
50.86 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of different methods among different human groups.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>table , we</head><label>,</label><figDesc></figDesc><table>can observe that the SG and MS models 
can achieve competitive accuracy on certain ques-
tion types (like Synonym) compared with HP. After 
incorporating knowledge into learning word embed-
ding, our RK model can improve the accuracy over 
all question types. Moreover, the table shows that 
RK can result in a big improvement over HP on the 
question types of Synonym and Classification. 

</table></figure>

			<note place="foot" n="1"> http://www.mturk.com/</note>

			<note place="foot" n="2"> http://en.wikipedia.org/wiki/Wikipedia: Database_download 3 http://wechsleradultintelligencescale. com/</note>

			<note place="foot" n="4"> It can be downloaded from https://www.dropbox. com/s/o0very1gwv3mrt5/VerbalQuestions.zip? dl=0.</note>

			<note place="foot" n="7"> With the t-test score p = 0.036.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The complete book of intelligence tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Carter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>John Wiley &amp; Sons Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Ultimate IQ Test Book: 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Carter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Practice Test Questions to Boost Your Brain Power</title>
		<imprint>
			<publisher>Kogan Page Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using latent semantic analysis to improve access to textual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGCHI</title>
		<meeting>SIGCHI</meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Incorporating both distributional and relational semantics in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<idno>abs/1412.4369</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.1640</idno>
		<title level="m">Wordrep: A benchmark for research on learning word representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Times Book of IQ Tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Kogan Page Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushmany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilayy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Better word representations with recursive neural networks for morphology</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Original Cambridge Self Scoring IQ Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pape</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>The Magni Group, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automated word puzzle generation via topic dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Pintér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyula</forename><surname>Vörös</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltán</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Lörincz</surname></persName>
		</author>
		<idno>abs/1206.0377</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A computer program capable of passing i.q. tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritika</forename><surname>Sanghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint International Conference on Cognitive Science</title>
		<meeting>the Joint International Conference on Cognitive Science</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diagram understanding in geometry questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence<address><addrLine>Québec City, Québec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07" />
			<biblScope unit="page" from="2831" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Psychological Methods of Testing Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Stern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1914" />
			<pubPlace>Warwick &amp; York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An anthropomorphic method for number sequence problems. Cognitive Systems Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claes</forename><surname>Strannegard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Amirghasemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ulfsbacker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A uniform approach to analogies, synonyms, antonyms, and associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Coling</title>
		<meeting>the Coling</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analogy perception applied to seven tests of word comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental &amp; Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="362" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wechsler adult intelligence scale-fourth edition (wais-iv)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wechsler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>NCS Pearson</publisher>
			<pubPlace>San Antonio, TX</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.7973</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: a set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
