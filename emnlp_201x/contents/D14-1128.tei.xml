<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Contextual Predictions for Hard Sentiment Words</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
							<email>ebert@cis.lmu.de, inquiries@cislmu.org</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Contextual Predictions for Hard Sentiment Words</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1210" to="1215"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized. We provide evidence for this hypothesis in a case study for the adjective &quot;hard&quot; and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation. An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper deals with fine-grained sentiment anal- ysis. We aim to make three contributions. First, based on a detailed linguistic analysis of contexts of the word "hard" (Section 3), we give evidence that highly accurate sentiment analysis is only pos- sible if senses with different polarity are accu- rately recognized.</p><p>Second, based on this analysis, we propose to return to a lexicon-based approach to sentiment analysis that supports identifying sense distinc- tions relevant to sentiment. Currently available sentiment lexicons give the polarity for each word or each sense, but this is of limited utility if senses cannot be automatically identified in context. We extend the lexicon-based approach by introducing the concept of a contextually enhanced sentiment lexicon (CESL). The lexicon entry of a word w in CESL has three components: (i) the senses of w; (ii) a sentiment annotation of each sense; (iii) a data structure that, given a context in which w oc- curs, allows to identify the sense of w used in that context.</p><p>As we will see in Section 3, the CESL sense inventory -(i) above -should be optimized for sentiment analysis: closely related senses with the same sentiment should be merged whereas subtle semantic distinctions that give rise to different po- larities should be distinguished.</p><p>The data structure in (iii) is a statistical classi- fication model in the simplest case. We will give one other example for (iii) below: it can also be a set of centroids of context vector representations, with a mapping of these centroids to the senses.</p><p>If sentiment-relevant sense disambiguation is the first step in sentiment analysis, then power- ful contextual features are necessary to support making fine-grained distinctions. Our third con- tribution is that we experiment with deep learn- ing as a source of such features. We look at two types of deep learning features: word em- beddings and neural network language model pre- dictions (Section 4). We show that deep learn- ing features significantly improve the accuracy of context-dependent polarity classification (Sec- tion 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Initial work on sentiment analysis was either based on sentiment lexicons that listed words as posi- tive or negative sentiment indicators (e.g., <ref type="bibr" target="#b15">Turney (2002)</ref>, <ref type="bibr" target="#b20">Yu and Hatzivassiloglou (2003)</ref>), on statis- tical classification approaches that represent doc- uments as ngrams (e.g., <ref type="bibr" target="#b10">Pang et al. (2002)</ref>) or on a combination of both (e.g., <ref type="bibr" target="#b11">Riloff et al. (2003)</ref>, <ref type="bibr" target="#b16">Whitelaw et al. (2005)</ref>). The underlying assump- tion of lexicon-based sentiment analysis is that a word always has the same sentiment. This is clearly wrong because words can have senses with different polarity, e.g., "hard wood" (neutral) vs. "hard memory" (negative).</p><p>Ngram approaches are also limited because ngram representations are not a good basis for relevant generalizations. For example, the neu- tral adverbial sense 'intense' of "hard" ("laugh hard", "try hard") vs. the negative adjectival mean-  <ref type="table">Table 1</ref>: Sense inventory of "hard".</p><p>ing 'difficult' ("hard life", "hard memory") cannot be easily distinguished based on an ngram repre- sentation. Moreover, although ngram approaches could learn the polarity of these phrases they do not generalize to new phrases.</p><p>More recent compositional approaches to senti- ment analysis can outperform lexicon and ngram- based methods (e.g., <ref type="bibr" target="#b13">Socher et al. (2011)</ref>, <ref type="bibr" target="#b14">Socher et al. (2013)</ref>). However, these approaches conflate two different types of contextual effects: differ- ences in sense or lexical meaning ("hard memory" vs. "hard wood") on the one hand and meaning composition like negation on the other hand. From the point of view of linguistic theory, these are dif- ferent types of contextual effects that should not be conflated. Recognizing that "hard" occurs in the scope of negation is of no use if the basic po- larity of the contextually evoked sense of "hard" (e.g., negative in "no hard memories" vs. neutral in "no hard wood") is not recognized. <ref type="bibr" target="#b18">Wilson et al. (2009)</ref> present an approach to clas- sify contextual polarity building on a two-step pro- cess. First, they classify if a sentiment word is po- lar in a phrase and if so, second, they classify its polarity. Our approach can be seen as an exten- sion of this approach; the main difference is that we will show in our analysis of "hard" that the polarity of phrases depends on the senses of the words that are used. This is evidence that high- accuracy polarity classification depends on sense disambiguation.</p><p>There has been previous work on assigning po- larity values to senses of words taken from Word- Net (e.g., <ref type="bibr" target="#b0">Baccianella et al. (2010)</ref>, Wiebe and Mi- halcea <ref type="formula">(2006)</ref>). However, these approaches are not able to disambiguate the sense of a word given its context.</p><p>Previous work on representation learning for sentiment analysis includes <ref type="bibr" target="#b6">(Maas and Ng, 2010)</ref> and <ref type="bibr" target="#b7">(Maas et al., 2011</ref>). Their models learn word embeddings that capture semantic similarities and word sentiment at the same time. Their approach focuses on sentiment of entire sentences or docu- ments and does not consider each sentiment word instance at a local level.</p><p>We present experiments with one supervised and one semisupervised approach to word sense disambiguation (WSD) in this paper. Other WSD approaches, e.g., thesaurus-based WSD <ref type="bibr" target="#b19">(Yarowsky, 1992)</ref>, could also be used for CESL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linguistic analysis of sentiment contexts of "hard"</head><p>We took a random sample of 5000 contexts of "hard" in the Amazon Product Review Data (Jin- dal and <ref type="bibr" target="#b5">Liu, 2008</ref>). We use 200 as a test set and set aside 200 for future use. We analyzed the remain- ing 4600 contexts using a tool we designed for this study, which provides functionality for selecting and sorting contexts, including a keyword in con- text display. If a reliable pattern has been identi- fied (e.g., the phrase "die hard"), then all contexts matching the pattern can be labeled automatically.</p><p>Our goal is to identify the different uses of "hard" that are relevant for sentiment. The basis for our inventory is the Cobuild <ref type="bibr" target="#b12">(Sinclair, 1987)</ref> lexicon entry for "hard". We use Cobuild because it was compiled based on an empirical analysis of corpus data and is therefore more likely to satisfy the requirements of NLP applications than a tradi- tional dictionary.</p><p>Cobuild lists 16 senses. One of these senses (3) is split into two to distinguish the adverbial ("to accelerate hard") and adjectival ("hard accel- eration") uses of "hard" in the meaning 'intense'. We conflated five senses <ref type="bibr">(2,</ref><ref type="bibr">4,</ref><ref type="bibr">9,</ref><ref type="bibr">10,</ref><ref type="bibr">11)</ref> refer- ring to different types of difficulty: "hard ques- tion" (2), "hard work" (4), "hard life" (11) and two variants of "hard on": "hard on someone" (9), "hard on something" (10); and four differ- ent senses (3a, 5, 6, 7) referring to different types of intensity: "to work hard" (3a), "to look hard" (5), "to kick hard" (6), "to laugh hard" (7). Fur- thermore, we identified a number of noncompo- sitional meanings or phrases (lists NEGATIVE-P and NEUTRAL-P in the supplementary material 1 ) in addition to the four listed by <ref type="bibr">Cobuild (13, 14, 15, 16</ref>). In addition, new senses for "hard" are in- troduced for opposites of senses of "soft": the op- posite of 'quiet/gentle voice/sound' (7: MUSIC; e.g., "hard beat", "not too hard of a song") and the opposite of 'smooth surface/texture' (8: CON- TRAST; e.g., "hard line", "hard edge"). <ref type="table">Table 1</ref> lists the 10 different uses that are the re- sult of our analysis. For each use, we give the cor- responding Cobuild sense numbers, syntactic in- formation, meaning, an example, typical patterns, polarity, and number of occurrences in training and test sets.</p><p>7 uses are neutral and 3 are negative. As "hard's" polarity in most sentiment lexicons is negative, but only 3 out of 7 senses are negative, "hard" provides evidence for our hypothesis that senses need to be disambiguated to allow for fine- grained and accurate polarity recognition.</p><p>We hired two PhD students to label each of the 200 contexts in the test set with one of the 10 la- bels in <ref type="table">Table 1</ref> (κ = .78). Disagreement was re- solved by a third person.</p><p>We have published the labeled data set of 4600+200 contexts as supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep learning features</head><p>We use two types of deep learning features to be able to make the fine-grained distinctions neces-sary for sense disambiguation. First, we use word embeddings similar to other recent work (see be- low). Second, we use a deep learning language model (LM) to predict the distribution of words for the position at which the word of interest occurs. For example, an LM will predict that words like "granite" and "concrete" are likely in the context "a * countertop" and that words like "serious" and "difficult" are likely in the context "a * problem". This is then the basis for distinguishing contexts in which "hard" is neutral (in the meaning 'firm, solid') from contexts in which it is a sentiment in- dicator (in the meaning 'difficult'). We will use the term predicted context distribution or PCD to refer to the distribution predicted by the LM.</p><p>We use the vectorized log-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>bilinear language model (vLBL) (Mnih and Kavukcuoglu, 2013) because it has three appealing features. (i) It learns state of the art word embeddings (Mnih and Kavukcuoglu, 2013). (ii) The model is a language model and can be used to calculate PCDs. (iii) As a linear model, vLBL can be trained much faster than other models (e.g., Bengio et al. (2003)).</head><p>The vLBL trains one set of word embeddings for the input space (R) and one for the target space (Q). We denote the input representation of word w as r w and the target representation as q w . For a given context c = w 1 , . . . , w n the model predicts a target representationˆqrepresentationˆ representationˆq by linearly combining the context word representations with position depen- dent weights:</p><formula xml:id="formula_0">ˆ q(c) = n i=1 d i r w i</formula><p>where d i ∈ D is the weight vector associated with position i in the context and is point- wise multiplication. Given the model parameters θ = {R, Q, D, b} the similarity betweenˆqbetweenˆ betweenˆq and the correct target word embedding is computed by the similarity function</p><formula xml:id="formula_1">s θ (w, c) = ˆ q(c) T q w + b w</formula><p>where b w is a bias term. We train the model with stochastic gradient descent on mini-batches of size 100, following the noise-contrastive estimation training proce- dure of <ref type="bibr" target="#b8">Mnih and Kavukcuoglu (2013)</ref>. We use AdaGrad ( <ref type="bibr" target="#b3">Duchi et al., 2011</ref>) with the initial learn- ing rate set to η = 0.5. The embeddings size is set to 100.   <ref type="table">Table 2</ref>: Classification results; bl: baseline During training we do not need to normalize the similarity explicitly, because the normalization is implicitly learned by the model. However, nor- malization is still necessary for prediction. The normalized PCD for a context c of word w is com- puted using the softmax function:</p><formula xml:id="formula_2">P c θ (w) = exp(s θ (w, c)) w exp(s θ (w , c))</formula><p>We use a window size of ws = 7 for training the model. We found that the model did not capture enough contextual phenomena for ws = 3 and that results for ws = 11 did not have better quality than ws = 7, but had a negative impact on the training time. Using a vocabulary of the 100,000 most frequent words, we train the vLBL model for 4 epochs on 1.3 billion 7-grams randomly selected from the English Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The lexicon entry of "hard" in CESL consists of (i) the senses, (ii) the polarity annotations (neutral or negative) and (iii) the sense disambiguation data structure. Components (i) and (ii) are shown in <ref type="table">Table 1</ref>. In this section, we evaluate two different options for (iii) on the task of sentiment classifica- tion. <ref type="table">Table 3</ref>: Significant differences of lines 1-8 in Ta- ble 2; ‡: p=0.01, *: p=0.05, ·: p=0.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">2 3 4 5 6 7 8 2 ‡</head><formula xml:id="formula_3">3 ‡ 4 ‡ ‡ · 5 ‡ ‡ 6 ‡ ‡ 7 ‡ ‡ * ‡ ‡ 8 ‡ * * ‡ * ‡</formula><p>The first approach is to use a statistical classi- fication model as the sense disambiguation struc- ture. We use liblinear <ref type="bibr" target="#b4">(Fan et al., 2008</ref>) with stan- dard parameters for classification based on three different feature types: ngrams, embeddings (em- bed) and PCDs. Ngram features are all n-grams for n ∈ {1, 2, 3}. As embedding features we use (i) the mean of the input space (R) embed- dings and (ii) the mean of the target space (Q) em- beddings of the words in the context (see <ref type="bibr" target="#b2">Blacoe and Lapata (2012)</ref> for justification of using simple mean). As PCD features we use the PCD predicted by vLBL for the sentiment word of interest, in our case "hard".</p><p>We split the set of 4600 contexts introduced in Section 3 into a training set of 4000 and a devel- opment set of 600. <ref type="table">Table 2</ref> (lines 1-8) shows the classification re- sults on the development set for all feature type combinations. Significant differences between re- sults -computed using the approximate random- ization test <ref type="bibr" target="#b9">(Padó, 2006</ref>) -are given in <ref type="table">Table 3</ref>. The majority baseline (bl), which assigns a nega- tive label to all examples, reaches F 1 = .76. The classifier is significantly better than the baseline for all feature combinations with F 1 ranging from .89 to .94. We obtain the best classification result (.94) when all three feature types are combined (significantly better than all other feature combi- nations except for 5).</p><p>Manually labeling all occurrences of a word is expensive. As an alternative we investigate clustering of the contexts of the word of interest. Therefore, we represent each of the 4000 con- texts of "hard" in the training set as its PCD 2 , use kmeans clustering with k = 100 and then label each cluster. This decreases the cost of labeling by an order of magnitude since only 100 clusters have to be labeled instead of 4000 training set con- texts. <ref type="table">Table 2 (lines 9-15)</ref> shows results for this semisupervised approach to classification, using the same classifier and the same feature types, but the cluster-based labels instead of manual labels.</p><p>For most feature combinations, F 1 drops com- pared to fully supervised classification. The best performing model for supervised classification (ngram+PCD+embed) loses 5%. This is not a large drop considering the savings in manual labeling effort. All results are signifi- cantly better than the baseline. There are no signif- icant differences between the different feature sets (lines 9-15) with the exception of embed, which is significantly worse than the other 6 sets.</p><p>The centroids of the 100 clusters can serve as an alternative sense disambiguation structure for the lexicon entry of "hard" in CESL. <ref type="bibr">3</ref> Each sense s is associated with the centroids of the clusters whose majority sense is s.</p><p>As final experiment (lines 16-18 in <ref type="table">Table 2</ref>), we evaluate performance for the baseline and for PCD+ngram+embed -the best feature set -on the test set. On the test set, baseline performance is .80 (.04 higher than .76 on line 1, <ref type="table">Table 2</ref>); F 1 of PCD+ngram+embed is .92 (.02 less than develop- ment set) for supervised classification and is .88 (.01 less) for semisupervised classification (com- paring to lines 8 and 15 in <ref type="table">Table 2</ref>). Both results (.92 and .88) are significantly higher than the base- line (.80).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The sentiment of a sentence or document is the output of a causal chain that involves complex lin- guistic processes like contextual modification and negation. Our hypothesis in this paper was that for high-accuracy sentiment analysis, we need to model the root causes of this causal chain: the meanings of individual words. This is in contrast to other work in sentiment analysis that conflates different linguistic phenomena (word sense ambi- guity, contextual effects, negation) and attempts to address all of them with a single model. For sense disambiguation, the first step in the causal chain of generating sentiment, we proposed <ref type="bibr">3</ref> Included in supplementary material. CESL, a contextually enhanced sentiment lexi- con that for each word w holds the inventory of senses of w, polarity annotations of these senses and a data structure for assigning contexts of w to the senses. We introduced new features for sentiment analysis to be able to perform the fine- grained modeling of context needed for CESL. In a case study for the word "hard", we showed that high accuracy in sentiment disambiguation can be achieved using our approach. In future work, we would like to show that our findings generalize from the case of "hard" to the entire sentiment lex- icon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ngram</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="1"> All supplementary material is available at http:// www.cis.lmu.de/ebert .</note>

			<note place="foot" n="2"> To transform vectors into a format that is more appropriate for the underlying Gaussian model of kmeans, we take the square root of each probability in the PCD vectors.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by DFG (grant SCHU 2246/10). We thank Lucia Krisnawati and Sascha Rothe for their help with annotation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2200" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Opinion spam and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Web Data Mining</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A probabilistic model for semantic word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Advances in Neural Information Processing Systems: Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">User&apos;s guide to sigf: Significance testing by approximate randomisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Thumbs up?: Sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning subjective nouns using extraction pattern bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><forename type="middle">Ann</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Looking Up: Account of the Cobuild Project in Lexical Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sinclair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Collins CoBUILD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using appraisal groups for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Whitelaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navendu</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="625" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word sense and subjectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="433" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="454" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
