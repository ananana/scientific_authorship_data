<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Language Processing with Small Feed-Forward Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakalov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Salcianu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Language Processing with Small Feed-Forward Networks</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2879" to="2885"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of un-structured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we show-case simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep and recurrent neural networks with large net- work capacity have become increasingly accurate for challenging language processing tasks. For ex- ample, machine translation models have been able to attain impressive accuracies, with models that use hundreds of millions ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b21">Wu et al., 2016</ref>) or billions ( <ref type="bibr" target="#b13">Shazeer et al., 2017)</ref> of parameters. These models, however, may not be feasible in all computational settings. In partic- ular, models running on mobile devices are often constrained in terms of memory and computation.</p><p>Long Short-Term Memory (LSTM) mod- els <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) have achieved good results with small memory foot- prints by using character-based input representa- tions: e.g., the part-of-speech tagging models of <ref type="bibr">Gillick et al. (2016)</ref> have only roughly 900,000 parameters. Latency, however, can still be an is- sue with LSTMs, due to the large number of ma- trix multiplications they require (eight per LSTM cell): <ref type="bibr" target="#b1">Kim and Rush (2016)</ref> report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone.</p><p>Feed-forward neural networks have the poten- tial to be much faster. In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of mag- nitude speedup over an LSTM-based approach.</p><p>We begin by introducing the network model structure and the character-based representations we use throughout all tasks ( §2). The four tasks that we address are: language identification (Lang- ID), part-of-speech (POS) tagging, word segmen- tation, and preordering for translation. In order to use feed-forward networks for structured pre- diction tasks, we use transition systems <ref type="bibr">Henderson, 2007, 2010</ref>) with feature embeddings as proposed by <ref type="bibr">Chen and Manning (2014)</ref>, and in- troduce two novel transition systems for the last two tasks. We focus on budgeted models and ab- late four techniques (one on each task) for improv- ing accuracy for a given memory budget:</p><p>1. Quantization: Using more dimensions and less precision (Lang-ID: §3.1).</p><p>2. Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: §3.2).</p><p>3. Selected features: Adding explicit feature conjunctions (segmentation: §3.3).</p><p>4. Pipelines: Introducing another task in a pipeline and allocating parameters to the aux- iliary task instead (preordering: §3.4).</p><p>We achieve results at or near state-of-the-art with small (&lt; 3 MB) models on all four tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Small Feed-Forward Network Models</head><p>The network architectures are designed to limit the memory and runtime of the model. <ref type="figure">Figure 1</ref> illus- trates the model architecture:</p><p>1. Discrete features are organized into groups (e.g., E bigrams ), with one embedding matrix E g ∈ R Vg×Dg per group.</p><p>2. Embeddings of features extracted for each group are reshaped into a single vector and concatenated to define the output of the em- bedding layer as</p><formula xml:id="formula_0">h 0 = [X g E g | ∀g].</formula><p>3. A single hidden layer, h 1 , with M rectified linear units <ref type="bibr" target="#b8">(Nair and Hinton, 2010</ref>) is fully connected to h 0 .</p><p>4. A softmax function models the probability of an output class y: P (y) ∝ exp(β T y h 1 + b y ), where β y ∈ R M and b y are the weight vector and bias, respectively. Hashed Character n-grams Previous applica- tions of this network structure used (pretrained) word embeddings to represent words <ref type="bibr">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b20">Weiss et al., 2015)</ref>. However, for word embeddings to be effective, they usu- ally need to cover large vocabularies (100,000+) and dimensions (50+). Inspired by the success of character-based representations ( <ref type="bibr" target="#b4">Ling et al., 2015)</ref>, we use features defined over character n-grams in- stead of relying on word embeddings, and learn their embeddings from scratch.</p><p>We use a distinct feature group g for each n- gram length N , and control the size V g directly by applying random feature mixing ( <ref type="bibr">Ganchev and Dredze, 2008)</ref>. That is, we define the feature value v for an n-gram string x as v = H(x) mod V g , where H is a well-behaved hash function. Typical values for V g are in the 100-5000 range, which is far smaller than the exponential number of unique raw n-grams. A consequence of these small fea- ture vocabularies is that we can also use small fea- ture embeddings, typically D g =16.</p><p>Quantization A commonly used strategy for compressing neural networks is quantization, us- ing less precision to store parameters ( <ref type="bibr">Han et al., 2015)</ref>. We compress the embedding weights (the vast majority of the parameters for these shal- low models) by storing scale factors for each em- bedding (details in the supplementary material). In §3.1, we contrast devoting model size to higher precision and lower dimensionality versus lower precision and more network dimensions.</p><p>Training Our objective function combines the cross-entropy loss for model predictions relative to the ground truth with L2 regularization of the bi- ases and hidden layer weights. For optimization, we use mini-batched averaged stochastic gradient descent with momentum <ref type="bibr">(Bottou, 2010;</ref><ref type="bibr">Hinton, 2012)</ref> and exponentially decaying learning rates. The mini-batch size is fixed to 32 and we perform a grid search for the other hyperparameters, tun- ing against the task-specific evaluation metric on held-out data, with early stopping. Full feature templates and optimal hyperparameter settings are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We experiment with small feed-forward networks for four diverse NLP tasks: language identifica- tion, part-of-speech tagging, word segmentation, and preordering for statistical machine translation.</p><p>Evaluation Metrics In addition to standard task-specific quality metrics, our evaluations also consider model size and computational cost. We skirt implementation details by calculating size as the number of kilobytes (1KB=1024 bytes) needed to represent all model parameters and resources. We approximate the computational cost as the number of floating-point operations (FLOPs) per- formed for one forward pass through the network given an embedding vector h 0 . This cost is dom- inated by the matrix multiplications to compute (unscaled) activation unit values, hence our metric excludes the non-linearities and softmax normal-ization, but still accounts for the final layer log- its. To ground this metric, we also provide indica- tive absolute speeds for each task, as measured on a modern workstation CPU (3.50GHz Intel Xeon E5-1650 v3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Identification</head><p>Recent shared tasks on code-switching ( <ref type="bibr" target="#b7">Molina et al., 2016</ref>) and dialects ( <ref type="bibr" target="#b6">Malmasi et al., 2016)</ref> have generated renewed interest in language iden- tification. We restrict our focus to single language identification across diverse languages, and com- pare to the work of <ref type="bibr">Baldwin and Lui (2010)</ref> on pre- dicting the language of Wikipedia text in 66 lan- guages. For this task, we obtain the input h 0 by separately averaging the embeddings for each n- gram length (N = <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>), as summation did not produce good results. <ref type="table">Table 1</ref> shows that we outperform the low- memory nearest-prototype model of <ref type="bibr">Baldwin and Lui (2010)</ref>. Their nearest neighbor model is the most accurate but its memory scales linearly with the size of the training data.</p><p>Moreover, we can apply quantization to the em- bedding matrix without hurting prediction accu- racy: it is better to use less precision for each dimension, but to use more dimensions. Our subsequent models all use quantization. There is no noticeable variation in processing speed when performing dequantization on-the-fly at in- ference time. Our 16-dim Lang-ID model runs at 4450 documents/second (5.6 MB of text per sec- ond) on the preprocessed Wikipedia dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship to Compact Language Detector</head><p>These techniques back the open-source Com- pact Language Detector v3 (CLD3) 1 that runs in Google Chrome browsers. <ref type="bibr">2</ref> Our experimental Lang-ID model uses the same overall architecture as CLD3, but uses a simpler feature set, less in- volved preprocessing, and covers fewer languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">POS Tagging</head><p>We apply our model as an unstructured classifier to predict a POS tag for each token independently, and compare its performance to that of the byte- to-span (BTS) model ( <ref type="bibr">Gillick et al., 2016</ref>  model size by using small input vocabularies: byte values in the case of BTS, and hashed character n- grams and (optionally) cluster ids in our case.</p><p>Bloom Mapped Word Clusters It is well known that word clusters can be powerful features in linear models for a variety of tasks ( <ref type="bibr" target="#b3">Koo et al., 2008;</ref><ref type="bibr" target="#b19">Turian et al., 2010)</ref>. Here, we show that they can also be useful in neural network mod- els. However, naively introducing word cluster features drastically increases the amount of mem- ory required, as a word-to-cluster mapping file with hundreds of thousands of entries can be sev- eral megabytes on its own. 3 By representing word clusters with a Bloom map <ref type="bibr" target="#b15">(Talbot and Talbot, 2008)</ref>, a key-value based generalization of Bloom filters, we can reduce the space required by a fac- tor of ∼15 and use 300KB to (approximately) rep- resent the clusters for 250,000 word types. In order to compare against the monolingual setting of <ref type="bibr">Gillick et al. (2016)</ref>, we train models for the same set of 13 languages from the Univer- sal Dependency treebanks v1.1 (Nivre et al., 2016) corpus, using the standard predefined splits.</p><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, our best models are 0.3% more accuate on average across all languages than the BTS monolingual models, while using 6x fewer parameters and 36x fewer FLOPs. The clus- ter features play an important role, providing a 15% relative reduction in error over our vanilla model, but also increase the overall size. Halv- ing all feature embedding dimensions (except for the cluster features) still gives a 12% reduction in error and trims the overall size back to 1.1x the vanilla model, staying well under 1MB in total. This halved model configuration has a throughput of 46k tokens/second, on average. Two potential advantages of BTS are that it does not require tokenized input and has a more accu- rate multilingual version, achieving 95.85% accu- racy. From a memory perspective, one multilin- gual BTS model will take less space than separate FF models. However, from a runtime perspective, a pipeline of our models doing language identi- fication, word segmentation, and then POS tag- ging would still be faster than a single instance of the deep LSTM BTS model, by about 12x in our FLOPs estimate. <ref type="bibr">4</ref> </p><formula xml:id="formula_1">Transition SPLIT ([σ], [i|β]) → ([σ|i], [β]) MERGE ([σ], [i|β]) → ([σ], [β])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Segmentation</head><p>Word segmentation is critical for processing Asian languages where words are not explicitly sep- arated by spaces. Recently, neural networks have significantly improved segmentation accu- racy ( <ref type="bibr" target="#b24">Zhang et al., 2016;</ref><ref type="bibr">Cai and Zhao, 2016;</ref><ref type="bibr" target="#b23">Yang et al., 2017;</ref><ref type="bibr" target="#b2">Kong et al., 2015)</ref>. We use a structured model based on the transition system in <ref type="table" target="#tab_2">Table 3</ref>, and similar to the one proposed by <ref type="bibr" target="#b25">Zhang and Clark (2007)</ref>. We conduct the seg- mentation experiments on the Chinese Treebank 6.0 with the recommended data splits. No exter- nal resources or pretrained embeddings are used. Hashing was detrimental to quality in our prelim- inary experiments, hence we do not use it for this task. To learn an embedding for unknown charac- ters, we cast characters occurring only once in the training set to a special symbol.</p><p>Selected Features Because we are not using hashing here, we need to be careful about the size of the input vocabulary. The neural network with its non-linearity is in theory able to learn bigrams by conjoining unigrams, but it has been   shown that explicitly using character bigram fea- tures leads to better accuracy ( <ref type="bibr" target="#b24">Zhang et al., 2016;</ref><ref type="bibr" target="#b12">Pei et al., 2014</ref>). <ref type="bibr" target="#b24">Zhang et al. (2016)</ref> suggests that embedding manually specified feature con- junctions further improves accuracy (' <ref type="bibr" target="#b24">Zhang et al. (2016)</ref>-combo' in <ref type="table" target="#tab_4">Table 4</ref>). However, such embed- dings could easily lead to a model size explosion and thus are not considered in this work.</p><formula xml:id="formula_2">Transition Precondition APPEND ([σ|i|j], [β]) → ([σ|[ij]], [β]) SHIFT ([σ], [i|β]) → ([σ|i], [β]) SWAP ([σ|i|j], [β]) → [σ|j], [i|β]); i &lt; j</formula><p>The results in <ref type="table" target="#tab_4">Table 4</ref> show that spending our memory budget on small bigram embeddings is more effective than on larger character embed- dings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Preordering</head><p>Preordering source-side words into the target-side word order is a useful preprocessing task for statis- tical machine translation ( <ref type="bibr" target="#b22">Xia and McCord, 2004;</ref><ref type="bibr">Collins et al., 2005;</ref><ref type="bibr" target="#b9">Nakagawa, 2015;</ref><ref type="bibr">de Gispert et al., 2015</ref>). We propose a novel transition sys- tem for this task ( <ref type="table" target="#tab_5">Table 5</ref>), so that we can repeat- edly apply a small network to produce these per- mutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span). For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FRS</head><p>Size Nakagawa <ref type="formula">(2015)</ref> 81.6 - Small FF 75.2 0.5MB Small FF + POS tags 81.3 1.3MB Small FF + Tagger input fts. 76.6 3.7MB <ref type="table">Table 6</ref>: Preordering results for English → Japanese. FRS (in <ref type="bibr">[0,</ref><ref type="bibr">100]</ref>) is the fuzzy reordering score <ref type="bibr" target="#b14">(Talbot et al., 2011</ref>).</p><p>Pipelines For preordering, we experiment with either spending all of our memory budget on re- ordering, or spending some of the memory budget on features over predicted POS tags, which also requires an additional neural network to predict these tags. Full feature templates are in the supple- mentary material. As the POS tagger network uses features based on a three word window around the token, another possibility is to add all of the fea- tures that would have affected the POS tag of a token to the reorderer directly. <ref type="table">Table 6</ref> shows results with or without using the predicted POS tags in the preorderer, as well as including the features used by the tagger in the reorderer directly and only training the down- stream task. The preorderer that includes a sep- arate network for POS tagging and then extracts features over the predicted tags is more accurate and smaller than the model that includes all the features that contribute to a POS tag in the re- orderer directly. This pipeline processes 7k to- kens/second when taking pretokenized text as in- put, with the POS tagger accounting for 23% of the computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper shows that small feed-forward net- works are sufficient to achieve useful accuracies on a variety of tasks. In resource-constrained en- vironments, speed and memory are important met- rics to optimize as well as accuracies. While large and deep recurrent models are likely to be the most accurate whenever they can be afforded, feed-foward networks can provide better value in terms of runtime and memory, and should be con- sidered a strong baseline. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Memory</head><label></label><figDesc>needs are dominated by the embedding matrix sizes ( g V g D g , where V g and D g are the vocabulary sizes and dimensions respectively for each feature group g), while runtime is strongly influenced by the hidden layer dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>⨁</head><label></label><figDesc>Figure 1: An example network structure for a model using bigrams of the previous, current and next word, and trigrams of the current word. Does not illustrate hashing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>POS tagging. Embedded word clusters improves ac- curacy and allows the use of smaller embedding dimensions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Segmentation Transition system.</head><label>3</label><figDesc></figDesc><table>Initially all 
characters are on the buffer β and the stack σ is empty: 
([], [c1c2...cn]). In the final state the buffer is empty and the 
stack contains the first character for each word. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Segmentation results. Explicit bigrams are useful. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Preordering Transition system. Initially all words are 
part of singleton spans on the buffer: ([], [[w1][w2]...[wn]]). 
In the final state the buffer is empty and the stack contains a 
single span. 

</table></figure>

			<note place="foot" n="1"> github.com/google/cld3 2 As of the date of this writing in 2017.</note>

			<note place="foot" n="3"> For example, the commonly used English clusters from the BLLIP corpus is over 7 MB-people.csail.mit. edu/maestro/papers/bllip-clusters.gz</note>

			<note place="foot" n="4"> Our calculation of BTS FLOPs is very conservative and favorable to BTS, as detailed in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kuzman Ganchev, Fernando Pereira, and the anonymous reviewers for their useful com-ments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1511.06018</idno>
		<title level="m">Segmental recurrent neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring segment representations for neural segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="2880" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminating between similar languages and Arabic dialect identification: A report on the third DSL shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Ljubeši´ljubeši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3)</title>
		<meeting>the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview for the second shared task on language identification in code-switched data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Ghoneim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelati</forename><surname>Hawwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Reyvillamizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Computational Approaches to Code Switching</title>
		<meeting>the Second Workshop on Computational Approaches to Code Switching<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient top-down BTG parsing for machine translation preordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="208" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing in expected linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal Dependencies v1: A Multilingual Treebank Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A lightweight evaluation framework for machine translation reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ichikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Katz-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT &apos;11</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation, WMT &apos;11<address><addrLine>Stroudsburg, PA, USA. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bloom maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Meeting on Analytic Algorithmics and Combinatorics</title>
		<meeting>the Meeting on Analytic Algorithmics and Combinatorics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast and robust multilingual dependency parsing with a generative latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<biblScope unit="page" from="947" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A latent variable model for generative dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Parsing Technology: Dependency Parsing, Domain Adaptation, and Deep Parsing</title>
		<editor>Harry Bunt, Paola Merlo, and Joakim Nivre</editor>
		<meeting><address><addrLine>Netherlands; Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="35" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word Representations: A Simple and General Method for Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving a statistical MT system with automatically learned rewrite patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">508</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural word segmentation with rich pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<idno>abs/1704.08960</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transition-based neural word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="421" to="431" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="840" to="847" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
