<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Corpus-level Fine-grained Entity Typing Using Contextual Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
							<email>yadollah@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Corpus-level Fine-grained Entity Typing Using Contextual Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as &quot;food&quot; or &quot;artist&quot;. The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that first scores the individual occurrences of an entity and then aggregates the scores. In our evaluation, FIGMENT strongly out-performs an approach to entity typing that relies on relations obtained by an open information extraction system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language understanding (NLU) is not pos- sible without knowledge about the world -partly so because world knowledge is needed for many NLP tasks that must be addressed as part of NLU; e.g., many coreference ambiguities can only be re- solved based on world knowledge. It is also true because most NLU applications combine a vari- ety of information sources that include both text sources and knowledge bases; e.g., question an- swering systems need access to knowledge bases like gazetteers. Thus, high-quality knowledge bases are critical for successful NLU.</p><p>Unfortunately, most knowledge bases are in- complete. The effort required to create knowledge bases is considerable and since the world changes, it will always continue. Knowledge bases are therefore always in need of updates and correc- tions. To address this problem, we present an in- formation extraction method that can be used for knowledge base completion. In contrast to most other work on knowledge base completion, we fo- cus on fine-grained classification of entities as op- posed to relations between entities.</p><p>The goal of knowledge base completion is to acquire knowledge in general as opposed to de- tailed analysis of an individual context or sen- tence. Therefore, our approach is corpus-level: We infer the types of an entity by considering the set of all of its mentions in the corpus. In contrast, named entity recognition (NER) is context-level or sentence-level: NER infers the type of an entity in a particular context. As will be discussed in more detail in the following sections, the problems of corpus-level entity typing vs. context/sentence- level entity typing are quite different. This is partly because the objectives of optimizing ac- curacy on the context-level vs. optimizing accu- racy on the corpus-level are different and partly because evaluation measures for corpus-level and context-level entity typing are different.</p><p>We define our problem as follows. Let K be a knowledge base that models a set E of entities, a set T of fine-grained classes or types and a mem- bership function m : E × T → {0, 1} such that m(e, t) = 1 iff entity e has type t. Let C be a large corpus of text. Then, the problem we address in this paper is corpus-level entity typing: For a given pair of entity e and type t determine -based on the evidence available in C -whether e is a member of type t (i.e., m(e, t) = 1) or not (i.e., m(e, t) = 0) and update the membership relation m of K with this information.</p><p>We investigate two approaches to entity typing: a global model and a context model.</p><p>The global model aggregates all contextual in- formation about an entity e from the corpus and then based on that, makes a classification deci- sion on a particular type t -i.e., m(e, t) = 0 vs. m(e, t) = 1.</p><p>The context model first scores each individual context of e as expressing type t or not. A final de- cision on the value of m(e, t) is then made based on the distribution of context scores. One diffi- culty in knowledge base completion based on text corpora is that it is too expensive to label large amounts of text for supervised approaches. For our context model, we address this problem using distant supervision: we treat all contexts of an en- tity that can have type t as contexts of type t even though this assumption will in general be only true for a subset of these contexts. Thus, as is typi- cal for distant supervision, the labels are incorrect in some contexts, but we will show that the label- ing is good enough to learn a high-quality context model. The global model is potentially more robust since it looks at all the available information at once. In contrast, the context model has the advan- tage that it can correctly predict types for which there are only a small number of reliable contexts. For example, in a large corpus we are likely to find a few reliable contexts indicating that "Barack Obama" is a bestselling author even though this evidence may be obscured in the global distri- bution because the vast majority of mentions of "Obama" do not occur in author contexts.</p><p>We implement the global model and the con- text model as well as a simple combination of the two and call the resulting system FIGMENT: FIne-Grained eMbedding-based Entity Typing. A key feature of FIGMENT is that it makes exten- sive use of distributed vector representations or embeddings. We compute embeddings for words as is standard in a large body of NLP literature, but we also compute embeddings for entities and for types. The motivation for using embeddings in these cases is (i) better generalization and (ii) more robustness against noise for text types like web pages. We compare the performance of FIG- MENT with an approach based on Open Informa- tion Extraction (OpenIE).</p><p>The main contributions of this paper can be summarized as follows.</p><p>• We address the problem of corpus-level en- tity typing in a knowledge base completion setting. In contrast to other work that has fo- cused on learning relations between entities, we learn types of entities.</p><p>• We show that context and global models for entity typing provide complementary infor- mation and combining them gives the best re- sults.</p><p>• We use embeddings for words, entities and types to improve generalization and deal with noisy input.</p><p>• We show that our approach outperforms a system based on OpenIE relations when the input corpus consists of noisy web pages.</p><p>In the following, we first discuss related work. Then we motivate our approach and define the problem setting we adopt. We then introduce our models in detail and report and analyze experi- mental results. Finally, we discuss remaining chal- lenges and possible future work and present our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Named entity recognition (NER) is the task of detecting and classifying named entities in text. While most NER systems (e.g., <ref type="bibr" target="#b8">Finkel et al. (2005)</ref>) only consider a small number of entity classes, recent work has addressed fine-grained NER ( <ref type="bibr" target="#b30">Yosef et al., 2012;</ref><ref type="bibr" target="#b14">Ling and Weld, 2012)</ref>. These methods use a variety of lexical and syn- tactic features to segment and classify entity men- tions. Some more recent work assumes the seg- mentation is known and only classifies entity men- tions. <ref type="bibr" target="#b4">Dong et al. (2015)</ref> use distributed repre- sentations of words in a hybrid classifier to clas- sify mentions to 20 types. <ref type="bibr" target="#b29">Yogatama et al. (2015)</ref> classify mentions to more fine-grained types by using different features for mentions and embed- ding labels in the same space. These methods as well as standard NER systems try to maxi- mize correct classification of mentions in individ- ual contexts whereas we aggregate individual con- texts and evaluate on accuracy of entity-type as- signments inferred from the entire corpus. In other words, their evaluation is sentence-level whereas ours is corpus-level.</p><p>Entity set expansion (ESE) is the problem of finding entities in a class (e.g., medications) given a seed set (e.g., {"Ibuprofen", "Maalox", "Prozac"}). The standard solution is pattern-based bootstrapping <ref type="bibr" target="#b26">(Thelen and Riloff, 2002;</ref><ref type="bibr" target="#b10">Gupta and Manning, 2014)</ref>. ESE is different from the prob- lem we address because ESE starts with a small seed set whereas we assume that a large number of examples from a knowledge base (KB) is avail- able. Initial experiments with the system of Gupta and Manning <ref type="bibr">(2014)</ref> showed that it was not per- forming well for our task -this is not surprising given that it is designed for a task with properties quite different from entity typing.</p><p>More closely related to our work are the OpenIE systems NNPLB ( <ref type="bibr" target="#b13">Lin et al., 2012</ref>) and PEARL ( <ref type="bibr" target="#b19">Nakashole et al., 2013</ref>) for fine-grained typing of unlinkable and emerging entities. Both sys- tems first extract relation tuples from a corpus and then type entities based on the tuples they occur in (where NNPLB only uses the subject position for typing). To perform typing, NNPLB propa- gates activation from known members of a class to other entities whereas PEARL assigns types to the argument slots of relations. The main differ- ence to FIGMENT is that we do not rely on re- lation extraction. In principle, we can make use of any context, not just subject and object posi- tions. FIGMENT also has advantages for noisy text for which relation extraction can be challeng- ing. This will be demonstrated in our evaluation on web text. Finally, our emphasis is on making yes-no decisions about possible types (as opposed to just ranking possibilities) for all entities (as op- posed to just emerging or unlinkable entities). Our premise is that even existing entities in KBs are of- ten not completely modeled and have entries that require enhancement. We choose NNPLB as our baseline.</p><p>The fine-grained typing of entities performed by FIGMENT can be used for knowledge base completion (KBC). Most KBC systems focus on relations between entities, not on types as we do. Some generalize the patterns of relation- ships within the KB ( ) while others use a combination of within-KB generalization and information extrac- tion from text ( <ref type="bibr" target="#b23">Socher et al., 2013;</ref><ref type="bibr" target="#b12">Jiang et al., 2012;</ref><ref type="bibr" target="#b22">Riedel et al., 2013;</ref><ref type="bibr" target="#b27">Wang et al., 2014</ref>). <ref type="bibr" target="#b20">Neelakantan and Chang (2015)</ref> ad- dress entity typing in a way that is similar to FIG- MENT. Their method is based on KB information, more specifically entity descriptions in Wikipedia and Freebase. Thus, in contrast to our approach, their system is not able to type entities that are not covered by existing KBs. We infer classes for en- tities from a large corpus and do not assume that these entities occur in the KB.</p><p>Learning embeddings for words is standard in a large body of NLP literature (see <ref type="bibr" target="#b0">Baroni et al. (2014)</ref> for an overview). In addition to words, we also learn embeddings for entities and types. Most prior work on entity embeddings (e.g., , ) and entity and type embeddings ( <ref type="bibr" target="#b31">Zhao et al., 2015</ref>) has mainly used KB information as opposed to text corpora. <ref type="bibr" target="#b27">Wang et al. (2014)</ref> learn embeddings of words and entities in the same space by replacing Wikipedia anchors with their corresponding entities. For our global model, we learn entity embedding in a sim- ilar way, but on a corpus with automatically anno- tated entities. For our context model, we learn and use type embeddings jointly with corpus words to improve generalization, a novel contribution of this paper to the best of our knowledge. We learn all our embeddings using word2vec ( <ref type="bibr" target="#b17">Mikolov et al., 2013)</ref>.</p><p>Our problem can be formulated as multi- instance multi-label (MIML) learning ( <ref type="bibr" target="#b32">Zhou and Zhang, 2006</ref>), similar to the formulation for re- lation extraction by <ref type="bibr" target="#b25">Surdeanu et al. (2012)</ref>. In our problem, each example (entity) can have sev- eral instances (contexts) and each instance can have several labels (types). Similar to <ref type="bibr" target="#b32">Zhou and Zhang (2006)</ref>'s work on scene classification, we also transform MIML into easier tasks. The global model transforms MIML into a multi-label prob- lem by merging all instances of an example. The context model solves the problem by combining the instance-label scores to example-label scores.</p><p>3 Motivation and problem definition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Freebase</head><p>Large scale KBs like Freebase ( <ref type="bibr" target="#b2">Bollacker et al., 2008)</ref>, <ref type="bibr">YAGO (Suchanek et al., 2007)</ref> and Google knowledge graph are important NLP resources. Their structure is roughly equivalent to a graph in which entities are nodes and edges are relations between entities. Each node is also associated with one or more semantic classes, called types. These types are the focus of this paper.</p><p>We use Freebase, the largest available KB, in this paper. In Freebase, an entity can belong to several classes, e.g., "Barack Obama" is a mem- ber of 37 types including "US president" and "au- thor". One notable type is also defined for each entity, e.g., "US-president" for "Obama" since it is regarded as his most prominent characteristic and the one that would be used to disambiguate refer- ences to him, e.g., to distinguish him from some- body else with the same name.</p><p>There are about 1500 types in Freebase, or-ganized by domain; e.g., the domain "food" has types like "food", "ingredient" and "restaurant". Some types like "location" are very general, some are very fine-grained, e.g., "Vietnamese urban dis- trict". There are types that have a large number of instances like "citytown" and types that have very few like "camera sensor". Entities are defined as instances of types. They can have several types based on the semantic classes that the entity they are referring to is a member of -as in the above example of Barack Obama.</p><p>The types are not organized in a strict taxon- omy even though there exists an included type re- lationship between types in Freebase. The reason is that for a user-generated KB it is difficult to maintain taxonomic consistency. For example, al- most all instances of "author" are also instances of "person", but sometimes organizations author and publish documents. We follow the philosophy of Freebase and assume that the types do not have a hierarchical organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Incompleteness of knowledge base</head><p>Even though Freebase is the largest publicly avail- able KB of its kind, it still has significant coverage problems; e.g., 78.5% of persons in Freebase do not have nationality ( <ref type="bibr" target="#b18">Min et al., 2013)</ref>. This is unavoidable, partly because Freebase is user-generated, partly because the world changes and Freebase has to be updated to reflect those changes. All existing KBs that attempt to model a large part of the world suffer from this incomplete- ness problem. Incompleteness is likely to become an even bigger problem in the future as the number of types covered by KBs like Freebase increases. As more and more fine-grained types are added, achieving good coverage for these new types us- ing only human editors will become impossible.</p><p>The approach we adopt in this paper to address incompleteness of KBs is extraction of informa- tion from large text corpora. Text can be argued to be the main repository of the type of knowledge represented in KBs, so it is reasonable to attempt completing them based on text. There is in fact a significant body of work on corpus-based meth- ods for extracting knowledge from text; however, most of it has addressed relation extraction, not the acquisition of type information -roughly cor- responding to unary relations (see Section 2). In this paper, we focus on typing entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity linking</head><p>The first step in extracting information about en- tities from text is to reliably identify mentions of these entities. This problem of entity linking has some mutual dependencies with entity typing. Indeed, some recent work shows large improve- ments when entity typing and linking are jointly modeled ( <ref type="bibr" target="#b15">Ling et al., 2015;</ref><ref type="bibr" target="#b6">Durrett and Klein, 2014</ref>). However, there are constraints that are im- portant for high-performance entity linking, but that are of little relevance to entity typing. For ex- ample, there is a large literature on entity linking that deals with coreference resolution and inter- entity constraints -e.g., "Naples" is more likely to refer to a US (resp. an Italian) city in a context mentioning "Fort Myers" (resp. "Sicily").</p><p>Therefore, we will only address entity typing in this paper and consider entity linking as an in- dependent module that provides contexts of en- tities for FIGMENT. More specifically, we build FIGMENT on top of the output of an existing en- tity linking system and use FACC1, 1 an automatic Freebase annotation of ClueWeb ( <ref type="bibr" target="#b9">Gabrilovich et al., 2013</ref>). According to the FACC1 distributors, precision of annotated entities is around 80-85% and recall is around 70-85%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">FIGER types</head><p>Our goal is fine-grained typing of entities, but types like "Vietnamese urban district" are too fine- grained. To create a reliable setup for evaluation and to make sure that all types have a reasonable number of instances, we adopt the FIGER type set ( <ref type="bibr" target="#b14">Ling and Weld, 2012</ref>) that was created with the same goals in mind. FIGER consists of 112 tags and was created in an attempt to preserve the di- versity of Freebase types while consolidating in- frequent and unusual types through filtering and merging. For example, the Freebase types "dish", "ingredient", "food" and "cheese" are mapped to one type "food". See (Ling and Weld, 2012) for a complete list of FIGER types. We use "type" to refer to FIGER types in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Global, context and joint models</head><p>We address a problem setting in which the fol- lowings are given: a KB with a set of entities E, a set of types T and a membership function m : E × T → {0, 1} such that m(e, t) = 1 iff entity e has type t; and a large annotated corpus C in which mentions of E are linked. As mentioned before, we use FACC1 as our corpus.</p><p>In this problem setting, we address the task of corpus-level fine-grained entity typing: we want to infer from the corpus for each pair of entity e and type t whether m(e, t) = 1 holds, i.e., whether entity e is a member of type t.</p><p>We use three scoring models in FIGMENT: global model, context model and joint model. The models return a score S(e, t) for an entity-type pair (e, t). S(e, t) is an assessment of the extent to which it is true that the semantic class t contains e and we learn it by training on a subset of E. The trained models can be applied to large corpora and the resulting scores can be used for learning new types of entities covered in the KB as well as for typing new or unknown entities -i.e., entities not covered by the KB. To work for new or unknown entities, we would need an entity linking system such as the ones participating in TAC KBP <ref type="bibr" target="#b16">(McNamee and Dang, 2009</ref>) that identifies and clus- ters mentions of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Global model</head><p>The global model (GM) scores possible types of entity e based on a distributed vector representa- tion or embedding v(e) ∈ R d of e. v(e) can be learned from the entity-annotated corpus C.</p><p>Embeddings of words have been widely used in different NLP applications. The embedding of a word is usually derived from the distribution of its context words. The hypothesis is that words with similar meanings tend to occur in similar contexts <ref type="bibr" target="#b11">(Harris, 1954)</ref> and therefore cooccur with similar context words. By extension, the assumption of our model is that entities with similar types tend to cooccur with similar context words.</p><p>To learn a score function S GM (e, t), we use a multilayer perceptron (MLP) with one shared hid- den layer and an output layer that contains, for each type t in T, a logistic regression classifier that predicts the probability of t:</p><formula xml:id="formula_0">S GM (e, t) = G t tanh W input v(e)</formula><p>where W input ∈ R h×d is the weight matrix from v(e) ∈ R d to the hidden layer with size h. G t is the logistic regression classifier for type t that is applied on the hidden layer. The shared hid- den layer is designed to exploit the dependen- cies among labels. Stochastic gradient descent (SGD) with AdaGrad (Duchi et al., 2011) and minibatches are used to learn the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context model</head><p>For the context model (CM), we first learn a scor- ing function S c2t (c, t) for individual contexts c in the corpus. S c2t (c, t) is an assessment of how likely it is that an entity occurring in context c has type t. For example, consider the contexts c 1 = "he served SLOT cooked in wine" and c 2 = "she loves SLOT more than anything". SLOT marks the oc- currence of an entity and it also shows that we do not care about the entity mention itself but only its context. For the type t = "food", S c2t (c 1 , t) is high whereas S c2t (c 2 , t) is low. This example demon- strates that some contexts of an entity like "beef" allow specific inferences about its type whereas others do not. We aim to learn a scoring function S c2t that can distinguish these cases. Based on the context scoring function S c2t , we then compute the corpus-level CM scoring func- tion S CM that takes the scores S c2t (c i , t) for all contexts of entity e in the corpus as input and re- turns a score S CM (e, t) that assesses the appropri- ateness of t for e. In other words, S CM is:</p><formula xml:id="formula_1">S CM (e, t) = g(U e,t )<label>(1)</label></formula><p>where U e,t = {S c2t (c 1 , t), . . . , S c2t (c n , t)} is the set of scores for t based on the n contexts c 1 . . . c n of e in the corpus. The function g is a sum- mary function of the distribution of scores, e.g., the mean, median or maximum. We use the mean in this paper. We now describe how we learn S c2t . For train- ing, we need contexts that are labeled with types. We do not have such a dataset in our problem set- ting, but we can use the contexts of linked entities as distantly supervised data. Specifically, assume that entity e has n types. For each mention of e in the corpus, we generate a training example with n labels, one for each of the n types of e.</p><p>For training S c2t , a context c of a mention is represented as the concatenation of two vectors. One vector is the average of the embeddings of the 2l words to the left and right of the mention. The other vector is the concatenation of the em- beddings of the 2k words to the left and right of the mention. E.g., for k = 2 and l = 1 the context c is represented as the vector:</p><formula xml:id="formula_2">Φ(c) = x −2 , x −1 , x +1 , x +2 , avg(x −1 , x +1 )</formula><p>where x i ∈ R d is the embedding of the context word at posi- tion i relative to the entity in position 0.</p><p>We train S c2t on context representations that consist of embeddings because our goal is a robust model that works well on a wide variety of genres, including noisy web pages. If there are other enti- ties in the contexts, we first replace them with their notable type to improve generalization. We learn word and type embeddings from the corpus C by replacing train entities with their notable type.</p><p>The next step is to score these examples. We use an MLP similar to the global model to learn S c2t , which predicts the probability of type t occurring in context c:</p><formula xml:id="formula_3">S c2t (c, t) = G t tanh W input Φ(c)</formula><p>where Φ(c) ∈ R n is the feature vector of the con- text c as described above, n = (2k + 1) * d and W input ∈ R h×n is the weight matrix from input to hidden layer with h units. Again, we use SGD with AdaGrad and minibatch training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint model</head><p>Global model and context model have comple- mentary strengths and weaknesses. The strength of CM is that it is a direct model of the only source of reliable evidence we have: the context in which the entity occurs. This is also the way a human would ordinarily do entity typ- ing: she would determine if a specific context in which the entity occurs implies that the entity is, say, an author or a musician and type it accord- ingly. The order of words is of critical importance for the accurate assessment of a context and CM takes it into account. A well-trained CM will also work for cases for which GM is not applicable. In particular, if the KB contains only a small number of entities of a particular type, but the corpus con- tains a large number of contexts of these entities, then CM is more likely to generalize well.</p><p>The main weakness of CM is that a large pro- portion of contexts does not contain sufficient in- formation to infer all types of the entity; e.g., based on our distant supervised training data, we label every context of "Obama" with "author", "politician" and Obama's other types in the KB. Thus, CM is trained on a noisy training set that contains only a relatively small number of infor- mative contexts.</p><p>The main strength of GM is that it bases its de- cisions on the entire evidence available in the cor- pus. This makes it more robust. It is also more efficient to train since its training set is by a factor of |M | smaller than the training set of CM where |M | is the average number of contexts per entity.</p><p>The disadvantage of GM is that it does not work well for rare entities since the aggregated repre- sentation of an entity may not be reliable if it is based on few contexts. It is also less likely to work well for non-dominant types of an entity which might be swamped by dominant types; e.g., the author contexts of "Obama" may be swamped by the politician contexts and the overall context signature of the entity "Obama" may not contain enough signal to infer that he is an author. Finally, methods for learning embeddings like word2vec are bag-of-word approaches. Therefore, word or- der information -critical for many typing deci- sions -is lost.</p><p>Since GM and CM models are complementary, a combination model should work better. We test this hypothesis for the simplest possible joint model (JM), which adds the scores of the two in- dividual models:</p><formula xml:id="formula_4">S JM (e, t) = S GM (e, t) + S CM (e, t)</formula><p>5 Experimental setup and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Baseline: Our baseline system is the OpenIE sys- tem no-noun-phrase-left-behind (NNPLB) by <ref type="bibr" target="#b13">Lin et al. (2012)</ref> (see Section 2). Our reimplementa- tion performs on a par with published results. <ref type="bibr">2</ref> We use NNPBL as an alternative way of computing scores S(e, t). Scores of the four systems we com- pare -NNPBL, GM, CM, JM -are processed the same way to perform entity typing (see below).</p><p>Corpus: We select a subset of about 7.5 mil- lion web pages, taken from the first segment of ClueWeb12, 3 from different crawl types: 1 million Twitter links, 120,000 WikiTravel pages and 6.5 million web pages. This corpus is preprocessed by eliminating HTML tags, replacing all numbers with "7" and all web links and email addresses with "HTTP", filtering out sentences with length less than 40 characters, and finally doing a simple tokenization. We merge the text with the FACC1 annotations. The resulting corpus has 4 billion tokens and 950,000 distinct entities. We use the 2014-03-09 Freebase data dump as our KB.</p><p>Entity datasets: We consider all entities in the corpus whose notable types can be mapped to one of the 112 FIGER types, based on the mapping provided by FIGER. 750,000 such entities form our set of entities. 10 out of 112 FIGER types have no entities in this set. <ref type="bibr">4</ref> We run the OpenIE system <ref type="bibr">Reverb (Fader et al., 2011</ref>) to extract relation triples of the form &lt;subject, relation, object&gt;. Since NNPLB only considers entities in the subject position, we filter out triples whose subject is not an entity. The size of the remaining set of triples is 4,000,000. For a direct comparison with NNPLB, we divide the 750,000 entities into those that occur in subject po- sition in one of the extracted triples (about 250,000 subject entities or SE) and those that do not (about 500,000 non-subject entities or NSE). We split SE 50:20:30 into train, dev and test sets. The average and median number of FIGER types of the training entities are 1.8 and 2, respectively. We use NSE to evaluate performance of FIGMENT on entities that do not occur in subject position. <ref type="bibr">5</ref> Context sampling: For S c2t , we create train', dev' and test' sets of contexts that correspond to train, dev and test sets of entities. Because the number of contexts is unbalanced for both entities and types and because we want to accelerate train- ing and testing, we downsample contexts. For the set train', we use the notable type feature of Free- base: For each type t, we take contexts from the mentions of those entities whose notable type is t. Recall, however, that each context is labeled with all types of its entity -see Section 4.2.</p><p>Then if the number of contexts for t is larger than a minimum, we sample the contexts based on the number of training entities of t. We set the minimum to 10,000 and constrain the number of samples for each t to 20,000. Also, to reduce the effect of distant supervision, entities with fewer distinct types are preferred in sampling to provide discriminative contexts for their notable types. For test' and dev' sets, we sample 300 and 200 random contexts, respectively, for each entity.</p><p>System setup: As the baseline, we apply NNPLB to the 4 million extracted triples. To learn entity embeddings for GM, we run word2vec (skipgram, 200 dimensions, window size 5) on a version of the corpus in which entities have been replaced by their Freebase IDs, based on the FACC1 annotation. We then train MLP with num- ber of hidden units h = 200 on the embeddings of training entities until the error on dev entities stops decreasing.</p><p>Our reasoning for the unsupervised training setup is that we do not use any information about the types of entities (e.g., no entities annotated by humans with types) when we run an unsupervised algorithm like word2vec. In a real-world appli- cation of FIGMENT to a new corpus, we would first run word2vec on the merger of our corpus and the new corpus, retrain GM on training entities and finally apply it to entities in the new corpus. This scenario is simulated by our setup.</p><p>Recall that the input to CM consists of 2k unit embeddings and the average of 2l unit embeddings where we use the term unit to refer to both words and types. We set k to 4 and l to 5. To learn em- beddings for units, we first exclude lines contain- ing test entities, and then replace each entity with its notable type. Then, we run word2vec (skip- gram, 100 dimensions, window size 5) on this new corpus and learn embeddings for words and types.</p><p>Using the embeddings as input representations, we train S c2t on train' until error on dev' stops de- creasing. We set the number of hidden units to 300. We then apply the trained scoring function S c2t to test' and get the scores S c2t (c, t) for test' contexts. As explained in Section 4.2, we compute the corpus-level scores S CM (e, t) for each entity by averaging its context-level scores (see Equation 1).</p><p>Ranking evaluation: This evaluation shows how well the models rank types for entities. The ranking is based on the scores S(e, t) produced by the different models and baselines. Similar to the evaluation performed by <ref type="bibr" target="#b13">Lin et al. (2012)</ref>, we use precision at 1 (P@1) and breakeven point <ref type="bibr">(BEP, Boldrin and Levine (2008)</ref>). BEP is F 1 at the point in the ranked list at which precision and recall have the same value.</p><p>Classification evaluation: This evaluation demonstrates the quality of the thresholded assign- ment decisions produced by the models. These measures more directly express how well FIG- MENT would succeed in enhancing the KB with new information since for each pair (e, t), we have to make a binary decision about whether to put it in the KB or not. We compare our decisions with the gold KB information.  Our evaluation measures are (i) accuracy: an entity is correct if all its types and no incorrect types are assigned to it; (ii) micro average: F 1 of all type-entity assignment decisions; (iii) entity macro average F 1 : F 1 of types assigned to an en- tity, averaged over entities; (iv) type macro aver- age F 1 : F 1 of entities assigned to a type, averaged over types.</p><p>The assignment decision is made based on thresholds, one per type, for each S(e, t). We se- lect the threshold that maximizes F 1 of entities as- signed to the type on dev. <ref type="table">Table 1</ref> presents results for the ranking evaluation as well as for the first three measures of the clas- sification evaluation. MFT is the most frequent type baseline that ranks types according to their frequency in train. We also show the results for head entities (frequency higher than 100) and tail entities (frequency less than 5). The performance of the systems is in this order: JM &gt; GM &gt; CM &gt; NNPLB &gt; MFT. <ref type="table" target="#tab_1">Table 2</ref> shows the results of the fourth classi- fication measure, type macro average F 1 , for all, head (more than 3000 train entities, 11 types), and tail (less than 200 train entities, 36 types) types. The ordering of models for <ref type="table" target="#tab_1">Table 2</ref> is in line with <ref type="table">Table 1</ref>: JM &gt; GM &gt; CM &gt; NNPLB &gt; MFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We can easily run FIGMENT for non-subject entities (NSE) exactly the same way we have run it for subject entities. We test our JM on the 67,000 NSE entities with a frequency of more than 10. The top ranked type returned for 73.5% of enti- ties was correct. Thus, due to our ability to deal with NSE, we can type an additional 50,000 enti- ties correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>Effect of window size in CM:  Poor results of NNPLB: NNPLB is mostly hampered by Reverb, which did not work well on the noisy web corpus. As a result, the quality of the extracted relations -which NNPLB entity typ- ing is based on -is too low for reliable typing decisions. The good results of NNPLB on their non-noisy published relation triples confirm that. On the three million relation triples, when map- ping Freebase types to FIGER, P@1 of NNPLB is .684; when limiting entities to those with more than 10 relations, the results improve to .776.</p><p>GM performs better than CM and JM per- forms best: The fact that GM outperforms CM shows that decisions based on one global vector of an entity work better than aggregating multiple weak decisions on their contexts. That is clear- est for tail entities -where one bad context can highly influence the final decision -and for tail types, which CM was not able to distinguish from other similar types. However, the good results of the simple JM confirm that the score distributions in CM do help. As an example, consider one of the test entities that is an "author". GM and CM wrongly predict "written work" and "artist", re- spectively, but JM correctly outputs "author".</p><p>Errors of CM: Many CM errors are caused by its simple input representation: it has to learn all linguistic abstractions that it wants to rely on from the training set. One manifestation of this problem is that CM confuses the types "food" and "restau- rant". There are only few linguistic contexts in which entities of these types can be exchanged for each other. On the other hand, the context words they cooccur with in a bag-of-words (BOW) sense are very similar. Thus, this indicates that CM pays too much attention to BOW information and that its representation of contexts is limited in terms of generalization. Assumptions that result in errors: The per- formance of all models suffers from a number of assumptions we made in our training / evaluation setup that are only approximately true.</p><p>The first assumption is that FACC1 is correct. But it has a precision of only 80-85% and this caused many errors. An example is the lunar crater "Buffon" in Freebase, a "location". Its predicted type is "athlete" because some FACC1 annotations of the crater link it to the Italian goalkeeper.</p><p>The second assumption of our evaluation setup is the completeness of Freebase. There are about 2,600 entities with the single type "person" in SE test. For 62% of the errors on this subset, the top predicted type is a subtype of person: "author", "artist" etc. We manually typed a random subset of 50 and found that the predicted type is actually correct for 44 of these entities.</p><p>The last assumption is the mapping from Free- base to FIGER. Some common Freebase types like "award-winner" are not mapped. This negatively affects evaluation measures for many entities. On the other hand, the resulting types do not have a balanced number of instances. Based on our train- ing entities, 11 types (e.g., "law") have less than 50 instances while 26 types (e.g., "software") have more than 1000 instances. Even sampling the con- texts could not resolve this problem and this led to low performance on tail types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future work</head><p>The performance of FIGMENT is poor for tail types and entities. We plan to address this in the future (i) by running FIGMENT on larger corpora, (ii) by refining the FIGER type set to cover more Freebase entities, (iii) by exploiting a hierarchy over types and (iv) by exploring more complex in- put representations of the context for the CM.</p><p>FIGMENT's context model can in principle be based on any system that provides entity-type as- sessment scores for individual contexts. Thus, as an alternative to our scoring model S c2t (c, t), we could use sentence-level entity classification systems such as FIGER ( <ref type="bibr" target="#b14">Ling and Weld, 2012)</ref> and ( <ref type="bibr" target="#b29">Yogatama et al., 2015</ref>)'s system. These sys- tems are based on linguistic features different from the input representation we use, so a comparison with our embedding-based approach is interesting.</p><p>Our assumption is that FIGMENT is more robust against noise, but investigation is needed.</p><p>The components of the version of FIGMENT we presented, in particular, context model and global model, do not use features derived from the mention of an entity. Our assumption was that such features are less useful for fine-grained en- tity typing. However, there are clearly some types for which mention-based features are useful (e.g., medications or organizations referred to by abbre- viations), so we will investigate the usefulness of such features in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented FIGMENT, a corpus-level system that uses contextual information for entity typing. We designed two scoring models for pairs of en- tities and types: a global model that scores based on aggregated context information and a context model that aggregates the scores of individual con- texts. We used embeddings of words, entities and types to represent contextual information. Our experimental results show that global model and context model provide complementary informa- tion for entity typing. We demonstrated that, com- paring with an OpenIE-based system, FIGMENT performs well on noisy web pages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Type macro average F 1 for all, head and 
tail types 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 explores</head><label>3</label><figDesc></figDesc><table>the effect of using different context sizes. Recall 
that CM was trained with 2k = 8 for the concatena-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Effect of the context size 2k in CM (2k: context size, h: number of hidden units in MLP)</head><label>3</label><figDesc></figDesc><table>tion and 2l = 10 for the average window size. We 
change 2k from 0 to 14 while keeping 2l = 10. The 
number of hidden units used in each model is also 
reported. The table shows that CM can leverage 
larger context sizes well. 
</table></figure>

			<note place="foot" n="1"> lemurproject.org/clueweb12/FACC1</note>

			<note place="foot" n="2"> The precision of our implementation on the dataset of three million relation triples distributed by (Lin et al., 2012) is 60.7% compared to 59.8% and 61% for tail and head entities reported by Lin et al. (2012). 3 http://lemurproject.org/clueweb12</note>

			<note place="foot" n="4"> The reason is that the FIGER mapping uses Freebase user-created classes. The 10 missing types are not the notable type of any entity in Freebase. 5 The entity datasets are available at http: //cistern.cis.lmu.de/figment</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Against intellectual monopoly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Boldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-10" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Irreflexive and hierarchical relations as translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcíadurán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<idno>abs/1304.7158</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hybrid neural model for type classification of entity mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the TwentyFourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="1243" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John McIntyre Conference Centre</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-30" />
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Facc1: Freebase annotation of clueweb corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved pattern learning for bootstrapped entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="98" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Link prediction in multirelational graphs using additive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Technologies meet Recommender Systems &amp; Big Data</title>
		<meeting>the International Workshop on Semantic Technologies meet Recommender Systems &amp; Big Data<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-11-11" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">No noun phrase left behind: Detecting and typing unlinkable entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-12" />
			<biblScope unit="page" from="893" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview of the tac 2009 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Analysis Conference (TAC)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="111" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Westin Peachtree Plaza Hotel; Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-09" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained semantic typing of emerging entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Tylenda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Sofia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08-09" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1488" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring missing entity type instances for knowledge base completion: New dataset and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="515" to="525" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-09" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2007-05-08" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-12" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bootstrapping method for learning semantic lexicons using extraction pattern contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL-02 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Embedding methods for fine grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HYENA: hierarchical type classification for entity names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Amir Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Posters</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-08-15" />
			<biblScope unit="page" from="1361" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation learning for measuring entity relatedness with rich information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the TwentyFourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="1412" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiinstance multi-label learning with application to scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12-04" />
			<biblScope unit="page" from="1609" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
