<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Depth-bounding is effective: Improvements and evaluation of unsupervised PCFG induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Boston Children&apos;s Hospital &amp; Harvard Medical School</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">Department of Linguistics University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
							<email>finale@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Boston Children&apos;s Hospital &amp; Harvard Medical School</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">Department of Linguistics University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
							<email>timothy.miller@childrens.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Boston Children&apos;s Hospital &amp; Harvard Medical School</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">Department of Linguistics University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
							<email>schuler@ling.osu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Boston Children&apos;s Hospital &amp; Harvard Medical School</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">Department of Linguistics University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
							<email>lanes@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Linguistics</orgName>
								<orgName type="department" key="dep2">Boston Children&apos;s Hospital &amp; Harvard Medical School</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">Department of Linguistics University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Depth-bounding is effective: Improvements and evaluation of unsupervised PCFG induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2721" to="2731"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2721</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depth-bounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and without bounding. 1 Results show that depth-bounding is indeed significantly effective in limiting the search space of the inducer and thereby increasing the accuracy of the resulting parsing model. Moreover, parsing results on English, Chinese and German show that this bounded model with a new inference technique is able to produce parse trees more accurately than or competitively with state-of-the-art constituency-based grammar induction models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised grammar inducers hypothesize hi- erarchical structures for strings of words. Us- ing context-free grammars (CFGs) to define these structures, previous attempts at either CFG param- eter estimation <ref type="bibr" target="#b4">(Carroll and Charniak, 1992;</ref><ref type="bibr" target="#b28">Schabes and Pereira, 1992;</ref><ref type="bibr" target="#b15">Johnson et al., 2007b</ref>) or directly inducing a CFG as well as its probabilities ( <ref type="bibr" target="#b20">Liang et al., 2009;</ref><ref type="bibr" target="#b35">Tu, 2012)</ref> have not achieved as much success as experiments with other kinds of formalisms ( <ref type="bibr" target="#b19">Klein and Manning, 2004;</ref><ref type="bibr" target="#b31">Seginer, 2007;</ref><ref type="bibr" target="#b26">Ponvert et al., 2011</ref>). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner <ref type="bibr" target="#b8">(Gold, 1967;</ref><ref type="bibr" target="#b7">Cramer, 2007;</ref><ref type="bibr" target="#b20">Liang et al., 2009)</ref>.</p><p>One constraint that has been applied is recur- sion depth ( <ref type="bibr" target="#b30">Schuler et al., 2010;</ref><ref type="bibr" target="#b26">Ponvert et al., 2011;</ref><ref type="bibr" target="#b32">Shain et al., 2016;</ref><ref type="bibr" target="#b24">Noji and Johnson, 2016;</ref><ref type="bibr" target="#b12">Jin et al., 2018)</ref>, motivated by human cognitive constraints on memory capacity <ref type="bibr" target="#b6">(Chomsky and Miller, 1963)</ref>. Recursion depth can be defined in a left-corner parsing paradigm <ref type="bibr" target="#b27">(Rosenkrantz and Lewis, 1970;</ref><ref type="bibr" target="#b16">Johnson-Laird, 1983)</ref>. Left-corner parsers require only minimal stack memory to pro- cess left-branching and right-branching structures, but require an extra stack element to process each center embedding in a structure. For example, a left-corner parser must add a stack element for each of the first three words in the sentence, For parts the plant built to fail was awful, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. These kinds of depth bounds in sentence processing have been used to explain the relative difficulty of center-embedded sentences compared to more right-branching paraphrases like It was awful for the plant's parts to fail.</p><p>However, depth-bounded grammar induction has never been compared against unbounded in- duction in the same system, in part because most previous depth-bounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. In order to compare the effects of depth-bounding more directly, this work extends a chart-based Bayesian PCFG induction model <ref type="bibr" target="#b15">(Johnson et al., 2007b</ref>) to include depth bounding, which allows both bounded and unbounded PCFGs to be in- duced from unannotated text.</p><p>Experiments reported in this paper confirm that depth-bounding does empirically have the effect of significantly limiting the search space of the in- ducer. Analyses of this model also show that the posterior samples are indicative of implicit depth limits in the data. This work also shows for the first time that it is possible to induce an accurate unbounded PCFG from raw text with no strong linguistic constraints. With a novel grammar- level marginalization in posterior inference, com- parisons of the accuracy of bounded grammar in- duction using this model against other recent con- stituency grammar inducers show that this model is able to achieve state-of-the-art or competitive results on datasets in multiple languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Induction of PCFGs has long been considered a difficult problem <ref type="bibr" target="#b4">(Carroll and Charniak, 1992;</ref><ref type="bibr" target="#b15">Johnson et al., 2007b;</ref><ref type="bibr" target="#b20">Liang et al., 2009;</ref><ref type="bibr" target="#b35">Tu, 2012)</ref>. Lack of success for direct estimation was attributed either to a lack of correlation between the linguistic accuracy and the optimization objec- tive ( <ref type="bibr" target="#b15">Johnson et al., 2007b)</ref>, or the likelihood func- tion or the posterior being filled with weak local optima <ref type="bibr" target="#b34">(Smith, 2006;</ref><ref type="bibr" target="#b20">Liang et al., 2009</ref>). Much of this grammar induction work used strong linguisti- cally motivated constraints or direct linguistic an- notation to help the inducer eliminate some local optima. <ref type="bibr" target="#b28">Schabes and Pereira (1992)</ref> use brack- eted corpora to provide extra structural informa- tion to the inducer. Use of part-of-speech (POS) sequences in place of word strings is popular in the dependency grammar induction literature ( <ref type="bibr">Manning, 2002, 2004;</ref><ref type="bibr" target="#b0">Berg-Kirkpatrick et al., 2010;</ref><ref type="bibr" target="#b11">Jiang et al., 2016;</ref><ref type="bibr" target="#b24">Noji and Johnson, 2016)</ref>. Combinatory Categorial Grammar (CCG) induc- tion also relies on POS tags to assign basic cat- egories to words <ref type="bibr">Hockenmaier, 2012, 2013)</ref>, among other constraints such as CCG com- binators. Other linguistic constraints such as con- straints of root nodes <ref type="bibr" target="#b24">(Noji and Johnson, 2016)</ref>, attachment rules ( <ref type="bibr" target="#b23">Naseem et al., 2010</ref>) or acoustic cues <ref type="bibr" target="#b25">(Pate, 2013)</ref> have also been used in induction.</p><p>Depth-like constraints have been applied in work by <ref type="bibr" target="#b31">Seginer (2007)</ref> and <ref type="bibr" target="#b26">Ponvert et al. (2011)</ref> to help with the search. Both of these systems are successful in inducing phrase structure trees from only words, but only generate unlabeled con- stituents.</p><p>Depth-bounds are directly used by induction models in work by <ref type="bibr" target="#b24">Noji and Johnson (2016)</ref>, <ref type="bibr" target="#b32">Shain et al. (2016)</ref> and <ref type="bibr" target="#b12">Jin et al. (2018)</ref>, and are shown to be beneficial to induction. Noji and John- son (2016) apply depth-bounding to dependency grammar induction with POS tags. However the constituency parsing evaluation scores they report are low compared to other induction systems. The model in <ref type="bibr" target="#b32">Shain et al. (2016)</ref> is a hierarchical se- quence model instead of a PCFG. Although depth- bounding limits the search space, the sequence model has more parameters than a PCFG, there- fore benefits brought by depth-bounding may be offset by this larger parameter space. <ref type="bibr" target="#b12">Jin et al. (2018)</ref> also apply depth-bounding to a grammar inducer and induce depth-bounded PCFGs and show that the depth-bounded gram- mar inducer can learn labeled PCFGs competitive with state-of-the-art grammar inducers that only produce unlabeled trees. However, because of the cognitively motivated left-corner HMM sam- pler used in the model, its state space grows expo- nentially with the maximum depth and polynomi- ally with the number of categories. This renders the transition matrix and the trellis of the inducer too big to be practical in exploring models with higher depth limits, let alone unbounded models. By using Gibbs sampling for PCFGs <ref type="bibr" target="#b9">(Goodman, 1998;</ref><ref type="bibr" target="#b15">Johnson et al., 2007b</ref>), here described as the inside-sampling algorithm, the state space of the model proposed in this work grows only polyno- mially with both the maximum depth and the num- ber of categories. This allows experiments with more complex models and also achieves a faster processing speed due to an overall smaller state space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed model</head><p>The model described in this paper follows <ref type="bibr" target="#b12">Jin et al. (2018)</ref> to induce a depth-bounded PCFG by first inducing an unbounded PCFG and then de- terministically deriving the parameters of a depth- bounded PCFG from it. The main difference be- tween this model and the model in <ref type="bibr" target="#b12">Jin et al. (2018)</ref> is that they use the bounded PCFG to derive pa- rameters for a factored HMM sequence model, where a forward-filtering backward-sampling al- gorithm <ref type="bibr" target="#b5">(Carter and Kohn, 1996)</ref> can be used in inference. In contrast, the model described in this paper transforms the unbounded PCFG into a bounded PCFG, and then uses the inside-sampling algorithm <ref type="bibr" target="#b9">(Goodman, 1998)</ref> to sample from the posterior of the parse trees given the bounded PCFG in inference. This section first gives an overview of the model, then briefly reviews the depth-bounding algorithm for PCFGs (van <ref type="bibr" target="#b29">Schijndel et al., 2013;</ref><ref type="bibr" target="#b12">Jin et al., 2018)</ref>, and finally de- scribes the inference.</p><p>As defined in <ref type="bibr" target="#b12">Jin et al. (2018)</ref>, a Chomsky nor- mal form (CNF) unbounded PCFG is a matrix G of binary rule probabilities with one row for each of C parent symbols c and one column for each of C 2 +W combinations of left and right child sym- bols a and b, which can be pairs of nonterminals or observed words from vocabulary W followed by null symbols ⊥:</p><formula xml:id="formula_0">G = a,b,c P(c → a b | c) δ c (δ a ⊗ δ b ) (1)</formula><p>where δ c is a Kronecker delta (a vector with value one at index c and zeros elsewhere) and ⊗ is a Kro- necker product (multiplying two matrices 2 of di- mension m × n and o × p into a matrix of dimen- sion mo × np composed of products of all pairs of elements in the operands). A deterministic depth- bounding transform φ is then applied to G to create a depth-bounded version G D . A depth-bounded grammar is composed of a set of side-and depth- specific distributions G s,d :</p><formula xml:id="formula_1">G D = s∈{1,2} d∈{1..D} D s,d G s,d E s,d<label>(2)</label></formula><p>2 or vectors in case n and p equal one where side s ∈ {1, 2} indicates left (1) or right <ref type="formula" target="#formula_1">(2)</ref> child. Categories in G D are made to be side-and depth-specific using transforms D s,d and E s,d : 3</p><formula xml:id="formula_2">D s,d = δ s ⊗ δ d ⊗ I (3a) E 1,d = δ 1 ⊗ δ d ⊗ I ⊗ δ 2 ⊗ δ d ⊗ I (3b) E 2,d = δ 1 ⊗ δ d+1 ⊗ I ⊗ δ 2 ⊗ δ d ⊗ I (3c)</formula><p>The generative story of this model is as follows. The model first generates an unbounded grammar G from the Dirichlet prior. Distributions over ex- pansions P(c → a b | c) of each category c in this model are drawn from a Dirichlet with symmetric parameter β:</p><formula xml:id="formula_3">G ∼ Dirichlet(β)<label>(4)</label></formula><p>Trees for sentences 1..N are each drawn from a PCFG given parameters G D = φ(G):</p><formula xml:id="formula_4">τ 1..N ∼ PCFG(G D ) (5)</formula><p>Each tree τ is a set {τ , τ 1 , τ 2 , τ 11 , τ 12 , τ 21 , ...} of category labels τ η where η ∈ {1, 2} * is a Gorn ad- dress specifying a path of left or right branches from the root. Categories of every pair of left and right children τ η1 , τ η2 are drawn from a multino- mial defined by the grammar G D and the category of the parent τ η :</p><formula xml:id="formula_5">τ η1 , τ η2 ∼ Multinomial(δ τ η G D ) (6)</formula><p>where</p><formula xml:id="formula_6">P G D (a b | w) = P G D (a b | ⊥) = a, b=⊥, ⊥ for w ∈ W,</formula><p>and · is an indicator function. In inference, a Gibbs sampler can be used to it- eratively draw samples from the conditional pos- teriors of the unbounded grammar and the parse trees. For example, at iteration t:</p><formula xml:id="formula_7">G t ∼ P(G t | τ t−1 1..N , σ τ t−1 1..N , β) (7) τ t 1...N ∼ P(τ t 1..N | G t D , σ τ t 1..N ) (8)</formula><p>where σ τ denotes the terminals in τ. These distri- butions will be defined in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Depth-bounding a PCFG</head><p>This section summarizes the depth-bounding func- tion φ for PCFGs described in van <ref type="bibr" target="#b29">Schijndel et al. (2013)</ref> and <ref type="bibr" target="#b12">Jin et al. (2018)</ref>. Depth-bounding es- sentially creates a set of PCFGs with depth-and side-specific categories where no tree that exceeds its depth bound can be generated by the bounded grammar. Because depth increases when a left child of a right child of some parent category per- forms non-terminal expansion, the probability of such expansions at the maximum depth limit as well as non-depth-increasing expansions beyond the maximum depth limit must be removed from the unbounded grammar. Following van <ref type="bibr" target="#b29">Schijndel et al. (2013)</ref> and <ref type="bibr" target="#b12">Jin et al. (2018)</ref>, this can be done by iteratively defining a side-and depth-specific containment likelihood h (i) s,d for left-or right-side siblings s ∈ {1, 2} at depth d ∈ {1..D} at each iter- ation i ∈ {1..I}, as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each sym- bol generating a complete yield within depth d as an s-side sibling:</p><formula xml:id="formula_8">h (0) s,d = 0 (9a) h (i) 1,d =        G (1 ⊗ δ ⊥ + h (i−1) 1,d ⊗ h (i−1) 2,d ) if d ≤ D + 1 0 if d &gt; D + 1 (9b) h (i) 2,d =              δ T if d = 0 G (1 ⊗ δ ⊥ + h (i−1) 1,d+1 ⊗ h (i−1) 2,d ) if 0 &lt; d ≤ D 0 if d &gt; D (9c)</formula><p>where 'T' is a top-level category label at depth zero. Following previous work, experiments de- scribed in this paper use I = 20. A depth-bounded grammar G s,d can then be de- fined to be the original grammar G reweighted and renormalized by this containment likelihood:</p><formula xml:id="formula_9">G 1,d = G diag(1 ⊗ δ ⊥ + h (I) 1,d ⊗ h (I) 2,d ) h (I) 1,d (10a) G 2,d = G diag(1 ⊗ δ ⊥ + h (I) 1,d+1 ⊗ h (I) 2,d ) h (I) 2,d (10b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gibbs sampling of unbounded grammars and bounded trees</head><p>As defined above, this model samples itera- tively from the conditional posteriors of</p><formula xml:id="formula_10">P(G | τ 0..N , σ τ 0..N , β) and P(τ 0..N | G D , σ τ 0..N ) in infer-</formula><p>ence, extending the Gibbs sampling algorithm for PCFG induction introduced in <ref type="bibr" target="#b15">Johnson et al. (2007b)</ref> to depth-bounded grammars. The below equations will omit the superscript t for the itera- tion number of inference for clarity.</p><p>To sample from the conditional posterior of G, it is necessary to first sum over all rule applications in all sampled trees:</p><formula xml:id="formula_11">C D = τ∈τ 1..N τ η ∈τ δ τ η (δ τ η1 ⊗ δ τ η2 )<label>(11)</label></formula><p>then remove side-and depth-specificity from cate- gory labels:</p><formula xml:id="formula_12">C = s d D s,d C D E s,d<label>(12)</label></formula><p>A side-and depth-independent grammar is then sampled from these counts, plus the pseudo- count β:</p><formula xml:id="formula_13">G ∼ Dirichlet(β + C)<label>(13)</label></formula><p>Inside-sampling <ref type="bibr" target="#b9">(Goodman, 1998;</ref><ref type="bibr" target="#b15">Johnson et al., 2007b</ref>) is then used to sample from the pos- terior of trees</p><formula xml:id="formula_14">P(τ 0..N | G D , σ τ 0...N )</formula><p>. Given a depth- bounded grammar and a sentence, this algorithm first constructs the inside chart V ∈ R L×L×C , where L is the length of the sentence. A chart vector V [i, j,1..C] for the span i, j where i &lt; j ≤ L in some sentence w 1..L is the likelihood P G D (w i.. j | c) of the span for all side-and depth-specific categories c:</p><formula xml:id="formula_15">V [i, j,1..C] =        G D (δ w i ⊗ δ ⊥ ) if j−i = 1 k G D (V [i,k,1..C] ⊗ V [k, j,1..C] ) if j−i &gt; 1<label>(14)</label></formula><p>Trees are sampled iteratively from the top down by first choosing a split point k i, j for the current span i, j such that i &lt; k i, j &lt; j:</p><formula xml:id="formula_16">k i, j ∼ Mul         k δ k δ c i, j G D (V [i,k,1..C] ⊗ V [k, j,1..C] )         (15)</formula><p>The algorithm then samples pairs of category la- bels c i,k i, j and c k i, j , j adjacent at this split point k i, j :</p><formula xml:id="formula_17">c i,k , c k, j ∼ Mul δ c i, j G D diag(V [i,k,1..C] ⊗ V [k, j,1..C] )<label>(16)</label></formula><p>Empirically the sampler spends most of its time constructing the inside chart. The model described in this paper therefore efficiently computes the in- side chart using matrix multiplication, which is able to exploit GPU optimization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient inside score calculation</head><p>The complexity of the inside algorithm is cubic on the length of the sentence because it has to it- erate over all start points i, all end points j and all split points k of a span. For a dense PCFG with a large number of states, the explicit loop- ing is undesirable, especially when it can be for- mulated as matrix multiplication. The split point loop is therefore replaced with a matrix multipli- cation in order to take advantage of highly opti- mized GPU linear algebra packages like cuBLAS and cuSPARSE, whereas previous work explores how to parse efficiently on GPUs <ref type="bibr" target="#b13">(Johnson, 2011;</ref><ref type="bibr" target="#b3">Canny et al., 2013;</ref><ref type="bibr" target="#b10">Hall et al., 2014</ref>).</p><p>Inside likelihoods are propagated using a copy V of the inside likelihood tensor V with the first and second indices reversed:</p><formula xml:id="formula_18">V [ j,i,c] = V [i, j,c]<label>(17)</label></formula><p>This reversal allows the sum over split points k ∈ {i+1, ..., j−1} to be calculated as a product of con- tiguous matrices, which can be efficiently imple- mented on a GPU:</p><formula xml:id="formula_19">V [i, j,1..C] = G D vec(V [i,i+1.. j−1,1..C] V [ j,i+1.. j−1,1..C] )<label>(18)</label></formula><p>where vec(M) flattens a matrix M into a vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Posterior inference on constituents</head><p>Prior work <ref type="bibr" target="#b14">(Johnson et al., 2007a)</ref> shows that us- ing EM-like algorithms, which seek to maximize the likelihood of data marginalizing out the latent trees, does not yield good performance. Because trees are the main target for evaluation, it may be preferable to find the most probable tree struc- tures given the marginal posterior of tree struc- tures compared to finding the most probable gram- mar. Some recent work <ref type="bibr" target="#b22">(McClosky and Charniak, 2015;</ref><ref type="bibr" target="#b17">Keith et al., 2018</ref>) explores how to use marginal distributions of tree structures from supervised parsers to create more accurate parse trees. Based on these arguments, this model per- forms maximum a posteriori (MAP) inference on constituents (PIoC) using approximate conditional posteriors of spans to create final parses for evalu- ation.</p><p>Formally, let σ i, j be an MAP unlabeled span of words in a sentence from a corpus σ, with start point i and end point j, and σ i,k , σ k, j its possi- ble children. This algorithm iteratively looks for the best pair of children σ i,k , σ k, j according to the posterior of the children, using all posterior sam- ples. The spans are sentence-specific, but the be- low equations omit the sentence index for brevity:</p><formula xml:id="formula_20">σ i,k , σ k, j = arg max σ i,k ,σ k, j P(σ i,k , σ k, j | σ i, j , σ)</formula><p>= arg max</p><formula xml:id="formula_21">σ i,k ,σ k, j P(σ i,k , σ k, j , G | σ i, j , σ) dG ≈ arg max σ i,k ,σ k, j ˆ G∼P(G|σ) P(σ i,k , σ k, j , ˆ G | σ i, j , σ)<label>(19)</label></formula><p>where σ is the training corpus. Starting from the whole sentence σ 0,N , this algorithm finds the best children for a span from the Monte Carlo estimation of the marginal posterior distribution of children for the span, and then continues to split the found children spans. Because samples from different runs at different iterations can be used to approximate the span posteriors, the pro- cess marginalizes out sampled grammars, whole- sentence parse trees and constituent labels to only consider split points for spans. In terms of input and output, the PIoC algorithm takes in posterior samples of trees for a sentence, and outputs an un- labeled binary-branching tree.</p><p>There are a few benefits of doing posterior in- ference on constituents. First, the distribution P(σ i,k , σ k, j | σ i, j , σ) quantifies how much uncer- tainty there is in splitting a span σ i, j at all possible k's. One way of using this uncertainty information is to merge spans where uncertainty is high, ef- fectively weakening or removing the constraint of binary-branching from the grammar inducer. Sec- ond, this algorithm produces trees that may not be seen in the samples, potentially helping aggre- gate evidence across different iterations within a run and across runs. Third, the multimodal na-ture of the joint posterior of grammars and trees often makes the sampler get stuck at local modes, but doing MAP on constituents may allow infor- mation about trees from different modes to come together. If different grammars all consider cer- tain children for a span to be highly likely, then these children should be in the final parse output. Finally, it is a nonparametric way of doing model selection. As will be shown, model selection relies on the log likelihood of the data, but the log like- lihood of the data is only weakly correlated with parsing accuracy. Performing PIoC with multiple runs can increase accuracy without depending too heavily on log likelihood for model selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model analysis and evaluation</head><p>The model described above has hyperparameters for maximum depth D, number of categories C and the symmetric Dirichlet prior β. Following <ref type="bibr" target="#b12">Jin et al. (2018)</ref>, this evaluation uses the first half of the WSJ20 corpus as the development set (WSJ20dev) for all experiments. However instead of using the development set only to set the hyper- parameters of the model, this evaluation also uses it to explore interactions among parsing accuracy, model fit, depth limit and category domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of model behavior</head><p>The first experiment explores the effects of depth- bounding on linguistic grammar quality. The hy- pothesis is that depth-bounding limits the search space of possible grammars, so the inducer will be less likely to find low-quality local optima where cognitively implausible parse trees are assigned non-zero probabilities, because such local optima would be removed from the posterior by limiting the maximum depth of parse trees to a small num- ber d.</p><p>The effect of depth-bounding <ref type="figure" target="#fig_2">Figure 3</ref> shows the effect of depth bounding us- ing 60 data points of unlabeled PARSEVAL scores from 20 different runs for each of three different depth bounds: 2, 3, and ∞ (unbounded). The range of possible parsing accuracy scores is very wide, as mapped out by the runs. Although the un- bounded model is able to reach the performance upper bound seen from the figure, most of the time its results are in the middle of the range. By bounding the maximum depth to 2, the sampler is able to stay in the region of high parsing accuracy. This may be because the majority of the modes in the region of low parsing accuracy require higher depth limits, and humans who produce the sen- tences do not have access to those higher depth limits. The difference between depth ∞ and depth 2 is significant (p=0.017, Student's t test), show- ing that depth-bounding does have a positive ef- fect on the linguistic grammar quality of the in- duced grammars. Data from depth 3 also shows a positive trend of inducing better grammars than 575000 570000 565000 560000 555000</p><p>Data loglikelihood </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>unbounded.</head><p>A purely right-branching baseline achieves an F1 score of 48 on the WSJ20 development dataset. A majority of induction runs perform better than this baseline, which indicates that the PCFG in- duction model with the inside-sampling algorithm is able to find good solutions, most of the time much better than the right-branching baseline. This is especially interesting when the grammar is unbounded with almost no other constraint, which had previously been shown to converge to weak local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation of model fit and parsing accuracy</head><p>Model fit, or data likelihood, has been reported not to be correlated or to be correlated only weakly with parsing accuracy for some unsuper- vised grammar induction models <ref type="bibr" target="#b34">(Smith, 2006;</ref><ref type="bibr" target="#b15">Johnson et al., 2007b;</ref><ref type="bibr" target="#b20">Liang et al., 2009</ref>) when the model has converged to a local maximum. <ref type="figure" target="#fig_3">Figure  4</ref> shows the correlation between data likelihood and parsing accuracy at convergence for all the runs. There is a significant (p = 0.007) positive correlation (Pearson's r=0.39) between data likeli- hood and parsing accuracy at convergence for our model. This indicates that although noisy and un- reliable, the data likelihood can be used as a metric to do preliminary model selection.</p><p>The bounded unbounded PCFG We also examine the distribution of tree depths in unbounded runs. For a run, we compute the per- centage of parse trees with a certain depth, and then examine how these percentages vary across different runs. Theoretically the possible maxi- mum depth of a parse for a sentence is the sentence length divided by 2. For example, a 20-word sen- tence can have a parse of depth 10 because at least two words are needed to create a new depth with a center embedded phrase, but under most PCFGs this maximally center embedded configuration is not very likely. <ref type="figure" target="#fig_4">Figure 5</ref> shows the percentage of tree depths from samples in the beginning of each unbounded run and at convergence. It shows that at the beginning of the sampling process with a random model sampled from the prior, the dis- tribution of parse tree depths seems to be cen- tered around depth 2 and 3, with non-negligible probability mass at other depth levels. At con- vergence, the distribution of parse tree depths is very peaked with a large portion of the proba- bility mass concentrated at depth 2. Given that an unbounded PCFG has no constraint on depth, this convergence of the marginal posterior distri- bution of parse tree depth shows that the depth limit seems to be a natural tendency in the data, rather than an arbitrary preference of corpus anno- tators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Posterior uncertainty of constituents</head><p>Experiments were also conducted to determine whether posterior inference on constituents (PIoC) has any effect on parsing accuracy. These experi- ments use 10 runs on WSJ20dev with depth 2 that have the highest log-likelihoods for exploration. In this data, some spans have a strikingly higher degree of uncertainty than other spans. For exam- ple, the posterior probability of splitting the phrase the old story, into the old and story is 0.55, and the probability of splitting it into the and old story is 0.45. Some other spans like use old tools have vir- tually no uncertainty in how the inducer evaluates the splits. Many such spans with high uncertainty are noun phrases, which are not annotated with subconstituents in the Penn Treebank annotation. The parser can therefore avoid precision losses by not splitting constituents with 3 or 4 words if there is large uncertainty in this posterior. <ref type="bibr">4</ref> This exper- iment only merges spans that would cover 3 or 4 words and leave merging spans with larger cover- age to future work. <ref type="table">Table 1</ref> shows parsing results on the WSJ20dev dataset. The Best result is from an arbitrary sam- ple at convergence of the oracle best run. The Best with PIoC is the same run, but with PIoC to ag- gregate 100 posterior samples at convergence. All with PIoC uses 100 posterior samples from all of the 10 chosen runs, and finally All with PIoC with- out best excludes the best run in PIoC calculation.</p><p>There is almost a point of gain in precision go- ing from Best to Best with PIoC with virtually no recall loss, showing that the posterior uncertainty is helpful in flattening binary trees. As more sam- ples from the posterior are collected, as shown in All with PIoC without best, the precision gain is even more substantial. This shows that with PIoC there is no need to know which sample from which run is the best. Model selection in this case is only needed to weed out the runs with very low likeli- hood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multilingual PARSEVAL</head><p>A final set of experiments compare the proposed model with several state-of-the-art constituency grammar induction systems on three different lan- guages. The competing systems are CCL <ref type="bibr" target="#b31">(Seginer, 2007)</ref>  <ref type="bibr">5</ref> and UPPARSE <ref type="bibr" target="#b26">(Ponvert et al., 2011</ref>). <ref type="bibr">6</ref> We also include the published results of DB-PCFG ( <ref type="bibr" target="#b12">Jin et al., 2018)</ref> on English for comparison. <ref type="bibr">7</ref> The corpora used are the WSJ20test dataset used in <ref type="bibr" target="#b12">Jin et al. (2018)</ref>, the CTB20 (sentences with 20 words or fewer from the Chinese Treebank) and NE- GRA20 (sentences with 20 words or fewer from the German NEGRA Treebank) datasets used in <ref type="bibr" target="#b31">Seginer (2007)</ref>. All systems are trained and evalu- ated on the same datasets to ensure fair and direct comparison. Five different induction runs were run on each dataset with the same hyperparame- ters D=2, C=15, β=0.2 as tuned on the develop- ment set, and three runs with the highest likeli- hood at convergence were chosen for comparison with other models. Parse trees were then calcu- lated using PIoC as previously described, remov- ing punctuation to calculate the unlabeled PARSE- VAL scores with EVALB. Multiple runs of CCL and UPPARSE on the same data yield the same results. <ref type="table" target="#tab_1">Table 2</ref> shows the unlabeled PARSEVAL scores for the competing systems. The model described in this paper shows strong performance in all lan- guages. On English and Chinese, this model achieves the new state-of-the-art recall and F1 numbers. On German, this model also achieves the best recall scores among all models, showing that more constituents found in the gold annotation are discovered. It is worth noting that the CCL and UPPARSE models do take advantage of additional linguistic constraints, e.g. using punctuation as de- limiters of constituents. Experiments described in this paper show that this system can perform bet- ter than or competitive with these existing models without similar heuristics and constraints.</p><p>The model described in this paper performs rel- atively poorly on precision due to the fact that trees produced by this system are mostly binary- branching with some constituents flattened by PIoC. This issue is most evident on Negra, where fully binary-branching trees have nearly twice as many constituents as are annotated in gold. This puts any system that produces binary-branching trees under a precision celling of 0.51, and F1 celling of 0.675.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Experiments in this work confirm that depth- bounding does empirically have the effect of limit- ing the search space of an unsupervised PCFG in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>WSJ20test CTB20 NEGRA20 Rec Prec F1</p><p>Rec Prec F1 Rec Prec F1 CCL 61.7 60.1 60.9 35.3 39.2 37.1 44.4 27.2 33.7 UPPARSE 40.5 47.8 43.9 33.8 44.0 38.2 55.5 41.9 47.7 DB-PCFG 70.5 53.0 60.5 - - - - - - this work 73.1 55.6 63.1 43.8 35.1 38.9 59.1 31.2 40.8 ducer. Analysis of a depth-bounded model demon- strates desirable engineering properties, includ- ing a significant correlation between parsing accu- racy and data likelihood, and interesting linguis- tic properties such as implicit bounding for un- bounded grammars. This paper also introduces the Posterior Inference on Constituents technique for model selection and shows for the first time that it is possible to accurately induce a PCFG with no strong universal linguistic constraints. Compar- isons of the proposed model with other state-of- the-art constituency grammar inducers show that this model is able to achieve state-of-the-art or competitive results on datasets in multiple lan- guages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Stack elements after the word the in a leftcorner parse of the sentence For parts the plant built to fail was awful.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of matrix multiplication in place of looping over break points for the span (0,5). Each chart cell represents a likelihood vector for the span between i and j where i is the leftmost delimiting index of the span and j the rightmost. The arrows represent the order in which the cells are stored in the chart matrices V and V .</figDesc><graphic url="image-1.png" coords="5,90.43,62.81,181.42,84.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PARSEVAL scores for runs with different depth limits. The difference of all PARSEVAL scores between depth ∞ and depth 2 is significant (p=0.017, Student's t test).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The correlation between data likelihood and parsing accuracy of all 60 runs. Calculations show that there is a significant (p = 0.007) positive correlation (Pearson's r=0.39) between data likelihood and parsing accuracy at convergence for our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The usage of different depths for parse trees in the samples from 20 runs with the unbounded grammar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>PARSEVAL scores for different constituency grammar induction systems.</figDesc><table></table></figure>

			<note place="foot" n="1"> The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18.</note>

			<note place="foot" n="3"> Note that this correctly stipulates depth increases for left children of right children.</note>

			<note place="foot" n="4"> I.e. if the difference between the first and the second highest posterior probabilities is smaller than 0.3. 5 https://github.com/DrDub/cclparser 6 https://github.com/eponvert/upparse</note>

			<note place="foot" n="7"> We are not able to run DB-PCFG on the other languages due to its substantial resource requirements.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their helpful comments. Compu-tations for this project were partly run on the Ohio Supercomputer <ref type="bibr">Center (1987)</ref>. This research was funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022. The content of the information does not necessarily re-flect the position or the policy of the Government, and no official endorsement should be inferred.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Simple Robust Grammar Induction with Combinatory Categorial Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="1643" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An HDP Model for Inducing Combinatory Categorial Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions Of The Association For Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="75" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-Teraflop Constituency Parser using GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1898" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Two experiments on learning probabilistic dependency grammars from corpora. Working Notes of the Workshop on Statistically-Based NLP Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-03" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Markov chain Monte Carlo in conditionally Gaussian state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="589" to="601" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introduction to the formal analysis of natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Mathematical Psychology</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1963" />
			<biblScope unit="page" from="269" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Limitations of current grammar induction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL: Student Research Workshop on-ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the ACL: Student Research Workshop on-ACL &apos;07</meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language Identification in the Limit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="447" to="474" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Parsing Inside-Out</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparser, Better, Faster GPU Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canny</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">208217</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="763" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised Grammar Induction with Depth-bounded PCFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing in parallel on multiple cores and gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Australasian Language Technology Association Workshop</title>
		<meeting>Australasian Language Technology Association Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">641</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian Inference for PCFGs via Markov chain Monte Carlo. Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference (ACL)</title>
		<meeting>the Main Conference (ACL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mental models: Towards a cognitive science of language, inference, and consciousness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip N Johnson-Laird</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Harvard University Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monte Carlo Syntax Marginals for Exploring and Using Dependency Parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katherine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Lin</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A generative constituent-context model for improved grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probabilistic Grammars and Hierarchical Dirichlet Processes. The Handbook of Applied Bayesian Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Syntactic Parse Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1360" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using Universal Linguistic Knowledge to Guide Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using Leftcorner Parsing to Encode Universal Structural Constraints in Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="\url{http://osc.edu/ark:/19495/f5s1ph73}" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>The Ohio Supercomputer Center</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised Dependency Parsing with Acoustic Cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John K Pate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="74" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple unsupervised grammar induction from raw text with cascaded finite state models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Ponvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deterministic left corner parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D J Rosenkrantz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P M</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Annual Symposium on Switching and Automata Theory (swat 1970)</title>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="page" from="139" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">InsideOutside Reestimation From Partially Bracketed Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Schabes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 30th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Model of language processing as hierarchic sequential prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Exley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="522" to="540" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Broad-coverage parsing using human-Like memory constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast Unsupervised Incremental Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Seginer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="384" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Memory-bounded left-corner unsupervised grammar induction on child-directed input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Shain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Bryce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Krakovna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="964" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Linguistically Interpreted Corpus of German Newspaper Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigitte</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESSLLI Workshop on Recent Advances in Corpus Annotation</title>
		<meeting>the ESSLLI Workshop on Recent Advances in Corpus Annotation</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah Ashton</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="228" />
		</imprint>
	</monogr>
<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Ocurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kovarik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Kroch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
		<title level="m">Developing Guidelines and Ensuring Consistency for</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Chinese Text Annotation. Proceedings of the Second Language Resources and Evaluation Conference</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
