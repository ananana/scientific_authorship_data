<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Search-Aware Tuning for Hierarchical Phrase-based Decoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Queens College &amp; Graduate Center City</orgName>
								<orgName type="institution" key="instit1">Queens College City University of New York</orgName>
								<orgName type="institution" key="instit2">University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Queens College &amp; Graduate Center City</orgName>
								<orgName type="institution" key="instit1">Queens College City University of New York</orgName>
								<orgName type="institution" key="instit2">University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Queens College &amp; Graduate Center City</orgName>
								<orgName type="institution" key="instit1">Queens College City University of New York</orgName>
								<orgName type="institution" key="instit2">University of New York</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Search-Aware Tuning for Hierarchical Phrase-based Decoding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Parameter tuning is a key problem for statistical machine translation (SMT). Most popular parameter tuning algorithms for SMT are agnostic of decoding, resulting in parameters vulnerable to search errors in decoding. The recent research of &quot;search-aware tuning&quot; (Liu and Huang, 2014) addresses this problem by considering the partial derivations in every decoding step so that the promising ones are more likely to survive the inexact decoding beam. We extend this approach from phrase-based translation to syntax-based translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show significant BLEU improvements on MERT, MIRA and PRO.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Efforts in parameter tuning algorithms for SMT, such as MERT <ref type="bibr" target="#b14">(Och, 2003;</ref><ref type="bibr" target="#b6">Galley et al., 2013</ref>), MIRA ( <ref type="bibr" target="#b16">Watanabe et al., 2007;</ref><ref type="bibr" target="#b1">Chiang et al., 2009;</ref><ref type="bibr" target="#b0">Cherry and Foster, 2012;</ref><ref type="bibr" target="#b3">Chiang, 2012)</ref>, and PRO ( <ref type="bibr" target="#b8">Hopkins and May, 2011</ref>) have improved the translation quality considerably in the past decade. These tuning algorithms share the same characteristic that they treat the decoder as a black box. This decoding insensitiveness has two ef- fects: 1) the parameter tuning algorithm can be more general to choose the most effective decod- ing paradigm for different language pairs; 2) how- ever, it also means that the learned parameters may not fit the decoding algorithm well, so that promis- ing partial translations might be pruned out due to the beam search decoding.</p><p>Recent researches reveal that the parameter tun- ing algorithms tailored for specific decoding al- gorithms can be beneficial in general structured prediction problems ( <ref type="bibr" target="#b9">Huang et al., 2012)</ref>, and in machine translation ( <ref type="bibr" target="#b17">Yu et al., 2013;</ref><ref type="bibr" target="#b18">Zhao et al., 2014</ref>; <ref type="bibr" target="#b12">Liu and Huang, 2014</ref>). Particularly, <ref type="bibr" target="#b12">Liu and Huang (2014)</ref> show that by requiring the conven- tional parameter tuning algorithms to consider the final decoding results (full translations) as well as the intermediate decoding states (partial transla- tions) at the same time, the inexact decoding can be significantly improved so that correct interme- diate partial translations are more likely to survive the beam. However, the underlying phrase-based decoding model suffers from limited distortion, and thus, may not be flexible enough for translat- ing language pairs that are syntactically different, which require long distance reordering.</p><p>In order to better handle long distance reorder- ing which beyonds the capability of phrase-based MT, we extend the search-aware tuning frame- work from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based transla- tion model (HIERO) <ref type="bibr" target="#b2">(Chiang, 2007)</ref>.</p><p>One key advantage of search-aware tuning for previous phrase-based MT is the minimal change to existing parameter tuning algorithms, which is achieved by defining BLEU-like metrics for the intermediate decoding states with sequence- structured derivations. To keep our approach simple, we generalize these BLEU-like metrics to handle intermediate decoding states with tree- structured derivations in HIERO, which are cal- culated by dynamic programming algorithms in- spired by the inside-outside algorithm.</p><p>We make the following contributions:</p><p>1. We extend the framework of search-aware tuning methods from phrase-based transla- tion to syntax-based translation. This exten- sion is simple to be applied to most conven- tional parameter tuning methods, requiring minimal extra changes to existing algorithms.</p><p>2. We propose two BLEU metrics and their vari- ants to evaluate partial derivations. In order to efficiently compute the new BLEU metrics, we investigate dynamic programming based algorithms to recover the good partial deriva- tions for search-aware tuning.</p><p>3. Our method obtains significant improve- ments on large-scale Chinese-to-English translation on top of MERT, MIRA and PRO baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hierarchical Phrase-based Decoding</head><p>We first briefly describe the hierarchical phrase- based translation <ref type="bibr" target="#b2">(Chiang, 2007)</ref>, HIERO, by a Chinese-English translation example <ref type="figure" target="#fig_0">(Figure 1)</ref>. A HIERO decoder translates a source sentence by using a synchronous grammar <ref type="figure" target="#fig_0">(Figure 1 (a)</ref>) to parse it into a bilingual derivation tree, as shown in </p><formula xml:id="formula_0">r 8 : X → X 1 qian X 2 , X 2 before X 1 ,</formula><p>where the left-hand-side is a non-terminal symbol, and the right-hand-side is a pair of source and tar- get strings, each of which consists a sequence of lexical terminals and non-terminals. Specifically, the same subscript on both sides denotes a one-to- one correspondence between their non-terminals. We use |r| to denote the arity of rule r, i.e., the number of non-terminals. Usually, the rule used in HIERO system has a maximum arity 2. Let x, y be a Chinese-English sentence pair in tuning set, the HIERO decoder will generate a derivation tree for it <ref type="figure" target="#fig_0">(Figure 1 (b)</ref>), which can be defined recursively in a functional way:</p><formula xml:id="formula_1">d =    r |r| = 0 r(d 1 ) |r| = 1 r(d 1 , d 2 ) |r| = 2 ,<label>(1)</label></formula><p>where d is a (partial) derivation, i.e., the (sub-) derivation tree. When r is a fully lexicalized rule (|r| = 0), the decoder generates a tree node di- rectly (e.g., X <ref type="bibr">[3,</ref><ref type="bibr">5]</ref> in <ref type="figure" target="#fig_0">Figure 1 (b)</ref>). If |r| &gt; 0, a new (partial) derivation will be created by apply- ing r to its children nodes. In our notation, we denote this process as applying a function of rule r to its arguments. For example, node X <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> in <ref type="figure" target="#fig_0">Figure 1</ref>(b) is created by applying rule r 8 in <ref type="figure" target="#fig_0">Fig- ure 1(a)</ref>, where the arguments are d 1 = X <ref type="bibr">[1,</ref><ref type="bibr">2]</ref> and d 2 = X <ref type="bibr">[3,</ref><ref type="bibr">5]</ref> respectively.</p><p>Practically, we organize the partial derivations based on spans in HIERO decoder, and use a beam B <ref type="bibr">[i,j]</ref> to keep the k-best partial derivations for each span <ref type="bibr">[i, j]</ref>:</p><formula xml:id="formula_2">B [i,j] = top k w (D [i,j] ),</formula><p>where D <ref type="bibr">[i,j]</ref> is the set of all possible partial deriva- tions for span <ref type="bibr">[i, j]</ref>, and top k w (·) returns the top k ones according to the current model w. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of the HIERO beam-search de- coding.</p><p>3 Search-Aware Tuning for HIERO As we discussed in Section 1, current tuning meth- ods for HIERO system (MERT, MIRA, or PRO) are mostly search-agnostic. They only consider the complete translations in final beam B <ref type="bibr">[1,|x|]</ref> , but ig- nore the partial ones in the intermediate beams.</p><p>However, because MT decoding is inexact (beam- search), many potentially promising derivations might be pruned before reaching the final beam (e.g., the partial derivation X <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> of <ref type="figure" target="#fig_0">Figure 1</ref>(b) is pruned in beam B <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> in <ref type="figure" target="#fig_1">Figure 2</ref>). Consequently, once we lose these good partial derivations in de- coding, it is hard to promote them by these search- agnostic tuning methods.</p><p>In order to address this problem, search-aware tuning ( <ref type="bibr" target="#b12">Liu and Huang, 2014</ref>) aims to promote not only the accurate complete translations in the final beam, but more importantly those promising par- tial derivations in non-final beams.</p><p>In this section, we apply search-aware tuning to HIERO system. Similar to the phrase-based one, the key challenge here is also the evaluation of par- tial derivations. Following ( <ref type="bibr" target="#b12">Liu and Huang, 2014)</ref>, we design two different evaluation metrics, partial BLEU and potential BLEU respectively for HIERO decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partial BLEU</head><p>Given a partial derivation d of span <ref type="bibr">[i, j]</ref>, partial BLEU evaluates it by comparing its partial trans- lation e(d) directly with the (full) reference. We explore two different partial BLEU measures here: full partial BLEU and span partial BLEU.</p><p>Full partial BLEU is similar to the one used in ( <ref type="bibr" target="#b12">Liu and Huang, 2014)</ref>, which compares e(d) with the full reference y and computes BLEU score us- ing an adjusted effective reference length propor- tional to the length of the source span <ref type="bibr">[i, j]</ref>   <ref type="bibr">[0,</ref><ref type="bibr">5]</ref> X <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> X <ref type="bibr">[3,</ref><ref type="bibr">5]</ref> bolin 5 zhuzai 4 qian 3 X <ref type="bibr">[1,</ref><ref type="bibr">2]</ref> jiehun 2</p><formula xml:id="formula_3">(i.e., id rule r 0 S → X 1 , X 1 r 1 S → S 1 X 1 , S 1 X 1 r 2 X → Aimi, Amy</formula><formula xml:id="formula_4">r 8 X → X 1 qian X 2 ,X 2 before X 1 r 9 X → X 1 qian X 2 ,X 1 before X 2 S</formula><formula xml:id="formula_5">S [0,1] X [0,1] 0 Aimi 1 S X X married getting before X Berlin in lived S X Amy S [0,5]</formula><p>X <ref type="bibr">[3,</ref><ref type="bibr">5]</ref> bolin 5 zhuzai 4 S <ref type="bibr">[0,</ref><ref type="bibr">3]</ref> X <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> qian 3 S <ref type="bibr">[0,</ref><ref type="bibr">2]</ref> X <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>   ) for more details. Span partial BLEU differs from full partial BLEU by creating a span reference for span <ref type="bibr">[i, j]</ref> to evaluate its partial derivations, rather than us- ing the full reference. We use word alignment to determine the span references of different spans.</p><p>Specifically, we first add the tuning set into train- ing data, and then run GIZA++ to get the word alignment. For a pair of source and target span which are consistent with word alignment 1 , the string of target span is used as the span reference for the source span. <ref type="bibr">2</ref> For example, in <ref type="figure" target="#fig_0">Figure 1</ref>(b), the span reference of source span <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> is "lived in Berlin before getting married".</p><p>Partial BLEU is quite an intuitive choice for evaluating partial derivations. However, as <ref type="bibr" target="#b12">Liu and Huang (2014)</ref> discussed, partial BLEU only evaluates the partial derivation itself without con- sidering any of its context information, leading to a performance degradation. Thus, in order to take the context information into account, we investi- gate a more reasonable metric, potential BLEU, in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Potential BLEU</head><p>Potential BLEU evaluates the partial derivation d by assessing its potential, which is a complete translation generated from d, rather than e(d) as in partial BLEU. In phrase-based decoding, Liu and Huang (2014) introduce a future string f (d) to denote the translation of the untranslated part, and get the potential of d by concatenating e(d) and f (d).</p><p>Different from their work, we define an outside string for d in HIERO system. Suppose a complete translation generated from d is ¯ e(d), it can be de- composed as follows:</p><formula xml:id="formula_6">¯ e(d) = η(d) • e(d) • ξ(d) (2)</formula><p>where • is an operator that concatenates strings monotonically, η(d) and ξ(d) are the left and right parts of ¯ e(d) apart from e(d), which we call left and right future strings of d. The pair of η(d) and ξ(d) is defined as the outside string of d:</p><formula xml:id="formula_7">O(d) = η(d), ξ(d)</formula><p>An example of the outside string is shown in <ref type="figure">Fig- ure 3</ref>. Assume we are evaluating the partial deriva- tion which translates Chinese word "qian" to En- glish word "before", and get a complete transla- tion "Amy lived in Berlin before getting married" from it, then its outside string would be: Figure 3: Example of outside string when we use"Amy lived in Berlin before getting married" as the potential for the partial derivation which trans- lates word "qian" to "before".</p><formula xml:id="formula_8">Amy lived in Berlin, getting married ⌘(d) ⇠(d) 8 &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; :</formula><p>Theoretically, different partial derivations of the same span could have different outside strings. To simplify the problem, we use the same outside string for all partial derivations of the same span. The outside string for span <ref type="bibr">[i, j]</ref> is defined as:</p><formula xml:id="formula_9">O([i, j]) = η([i, j]), ξ([i, j])</formula><p>For a specific partial derivation d of span <ref type="bibr">[i, j]</ref>, by combining O( <ref type="bibr">[i, j]</ref>) and e(d), we can compute its potential BLEU against the reference. Appar- ently, different outside string will lead to different potential BLEU score. In the rest of this section, we will explore three different methods to gener- ate outside strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Concatenation</head><p>In order to generate outside string, one simple and straightforward way is concatenation. For a spe- cific span <ref type="bibr">[i, j]</ref>, we first get the best translation of its adjacent spans <ref type="bibr">[0, i]</ref> and <ref type="bibr">[j, |x|]</ref> (e( <ref type="bibr">[0, i]</ref>) and e( <ref type="bibr">[j, |x|]</ref>) respectively). <ref type="bibr">3</ref> Then for a partial deriva- tion d of span <ref type="bibr">[i, j]</ref>, we generate the outside string of d by concatenation</p><formula xml:id="formula_10">η([i, j]) = e([0, i]) ξ([i, j]) = e([j, |x|]) ¯ e x (d) = e([0, i]) • e(d) • e([j, |x|])</formula><p>Also take the example of <ref type="figure">Figure 3</ref>, if we per- form concatenation on it, the outside string of the corresponding partial derivation is:</p><p>Amy getting married, lived in Berlin.</p><p>Obviously, this outside string is not good, since it does not consider the reordering between spans. In order to incorporate reordering into outside string, we propose a new top-down algorithm in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Top-Down</head><p>Top-down method (Algorithm 1) is defined over the derivation tree, and takes the complete transla- tions in the final beam as the potential for comput- ing potential BLEU.</p><p>Suppose we have a partial derivation d = r(d 1 , d 2 ) as shown in Formula 1, and the target side string of rule r is:</p><formula xml:id="formula_11">w 1 · · · w p X 1 w q · · · w m X 2 w n · · · w l</formula><p>The corresponding (partial) translation e(d) of d could be generated by applying an r-based func- tion e r (·) with d 1 and d 2 as the arguments:</p><formula xml:id="formula_12">e r (d 1 , d 2 ) = w 1 ···w p e(d 1 ) w q ···w m e(d 2 ) w n ···w l</formula><note type="other">where e(d 1 ) and e(d 2 ) are the partial translations of d 1 and d 2 . We can further decompose e r (d 1 , d 2 ) based on e(d 1 ):</note><formula xml:id="formula_13">w 1 · · · w p η d (d 1 ) e(d 1 ) w q · · · w m e(d 2 ) w n · · · w l ξ d (d 1 ) .</formula><p>where we call η d (d 1</p><note type="other">) and ξ d (d 1 ) the partial left and right future strings of d 1 under d. Similar Algorithm 1 Top-Down Outside String. 1: function TOPDOWN(d) 2: r ← the rule that generates d 3: for each non-terminal x in rule r do 4:</note><formula xml:id="formula_14">η(dx) = η(d) • η d (dx) 5: ξ(dx) = ξ d (dx) • ξ(d) 6:</formula><p>TOPDOWN(dx) 7: return to the outside cost of inside-outside algorithm, we compute the outside string η(d 1 ), ξ(d 1 ) for d 1 based on the decomposition:</p><formula xml:id="formula_15">η(d 1 ) = η(d) • η d (d 1 ) ξ(d 1 ) = ξ d (d 1 ) • ξ(d).</formula><p>Similarly, if we decompose e r (d 1 , d 2 ) based on e(d 2 ) we have:</p><formula xml:id="formula_16">w 1 · · · w p e(d 1 ) w q · · · w m η d (d 2 ) e(d 2 ) w n · · · w l ξ d (d 2 )</formula><p>, and the outside string η(d 2 ), ξ(d 2 ) for d 2 is:</p><formula xml:id="formula_17">η(d 2 ) = η(d) • η d (d 2 ) ξ(d 2 ) = ξ d (d 2 ) • ξ(d).</formula><p>The top-down method works based on the above decompositions. For a complete translation in the final beam B <ref type="bibr">[0,|x|]</ref> , the algorithm tracebacks on its derivation tree, and gets the outside string for all spans in the derivation. Taking the span <ref type="bibr">[1,</ref><ref type="bibr">2]</ref> in <ref type="figure" target="#fig_0">Figure 1(b)</ref> as an example, if we do top-down, its outside string would be:</p><formula xml:id="formula_18">Amy lived in Berlin before, .</formula><p>where denotes the empty string. Compared to the concatenation method, top-down is able to consider the reordering between spans, and thus would be much better. However, since it bases on the k-best list in the final beam, it can only handle the spans appearing in the final k-best derivations. In our experiments, top-down algorithm only cov- ers about 30% of all spans. <ref type="bibr">4</ref> In order to incorpo- rate more spans into search-aware tuning, we en- hance the top-down algorithm and propose guided backtrace algorithm in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Guided Backtrace</head><p>The guided backtrace algorithm is a variation of top-down method. In guided backtrace, we first introduce a container s <ref type="bibr">[i,j]</ref> to keep a best partial derivation for each split point of [i, j] during de- coding. <ref type="bibr">5</ref> Hence, there will be at most j − i par- tial derivations in s <ref type="bibr">[i,j]</ref> . <ref type="bibr">6</ref> For instance, suppose the span is <ref type="bibr">[2,</ref><ref type="bibr">5]</ref>, we will keep three partial derivations in s <ref type="bibr">[i,j]</ref> for split point 2, 3, and 4 respectively. Dif- ferent from the similar k-best list in the decoding beam, s <ref type="bibr">[i,j]</ref> introduces more diverse partial deriva- tions for backtracing, and thus could help to cover more spans.</p><p>After decoding, we employ algorithm 2 to do backtrace. The algorithm starts from the best translation in the final beam. At first, we get the corresponding span of the input partial derivation d (line 2), and the outside string for this span (line 5-6). We then sort the partial derivations in s <ref type="bibr">[i,j]</ref> based on their potential Bleu +1 (line 12). Thus, the sorting will guide us to first backtrace the partial derivations with better potential Bleu +1 scores. Then we traverse all these partial deriva- tions (line <ref type="bibr">[13]</ref><ref type="bibr">[14]</ref>, and do guided backtrace recur- sively on its child partial derivations (line <ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref>. In this process, about 90% of all spans are visited in our experiments. We demand each span will only be visited once (line 3-4), During the above process, guided backtrace will collect good partial derivations which have bet- ter potential Bleu +1 scores than the best Bleu +1 score MaxSenBleu of the final beam (line 10-11) into goodcands. Meanwhile, we also collect the good partial derivations from descendant nodes (line 19), and apply rule r to them to form good partial derivations for span <ref type="bibr">[i, j]</ref> (line 20-21). At last, we add the top 50 good partial derivations to the beam B <ref type="bibr">[i,j]</ref> for tuning (line 22).</p><p>The purpose of adding good partial derivations for tuning is to recover the good ones pruned out of the beam. For example, the partial derivation X <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> in <ref type="figure" target="#fig_0">Figure 1</ref> has been pruned out of the beam B <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> in <ref type="figure" target="#fig_1">Figure 2</ref>. If we only consider the partial derivations in the beam, it is still hard to promote it. After adding good partial derivations, we will have more good targets to do better tuning.</p><p>From Algorithm 2, we can also get a new way to compute oracle BLEU score of a translation sys- tem. We memorize the string that has the maxi- mum potential Bleu +1 score of all strings (line 9). We then collect the best string of all source sen-Algorithm 2 Guided Backtrace Outside String.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1: function GUIDED(d) 2: [i, j] ← the source span that d belongs to 3: if [i, j] has been visited then 4:</head><p>return</p><formula xml:id="formula_19">∅ 5: η([i, j]) = η(d) 6: ξ([i, j]) = ξ(d) 7: goodcands ← ∅ 8: for each partial derivation d in B [i,j] do 9: bleu = Bleu +1 (η([i, j]) • e(d) • ξ([i, j])) 10:</formula><p>if bleu &gt; MaxSenBleu then 11:</p><p>add d to goodcands 12: sort s <ref type="bibr">[i,j]</ref> based on potential Bleu +1 13: for each partial derivation d of span <ref type="bibr">[i, j]</ref> do 14:</p><p>r ← the rule that generates d 15:</p><p>D d ← the set of partial derivations r use to form d 16:</p><p>for each non-terminal x in rule r do 17:</p><formula xml:id="formula_20">η(dx) = η([i, j]) • η d (dx) 18: ξ(dx) = ξ d (dx) • ξ([i, j]) 19: cands ← GUIDED(dx) 20:</formula><p>for each cand dg in cands do 21:</p><p>add r(dg, D k \dx) to goodcands 22: add top 50 partial derivations of goodcands to B <ref type="bibr">[i,j]</ref> 23: return goodcands tences in the tuning or test set, and use them to compute BLEU score of the set. This can be seen as an approximation upper bound of the current model and decoder, which we call guided oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>The major process of search-aware tuning is sim- ilar to traditional tuning pipeline. We first decode the source sentence, and then output both the com- plete translations in the final beam and the partial derivations from the shorter spans as training in- stances. For potential BLEU, the outputs of par- tial derivations would be the corresponding com- plete translations generated by Equation (2). If we use partial BLEU, the outputs are the correspond- ing partial translations. Each span will serve as a single tuning instance. Different from ( <ref type="bibr" target="#b12">Liu and Huang, 2014</ref>), we only use the features from par- tial derivations for tuning.</p><p>For the spans that top-down or guided backtrace algorithm cannot get outside strings, we use con- catenation for them to maintain consistent number of tuning instances between different tuning iter- ations. However, since we do not want to spent much effort on them, we only use the one-best par- tial derivation for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate our method, we conduct experiments on Chinese-to-English translation. The train- ing data includes 1.8M bilingual sentence pairs, with about 40M Chinese words and 48M English words. We generate symmetric word alignment using GIZA++ and the grow-diag-final-and strat- egy. We train a 4-gram language model on the Xinhua portion of English Gigaword corpus by SRILM toolkit <ref type="bibr" target="#b15">(Stolcke, 2002</ref>). We use BLEU 4 with "average reference length" to evaluate the translation performance for all experiments.</p><p>We use the NIST MT 2002 evaluation data (878 sentences) as the tuning set, and adopt NIST MT04 (1,789 sentences), MT05 (1,082 sentences), MT06 (616 sentences from new portion) and MT08 (691 sentences from new portion) data as the test set.</p><p>Our baseline system is an in-house hierarchical phrase-based system. The translation rules are ex- tracted with Moses toolkit ( <ref type="bibr" target="#b10">Koehn et al., 2007</ref>) by default settings. For the decoder, we set the beam size to 30, nbest list to 50, and 20 as the maximum length of spans for using non-glue rules.</p><p>The baseline tuning methods are batch tun- ing methods based on k-best translations, includ- ing MERT <ref type="bibr" target="#b14">(Och, 2003)</ref>, MIRA (Cherry and Fos- ter, 2012) and PRO (Hopkins and May, 2011) from Moses. Another baseline tuning method is hypergraph-MERT from cdec toolkit ( <ref type="bibr" target="#b5">Dyer et al., 2010)</ref>. To guarantee the tuning efficiency, we con- strain the minimum length of spans for search- aware tuning to restrict the number of training in- stances. For the sentences with less than 20 words, we only use the spans longer than 0.75 times sen- tence length. For the ones with more than 20 words, the minimum span length is set to 18. 7 All results are achieved by averaging three indepen- dent runs for fair comparison <ref type="bibr" target="#b4">(Clark et al., 2011</ref>). <ref type="table" target="#tab_2">Table 1</ref> compares the main results of our MERT- based search-aware tuning with traditional tuning: MERT and hypergraph-MERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Translation Results</head><p>From the results, we can see that hypergraph- MERT is better than MERT by 0.5 BLEU points, verifying the result of ( <ref type="bibr" target="#b11">Kumar et al., 2009</ref>). For search-aware tuning, partial BLEU (both full and span one) only gets comparable results with base- line tuning method, confirming our previous anal- ysis in section 3.1, and the results are also consis- tent with ( <ref type="bibr" target="#b12">Liu and Huang, 2014</ref>   Compared to partial BLEU, potential BLEU is more helpful. Both concatenation and top-down method are better than MERT on all five test sets. Guided backtrace gets the best performance over all methods. It outperforms traditional MERT by 1.1 BLEU points on average, and better than hypergraph-MERT by 0.6 BLEU points.</p><note type="other">34.3 36.4 34.6 33.1 28.5 33.4 top-down 34.5 36.6 34.5 33.1 28.9 33.5 guided backtrace 34.9 36.9 35.2 33.7 29.1 34.0</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>In this section, we analyze the results of search- aware tuning by comparing MERT-based baseline and guided backtrace in detail.</p><p>Oracle BLEU We first compare the oracle BLEU scores of baseline and guided backtrace in <ref type="table" target="#tab_3">Table 2</ref>. In order to get the k-best oracle, we first look for the best Bleu +1 translation in the k-best list for each source sentence, and then use these best translations to compute the BLEU score of the entire set. To get the guided oracle, we use the weights from baseline MERT to run Algorithm 2 on tuning set (nist02) and nist04 test set, and gen- erate the best oracle translation (section 3.2.3) for each source sentence for evaluation.</p><p>The final oracle BLEU comparison is shown in <ref type="table" target="#tab_3">Table 2</ref>. On both nist02 tuning set and nist04 test set, guided backtrace method gains at lease 1.0 nist02 nist04 MERT 0.3960 0.3791 guided backtrace 0.4098 0.3932 <ref type="table">Table 3</ref>: The diversity comparison based on k-best list in the final beam on both tuning set (nist02) and nist04 test set. The higher the value is, the more diverse the k-best list is.</p><p>BLEU points improvements over traditional MERT on both k-best oracle and guided oracle. More- over, k-best and guided oracle get more improve- ments than 1-best, indicating that by search-aware tuning, the decoder could generate a better k-best list, and has a higher upper bound.</p><p>Diversity As shown in ( <ref type="bibr" target="#b7">Gimpel et al., 2013;</ref><ref type="bibr" target="#b12">Liu and Huang, 2014</ref>), diversity is important for MT tuning. An k-best list with higher diversity can better represent the entire decoding space, and thus tuning on it will lead to a better performance.</p><p>Similar as ( <ref type="bibr" target="#b12">Liu and Huang, 2014)</ref>, our method encourages the diversity of partial translations in each beam, including the ones in the final beam. We use the metric in ( <ref type="bibr" target="#b12">Liu and Huang, 2014</ref>) to compare the diversity of traditional MERT and guided backtrace. The metric is based on the n- gram matches between two sentences y and y :</p><formula xml:id="formula_21">∆(y, y ) = − |y|−q i=1 |y |−q j=1 y i:i+q = y j:j+q d(y, y ) = 1 − 2 × ∆(y, y ) ∆(y, y) + ∆(y , y ) ,</formula><p>where q = n − 1 and x equals to 1 if x is true, 0 otherwise. The final diversity results are shown in <ref type="table">Table 3</ref>. We can see guided backtrace gets a better diversity than traditional MERT. Beam Size As we discussed before, search- aware tuning helps to accommodate search errors in decoding, and promotes good partial deriva- tions. Thus, we believe that even with a small beam, these good partial derivations can still sur- vive with search-aware tuning, resulting in a good translation quality. <ref type="figure" target="#fig_3">Figure 4</ref> compares the results of different beam sizes <ref type="bibr">(2,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">30)</ref> between traditional MERT and guided backtrace. The com- parison shows that guided backtrace achieves bet- ter result than baseline MERT, and when the beam is smaller, the improvement is bigger. More- over, guided backtrace method with a beam size 8 could achieve comparable BLEU score to tradi- tional MERT with beam size 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1288</head><p>Span Size For a big tuning set, in order to make the tuning tractable, we constrain the length of spans for search-aware tuning. Intuitively, towards search-aware tuning, more spans will get better re- sults because we have more training instances to guide the tuning process, and accommodate search errors in early decoding stage (shorter spans). To verify this intuition, we perform experiments on a small tuning set, which is generated by select- ing the sentences with less than 20 words from the original NIST MT 02 tuning set. <ref type="figure" target="#fig_4">Figure 5</ref> compares the results of traditional MERT and guided backtrace over different mini- mum span length. The curve in the figure shows that by using more spans for search-aware tuning, we can achieve better translation performance. The value of x-axis refers to the minimum length of spans used for search-aware tuning. More spans will be used in tuning with smaller minimum span length. <ref type="table" target="#tab_5">Table 4 and Table 5</ref> shows the final results of MIRA and PRO for traditional tuning and search- aware tuning using potential BLEU. We can see that potential BLEU is helpful for tuning. Guided backtrace is also the best one, which outperforms the baseline MIRA and PRO by 0.9 and 0.8 BLEU points on average. <ref type="table" target="#tab_8">Table 6</ref> shows the training time comparisons be- tween search-aware tuning and traditional tuning. From this <ref type="table">Table,</ref> we can see that search-aware tun- ing slows down the training speed. <ref type="bibr">8</ref> The slow training is due to three reasons. Similar to ( <ref type="bibr" target="#b12">Liu and Huang, 2014</ref>), as search- aware tuning considers partial translations of spans besides the complete translations, it has much more training instances than traditional tun- ing. In our experiments, although we have con- strained the length of spans, we get a total of 38,539 instances for search-aware tuning, while it is 878 for traditional tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MIRA and PRO Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Efficiency</head><p>Another time consuming factor is the compu- tation of Bleu +1 scores. In guided backtrace set- ting, we need to compute the Bleu +1 scores for all candidates of spans we consider. This is why the nist03 nist04 nist05 nist06 nist08 avg.     , there is also an monster phenomenon ( <ref type="bibr" target="#b13">Nakov et al., 2013</ref>) in our search-aware tuning setting. Therefore, here we perform search-aware tuning on only 109 short sentences (with less than 10 words) from nist02.  decoding of guided backtrace is much slower than baseline or top-down. Finally, adding good candidates enlarges the k- best lists of training instances, which further slows down the tuning process of guided backtrace.</p><p>It should be noted that although search-aware tuning is slower than traditional tuning method, since the decoding is all the same for them in test- ing time, it does not slow down the testing speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an extension of "search-aware tuning" to accommodate search errors in hier- archical phrase-based decoding and promote the promising partial derivations so that they are more likely to survive in the inexact beam search. In order to handle the tree-structured derivations for HIERO system, we generalize the BLEU metrics and propose corresponding partial BLEU and po- tential BLEU to evaluate partial derivations. Our approach can be used as a plugin for most popu- lar parameter tuning algorithms including MERT, MIRA and PRO. Extensive experiments confirmed substantial BLEU gains from our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 (</head><label>1</label><figDesc>b) and Figure 1 (c). An example rule of the synchronous grammar is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>r 2 X</head><label>2</label><figDesc>→ Aimi, Amy has r 3 X → zhuzai bolin, lived in Berlin r 3 X → zhuzai bolin, living in Berlin r 4 X → qian, before r 5 X → qian, former r 6 X → jiehun, got married r 7 X → jiehun, getting married</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: An example of HIERO translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The BLEU comparison between MERT and guided backtrace on nist04 test set over different beam sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The BLEU comparison between MERT and guided backtrace on nist04 test set over different span sizes. L denotes the sentence length. The value of x-axis refers to the minimum length of spans used for search-aware tuning. More spans will be used in tuning with smaller minimum span length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>MIRA</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>MERT results: BLEU scores the test sets (nist03, nist04, nist05, nist06, and nist08). MERT-S is 
an enhanced version of MERT, which uses a beam size 200 and nbest list 200 for tuning, and beam size 
30 for testing. 

methods 
nist02 nist04 

1-best 
MERT 
36.1 
35.9 
guided backtrace +0.5 
+1.0 

k-best Oracle 
MERT 
43.3 
42.8 
guided backtrace +1.1 
+1.2 

Guided Oracle 
MERT 
48.0 
47.2 
guided backtrace +1.4 
+1.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The oracle BLEU comparison between 
baseline MERT and guided backtrace. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>MIRA results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08). 

nist03 nist04 nist05 nist06 nist08 avg. 
PRO 
34.2 
36.1 
33.9 
32.3 
28.7 33.1 
concatenation 
34.6 
36.2 
34.7 
32.3 
28.5 33.2 
top-down 
34.8 
36.4 
34.7 
32.7 
29.0 33.5 
guided backtrace 35.0 
36.8 
34.7 
33.4 
29.6 33.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>PRO results: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08). Similar to (Liu 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison on training efficiency. The 
time (in minutes) is measured at the last iteration 
of tuning. Column "Total" refers to the time for an 
entire iteration, while "Optim." is the time of op-
timization. We use a parallelized MERT for tuning 
by 24 cores. 

</table></figure>

			<note place="foot" n="1"> Two spans are consistent with word alignment means that words in one span only align to words in the other span via word alignment, and vice versa. 2 In practice, there might be several different consistent target spans for a source span due to the unaligned words. We use the longest target span as the span reference which shows the best performance in our experiments.</note>

			<note place="foot" n="3"> For the spans not having translations, we compute a best possible translation for each of them by concatenating the translations of its children spans monotonically.</note>

			<note place="foot" n="4"> We do not consider the hypothesis recombination in topdown algorithm.</note>

			<note place="foot" n="5"> If the partial derivation is generated by HIERO rules, we set the split point as the last position of the first non-terminal. 6 Some split points might not have corresponding partial derivations.</note>

			<note place="foot" n="7"> As the decoder demands that spans longer than 20 can only be translated by glue rule, for these spans, we only need to consider the ones beginning with 0 in search-aware tuning.</note>

			<note place="foot" n="8"> Since the tuning set is different for traditional PRO tuning and search-aware tuning, we do not compare them here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgment</head><p>We thank the three anonymous reviewers for the valuable suggestions. This project was supported in part by DARPA FA8750-13-2-0041 (DEFT), NSF IIS-1449278, and a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Batch tuning strategies for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">11,001 new features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="208" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hope and fear for discriminative training of statistical translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1159" to="1187" />
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regularized minimum error rate training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1948" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic exploration of diversity in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1100" to="1111" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured perceptron with inexact search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Demonstrations</title>
		<meeting>ACL: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Search-aware tuning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1942" to="1952" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tale about pro and monsters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Joseph</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online large-margin training for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Max-violation perceptron and forced decoding for scalable MT training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical mt training using maxviolation perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="785" to="790" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
