<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Networks for Open Domain Targeted Sentiment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Networks for Open Domain Targeted Sentiment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Open domain targeted sentiment is the joint information extraction task that finds target mentions together with the sentiment towards each mention from a text corpus. The task is typically modeled as a sequence labeling problem, and solved using state-of-the-art labelers such as CRF. We empirically study the effect of word embeddings and automatic feature combinations on the task by extending a CRF baseline using neural networks, which have demonstrated large potentials for sentiment analysis. Results show that the neural model can give better results by significantly increasing the recall. In addition , we propose a novel integration of neural and discrete features, which combines their relative advantages, leading to significantly higher results compared to both baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Targeted sentiment analysis has drawn growing re- search interests over the past few years. Compared with traditional sentiment analysis tasks, which extract the overall sentiment of a document, a sen- tence or a tweet, targeted sentiment analysis ex- tracts the sentiment over given targeted entities from a text, and therefore is practically more infor- mative. An example is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. There are at least two practical scenarios:</p><p>(1) Certain entities of concern are specified, and the requirement is to extract the sentiment to- wards their mentions in a text. For exam- ple, one can be interested in the sentiment towards Google Inc., Microsoft and Face- book in financial news texts, or the sentiment towards Manchester United, Liverpool and Chelsea in tweets. [AW service] 0 will be back at work . (2) No specified target is given, and the require- ment is to find sentiments towards entities in the open domain. For example, one might be interested extracting the mentions to all per- sons and organizations, together with the sen- timents towards each mention, from a news archive or a collection of novels.</p><p>There are two sub tasks in targeted sentiment analysis, namely entity recognition and sentiment classification for each entity mention which ap- ply to both scenarios above. In scenario (1), en- tity recognition is relatively trivial, and can typ- ically be achieved by pattern matching. Partly due to this reason, most previous work has ad- dressed targeted sentiment analysis as a pure clas- sification task, assuming that target mentions have been given <ref type="bibr" target="#b11">(Jiang et al., 2011;</ref><ref type="bibr" target="#b1">Chen et al., 2012;</ref><ref type="bibr" target="#b4">Dong et al., 2014;</ref><ref type="bibr" target="#b25">Vo and Zhang, 2015</ref>). For scenario (2), a named entity recognition (NER) system can be used to extract targets, before the same targeted sentiment classification algorithms are applied. There has also been work that con- centrates on extracting opinion targets ( <ref type="bibr" target="#b12">Jin et al., 2009;</ref><ref type="bibr" target="#b10">Jakob and Gurevych, 2010</ref>). In both cases, the data in <ref type="figure" target="#fig_0">Figure 1</ref> can be used for training senti- ment classifiers. <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref> took a different ap- proach, extracting named entities and their senti- ment classes jointly. They model the joint task sentence: So excited to meet my baby Farah !!! entity:</p><p>sentiment: as an extension to the NER task, where an extra sentiment label is assigned to each named entity, in addition to the entity label. As a result, the task can be solved using sequence labeling meth- ods. As claimed by <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref>, the joint task is particularly suitable when no extra re- sources are available for training separate syntac- tic analyzers or name entity recognizers. Such sit- uations can include tweets and low-resource lan- guages/domains. Interestingly, because of con- taining entity information, the annotation in Fig- ure 1 suffices for training joint entity and senti- ment labels even if it is the only resource available. The annotations in <ref type="figure" target="#fig_0">Figure 1</ref> can be transformed into label sequences, as shown in <ref type="figure">Figure 2</ref>. <ref type="figure">Fig- ure 2</ref> consists of two types of labels, where the B/I/O labels indicate span boundaries, and the +/- /0 labels indicate sentiment classes. The two types of labels can be assigned in a span→sentiment pipeline, or jointly as a multi-label task. Alterna- tively, as shown in <ref type="figure">Figure 2</ref>(b), the two types of la- bels can be collapsed into a joint label, such as B+ and I-, indicating the beginning of a positive entity and the middle of a negative entity, respectively. The collapsed labels allow joint entity recognition and sentiment classification to be achieved using a standard sequence labeler. <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref> compare a pipeline model, a joint model and a collapsed model under the same conditional random field (CRF) framework, finding that the pipeline method outperforms the joint model on a tweet dataset. Intuitively, the in- teraction between entity boundaries and sentiment classes might not be as strong as that between more closely-coupled sources of information, such as word boundaries and POS <ref type="bibr" target="#b34">(Zhang and Clark, 2008)</ref>, or named entities and constituents <ref type="bibr" target="#b7">(Finkel and Manning, 2009)</ref>, for which joint models sig- nificantly outperform pipeline models. On the other hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as 'I like X.', the contextual pattern indicate both a positive sen- timent and an entity in the place of X.</p><formula xml:id="formula_0">O O O O O B I O Φ Φ Φ Φ Φ + + Φ<label>(</label></formula><p>Recently, neural network models have been in- creasingly used for sentiment analysis <ref type="bibr" target="#b23">(Socher et al., 2013;</ref><ref type="bibr" target="#b13">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b5">dos Santos and Gatti, 2014)</ref>, achieving highly competi- tive results, which show large potentials of neu- ral network models for this task. The main ad- vantages of neural networks are two-fold. First, neural models use real-valued hidden layers to au- tomatically learn feature combinations, which can capture complex semantic information that are dif- ficult to express using traditional discrete man- ual features. Second, neural networks take dis- tributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to some extent. In this paper, we exploit structured neural models for open targeted sentiment.</p><p>We take the CRF model of <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref> as the baseline, and explore two research ques- tions. First, we make an empirical comparison be- tween discrete and neural CRF models, and fur- ther combine the strengths of each model via fea- ture integration. Second, we compare the effects of the pipeline, joint and collapsed models for open targeted sentiment analysis under the neu- ral model settings. Our experiments show that the neural model gives competitive results compared with the discrete baseline, with relatively higher recalls. In addition, the integrated model signifi- cantly improves over both the discrete and the neu- ral models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Targeted sentiment analysis is closely related prior work on aspect-oriented ( <ref type="bibr" target="#b9">Hu and Liu, 2004</ref>), feature-oriented ( <ref type="bibr" target="#b21">Popescu and Etzioni, 2007)</ref> and topic-oriented ( <ref type="bibr" target="#b32">Yi et al., 2003</ref>) sentiment analysis. These related tasks are typically concentrated on product review settings. In contrast, targeted sen- timent analysis has a more general setting.</p><p>Recently, <ref type="bibr" target="#b29">Wang et al. (2011)</ref> proposed a topic- oriented model, which extracts sentiments towards certain topics from tweets. Topics in their model resemble targets in our work, although topics are represented by hashtags, which exists in 14.6% tweets and 27.5% subjective tweets ( <ref type="bibr" target="#b29">Wang et al., 2011</ref>). In contrast, targeted sentiment analysis can identify all the mentions to target entities in tweets, thereby having a larger coverage. The drawback is that the identification of mentions is subject to errors, and thus suffers a lower preci- sion compared to hashtag matching.</p><p>Sequence labeling models have been used for extracting opinions and target entities as a joint task. <ref type="bibr" target="#b12">Jin et al. (2009)</ref> use HMM to extract opinion- baring expressions and opinion targets. <ref type="bibr" target="#b16">Li et al. (2010)</ref> improve the results by using CRF to iden- tify the opinion expressions and targets jointly. The task is sometimes referred to as fine-grained sentiment analysis ( <ref type="bibr" target="#b30">Wiebe et al., 2005</ref>). It is differ- ent from our setting in that the predicate-argument relation between opinion-baring expressions and target entities are not explicitly modeled.</p><p>Recently, <ref type="bibr" target="#b31">Yang and Cardie (2013)</ref> use CRF to extract opinion-baring expressions, opinion hold- ers and opinion targets simultaneously. Their method is also centralized on opinion-baring ex- pressions and therefore in line with <ref type="bibr" target="#b12">Jin et al. (2009)</ref> and <ref type="bibr" target="#b16">Li et al. (2010)</ref>. In contrast, targeted sentiment analysis directly studies entity mentions and the sentiment on each mention, without ex- plicitly modeling the way in which the opinion is expressed. As a result, our task is more useful for applications such as broad-stroke reputation man- agement, but offer less fine-grained operational in- sight. It requires less fine-grained manual annota- tion.</p><p>As discussed in the introduction, targeted sen- timent analysis falls into two main settings. The first is targeted sentiment classification, assum- ing that entity mentions are given. Most previous work fall under this category <ref type="bibr" target="#b11">(Jiang et al., 2011;</ref><ref type="bibr" target="#b1">Chen et al., 2012;</ref><ref type="bibr" target="#b4">Dong et al., 2014</ref>). The sec- ond is open domain targeted sentiment, which has been discussed by <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref>. The task jointly extracts entities and sentiment classes, and is analogous to joint entity and relation extraction ( <ref type="bibr" target="#b15">Li and Ji, 2014</ref>) in that both are information ex- traction tasks with multi-label outputs.</p><p>Our work is related to the line of work on us- ing neural networks for sentiment analysis.  of a sentence automatically, before classifying its sentiment.  apply deep belief networks for semi-supervised sentiment classifica- tion. dos <ref type="bibr" target="#b5">Santos and Gatti (2014)</ref> use deep convo- lution neural networks with rich features to clas- sify sentiments over tweets and movie reviews. These methods use different models to represent sentence structures, performing sentiment analysis on the sentence level, without modeling targets. <ref type="bibr" target="#b4">Dong et al. (2014)</ref> perform targeted sentiment classification by using a recursive neural network to model the transmission of sentiment signal from opinion baring expressions to a target. They as- sume that the target mention is given, and perform three-way sentiment classification. In contrast, we apply a structural neural model for open domain targeted sentiment analysis, identifying and clas- sifying all targets in a sentence simultaneously. the pipeline and collapsed tasks, there is a single output label sequence y. For the joint task, there are two label sequences y and z, for entity and sen- timent labels, respectively. We take the models of <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref> as our baseline, which are standard CRFs with discrete manual features. To facilitate comparison between the discrete base- line and our neural models, we give a unified for- mulation to all the models in this paper, introduc- ing the neural and integrated models as extensions to the discrete models.</p><formula xml:id="formula_1">O · · · my B · · · baby I · · · Farah · · · · · · step 1: entity Φ · · · my (O) + · · · baby (B) + · · · Farah (I) · · · · · · step 2: sentiment (a) pipeline O · · · my B · · · baby I · · · Farah · · · · · · Φ + + · · · · · · (b) joint O · · · my B+ · · · baby I+ · · · Farah · · · · · · (c) collapsed</formula><p>The baseline CRF structures for pipeline, joint and collapsed targeted sentiment analysis are shown in <ref type="figure" target="#fig_3">Figure 3</ref>(a), 3(b) and 3(c), respectively. In the figures, the input features are represented as black and white circles, indicating that they take 0/1 binary values. The labels O, B and I indi- cate a non-target, the beginning of a target, and part of a target, respectively. The labels +, −, 0 and Φ indicate positive, negative, neutral and NULL sentiments, respectively. The NULL sen- timent is assigned to O entities automatically, and modeled as a hidden variable in the pipeline and joint CRFs. <ref type="bibr">1</ref> The collapsed labels take combined meanings from their components.</p><p>The links between labels and inputs represent output clique potentials:</p><formula xml:id="formula_2">Ψ( x, y i ) = exp θ · f ( x, y i ) ,</formula><p>where f ( x, y i ), is a discrete manual feature vector, and θ is the model parameter vector. The links between labels represent edge clique potentials:</p><formula xml:id="formula_3">Φ( x, y i , y i−1 ) = exp τ (y i , y i−1 ) ,</formula><p>where τ (y i , y i−1 ) is the transition weight, which is also a model parameter. For both the pipeline and collapsed models, the conditional probability of a label sequence given an input sequence is:</p><formula xml:id="formula_4">P ( y| x) = |x| i=1 Ψ( x, y i ) |x| j=1 Φ( x, y i , y i−1 ) Z( x) ,</formula><p>surface features word identity; word length; message length; punctuation characters; has digit; has dash; is lower case; is 3 or 4 letters; first letter capitalized; sentence position; more than one letter capitalized; Jerboa features; linguistic features function words; can syllabify; curse words; laugh words; words for good, bad, no, my; intensifiers; slang words; abbreviations; common verb endings; common noun endings; subjective suffixes and prefixes;</p><p>cluster features Brown cluster at length 3; Brown cluster at length 5; sentiment features is sentiment-bearing word; prior sentiment polarity; where Z( x) is the partition function:</p><formula xml:id="formula_5">Z( x) = y |x| i=1 Ψ( x, y i ) |x| j=1 Φ( x, y i , y i−1 ) ,</formula><p>For the joint model, we apply a multi-label CRF structure, where there are two separate sets of output clique potentials Ψ 1 ( x, y i ) and Ψ 2 ( x, z i ) and two separate sets of edge clique potentials Φ 1 ( x, y i , y i−1 ) and Φ 2 ( x, z i , z i−1 ) for the label sets {B, I, O} and {+, −, 0}, respectively. In the <ref type="figure" target="#fig_3">Figure 3(b)</ref>, there are also links between the span label y i and the sentiment label z i for each word x i . These links indicate label dependencies, which are constraints for decoding. For example, if y i = O, then z i must be φ.</p><p>We apply Viterbi decoding for all tasks, and training is performed using a max-margin objec- tive, which is discussed in Section 6. Our training algorithm is different from that of <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref>, but gives similar discrete CRF accuracies in our experiments. <ref type="bibr" target="#b28">Wang and Mori (2009)</ref> also applied a max-margin trainig strategy to train CRF models. The set of features is taken from <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref> without changes, as shown in <ref type="table" target="#tab_1">Table  1</ref>. Here the cluster features refer to Brown word clusters ( <ref type="bibr" target="#b0">Brown et al., 1992</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Models</head><p>We extend the discrete baseline system with two salient changes, which are illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. First, the input discrete features are replaced with continuous word embeddings. Each node in the input takes a real value between 0 and 1, as repre- sented by grey nodes in <ref type="figure" target="#fig_4">Figure 4</ref>. Second, a hidden neural layer h is added between the input nodes x and the label nodes y i . Formally, the links between the input nodes x and the hidden nodes h i for the node y i in <ref type="figure" target="#fig_4">Figure  4</ref> represent a feature combination function:</p><formula xml:id="formula_6">O · · · · · · my B · · · · · · baby I · · · · · · Farah · · · · · · step 1: entity Φ · · · · · · my (O) + · · · · · · baby (B) + · · · · · · Farah (I) · · · · · · step 2: sentiment (a) pipeline O · · · · · · my B · · · · · · baby I · · · · · · Farah · · · · · · Φ + + · · · · · · (b) joint O · · · · · · my B+ · · · · · · baby I+ · · · · · · Farah · · · · · · (c) collapsed</formula><formula xml:id="formula_7">h i =tanh W.(e( x i−2 ) ⊕ e( x i−1 ) ⊕ e( x i ) ⊕ e( x i+1 ) ⊕ e( x i+2 )) + b</formula><p>where e is the embedding lookup function, ⊕ is the vector concatenation function, the matrix W and vector b are model parameters and tanh is the activation function. The output clique potential of y i becomes:</p><formula xml:id="formula_8">Ψ( x, y i ) = exp σ · h i</formula><p>where σ is a model parameter, and the edge clique potentials remain the same as the baseline. By  <ref type="bibr" target="#b27">Wang and Manning (2013b)</ref>. The main differences between our model and the prior work are in the multi-label settings and training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Integrated Models</head><p>Gleaning different sources of information, neu- ral features and discrete linear features comple-ments each other. As a result, a model that in- tegrates both features can potentially achieve per- formance improvements. Most work attempts to add neural word embeddings into a discrete linear model ( <ref type="bibr" target="#b24">Turian et al., 2010;</ref><ref type="bibr" target="#b33">Yu et al., 2013;</ref><ref type="bibr" target="#b8">Guo et al., 2014</ref>), or add discreted features into a neural model ( <ref type="bibr" target="#b17">Ma et al., 2014</ref>). We make a novel combi- nation of the discrete models and the neural mod- els by integrating both types of inputs into a same CRF framework. <ref type="bibr">2</ref> The architectures of the integrated models are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. The main difference between <ref type="figure" target="#fig_5">Figure 5</ref> and <ref type="figure" target="#fig_3">Figure 3</ref> is the input layer. The inte- grated model takes both continuous word embed- dings, which are shown in grey nodes, and dis- crete manual features, which are shown in black or white nodes, as the input.</p><p>A separate hidden layer is given to each type of input nodes, with the hidden layer for the embed- dings being the same as the neural baseline:</p><formula xml:id="formula_9">h i =tanh W · (e( x i−2 ) ⊕ e( x i−1 ) ⊕ e( x i ) ⊕ e( x i+1 ) ⊕ e( x i+2 )) + b</formula><p>The hidden nodes g i between the discrete features and the node y i are:</p><formula xml:id="formula_10">g i = tanh θ · f ( x, y i )</formula><p>Finally, the output clique potential of y i becomes:</p><formula xml:id="formula_11">Ψ( x, y i ) = exp σ · ( h i ⊕ g i )</formula><p>The edge clique potentials remain the same as the baseline models; the same training and decoding algorithms are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training</head><p>We use a max-margin objective to train our model parameters Θ, which consist of θ, τ , W, b and σ for each model. The objective function is defined as:</p><formula xml:id="formula_12">L(Θ) = 1 N N n=1 l( x n , y n , Θ) + λ 2 Θ 2 , 2</formula><p>Wang and Manning (2013a) also investigated the inte- gration of discrete and neural features in CRF models. They compared the effect of integration without hidden layers (i.e. <ref type="bibr" target="#b24">Turian et al. (2010)</ref>) and with hidden layers (i.e. our meth- ods) for NER and chunking, finding that the formal outper- forms the latter. Our results are different from theirs, and a hidden layer gives significant improvements to the targeted sentiment analysis task.</p><p>where ( x n , y n )| N n=1 are the set of training ex- amples, λ is a regularization parameter, and l( x n , y n , Θ) is the loss function towards one ex- ample ( x n , y n ).</p><p>The loss function is defined as:</p><formula xml:id="formula_13">l( x n , y n , Θ) = max y (s( x n , y, Θ) + δ( y, y n )) − s( x n , y n , Θ),</formula><p>where s( x, y, Θ) = logP ( y| x) is the log proba- bility of y, and δ( y, y n ) is the Hamming distance between y and y n . We use online learning to train model parame- ters, updating the parameters using the AdaGrad algorithm <ref type="bibr" target="#b6">(Duchi et al., 2011)</ref>. One thing to note is that, our objective function is not differentiable because of the loss function l( x n , y n , Θ). Thus we use sub-gradients for l( x n , y n , Θ) instead, which can be computed by the formula:</p><formula xml:id="formula_14">∂l( x n , y n , Θ) ∂Θ = ∂s( x n , ˆ y, Θ) ∂Θ − ∂s( x n , y n , Θ) ∂Θ ,</formula><p>wherê y is the predicted label sequence which cor- responds to l( x n , y n , Θ).</p><p>Maximum-likelihood training is a commonly used alternative to max-margin training for neu- ral networks. It has been applied to the models of <ref type="bibr" target="#b3">Do et al. (2010)</ref> and <ref type="bibr" target="#b2">Collobert et al. (2011)</ref>, for example. However, our experiments show that maximum-likelihood training cannot be applied to open-domain targeted sentiment tasks. Although giving comparable overall accuracies in both en- tity and sentiment labels, it suffers from unbal- anced sentiment labels, assigning the neutral sen- timent to most entities. This problem can be ad- dressed by imposing a polarity-sensitive cost to the training, such as the sentence-level averaged F1-score between positive, negative and neutral la- bels. We skip these results due to space limita- tions. In contrast, max-margin training does not suffer from the label skew issue, thanks to the use of Hamming loss in the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Settings</head><p>Data: We use the data of <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref>  <ref type="bibr">3</ref> to conduct all the experiments, which consist of entity and sentiment annotations on both English and Spanish tweets. Simple normalizations are Domain #Sent #Entities #+ conducted to replace all usernames and URLs into the special tokens username and url, respec- tively. Following <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref>, we report ten-fold cross-validation results. During training, we split 10% of the training corpus as the devel- opment corpus to tune hyper-parameters. <ref type="table" target="#tab_2">Table 2</ref> shows the corpus statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#- #0 English 2,350 3,288 707 275 2,306 Spanish 5,145 6,658 1,555 1,007 4,096</head><p>Parameters: For all the neural models, we set the hidden layer size | h| for neural features to 200, the hidden layer size | g| for discrete features to 30, the initial learning rate for adagrad to 0.01 and the regularization parameter λ to 10 −8 . English and Spanish word embeddings are trained using the word2vec tool 4 , with respective corpora of 20 minion random tweets crawled by tweet API 5 . The size of word embeddings is 100. For English, there are 8,061 unique words, for which 25% are out of word embedding vocabulary (OOE) words, while for Spanish, there are 14,648 unique words, for which 15% are OOE words.</p><p>Metrics: We take full-span metrics for evalua- tion, which is different from <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref>, who evaluate mainly the beginning of spans. We measure the precision, recall and F-score of entity recognition (Entity), targeted sentiment analysis (SA) (both entity and sentiment), and targeted sub- jectivity detection (Subjectivity) (both entity and subjectivity, namely merging the + and -labels as "1" label, and performing two-way 0/1 subjectiv- ity classification on entities). For SA, an entity is taken as correct only when the span and the sen- timent are both correctly recognized. Similarly, for Subjectivity, an entity is taken as correct only when both the span and the subjectivity are cor- rectly recognized.</p><p>Code: We make the C++ implementations of the discrete, neural and combined models avail- able and GPL, at https://github.com/ SUTDNLP/OpenTargetedSentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Comparing Neural and Discrete Models</head><p>The main results on both the English and Span- ish dataset are shown in <ref type="table" target="#tab_4">Table 3</ref>, which are mea- sured on the pipeline, the joint and the collapsed tasks, respectively. As can be seen from the ta- ble, the neural models give higher F-scores than the discrete CRF models on the English dataset, while comparable overall F-scores on the Spanish dataset. The gains on English are mostly attributed to improved recalls, while the precision of the neu- ral CRF models are relatively lower. A likely rea- son for this observation is that the neural model takes embedding inputs, which allow semantically similar words to be represented with similar vec- tors. As a result, the neural model can better cap- ture patterns that do not occur in the training data. In contrast, the discrete model is based on man- ually defined binary features, which do not fire if not contained in the training data. Because dis- crete feature instantiation is based on exact match- ing, the discrete model gives a relatively higher precision.</p><p>To further contrast the discrete and neural mod- els, we draw the per-word accuracies of sentiment labels according to both models in <ref type="figure" target="#fig_6">Figure 6</ref>. In the figure, each dot represents the accuracy of a sentence, measured in the pipeline task. The dots for both English and Spanish are scattered from the reverse diagonal, showing that the two mod- els make very different errors, which suggests that model integration can lead to better accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">The Integrated Model</head><p>As shown in <ref type="table" target="#tab_4">Table 3</ref>, the integrated model com- bines the relative advantages of both pure models, improving the recall over the discrete model and the precision over the neural model. In most cases, it gives the best results in terms of both precision and recall. For the English pipeline model, the integrated model improves the entity recognition F-score from 43.84% to 55.67% (significant with p &lt; 10 −5 by pair-wise t-test) as compared to the discrete baseline, namely <ref type="bibr" target="#b18">Mitchell et al. (2013</ref>   The overall SA score is improved from 31.73% to 40.06% (p &lt; 10 −5 ). Similar improvements are achieved to the other test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Fine-tuning Word Embeddings</head><p>In the experiments above, word embeddings are fine-tuned for the neural models, but not for the integrated models. By fine-tuning, embeddings of in-vocabulary words are treated as model parame- ters, and updated with other parameters in super- vised training. This can improve the accuracy of the model by significantly enlarging the parameter space. However, it can make the embeddings of OOV words less useful to the model, because the hidden layers are tuned with adjusted embeddings. <ref type="figure" target="#fig_7">Figure 7</ref> shows the effectiveness of fine-tuning on the neural and integrated models using the Spanish data. Similar findings apply to the En- glish data. The neural model heavily relies on fine-tuning of embeddings, and a likely reason is that manual discrete features offer sufficient pa- rameters for capturing in-vocabulary patterns. On the other hand, thanks to the rich discrete features in parameter space, the integrated model does not rely on fine-tuning of word embeddings, which even caused slight overfitting and reduced the per- formances. This makes the non-fine-tuned inte- grated model potentially advantageous in handling test data with many OOV words. <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref> find that for discrete CRF, the pipeline task gives competitive overall perfor- mances compared with the joint task. This sug- gests a relatively weak connection between entity boundary information and sentiment classes. We re-examine the comparisons under the neural net- work setting, where automatic feature combina- tions can be useful in capturing more subtle cor- relations between two sources of information. As shown in <ref type="table" target="#tab_4">Table 3</ref>, the overall results are sim- ilar to those of <ref type="bibr" target="#b18">Mitchell et al. (2013)</ref>, with both the neural and the integrated models demonstrat- ing the same trends as the discrete baselines. A more detail analysis, however, shows some rela- tive strengths of the joint task. <ref type="table">Table 4</ref> give the precision, recall and F-scores of subjectivity, and those of SA excluding neutral sentiment labels on the Spanish data. Findings on the English dataset are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Comparing pipeline, joint and collapsed models</head><p>The latter metrics highlight sentiment polarities, which can be relatively more useful. The joint task gives better F-scores on both metrics, which sug- gest that is a considerable choice for open targeted sentiment. When there is external resource for en-  <ref type="table">Table 4</ref>: Results on subjectivity and polarity. tity recognition, the pipeline can be a favorable choice. On the other hand, although useful for some joint sequence labeling task ( <ref type="bibr" target="#b19">Ng and Low, 2004)</ref>, the collapsed task does not seem to address the joint sentiment task as effectively. We find this result empirical, but consistent across our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We explored open domain targeted sentiment analysis using neural network models, which gave competitive results when evaluated against a strong discrete CRF baseline, with relatively higher recalls. Given complementary error dis- tributions by the discrete and neural CRFs, we proposed a novel combination which significantly outperformed both models. Under the neural set- ting, we find that it is preferable to solve open tar- geted sentiment as a pipeline or joint multi-label task, but not as a joint task with collapsed labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Targeted sentiment analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Pipeline, joint and collapsed models for open targeted sentiment analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Socher et al. (2011) use recursive auto-encoders for senti- ment analysis on the sentence level. They further extend the method to a syntactic treebank anno- tated with sentiment labels (Socher et al., 2013). More recently, Kalchbrenner et al. (2014) use a dynamic pooling network to include the structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Discrete CRF models for pipeline, joint and collapsed targeted sentiment labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Neural networks for pipeline, joint and collapsed targeted sentiment labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Integrated models for pipeline, joint and collapsed targeted sentiment labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4Figure 6 :</head><label>6</label><figDesc>Figure 6: Labeling accuracy comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of fine-tuning (+T-with finetuning;-T-without fine-tuning).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Discrete features.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Experimental corpus statistics.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Model 

English 
Spanish 
Entity 
SA 
Entity 
SA 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 

Pipeline 
discrete 59.37 34.83 43.84 42.97 25.21 31.73 70.77 47.75 57.00 46.55 31.38 37.47 
neural 53.64 44.87 48.67 37.53 31.38 34.04 65.59 47.82 55.27 41.50 30.27 34.98 
integrated 60.69 51.63 55.67 43.71 37.12 40.06 70.23 62.00 65.76 45.99 40.57 43.04 

Joint 
discrete 59.55 34.06 43.30 43.09 24.67 31.35 71.08 47.56 56.96 46.36 31.02 37.15 
neural 54.45 42.12 47.17 37.55 28.95 32.45 65.05 47.79 55.07 40.28 29.58 34.09 
integrated 61.47 49.28 54.59 44.62 35.84 39.67 71.32 61.11 65.74 46.67 39.99 43.02 

Collapsed 
discrete 64.16 26.03 36.95 48.35 19.64 27.86 73.18 35.11 47.42 49.85 23.91 32.30 
neural 58.53 37.25 45.30 43.12 27.44 33.36 67.43 43.2 52.64 42.61 27.27 33.25 
integrated 63.55 44.98 52.58 46.32 32.84 38.36 73.51 53.3 61.71 47.69 34.53 40.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Main results.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>619</head><label>619</label><figDesc></figDesc><table>Model 

Subjectivity 
SA/0 
P 
R 
F 
P 
R 
F 
pipeline 47.92 42.26 44.84 42.93 18.02 25.14 
joint 
49.17 42.13 45.32 40.93 21.62 27.93 
collapsed 49.63 35.94 41.63 42.10 15.62 22.49 

</table></figure>

			<note place="foot" n="3"> Discrete CRF Baselines As shown in Figure 2, the input x to our tasks is a word sequence. Assuming no external resources, there is no POS given to each input word x i. For</note>

			<note place="foot" n="1"> Note the difference between neural and NULL sentiments. The former indicates that a target does not bare any sentiment, and the latter simply means that the term is not a part of a target.</note>

			<note place="foot" n="3"> http://www.m-mitchell.com/code/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their con-structive comments, which helped to improve the paper. This work is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting diverse sentiment expressions with target-dependent polarity from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meenakshi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit P</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trinh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Arti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting embedding features for simple semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting opinion targets in a single-and cross-domain setting with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1035" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel lexicalized hmm-based learning framework for web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hay</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure-aware review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Ju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="653" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tagging the web: Building a robust web tagger with neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqui</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1643" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Kiat</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="277" to="284" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1419" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orena</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing and text mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duy-Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1347" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effect of non-linear deep architecture in sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1285" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Max-margin hidden conditional random fields for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Topic sentiment analysis in twitter: a graph-based hashtag sentiment classification approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1031" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Nasukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Niblack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2003. ICDM 2003. Third IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compound embedding features for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="563" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint word segmentation and POS tagging using a single perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="888" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hybrid deep belief networks for semi-supervised sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoling</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1341" to="1349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
