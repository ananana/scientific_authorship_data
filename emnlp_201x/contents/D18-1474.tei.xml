<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speed Reading: Learning to Read ForBackward via Shuttle</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
							<email>ma@iis.sinica.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Academia Sinica No</orgName>
								<address>
									<addrLine>128, Sec. 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Academia Rd. Taipei</orgName>
								<address>
									<postCode>11529</postCode>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Academia</orgName>
								<address>
									<addrLine>Sinica No. 128, Sec. 2, Academia Rd. Taipei</addrLine>
									<postCode>11529</postCode>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speed Reading: Learning to Read ForBackward via Shuttle</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4439" to="4448"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4439</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present LSTM-Shuttle, which applies human speed reading techniques to natural language processing tasks for accurate and efficient comprehension. In contrast to previous work, LSTM-Shuttle not only reads shuttling forward but also goes back. Shuttling forward enables high efficiency, and going backward gives the model a chance to recover lost information, ensuring better prediction. We evaluate LSTM-Shuttle on sentiment analysis, news classification, and cloze on IMDB, Rotten Tomatoes, AG, and Children&apos;s Book Test datasets. We show that LSTM-Shuttle predicts both better and more quickly. To demonstrate how LSTM-Shuttle actually behaves, we also analyze the shuttling operation and present a case study.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, recurrent neural networks (RNNs) and long short-term memory (LSTM) cells and gate re- current unit (GRU) cells have achieved great suc- cess and are increasingly being applied in nature language processing tasks, e.g., part-of-speech (POS) tagging ( <ref type="bibr" target="#b24">Wang et al., 2015)</ref>, named-entity recognition ( <ref type="bibr" target="#b6">Chiu and Nichols, 2015)</ref>, sentiment analysis ( <ref type="bibr" target="#b28">Zhang et al., 2018)</ref>, document classi- fication <ref type="bibr" target="#b10">(Kim, 2014;</ref><ref type="bibr" target="#b13">Le and Mikolov, 2014a</ref>), cloze ( <ref type="bibr" target="#b22">Srinivasan et al., 2018)</ref>, machine trans- lation ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>, dialogue mod- eling ( <ref type="bibr" target="#b16">Mei et al., 2017)</ref>, document summariza- tion ( <ref type="bibr">Allahyari et al., 2017)</ref>, automatic knowledge extraction ( <ref type="bibr" target="#b7">Durme and Schubert, 2008)</ref>, and ques- tion answering <ref type="bibr" target="#b5">(Chen et al., 2017)</ref>.</p><p>Those tasks all call for text comprehension tech- niques. To solve these tasks, the proposed models read all the text available. That is, models read every token or word of the text from beginning to end. However, for some classification tasks, it is not necessary to treat each individual word equally. Take, for example, sentiment analysis: sentences such as "this movie is amazing" or "too boring" are sufficient to judge a sentiment without reading the entire comment. In addition, the fact that texts are often written redundantly also mo- tivates reading selectively, especially for certain NLP tasks.</p><p>In terms of human reading habits, although peo- ple tend to skim text when reading a newspaper or a novel, this does not significantly impair com- prehension. Speed reading, a reading technique, is used to improve one's ability to read quickly. Work has been done on modeling skimming be- havior along with the original sequence modeling RNN. LSTM-Jump ( <ref type="bibr" target="#b26">Yu et al., 2017</ref>) predicts how many words to skim based on the RNN hidden state. They show that neglecting some words in a document does not greatly harm prediction ac- curacy but does significantly accelerate the pro- cess. They also show that for certain tasks such as cloze, skimming even outperforms traditional methods. In addition, ( <ref type="bibr" target="#b27">Yu et al., 2018</ref>) use RNN hidden states to decide when to stop. If the RNN judges it has achieved sufficient comprehension of the context, it stops early and produce the answer directly.</p><p>However, strictly speakly, simply skimming and stopping early is not speed reading. For example, imagine that during a reading test, we first read the question and then the long article. We read quickly and skip some information. What do we do if we encounter text that we don't understand? We go back, read the previous text, and try to fill in the gaps in our understanding. For speed reading, go- ing back -or "reading backward" -is likewise im- portant as it helps us to recover lost information or correct misunderstandings, leading to better com- prehension of long documents. In fact, reading backward increases rather than decreases reading speed: given the opportunity to go back to correct misunderstandings, we skim more words and thus read faster without reducing our comprehension.</p><p>In this paper, we propose LSTM-Shuttle, which teaches the RNN model to speed read by mod- eling both forward and backward reading behav- ior. We evaluate the proposed method on senti- ment analysis, document classification, and cloze tasks. We use IMDB <ref type="bibr" target="#b12">(L. et al., 2011) and</ref><ref type="bibr">Rotten Tomatoes (Pang and</ref><ref type="bibr" target="#b18">Lee, 2005</ref>) as sentiment analysis datasets, AG New ( <ref type="bibr" target="#b20">Shang et al., 2015)</ref> as a document classification dataset, and Face- book Children's Book Test <ref type="bibr" target="#b8">(Hill et al., 2015</ref>) as a cloze dataset. The experiments show that the pro- posed method achieves better prediction perfor- mance and reads faster at the same time, in com- parison with the LSTM baseline <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) and LSTM-Jump. We also analyze the shuttling behavior under different set- tings, proving that reading forward and backward does help in reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The proposed method is inspired mainly by LSTM-Jump ( <ref type="bibr" target="#b26">Yu et al., 2017)</ref>, which predicts how many words should be neglected, accelerating the reading process using RNN. Both their work and ours is related to the idea of learning visual at- tention per <ref type="bibr" target="#b17">(Mnih et al., 2015)</ref>, where a recurrent model is used to decide which image part to watch seriatim. They train the model end-to-end using the REINFORCE algorithm <ref type="bibr" target="#b25">(Williams, 1992)</ref> and sample from a continuous Gaussian distribution. The difference between their and our methods is that we sample from a discrete distribution to re- flect the properties of text and image.</p><p>Many recent natural language processing appli- cations have explored the idea of filtering irrele- vant content. As in our work, instead of skimming some words, ( <ref type="bibr" target="#b19">Seo et al., 2018</ref>) consider all words but use a small RNN for irrelevant words and the original large RNN for relevant ones. ( <ref type="bibr" target="#b3">Campos et al., 2018</ref>) also attempt to dynamically control the RNN's computational costs, but they instead control the number of units to update at each time step. In our method, in contrast, we skim words, directly setting the amount of computation to be zero, which streamlines the reading process.</p><p>From another perspective, ( <ref type="bibr" target="#b27">Yu et al., 2018)</ref> at- tempt to model early stopping behavior, deciding whether the model can answer confidently based on the hidden states. This is very effective for tasks such as question answering. To ensure accu- racy, <ref type="bibr" target="#b21">(Shen et al., 2016</ref>) focuses on early stopping after multiple passes, and <ref type="bibr" target="#b21">(Shen et al., 2016</ref>) also using reinforcement learning to attempt to learn to reason. Both early stopping and our method ad- equately take into account research on sufficient comprehension. While, early stopping is not fast enough for classification, we can do better with text shuttling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Idea</head><p>In this section, we describe the proposed LSTM- Shuttle, first presenting its architecture. Then, we show that due to the nondifferentiability of the shuttle mechanism, we apply a policy gradi- ent ( <ref type="bibr" target="#b23">Sutton et al., 1999</ref>) to train it end-to-end. Fi- nally, we present the implementation details and the inference approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates, LSTM-Shuttle is based on an LSTM recurrent neural network to which is added an additional fully connected layer to predict for- ward or backward steps after a softmax distribu- tion.</p><p>Given a text which denoted as x 1 , x 2 , . . . , x L or x 1:L , LSTM-Shuttle first reads a fixed num- ber of words sequentially and outputs the hidden state. Then, based on the hidden state, LSTM- Shuttle computes the shuttle softmax distribution over the forward or backward steps on <ref type="bibr">[−K, K]</ref>. Given a negative step value, LSTM-Shuttle goes back to correct misunderstandings, and with a pos- itive step value, LSTM-Shuttle speed reads, skim- ming unimportant words. After shuttling, LSTM- Shuttle reads words sequentially and then pro- ceeds to shuttle again, iteratively. This process continues until one of the following occurs:</p><p>• The shuttle softmax samples a 0 • The number of shuttles exceeds the prede- fined upper limit • The model has arrived at the last word After stopping, the last hidden state is used to pre- dict the desired task. The post-processing depends on the task. For instance, for classification, the hidden state generates a softmax distribution over the target class, and for cloze, it is used to find the correlation between the question article and each candidate answer. The detailed settings of each task are described in Section 4. After reading the entire text, the last hidden state is used to answer prediction. Note that when going backward, the shuttle step is counted before reading sequentially.</p><p>As with LSTM-Jump ( <ref type="bibr" target="#b26">Yu et al., 2017)</ref>, we use the following notation:</p><p>• N : total number of allowed shuttles • R: number of words to read before shuttling • K: maximum shuttle size Whereas K is a fixed hyperparameter during train- ing, N and R can vary between training and test- ing. Note that as LSTM-Shuttle proceeds not only forwards but also backwards, the output shuttle size is 2K + 1: K forward, K backward, and 1 for stopping. In contrast to LSTM-Jump ( <ref type="bibr" target="#b26">Yu et al., 2017)</ref>, when going back, our shuttle step is counted before reading sequentially. In the exam- ple in <ref type="figure" target="#fig_0">Fig. 1</ref>, we set R = 2 and K = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training via Policy Gradient</head><p>In LSTM-Shuttle, there are two main parameter sets to compute: θ R and θ U . θ R includes the RNN along with the output prediction parameters, and θ U represents the parameters of the shuttle mech- anism.</p><p>We compute θ R via backpropagation directly by minimizing J 1 (θ R ), the cross entropy loss, which is differentiable over θ R and is the target objective function of the classification task.</p><p>However, this does not work for θ U . Since cross entropy isn't differentiable over θ U , we cannot use backpropagation to compute θ U . Therefore, we re- cast it as a reinforcement learning problem and ap- ply a policy gradient to train it: we seek to maxi- mize the reward function over θ U via the following formulation.</p><p>We first denote s 1:T as the shuttle action se- quence when training with text x 1:L . Assuming that h i is the hidden state of LSTM before the i-th shuttle s i , it is a function of s i:i−1 and thus can be denoted as h i (s 1:i−1 ). Also, the shuttle action can be sampled from the distribution of p(s t |h i (s 1:t−1 ); θ U ), which is determined by the shuttle softmax. We have the final prediction after LSTM-Shuttle processes text x 1:L under the cur- rent θ U shuttle strategy. As with ( <ref type="bibr" target="#b26">Yu et al., 2017)</ref>, we set the reward to 1 if the prediction is correct, and -1 otherwise.</p><formula xml:id="formula_0">R = 1 if predicted correctly −1 otherwise</formula><p>The objective function of θ U we seek to maxi- mize is the expected reward under the distribution over θ U shuttle strategy, i.e.,</p><formula xml:id="formula_1">J 2 (θ U ) = E p(s 1:T ;θ U ) [R],<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">p(s 1:T ;θ U ) = i p(s 1:i |h i (s 1:i−1 ; θ U ).</formula><p>To maximize the objective function, we must compute the gradient of Eq. (1). We compute an approximate gradient by running M exam- ples with the REINFORCE algorithm <ref type="bibr" target="#b25">(Williams, 1992)</ref>:</p><formula xml:id="formula_3">θ U J 2 (θ U ) = T i=1 E p(s 1:T ;θ U ) [ θ U log p(s 1:i |h i ; θ U )R] ≈ 1 M M m=1 T i=1 [ θ U log p(s m 1:i |h m i ; θ U )R m ],</formula><p>where the superscript m denotes that it belongs to the m-th example. Eventually, the term θ U log p(s 1:i |h i ; θ U ) is computed by backprop- agation as usual.</p><p>Though the approximation of θ U J 2 (θ U ) is un- biased, it may have very high variance <ref type="bibr" target="#b25">(Williams, 1992)</ref>. One common way to reduce this variance is to subtract a baseline value b from the reward function R, transforming the approximated gradi- ent into</p><formula xml:id="formula_4">θ U J 2 (θ U ) ≈ 1 M M m=1 T i=1 [ θ U log p(s m 1:i |h m i ; θ U )R m − b m i ].</formula><p>Here we apply same bias strategy as ( <ref type="bibr" target="#b15">Lewis et al., 2017)</ref>, treating the bias value b as the average re- ward from then until now. The final objective function for LSTM-Shuttle to minimize is</p><formula xml:id="formula_5">J(θ R , θ U ) = J 1 (θ R ) − J 2 (θ U ),</formula><p>which is entirely differentiable and can be com- puted by standard backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation detail and Inference</head><p>To simulate negative step selection, which corre- sponds to reading backward in the shuttle action, we set the shuttle output dimension to <ref type="bibr">[0, 2K]</ref>, where 0 maps to −K, 1 maps to −(K − 1), . . . , K maps to 0, . . . , 2K − 1 maps to +(K − 1), and 2K maps to +K.</p><p>We used the Adam optimizer ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) with a learning rate of 10 −3 for all exper- iments. For a fair comparison with ( <ref type="bibr" target="#b26">Yu et al., 2017)</ref>, the dropout rate between LSTM layers was set to 0.2 and the embedding dropout rate to 0.1. We implemented LSTM-Shuttle in Py- Torch ( ) on a GTX 1080Ti gpu.</p><p>During inference, we apply greedy sampling: we select the most probable shuttle step from the shuttle softmax distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed LSTM- Shuttle on three different tasks: sentiment analy- sis, news classification, and cloze on four different datasets. We use IMDB <ref type="bibr">(L. et al., 2011</ref>) and Rot- ten Tomatoes (Pang and <ref type="bibr" target="#b18">Lee, 2005</ref>) for sentiment analysis, AG (Shang et al., 2015) for news arti- cle classification, and Children's Book Test <ref type="bibr" target="#b8">(Hill et al., 2015</ref>) for cloze. <ref type="table">Table 1</ref> contains statistics for the tasks and datasets in our experiments.</p><p>To show the improvement in not only accuracy but also efficiency, we compared LSTM-Shuttle with three baselines: vanilla LSTM (Hochreiter and Schmidhuber, 1997), LSTM-Jump ( <ref type="bibr" target="#b26">Yu et al., 2017)</ref>, and bi-directional LSTM-Jump, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. For a fair comparison, we trained LSTM-Shuttle with the same LSTM settings as LSTM-Jump. For example, for sentiment analysis on IMDB, LSTM-Jump was trained with R=20, K=40, and N =80; we trained LSTM-Shuttle with the same parameters. Vanilla LSTM is the tradi- tional recurrent neural network using LSTM cells which reads the entire text and then outputs the prediction. LSTM-Jump has a skim mechanism which neglects some text. Bi-directional LSTM- Jump applies LSTM-Jump twice but starting from different directions, and concatenates the last hid- den state for answer prediction. To shorten the pre- sentation, for LSTM-Jump we selected only two results from the original paper directly on each dataset: one with the best accuracy and the other with the highest efficiency. This is to show the difference between reading in two directions and shuttling. The quantitative result of each dataset is shown in the following sections.</p><p>In addition to the quantitative results, we sought to investigate how the shuttle mechanism pro- gresses in reality. We present the shuttle statistics for different K settings, and show that because of the backward mechanism which affords a chance to recover lost information, LSTM-Shuffle shut- tles with larger steps, increasing the shuttle step size as it grows more and more confident in its pre- diction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Analysis on IMDB and Rotten Tomatoes</head><p>Sentiment analysis is a classic natural language processing task, in which we read an article and predict its latent sentiment as positive or negative.</p><p>It is widely implied in many forms or question- naires such as satisfaction surveys. Here we use IMDB (L. et al., 2011) and Rotten Tomatoes (Pang and <ref type="bibr" target="#b18">Lee, 2005</ref>) as our sentiment analysis datasets. We also show the result of a larger shuttle step (K = 75) version of LSTM-Shuttle. <ref type="table">Table 2</ref> shows the experimental results for IMDB, where the speedup ratio is compared with vanilla LSTM, conducted on a machine with a sin- gle GTX 1080Ti GPU. For bi-directional LSTM- Jump we used our own implementation, and the   <ref type="table">Table 2</ref>: Sentiment analysis results on IMDB. * means the best accuracy or highest speedup for each method given the same setting of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">IMDB Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB</head><p>"Backward" column in the table shows the fre- quency ratio of backward shuttles. Under the same (R, K, N ) setting, LSTM- Shuttle is a little slower than LSTM-Jump since the softmax size of K is larger, but the for- mer yields better prediction. For bi-directional LSTM-Jump, since it applies LSTM-Jump twice, it predicts better than the original.</p><p>How- ever, we doubt whether it is worth sacrificing so much efficiency for such a small increase in prediction accuracy (+0.2%).</p><p>Under the same (R, K, N ), LSTM-Shuttle is more accurate and faster than bi-directional LSTM-Jump, even though (80, 40, 8) is not in fact the best setting for LSTM-Shuttle. LSTM-Shuttle achieves the high- est accuracy (89.9%) with 2.08× acceleration un- der <ref type="bibr">(60,</ref><ref type="bibr">40,</ref><ref type="bibr">6</ref>). Due to the shuttle mechanism that can go back, LSTM-Shuttle does not need to read many words before each shuttle, thus accelerating the overall reading process.</p><p>In general, the combination of (R, N ) repre- sents a trade-off between accuracy and efficiency.</p><p>If we use a larger (R, N ), the model reads more words and predicts better but more slowly. Oth- erwise, for a smaller (R, N ), the model reads faster but yields predictions that are not as accu- rate. A similar tendency is found when it comes to the backward ratio. A smaller N means LSTM- Shuttle shuttles less often, so the model tends to read through as much as possible, making for a lower backward ratio. On the other hand, LSTM- Shuttle can shuttle many times so it is willing to go back to correct misunderstandings.</p><p>We also show the result for K = 75. With the larger shuttle step, fewer words are read be- fore shuttling, which accelerates reading but has little impact on accuracy. LSTM-Shuttle achieves 89.7% with 2.27× speedup under (50, 75, 4) and 89.1% with 2.45× times speedup under (50, 75, 2): both settings yield both high accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Rotten Tomatoes Results</head><p>The Rotten Tomatoes dataset <ref type="bibr" target="#b18">(Pang and Lee, 2005</ref>) is to IMDB. We chose to use a two-layer LSTM and 256 hidden units, and again used the pre-trained word2vec embeddings ( <ref type="bibr" target="#b14">Le and Mikolov, 2014b</ref>). We trained all models under R = 8, K = 10, and N = 3.</p><p>The experimental results are shown in <ref type="table">Table 3</ref>. LSTM-Shuttle achieves an accuracy of 79.5% with 1.55× speedup; a higher efficiency version accelerates to 1.89×. These results demonstrate a similar trade-off tendency with different (R, N ) combinations as those for IMDB. Since the aver- age comment length in Rotten Tomatoes is short, we used lower shuttle times (N = 2) for better speed but also maintained high accuracy. Because Method (R, K, N) Accuracy Speedup Backward LSTM - 79.1% 1x - LSTM-Jump <ref type="bibr">(7,</ref><ref type="bibr">10,</ref><ref type="bibr">4)</ref> 79.3% * 1.56x - LSTM-Jump (9, 10, 2) 78.3% 1.94x * - Bi-LSTM-Jump <ref type="bibr">(7,</ref><ref type="bibr">10,</ref><ref type="bibr">4)</ref> 79.4% * 1.32x - Bi-LSTM-Jump <ref type="bibr">(9,</ref><ref type="bibr">10,</ref><ref type="bibr">2)</ref> 78.9% 1.54x * - LSTM-Shuttle <ref type="bibr">(7,</ref><ref type="bibr">10,</ref><ref type="bibr">4)</ref> 79.5% 1.52x 0.41 LSTM-Shuttle <ref type="bibr">(8,</ref><ref type="bibr">10,</ref><ref type="bibr">3)</ref> 79.5% * 1.55x 0.41 LSTM- <ref type="figure" target="#fig_0">Shuttle (9, 10, 2)</ref> 79.3% 1.89x * 0.45 LSTM-Shuttle <ref type="bibr">(6,</ref><ref type="bibr">20,</ref><ref type="bibr">3)</ref> 79.8% * 1.74x 0.39 LSTM-Shuttle <ref type="bibr">(6,</ref><ref type="bibr">20,</ref><ref type="bibr">2)</ref> 79.2% 1.97x * 0.48 <ref type="table">Table 3</ref>: Sentiment analysis results for Rotten Tomatoes. * means the best accuracy or highest speedup for each method given the same setting of K.</p><p>of the shorter comments and lower shuttle times, LSTM-Shuttle prefers to shuttle over almost the entire text from the beginning, trying to see the last part of a comment, and then goes back to the mid- dle part; thus the backward ratio is much higher. We also show the results under a larger K (K = 20). On Rotten Tomatoes, a larger shuttle step seems more suitable. LSTM-Shuttle achieves the best accuracy (79.8%) with 1.74× high efficiency. A setting with fewer shuttles further accelerates up to 1.97× while maintaining a high accuracy of 79.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">News Article Classification on AG dataset</head><p>News classification is a common application of document comprehension. Given a news article, the model must recognize which field it belongs to. Modern topic classification is applied on dif- ferent target sources such as blog posts. We used AG ( <ref type="bibr" target="#b20">Shang et al., 2015)</ref> as news article classifica- tion dataset in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Result on AG dataset</head><p>We used the subset constructed by <ref type="bibr" target="#b20">(Shang et al., 2015</ref>) for classification at the character level. AG contains news covering four topics (World, Sports, Business, Sci/Tech), each of which in- cludes 30,000 training and 1,900 testing docu- ments. We used a single-layer LSTM with 64 hid- den units. We trained the character embedding with 16 dimensions for 70 characters in total, per LSTM-Jump ( <ref type="bibr" target="#b26">Yu et al., 2017)</ref>. We trained all mod- els using R = 30, K = 40, and N = 5.</p><p>As shown in <ref type="table">Table 4</ref>, LSTM-Shuttle still yields improvement at the character level. For both LSTM-Jump and bidirectional LSTM-Jump, the speedup effect is not obvious because of the com- putational overhead of skim being larger than <ref type="figure" target="#fig_5">(40, 40, 6)</ref> 88.4% 0.81x - LSTM-Shuttle (20, 40, 5) 90.1% * 1.34x * 0.27 LSTM-Shuttle <ref type="bibr">(30,</ref><ref type="bibr">40,</ref><ref type="bibr">5)</ref> 88.9% 1.16x 0.30 LSTM-Shuttle <ref type="figure" target="#fig_5">(40, 40, 6)</ref> 88.4% 0.82x 0.34 LSTM-Shuttle <ref type="figure" target="#fig_2">(20, 80, 4)</ref> 89.8% <ref type="table">Table 4</ref>: News classification result on AG. * means the best accuracy or highest speedup for each method given the same setting of K.</p><formula xml:id="formula_6">Method (R, K, N) Accuracy Speedup Backward LSTM - 88.1% 1x - LSTM-Jump (30, 40, 5) 88.5% * 1.24x * - LSTM-Jump (40, 40, 6) 87.4% 0.83x - Bi-LSTM-Jump (30, 40, 5) 89.5% * 1.08x * - Bi-LSTM-Jump</formula><formula xml:id="formula_7">1.63x * 0.26 LSTM-Shuttle (30, 80, 4) 90.1% * 1.29x 0.28</formula><p>when processing the entire text directly. On the other hand, LSTM-Shuttle yields accurate pre- dictions even when reading fewer characters be- fore shuttling, and it clearly yields accelerated performance. We reach an accuracy of 90.1% with 1.34× speedup, both far from LSTM-Jump. Interestingly, (20, 40, 5) reads fewer words than <ref type="bibr">(30,</ref><ref type="bibr">40,</ref><ref type="bibr">5)</ref>, but the former predicts better. This may be due to the character-level nature of the task and because many characters actually mislead the model. This can be seen with LSTM-Jump as well.</p><p>The backward ratio on the AG dataset is nearly to that of IMDB: almost three times forward with once backward. We also show that a larger setting of K (K = 80) yields further acceleration, even at the charac- ter level, achieving the highest (1.63×) speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cloze on Children's Book Test Name</head><p>Entity dataset</p><p>For the cloze task, we must supply the missing words in an article. In the Children's Book Test (CBT) ( <ref type="bibr" target="#b8">Hill et al., 2015)</ref>, the question includes a complete article and a query from which a specific word is deleted. The model must determine which of the ten candidate words is most suitable. In con- trast to previous tasks, which have a fixed class type, CBT provides different candidate words for each question. Thus we cannot train it as with a normal classification problem. Inspired by ), we formulate the task as</p><formula xml:id="formula_8">softmax(CW h o ) ∈ R 10 ,<label>(2)</label></formula><p>where C ∈ R 10×d is the word embedding matrix, h o is the latest LSTM hidden state, and W is a trainable weight variable. The output of the above equation is taken as the index of the answer word. We train LSTM-Shuttle to maximize the distribu- tion over a one-hot answer index. Therefore for</p><formula xml:id="formula_9">Method (R, K, N) Accuracy Speedup Backward LSTM - 45.3% 1x - LSTM-Jump (1, 5, 5) 46.8% * 3.05x - LSTM-Jump (1, 5, 1) 45.2% 6.28x * - Bi-LSTM-Jump (1, 5, 5) 47.0% * 2.64x - Bi-LSTM-Jump (1, 5, 1) 45.3% 6.19x * - LSTM-Shuttle (1, 5, 5) 47.2% * 2.98x 0.31 LSTM-Shuttle</formula><p>(1, 5, 1) 46.0% 6.16x * 0.18 LSTM-Shuttle <ref type="bibr">(1,</ref><ref type="bibr">10,</ref><ref type="bibr">5)</ref> 47.1% * 2.91x 0.36 LSTM- <ref type="figure" target="#fig_0">Shuttle (1, 10, 1)</ref> 46.6% 6.13x * 0.27 <ref type="table">Table 5</ref>: Cloze result on CBT-NE. * means the best accuracy or highest speedup for each method given the same setting of K.</p><p>different candidate words, we concatenate them as an embedding matrix, feed this into the above equation, and generate the prediction distribution. We used the named-entity (NE) part of CBT as the cloze dataset when evaluating LSTM-Shuttle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Result on CBT-NE</head><p>CBT-NE includes 120,769 questions for training and 2,500 for testing. We trained all models using Eq. (2) with a two-layer LSTM and 256 hidden units. Pre-trained word2vec embeddings were again applied directly. We trained them un- der R = 1, K = 5, and N = 5 at the sen- tence level, which means that LSTM-Shuttle read one sentence and shuttled several sentences five times. Vanilla LSTM, LSTM-Jump, Bi-LSTM- Jump, and LSTM-Shuttle all read the query, but only vanilla LSTM read the entire question article. Others decided how to skim or shuttle.</p><p>The result is reported in <ref type="table">Table 5</ref>. LSTM- Shuttle's best accuracy is 47.2% with 2.98× speedup, and the highest efficiency version achieved 46.0% accuracy with 6.16× speedup. Despite the modest acceleration effect, LSTM- Shuttle yields consistently better prediction accu- racy with minimal drops in efficiency. Accelera- tion is only modest because the average number of article sentences was only 20, and it thus did not need to read many sentences (R = 1) before shut- tling or shuttling so many times (N = 5). That is also why the backward ratio here is low in CBT- NE.</p><p>LSTM-Shuttle maintains accurate prediction and high efficiency under a larger K: 46.6% ac- curacy with 6.13× speedup. To demonstrate the proposed method, we offer a case study in Sec- tion 4.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Shuttle Mechanism</head><p>Here we analyze how LSTM-Shuttle actually op- erates. We compute the total average shuttle steps and the average shuttle steps for each shuttle ac- tion under same R = 20 and N = 6 but different K on the IMDB datasets. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the average total shuttle steps for different K, taking into consideration both for- ward and backward steps. For backward shuttling, we use the absolute value as its shuttle steps. For instance, shuttling -5 means it goes back 5 words, and the shuttle step is 5 indeed. We can see that a larger K, and thus a larger shuttle space, tends to shuttle larger steps, but also converges for large enough values of K. We see the same thing in the backward ratio. <ref type="figure">Fig. 4</ref> shows the backward ratio  In addition to the total average, we seek to un- derstand how LSTM-Shuttle shuttles for each. As above, both forward and backward are taken into consideration and the absolute value is used for the backward steps. We show each shuttle record for a total of 6 shuttles under K between <ref type="bibr">[40,</ref><ref type="bibr">50]</ref>. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, all settings of LSTM-Shuttle shuttle more steps after shuttling more times, but LSTM-Jump skims at an almost fixed frequency. Thus the model reads more words after more shut- tles since it tends to read sequential words before each shuttle, in turn yielding better comprehension for the model. For LSTM-Shuttle, with its back- ward mechanism to recover lost information, it shuttles with larger and larger steps. However, for LSTM-Jump, as it cannot go back, it reads more carefully and maintains a constant skim step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>Below, we show two examples of LSTM-Shuttle shuttling on the CBT-NE dataset. Example 1 in <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates a simple case. Based only on the query, "King should promise him his daugh- ter", the deleted word clearly should be "King". LSTM-Shuttle shows more confidence providing the answer given only this query, so it shuttles with large steps to read the last part of the article, from sentence 1 to 12. Also, it goes back to confirm its prediction: from sentence 15 to 10. Example 2 in <ref type="figure" target="#fig_6">Fig. 7</ref> is a more difficult task because the answer word "Nora" appears only once in the entire arti- cle. Thus LSTM-Shuttle must read more carefully. The shuttle steps are all smaller than 8 before dis- covering the answer in sentence 18, after which it goes back to see if it missed something. From these examples, we see that the shuttle mechanism is used in diverse manners for queries with differ- ent difficulties. For simple queries, LSTM-Shuttle shuttles in large steps, while for difficult queries, it shuttles more conservatively. In both cases we witness the ability to go back if necessary to make sure it understands correctly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of LSTM-Shuttle. In this example, we set the number of words read sequentially before shuttling R = 2, and the maximum shuttle size K = 10. The shuttle action is sampled from [−K, K]. After reading the entire text, the last hidden state is used to answer prediction. Note that when going backward, the shuttle step is counted before reading sequentially.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,453.55,67.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(L. et al., 2011), a well-known movie infor- mation website, also includes audience comments and their sentiments. It contains 25,000 training data and 25,000 testing data, where the average length is 241 words. Both baselines and LSTM- Shuttle used a single layer and 128 hidden units as LSTM cells. We used pre-trained word2vec em- beddings (Le and Mikolov, 2014b) as initial word embeddings and trained it along with LSTM. For a comparison with the baselines, all models were trained under R = 20, K = 40, and N = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Baseline architectures. Left: Vanilla LSTM: reads entire text sequentially. Middle: LSTMJump: skims to neglect some words. Right: Bi-direction LSTM-Jump: skims in two directions, concatenates two latest hidden states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average total shuttle steps</figDesc><graphic url="image-3.png" coords="7,329.10,62.81,174.60,113.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average steps per shuttle</figDesc><graphic url="image-5.png" coords="7,329.10,619.30,174.61,120.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example 1 for R = 1, K = 15, and N = 4, where read sentences are shown in bold</figDesc><graphic url="image-6.png" coords="8,77.46,62.81,207.36,267.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example 2 for R = 1, K = 15, and N = 4, where read sentences are shown in bold</figDesc><graphic url="image-7.png" coords="8,318.19,62.81,196.43,167.02" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present LSTM-Shuttle to use human speed reading techniques for text comprehension. In ad-dition to reading forward and skimming over text to accelerate, LSTM-Shuttle goes back to recover lost information or double-check its grasp of the text's meaning. We evaluate LSTM-Shuttle on sentiment analysis, news classification, and cloze on IMDB, Rotten Tomatoes, AG, and Children's Book Test datasets. We show that LSTM-Shuttle predicts better on all datasets with higher effi-ciency. We also analyze LSTM-Shuttle's behavior under different shuttle step restrictions, and pro-vide case studies that reveal the specific shuttle op-erations; these show how the model comprehends the context to achieve specific goals.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paszke</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gross</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chintala</forename><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanan</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devito</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zeming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmaison</forename><surname>Alban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antiga</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerer</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gutierrez, and Krys Kochut. 2017. Text summarization techniques: A brief survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Allahyari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyedamin</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Assefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">D</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1707.02268</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open knowledge extraction through compositional language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenhart</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Semantics in Text Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional neural net-works for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maas</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daly Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pham</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potts</forename><surname>Christopher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deal or no deal? end-to-end learning of negotiation dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coherent dialogue with attention-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural speed reading via skimrnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A simple and effective approach to the story cloze test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddarth</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Riedl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1803.05547</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging with bidirectional long short-term memory recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to skim text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongrae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast and accurate text classification: Skimming, rereading and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning for sentiment analysis : A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07883</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
