<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translation Invariant Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejun</forename><surname>Huang</surname></persName>
							<email>huang663@umn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Papalexakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
							<email>christos@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Sidiropoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
							<email>tom.mitchell@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Indian Institute of Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Minnesota</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">University of Minnesota</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit6">University of Minnesota</orgName>
								<orgName type="institution" key="instit7">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Translation Invariant Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work focuses on the task of finding latent vector representations of the words in a corpus. In particular, we address the issue of what to do when there are multiple languages in the corpus. Prior work has, among other techniques, used canonical correlation analysis to project pre-trained vectors in two languages into a common space. We propose a simple and scal-able method that is inspired by the notion that the learned vector representations should be invariant to translation between languages. We show empirically that our method outperforms prior work on multilingual tasks, matches the performance of prior work on monolingual tasks, and scales linearly with the size of the input data (and thus the number of languages being embedded).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing words as vectors in some latent space has long been a central idea in natural lan- guage processing. The distributional hypothesis, perhaps best stated as "You shall know a word by the company it keeps" <ref type="bibr" target="#b4">(Firth, 1957)</ref>, has had a long and productive history, as well as a recent revival in neural-network-based models ( <ref type="bibr" target="#b8">Mikolov et al., 2013</ref>). These methods generally construct a word by context matrix, then either use the vectors di- rectly (often weighted by term frequency and in- verse document frequency), perform some factor- * These authors contributed equally. ization of the matrix, or use it as input to a neu- ral network which produces vectors for each word. The resultant vectors can be used in a wide array of tasks, from information retrieval to part-of-speech tagging and parsing.</p><p>There has also been some recent work address- ing how to create these vectors when informa- tion from multiple languages is available. Two recent attempts involve using canonical correla- tion analysis (CCA) to project pre-trained vec- tors from each of two languages into a common space <ref type="bibr" target="#b3">(Faruqui and Dyer, 2014b)</ref> and using an alignment matrix to heuristically project the vec- tors from one language onto the words in another language ( <ref type="bibr" target="#b7">Guo et al., 2015</ref>). These methods gener- ally only work with two languages at a time, how- ever.</p><p>In this paper, we introduce a technique for con- structing multilingual word embeddings that is in- spired by the notion of translational invariance. CCA and the heuristic projection mentioned above both attempt to construct vectors such that words that are translations of each other are close in the vector space, but the method we introduce formal- izes this as part of the objective function of the original decomposition. We further show how to optimize this objective function with a method that scales linearly in the size of the input data. This results in a scalable, single-step method that is in- formed by both the monolingual corpus statistics and the multilingual alignment data. We show ex- perimentally that this results in vectors that outper- form prior work on multilingual tasks and match the performance of prior work on monolingual tasks.</p><p>The contributions of this paper are the follow- ing:</p><p>• Problem formulation: we formalize the no- tion of translation-invariance, regardless of the number of languages, as part of the ob- jective function of a standard matrix decom- position; • Scalable algorithm: we introduce scalable means of optimizing this augmented objec- tive functions; and • Effectiveness: we present state-of-the-art re- sults on a multilingual task using the vectors obtained by these methods. The code and data used in this paper are pub- licly available at https://sites.google. com/a/umn.edu/huang663/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>The informal problem definition is the following:</p><p>Informal Problem. Given a set of cooccurrence statistics between words in each of several lan- guages, and a translation table containing align- ment counts between words in each of these lan- guages, Find a latent representation for each word in each language that (1) captures information from the cooccurrence statistics and (2) is invari- ant to translations of the cooccurrence statistics between languages.</p><p>More formally, suppose we have M 1 words and N 1 contexts in the first language ("English"), and M 2 and N 2 for the second language ("Spanish"). Then, we are given two matrices of cooccurrence statistics (one for each language), with dimensions M 1 × N 1 and M 2 × N 2 , and two dictionary matri- ces containing translations from English to Span- ish, and from Spanish to English, repesctively. A more detailed description on how the data is ob- tained can be found in <ref type="bibr" target="#b3">(Faruqui and Dyer, 2014b</ref>).</p><p>For simplicity in what follows, we denote these matrices as</p><p>• X: a single multilingual cooccurrence matrix (with all the M 1 + M 2 words as the rows, and N 1 + N 2 contexts as columns). Entries in this matrix specify the cooccurrence between a word in any language and a context in any language.</p><p>• D 1 : a word dictionary matrix (with all the M 1 + M 2 English and Spanish words as both rows and columns). Entries in this ma- trix specify which words are translations of which other words, and is generally block- normalized, so that (e.g.) each Spanish word has a probability distribution over English words.</p><p>• D 2 : a context dictionary matrix (with all the N 1 +N 2 English and Spanish contexts as both rows and columns). This is similar to D 1 in its construction. We seek decompositions of X that are invari- ant to multiplications along each mode by its re- spective D matrix. Note that, while we only de- scribed the case where we have two languages, it is straightforward to extend this to having many languages in the combined X, D 1 and D 2 matri- ces, and we do this in some of the experiments described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Translation-invariant LSA</head><p>Without the side information provided by the dic- tionary matrices, the classic method for generat- ing word vectors finds a low-rank decomposition of the data matrix X:</p><formula xml:id="formula_0">min U,V X − UV T 2 F .</formula><p>With proper scaling (see our discussion in §4.2), the rows of U (or rows of V) are the word embed- dings (or "context embeddings"). It is well-known that the solution is given by the principal compo- nents of the singular value decomposition (SVD) of X. Generating word embeddings in this way is known as latent semantic analysis (LSA) <ref type="bibr" target="#b1">(Deerwester et al., 1990</ref>). Our method extends LSA to incorporate infor- mation from many languages at a time, with the constraint that the decomposition should be invari- ant to translation between these languages. We call this method translation-invariant LSA (TI- LSA).</p><p>In order to take the dictionary matrices D 1 and D 2 into consideration, we propose to seek a de- composition that can simultaneously explain the original matrix X and various translations of it. We can formalize this in the following objective function:</p><formula xml:id="formula_1">min U,V X − UV T 2 F + D 1 X − UV T 2 F + (1) XD T 2 − UV T 2 F + D 1 XD T 2 − UV T 2 F</formula><p>. By expanding and combining all four quadratic terms, we can see that the above problem is equiv- alent to (up to a constant difference)</p><formula xml:id="formula_2">min U,V ˜ X − UV T 2 F ,<label>(2)</label></formula><formula xml:id="formula_3">where˜X where˜ where˜X = 1 4 X + D 1 X + XD T 2 + D 1 XD T 2 = 1 4 (I + D 1 )X(I + D 2 ) T .</formula><p>Taking the SVD of˜Xof˜ of˜X does not seem numeri- cally appealing at first glance: even though D 1 , D 2 , and X are all very sparse, forming˜Xforming˜ forming˜X explic- itly will introduce a significant amount of nonze- ros. However, as we will explain below, it is not necessary to explicitly form˜Xform˜ form˜X in order to find a few principal components of it.</p><p>We propose to use the Lanczos algorithm (Golub and Van Loan, 1996, Chapter 9) to calcu- late the SVD of˜Xof˜ of˜X. The Lanczos method can be viewed as a generalization of the power method for computing an arbitrary number of principal components, and the basic operation required is only matrix-vector multiplication. For our prob- lem specifically, the required matrix-vector multi- plications˜Xµplications˜ plications˜Xµ and˜Xand˜ and˜X T ν can be carried out very efficiently with three sparse matrix-vector multi- plications, each with complexity linear in the num- ber of nonzeros in the sparse matrix involved, so that any dense intermediate matrix is avoided. As a result, by using our implementation of the Lanc- zos method, the time required for calculating the SVD of˜Xof˜ of˜X is not much more than that of X, even though˜Xthough˜ though˜X is significantly denser than X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present three experiments to evaluate the method introduced in this paper. The first experi- ment uses our word embeddings in a cross-lingual dependency parsing task; the second experiment looks at monolingual (English) performance on a series of word-similarity tasks; and the final ex- periment shows the scalability of our method by applying it to multiple languages. <ref type="bibr" target="#b7">Guo et al. (2015)</ref> recently introduced a method for using multilingual word embeddings to perform cross-lingual dependency parsing. They train a neural-network-based dependency parsing model using word vectors from one language, and then test the model using data and word vectors from another language. They used the embeddings ob- tained by <ref type="bibr" target="#b3">Faruqui and Dyer (2014b)</ref>, along with a heuristic projection. Because we used the same</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-lingual evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding method LAS UAS CCA (Faruqui &amp; Dyer) 60.7 69.8 Projection (Guo et al.) 61.3 71.1 TI-LSA</head><p>62.8 72.5 <ref type="table">Table 1</ref>: Labeled and unlabeled attachment score (LAS/UAS) on a cross-lingual dependency task. TI-LSA out- performs prior work on this task. data to obtain our embeddings, our method is di- rectly comparable to the CCA method of Faruqui and Dyer, and the projection method of Guo et al.</p><p>We used code and data graciously provided by Guo to run experiments, training a dependency parsing model on their English treebank, and test- ing it on the Spanish treebank. We report the re- sults below for the methods used by Guo et al. and the method introduced in this paper. We could not exactly reproduce Guo's result with the code we were provided, so we report all results from our use of the provided code, in case some parame- ter settings are different from those used in Guo's paper. The results are shown in <ref type="table">Table 1</ref>. As can be seen in the table, our first method for obtain- ing multilingual embeddings outperforms both the CCA method of Faruqui and Dyer, and the heuris- tic projection used by Guo et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Monolingual evaluation</head><p>While our focus is on generating embeddings that are invariant to translations (and thus most suited to multi-or cross-lingual tasks), we would hope that the addition of multiple languages would not hurt performance on monolingual tasks. We used wordvectors.org <ref type="bibr" target="#b2">(Faruqui and Dyer, 2014a)</ref> to evaluate our learned vectors on a variety of English-language word similarity tasks. The tasks are mostly all variations on performing word simi- larity judgments, finding the correlation between the system's output and human responses. We used the same data as that used by Faruqui and Dyer (2014b) (English-Spanish only), and thus our method for obtaining multilingual embeddings is directly comparable to their technique for do- ing the same (CCA). We used the first 11 tasks on wordvectors.org, and obtained Faruqui and Dyer's results from that website. Due to space con- straints, we only report the average performance across these 11 tasks for each of the methods we tested. The results are shown in <ref type="table">Table 2</ref>. To test statistical significance, we performed a paired per- mutation test, treating performance on each task as Method Average Correlation CCA <ref type="bibr">(Faruqui &amp; Dyer)</ref> 0.638 LSA 0.626 TI-LSA 0.628 <ref type="table">Table 2</ref>: Average correlation with human similarity judg- ments on 11 word-similarity tasks. The differences between these methods are not statistically significant, showing that the gains we see in cross-lingual tasks are not at the expense of monolingual tasks.</p><p>paired data. The important thing to note from the table is that the differences between the methods are all quite small, and none of them are statisti- cally significant. Note that LSA on just the English data performs on par with all of the other methods presented; we have not found a way to improve performance on this monolingual task from using multilingual data. 1 However, it is also important to note that our multilingual methods do not hurt performance on these monolingual tasks, either-we get the benefits described in our other evaluations without losing performance on English-only tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scalability</head><p>We mentioned in Section 3 that our method is lin- ear in the number of nonzeros in the data, as we are simply using the Lanczos algorithm to com- pute a sparse SVD. To show this in practice, we briefly present how the running time of our algo- rithm scales with the number of languages used. Each additional language adds roughly the same amount of data to the X matrix. <ref type="figure">Figure 1</ref> shows that our method does indeed scale linearly with the number of nonzeros in the matrix, and thus also with the number of languages used (assuming each language has roughly the same amount of data). All the experiments are performed in MATLAB 2013a on a Linux server with 32 Xeon 2.00GHz cores and 128GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We discuss here two points on the flexibility of the method we have introduced. First, note that the dictionary matrices we used contained infor- 1 This is in contrast to the results reported by Faruqui and Dyer, who by our evaluation also do not improve perfor- mance using multilingual data. To obtain word vectors from our decomposition, we used only the U component of the SVD; including the singular values, as Faruqui and Dyer did, gives worse performance. We confirmed this with the au- thors, and replicated their result for English-only LSA when using the singular values. English-Spanish-French-German <ref type="figure">Figure 1</ref>: TI-LSA is linear in the number of nonzeros in the data matrices, and can easily scale to many languages.</p><p>mation about translations between languages. It is also possible to include information about para- phrases in this dictionary. For instance, a resource such as the Paraphrase Database ( <ref type="bibr" target="#b5">Ganitkevitch et al., 2013)</ref> could be used to further constrain the embeddings obtained; this could be useful if the resource used to obtain a paraphrase dictionary contained more or different information than the corpus statistics used in the decomposition. Second, note that we have two dictionaries, one for the words and one for the contexts. These dictionaries correspond to the modes of the ma- trix; we have one dictionary matrix per mode, and we always multiply the dictionary along its corre- sponding mode. It would be easy to extend this method to a setting where the data is a 3-mode tensor instead of a matrix, e.g., if the data were (subject, verb, object) triples, or relation triples in some knowledge base. In these settings, the dictio- naries used for each mode might be more different; in the subject-verb-object example, one of the dic- tionaries would only have verbs, while the other two would only have nouns, for instance. Stan- dard tensor decompositions could be augmented with a translation-invariance term, similar to what we have done with matrices in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The most closely related work is that of <ref type="bibr" target="#b3">Faruqui and Dyer (2014b)</ref>, whose CCA-based method we have already mentioned; however, it is not obvi- ous how CCA-based methods can be applied to more than two languages at a time. Our work is also similar to prior work on multilingual latent semantic analysis; <ref type="bibr" target="#b0">Bader and Chew (2008)</ref> also include a translation dictionary when decompos- ing the X matrix, though their formulation uses a term-document matrix instead of a word-context matrix, and the way they use the translation dic- tionary is quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a new technique for gener- ating word embeddings from multilingual cor- pora. This technique formalizes the notion of translation invariance into the objective function of the matrix decomposition and provides flexi- ble and scalable means for obtaining word vec- tors where words that are translations of each other are close in the learned vector space. Through three separate evaluations, we showed that our technique gives superior performance on multilin- gual tasks, matches prior work on monolingual tasks, and scales linearly in the size of the input data. The code and data used in this paper are available at https://sites.google.com/ a/umn.edu/huang663/.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Research was supported by the National Science Foundation Grant No. IIS-1247489, IIS-1247632, and a gift from Google. This work was also sup-ported in part by a fellowship to Kejun Huang from the University of Minnesota Informatics In-stitute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding parties.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhancing multilingual latent semantic analysis with term alignment information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Deerwester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Community evaluation and exchange of word vectors at wordvectors.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Faruqui and Dyer2014b</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory 1930-1955</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Linguistic Analysis</title>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ppdb: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ganitkevitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Loan1996] Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computations</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
