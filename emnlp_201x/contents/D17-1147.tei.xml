<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Data Selection for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlies</forename><surname>Van Der Wees</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Informatics Institute University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">LIACS Leiden University</orgName>
								<orgName type="institution" key="instit3">Informatics Institute University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Informatics Institute University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">LIACS Leiden University</orgName>
								<orgName type="institution" key="instit3">Informatics Institute University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Informatics Institute University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">LIACS Leiden University</orgName>
								<orgName type="institution" key="instit3">Informatics Institute University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Data Selection for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1400" to="1410"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Ax-elrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine transla- tion (NMT) <ref type="bibr" target="#b36">(Sutskever et al., 2014;</ref>) as the most common machine translation paradigm. With large quan- tities of parallel data, NMT outperforms PBMT for an increasing number of language pairs <ref type="bibr">(Bojar et al., 2016)</ref>. Unfortunately, training an NMT model is often a time-consuming task, with train- ing times of several weeks not being unusual.</p><p>Despite its training inefficiency, most work in NMT greedily uses all available training data for a given language pair. However, it is unlikely * Work done while at University of Amsterdam that all data is equally helpful to create the best- performing system. In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality <ref type="bibr" target="#b31">(Moore and Lewis, 2010;</ref><ref type="bibr" target="#b0">Axelrod et al., 2011;</ref><ref type="bibr" target="#b18">Gascó et al., 2012)</ref>. Instead, for a given translation task, the training bitext likely contains sentences that are irrelevant or even harmful, making it ben- eficial to keep only the most relevant subset of the data while discarding the rest, with the additional benefit of smaller models and faster training.</p><p>Motivated by the success of data selection in PBMT, we investigate in this paper to what ex- tent and how NMT can benefit from data selec- tion as well. While data selection has been ap- plied to NMT to reduce the size of the data ( <ref type="bibr" target="#b29">Luong et al., 2015b</ref>), the effects on translation quality have not been investigated. In- tuitively, and confirmed by our exploratory exper- iments in Section 5.1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data ( <ref type="bibr">Zoph et al., 2016;</ref><ref type="bibr" target="#b14">Fadaee et al., 2017)</ref>, and do not have a separate large-scale target-side language model to compen- sate for smaller parallel training data.</p><p>To alleviate the negative effect of small training data on NMT, we introduce dynamic data selec- tion. Following conventional data selection, we still dramatically reduce the training data size, fa- voring parts of the data which are most relevant to the translation task at hand. However, we exploit the fact that the NMT training process iterates over the training corpus in multiple epochs, and we al- ter the quantity or the composition of the training data between epochs. The proposed method re- quires no modifications to the NMT architecture or parameters, and substantially speeds up training times while simultaneously improving translation quality with respect to a complete-bitext baseline.</p><p>In summary, our contributions are as follows: (i) We compare the effects of a commonly used data selection approach <ref type="bibr" target="#b0">(Axelrod et al., 2011</ref>) on PBMT and NMT using four different test sets. We find that this method is much less effective for NMT than for PBMT, while using the exact same training data subsets.</p><p>(ii) We introduce dynamic data selection as a way to make data selection profitable for NMT. We explore two techniques to alter the selected data subsets, and find that our method called grad- ual fine-tuning improves over conventional static data selection (up to +2.6 BLEU) and over a high-resource general baseline (up to +3.1 BLEU). Moreover, gradual fine-tuning approximates in- domain fine-tuning in ∼20% of the training time, even when no parallel in-domain data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Static data selection</head><p>As a first step towards dynamic data selection for NMT, we compare the effects of a commonly used, state-of-the-art data selection method (Axel- rod et al., 2011) on both neural and phrase-based MT. Briefly, this approach ranks sentence pairs in a large training bitext according to their difference in cross-entropy with respect to an in-domain cor- pus (i.e., a corpus representing the test data) and a general corpus. Next, the top n sentence pairs with the highest rank-thus lowest cross-entropy-are selected and used for training an MT system.</p><p>Formally, given an in-domain corpus I, we first create language models from the source side f of I (LM I,f ) and the target side e of I (LM I,e ). We then draw a random sample (similar in size to I) of the large general corpus G and create language mod- els from the source and target sides of G: LM G,f and LM G,e , respectively. Note that the data for creating these LMs need not be parallel but can be independent corpora in both languages.</p><p>Next, we compute for each sentence pair s in G four cross-entropy scores, defined as:</p><formula xml:id="formula_0">H C,s b = − p (s b ) log LM C,b (s b ) ,<label>(1)</label></formula><p>where C ∈ {I, G} is the corpus, b ∈ {f, e} refers to the bitext side, and s b is the bitext side b of sen- tence pair s in the parallel training corpus.</p><p>To find sentences that are similar to the in- domain corpus, i.e., have low H I , and at the same time dissimilar to the general corpus, i.e., have high H G , we compute for each sentence pair s the bilingual cross-entropy difference CED s fol- lowing <ref type="bibr" target="#b0">Axelrod et al. (2011)</ref>:</p><formula xml:id="formula_1">CED s = (H I,s f −H G,s f )+(H I,se −H G,se ). (2)</formula><p>Finally, we rank all sentence pairs s ∈ G accord- ing to their CED s , and then select only the top n sentence pairs with the lowest CED s .</p><p>Following related work by <ref type="bibr" target="#b31">Moore and Lewis (2010)</ref>, we restrict the vocabulary of the LMs to the words occurring at least twice in the in-domain corpus. To analyze the quality of the selected data subsets, we also run experiments on random se- lections, all performed in threefold. Finally, we always use the exact same selection of sentence pairs in equivalent PBMT and NMT experiments.</p><p>LSTM versus n-gram The described data se- lection method uses n-gram LMs to determine the domain-relevance of sentence pairs. We adhere to this setting for our comparative experiments on PBMT and NMT (Section 5.1). However, when applying data selection to NMT, we examine the potential benefit of replacing the conventional n- gram LMs with LSTMs <ref type="bibr">1</ref> . These have the advan- tage to remember longer histories, and do not have to back off to shorter histories when encountering out-of-vocabulary words. In this neural variant to rank sentences, the score for each sentence pair in G is still computed as the bilingual cross-entropy difference in Equation <ref type="bibr">(2)</ref>. In addition, we use the same in-domain and general corpora as with the n- gram method, and we again restrict the vocabulary to the most frequent words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic data selection</head><p>While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the 'long tail', which are major is- sues in NMT ( <ref type="bibr" target="#b29">Luong et al., 2015b;</ref><ref type="bibr" target="#b35">Sennrich et al., 2016b</ref>). In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios ( <ref type="bibr">Zoph et al., 2016;</ref><ref type="bibr" target="#b14">Fadaee et al., 2017;</ref><ref type="bibr" target="#b24">Koehn and Knowles, 2017)</ref>.</p><p>To overcome this problem, we introduce dy- namic data selection, in which we vary the se- lected data subsets during training. Unlike other MT paradigms, which require training data to be fixed during the entire training process, NMT it- erates over the training corpus in several epochs,  <ref type="figure">Figure 1</ref>: Illustration of two dynamic bitext selection techniques for NMT: sampling (left) and gradual fine-tuning (right). Measured over 16 training epochs (which is used in this work), the total training time of both examples would be ∼30% of the training time needed when using the complete bitext.</p><p>allowing to use a different subset of the training data in every epoch.</p><p>Dynamic data selection starts from a relevance- ranked bitext, which we create using CED scores as computed in Equation (2). Given this ranking, we investigate two dynamic data selection tech- niques 2 that vary per epoch the composition or the size of the selected training data. Both techniques aim to favor highly relevant sentences over less relevant sentences while not completely discard- ing the latter. In all experiments, we use a fixed vocabulary created from the complete bitext.</p><p>While we use in this work a domain-relevance ranking of the bitext following <ref type="bibr" target="#b0">Axelrod et al. (2011)</ref>, dynamic data selection can also be ap- plied using other ranking criteria, for example lim- iting redundancy in the training data ( <ref type="bibr" target="#b25">Lewis and Eetemadi, 2013)</ref> or complementing similarity with diversity <ref type="bibr" target="#b33">(Ruder and Plank, 2017)</ref>.</p><p>Sampling sentence pairs In the first technique, illustrated in <ref type="figure">Figure 1a</ref>, we sample for every epoch n sentence pairs from G, using a distribution computed from the domain-specific CED s scores. Concretely, this is done as follows:</p><p>First, since higher ranked sentence pairs have lower CED s scores, and they can be either nega- tive or positive, we scale and invert CED s scores such that 0 ≤ CED s ≤ 1 for each sentence pair s ∈ G:</p><formula xml:id="formula_2">CED s = 1 − CEDs − min(CEDG) max(CEDG) − min(CEDG) ,<label>(3)</label></formula><p>2 Code for bitext ranking and both selection techniques:</p><formula xml:id="formula_3">github.com/marliesvanderwees/dds-nmt.</formula><p>where CED G refers to the set of CED s scores for bitext G.</p><p>Next, we convert CED s scores to relative weights, such that s∈G w(s) = 1:</p><formula xml:id="formula_4">w(s) = CED s s i ∈G CED s i .<label>(4)</label></formula><p>We then use {w(s) : s ∈ G} to perform weighted sampling, drawing for each epoch n sentence pairs without replacement. While all selection weights are very close to zero, higher ranked sentences have a noticeably higher probability of being se- lected than lower-ranked sentences; in practice we find that top-ranked sentences get selected in nearly each epoch, while bottom-ranked sentence pairs get selected at most once. Note that the sam- pled selection for any epoch is independent of se- lections for all other epochs.  <ref type="bibr" target="#b16">Onaizan, 2016)</ref>, in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data. However, rather than training a full model on the complete bitext G, we gradually decrease the training data size, starting from G and keeping only the top n sentence pairs for the duration of η epochs, where the top n pairs are defined by their CED s scores. Given its re- semblance to fine-tuning, we refer to this variant as gradual fine-tuning.</p><p>During gradual fine-tuning, the selection size n is a function of epoch i:</p><formula xml:id="formula_5">n(i) = α · |G| · β (i−1)/η .<label>(5)</label></formula><p>Here 0 ≤ α ≤ 1 is the relative start size, i.e., the fraction of general bitext G used for the first se- lection, 0 ≤ β ≤ 1 is the retention rate, i.e., the fraction of data to be kept in each new selection, and η ≥ 1 is the number of consecutive epochs each selected subset is used. Note that i/η + 1 indicates rounding down i/η + 1 to the nearest in- teger. For example, if we start with the complete bitext (α = 1), select the top 60% (β = 0.6) every second epoch (η = 2), then we run epochs 1 and 2 with a subset of size |G|, epochs 3 and 4 with a subset of size 0.6 · |G|, epochs 5 and 6 with a subset of size 0.36 · |G|, and so on. For every size n, the actual selection contains the top n sentences pairs of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental settings</head><p>We evaluate static and dynamic data selection on a German→English translation task comprising four test sets. Below we describe the MT systems and data specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Machine translation systems</head><p>While the main aim of this paper is to improve data selection for NMT, we also perform comparative experiments using PBMT. Our PBMT system is an in-house system similar to Moses ( <ref type="bibr">Koehn et al., 2007)</ref>. To create optimal PBMT systems given the available resources, we apply test-set-specific parameter tuning using PRO ( <ref type="bibr" target="#b19">Hopkins and May, 2011</ref> For our NMT experiments we use an in-house encoder-decoder 3 model with global attention as described in <ref type="bibr" target="#b28">Luong et al. (2015a)</ref>. This choice comes at the cost of optimal translation quality but allows for a relatively fast realization of large- scale experiments given our available resources. Both the encoder and decoder are four-layer unidi- rectional LSTMs, with embedding and layer sizes <ref type="bibr">3</ref> github.com/ketranm/tardis of 1,000. We uniformly initialize all parameters, and use SGD with a mini-batch size of 64 and an initial learning rate of 1, which is decayed by a factor two every epoch after the fifth epoch. We use dropout with probability 0.3, and a beam size of 12. We train for 16 epochs and test on the model from the last epoch. All NMT experiments are run on a single NVIDIA Titan X GPU.   <ref type="table" target="#tab_3">Table 1</ref>. The in-domain LMs used to rank training sen- tences for data selection are trained on small por- tions of in-domain parallel data whenever avail- able (3.3M, 1.2M and 3.3M German tokens for EMEA, Movies and TED, respectively). Since no sizeable in-domain parallel text is available for WMT, we independently sample 200K sentences from the WMT monolingual News Crawl corpora (3.3M German tokens or 3.5M English tokens). This demonstrates the applicability of data selec- tion techniques even in cases where one lacks par- allel in-domain data.</p><p>Before running data selection, we preprocess our data by tokenizing, lowercasing and remov-  For our NMT experiments we use BPE- processed corpora on both bitext sides, while for PBMT we only apply BPE to the German side. Our NMT systems use a vocabulary size of 40K on both the source and target side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Below we discuss the results of our translation ex- periments using static and dynamic data selection, measuring translation quality with case-insensitive untokenized BLEU ( <ref type="bibr" target="#b32">Papineni et al., 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Static data selection for PBMT and NMT</head><p>We first compare the effects of static data selec- tion with n-gram LMs on both NMT and PBMT using various selection sizes. Concretely, we se- lect the top n sentence pairs such that the number of selected tokens t ∈ 5%, 10%, 20%, 50% of G, or t = |I| (the in-domain corpus size). <ref type="figure" target="#fig_2">Fig- ure 2</ref> shows German→English translation perfor- mance in BLEU for our four test sets. The benefits of n-gram-based data selection for PBMT (purple circles) are confirmed: In all test sets, the selec- tion of size |I| (dotted vertical line) yields better performance than using only the in-domain data of the exact same size (purple star), and at least one of the selected subsets-often using only 5% of the complete bitext-outperforms using the com- plete bitext (light purple line). We also show that the informed selections are superior to random se- lections of the same size (purple diamonds).</p><p>In NMT, results of n-gram-based data selection (green triangles) vary: While for Movies a selec- tion of only 10% outperforms the complete bitext (light green line), none of the selected subsets for other test sets is noticeably better than the full bi- text. <ref type="bibr">4</ref> Interestingly, the same selections of size |I| that proved useful in PBMT, never beat the sys- tem that uses exactly the available in-domain data (green star), indicating that the current selections can be further improved for NMT. In all scenarios we see that NMT suffers much more from small- data settings than PBMT. Finally, the random se-lections (green squares) show that NMT not only needs large quantities of data, but it is also affected when the selected data is of low quality. In PBMT, both low-quantity and low-quality scenarios ap- pear to be compensated for by the large monolin- gual LM on the target side.</p><p>When comparing the different test sets, we observe that the impact of domain mismatch in NMT with respect to PBMT is largest for the two domains that are most distinct from the gen- eral bitext, EMEA and Movies. For WMT, both MT systems achieve very similar baseline results, but translation quality deteriorates considerably in data selection experiments, which is likely caused by the lack of in-domain data in the general bitext.</p><p>LSTM versus n-gram Before proceeding with dynamic data selection for NMT, we test whether bitext ranking for NMT can be improved using LSTMs rather than conventional n-gram LMs. Ta- ble 2 shows NMT BLEU scores of a few differ- ent sizes of selected subsets created using n-gram LMs or LSTMs. While results vary among test sets and selection sizes, we observe an average im- provement of 0.4 BLEU when using LSTMs in- stead of n-gram LMs. For PBMT, similar results have been reported when replacing n-gram LMs with recurrent neural LMs ( <ref type="bibr" target="#b11">Duh et al., 2013</ref>  <ref type="table">Table 2</ref>: NMT BLEU comparison between using n-gram LMs and LSTMs for bitext ranking. Selec- tion sizes concern the selected bitext subsets; LMs are created from the exact same in-domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dynamic data selection for NMT</head><p>Equipped with a relevance ranking of sentence pairs in bitext G, we now examine two variants of dynamic data selection as described in Section 3. We are interested in reducing training time while limiting the negative effect on BLEU for various domains. Therefore we report BLEU as well as the relative training time of each exper- iment. Since wall-clock times depend on other factors such as the NMT architecture and memory speed, we define training time as the total number of tokens observed while training the NMT sys- tem, i.e., the sum of tokens in the selected subsets of all epochs. We report all training times relative to the training time of our complete-bitext base- line (i.e., 4.3M tokens × 16 epochs). Note that this measure of training time corresponds closely but not exactly to the number of model updates, as the latter relies on the number of sentences, which vary in length, rather than the number of tokens in the training data. For completeness: Training the 100% baseline takes 106 hours, while our fastest dynamic selection variant takes 19-21 hours. Computing CED scores takes ∼15 minutes when using n-gram LMs and 5-6 hours when us- ing LSTMs. <ref type="figure" target="#fig_4">Figure 3</ref> shows BLEU scores of some selected experiments as a function of relative training time. Compared to static data selection (blue lines), our weighted sampling technique (orange triangles) yields variable results. When sampling a subset of 20% of |G| from the top 50% of the ranked bi- text, we obtain small improvements for TED and WMT, but small drops for EMEA and Movies. Other selection sizes (30% and 40%, not shown) give similar results lacking a consistent pattern.</p><p>By contrast, our gradual fine-tuning method performs consistently better than static selection, and even beats the general baseline in three out of four test sets. The displayed version uses settings (α = 0.5, β = 0.7, η = 2) and is at least as fast as static selection using 20% of the bitext, yielding up to +2.6 BLEU improvement (for WMT news) over this static version. Compared to the com- plete baseline, this gradual fine-tuning method im- proves up to +3.1 BLEU (for TED talks). <ref type="table" target="#tab_5">Table 3</ref> provides detailed information on ad- ditional experiments using other settings. For all three test domains which are covered in the parallel data-EMEA, Movies and TED- improvements are highest when starting gradual fine-tuning with only the top 50% of the ranked bitext, which are also the fastest approaches. For WMT, which is not covered in the general bi- text, adding more data clearly benefits translation quality. These findings are consistent with the static data selection patterns; Using low-ranked sentences on top of the most relevant selection    <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>does not improve translation performance for any domain except WMT news. Finally, we compare our data selection exper- iments to domain-specific fine-tuning (light blue stars in <ref type="figure" target="#fig_4">Figure 3)</ref>, which is the current state-of-the- art for domain adaptation in NMT. To this end, we first train a model on the complete bitext, and then train for twelve additional epochs on available in- domain data, using an initial learning rate of 1 which halves every epoch. Depending on the test set, this approach yields +2.5-4.4 BLEU improve- ments over our baselines, however it does not speed up training and requires a parallel in-domain text which may not be available (e.g., for WMT). While none of our data selection experiments out- performs domain-specific fine-tuning, we obtain competitive translation quality in only 20% of the training time. In additional experiments we found that in-domain fine-tuning on top of our selection approaches does not yield improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Further analysis</head><p>In this section we conduct a few additional exper- iments and analyses. We restrict to one parameter setting per selection approach: Static selection and sampling with 20% of the data, and gradual fine- tuning using (α = 0.5, β = 0.7, η = 2). All have very similar training times.</p><p>First, we hypothesize that dynamic data selec- tion works well because more different sentence pairs are observed during training, and it therefore increases coverage with respect to static data se- lection. To verify this, we measure for each test set the number of unseen source word types in the training data of different selection methods. <ref type="figure" target="#fig_5">Fig- ure 4</ref> shows indeed that the average number of unseen word types is reduced noticeably in both of our dynamic selection techniques, being much closer to the complete bitext baseline than to static selection. Note that all methods use the same vo- cabulary during training. Next, following the static data selection exper- iments in Section 5.1, we examine how well dy- namic data selection performs using random selec- tions. To this end, we repeat all techniques using a bitext which is ranked randomly rather than by its relevance to the test sets. The results in <ref type="table">Table 4</ref> show that the bitext ranking plays a crucial role in the success of data selection. However, the results also show that even in the absence of an appropri- ate bitext ranking, dynamic data selection-and in particular gradual fine-tuning-is still superior to static data selection. We explain this result as fol- lows: Compared to static selection, both sampling and gradual fine-tuning have better coverage due to their improved exploration of the data. How- ever, sampling also suffers from a surprise effect of observing new data in every epoch. Gradual fine-tuning on the other hand gradually improves learning on a subset of the selected data, suggest- ing that repetition across epochs has a positive ef- fect on translation quality.  <ref type="table">Table 4</ref>: BLEU scores of data selection using rel- evance versus random ranking of the bitext. Grad- ual fine-tuning uses (α = 0.5, β = 0.7, η = 2), with relative training times of 18-20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMEA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Movies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking</head><p>One could expect that changing the data during training results in volatile training behavior. To test this, we inspect cross-entropy of our devel- opment sets after every training epoch. <ref type="figure" target="#fig_6">Figure 5</ref> shows these results for TED. Clearly, static data selection converges most steadily. However, both dynamic selection techniques eventually converge to a lower cross-entropy value which is reflected by higher translation quality of the test set. We ob- serve very similar behavior for the other test sets. By its nature, our gradual fine-tuning technique uses training epochs of different sizes, and there- fore also implicitly differs from other methods in its parameter optimization behavior. Since we decrease both the training data size and the SGD learning rate after finishing complete train- ing epochs, we automatically decay the learning rate at decreasing time intervals. We therefore study how this approach is affected when we (i) decay the learning rate after a fixed number of updates (i.e., the same as in static data selection) rather than per epoch, or (ii) keep the learning rate fixed. In the first scenario, we observe that trans- lation performance drops with -1.1-2.0 BLEU. When keeping a fixed learning rate, BLEU scores hardly change or even improve, indicating that the implicit change in search behavior may contribute to the success of gradual fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>A few research topics are related to our work. Regarding data selection for SMT, previous work has targeted two goals; to reduce model sizes and training times, or to adapt to new domains. Data selection methods for domain adaptation mostly employ information theory metrics to rank train- ing sentences by their relevance to the domain at hand. This has been applied monolingually ( <ref type="bibr" target="#b17">Gao et al., 2002</ref>) as well as bilingually ( <ref type="bibr">Yasuda et al., 2008)</ref>. In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data <ref type="bibr" target="#b31">(Moore and Lewis, 2010;</ref><ref type="bibr" target="#b0">Axelrod et al., 2011</ref><ref type="bibr" target="#b1">Axelrod et al., , 2015</ref>, favoring sentences that are similar to the test domain and at the same time dissimi- lar from the general domain. <ref type="bibr" target="#b11">Duh et al. (2013)</ref> and <ref type="bibr" target="#b7">Chen and Huang (2016)</ref> present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively.</p><p>Data selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still maintaining high vocabulary coverage <ref type="bibr" target="#b12">(Eck et al., 2005;</ref><ref type="bibr" target="#b18">Gascó et al., 2012;</ref><ref type="bibr" target="#b25">Lewis and Eetemadi, 2013)</ref>. In a com- parative study, <ref type="bibr" target="#b30">Mirkin and Besacier (2014)</ref> find that similarity-objected methods perform best if the test domain and general corpus are very differ- ent, while a coverage-objected method is superior if test and general corpus are relatively similar. A comprehensive survey on data selection for SMT is provided by <ref type="bibr" target="#b13">Eetemadi et al. (2015)</ref>. While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective.</p><p>In NMT, data selection can serve similar goals as in PBMT; increasing training efficiency or do- main adaptation. Domain adaptation in NMT typ- ically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus <ref type="bibr" target="#b27">(Luong and Manning, 2015;</ref><ref type="bibr">Zoph et al., 2016)</ref>. Other work combines fine-tuning with model ensembles <ref type="bibr">(Freitag and AlOnaizan, 2016</ref>) or with domain-specific tags in the training corpus ( <ref type="bibr" target="#b9">Chu et al., 2017)</ref>. Finally, Sen- nrich et al. (2016a) adapt their systems by back- translating in-domain data, which is then added to the training data and used for fine-tuning.</p><p>Some other previous work has addressed train- ing efficiency for NMT, for example by paral- lelizing models or data ( <ref type="bibr">Wu et al., 2016)</ref>, modi- fying the NMT network structure <ref type="bibr" target="#b20">(Kalchbrenner et al., 2016)</ref>, decreasing the number of parame- ters through knowledge distillation ( <ref type="bibr" target="#b10">Crego et al., 2016;</ref><ref type="bibr" target="#b21">Kim and Rush, 2016)</ref>, or by boosting parts of the data that are 'challenging' to the NMT sys- tem ( <ref type="bibr">Zhang et al., 2016</ref>). The latter is most related to our work since training data is also adjusted dur- ing training, however we reduce the training data size much more aggressively and study different techniques of data selection.</p><p>Finally, recent work comparing various aspects for PBMT and NMT includes ( <ref type="bibr" target="#b3">Bentivogli et al., 2016;</ref><ref type="bibr" target="#b15">Farajian et al., 2017;</ref><ref type="bibr">Toral and SánchezCartagena, 2017;</ref><ref type="bibr" target="#b24">Koehn and Knowles, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>With the recent increase in popularity of neural machine translation (NMT), we explored in this paper to what extent and how NMT can benefit from data selection. We first showed that a state- of-the-art data selection method yields unreliable results for NMT while consistently performing well for PBMT. Next, we have introduced dynamic data selection for NMT, which entails varying the selected subset of training data between different training epochs. We explored two techniques of dynamic data selection and found that our grad- ual fine-tuning technique, in which we gradu- ally reduce training size, improves consistently over conventional static data selection (up to +2.6 BLEU) and over a high-resource general base- line (up to +3.1 BLEU). Moreover, gradual fine- tuning approximates in-domain fine-tuning using only ∼20% of the training time, even when no par- allel in-domain data is available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PBMT (purple) and NMT (green) German→English results of Axelrod data selection and random data selection (average of three runs) for four domains. Purple and green stars indicate BLEU scores when only the available in-domain data is used. We use selections of the in-domain size |I|, and 5%, 10%, 20%, and 50% of the complete bitext, which are exactly the same for PBMT and NMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Selected German→English translation results of dynamic data selection methods (orange and red markers) compared to conventional static data selection (blue circles). Relative training time equals the total number of training tokens relative to the complete baseline, which takes 106 hours to train and is represented by the rightmost blue circle. Note that no parallel in-domain data is available for WMT news. All y-axes are scaled equally for easy comparison of BLEU differences across domains. Experiment Relative BLEU Start size Retention rate β Decrease every training time EMEA Movies TED WMT Static selection top 20% 20% 34.5 19.6 26.6 21.9 50% (α = 0.5) 0.7 η = 2 epochs 18-20% 36.1 (+1.6) 21.0 (+1.4) 29.1 (+2.5) 24.5 (+2.6) 50% (α = 0.5) 0.5 η = 4 epochs 21-23% 36.0 (+1.5) 21.2 (+1.6) 29.0 (+2.4) 25.0 (+3.1) 50% (α = 0.5) 0.6 η = 4 epochs 25-27% 35.6 (+1.1) 21.0 (+1.4) 28.5 (+1.9) 25.1 (+3.2) 100% (α = 1) 0.6 η = 2 epochs 29-31% 35.5 (+1.0) 21.1 (+1.5) 29.0 (+2.4) 25.6 (+3.7) 100% (α = 1) 0.7 η = 2 epochs 37-39% 35.9 (+1.4) 20.4 (+0.8) 28.2 (+1.6) 25.8 (+3.9) 100% (α = 1) 0.9 η = 1 epoch 50-52% 35.4 (+0.9) 19.6 (±0.0) 27.4 (+0.8) 26.1 (+4.2) Complete bitext baseline 100% 34.8 18.8 26.0 26.7 Gold: fine-tuning on in-domain data 101-103% 37.7 21.3 30.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test set source words not covered in the training data of different data selection methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: German→English cross-entropy of the TED dev set as a function of training time. Each data point represents a completed training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Data specifications with tokens counted 
on the German side. The WMT training cor-
pus contains Commoncrawl, Europarl, and News 
Commentary but no in-domain news data. 

4.2 Training and evaluation data 

We evaluate all experiments on four domains: (i) 
EMEA medical guidelines (Tiedemann, 2009), (ii) 
movie dialogues (van der Wees et al., 2016) con-
structed from OpenSubtitles (Lison and Tiede-
mann, 2016), (iii) TED talks (Cettolo et al., 
2012), and (iv) WMT news. For TED, we use 
IWSLT2010 as development set and IWSLT2011-
2014 as test set, and for WMT we use new-
stest2013 as development set and newstest2016 
as test set. We train our systems on a mixture 
of domains, comprising Commoncrawl, Europarl, 
News Commentary, EMEA, Movies, and TED. 
Corpus specifications are listed in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). In all subsequent experiments we use relevance rankings computed with LSTMs instead of n-gram LMs.</figDesc><table>Selection LM type EMEA Movies TED WMT 

5% 
n-gram 
29.8 
17.4 
22.6 
8.1 
LSTM 
30.0 
17.8 
22.6 
9.6 

10% 
n-gram 
33.0 
19.6 
24.5 
16.6 
LSTM 
33.0 
19.7 
24.7 
17.4 

20% 
n-gram 
34.8 
19.0 
25.6 
21.9 
LSTM 
34.5 
19.6 
26.6 
21.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>German→English BLEU results of various gradual fine-tuning experiments sorted by relative 
training time. Indicated improvements are with respect to static selection using 20% of the bitext, and 
highest scores per test set are bold-faced. Results from the first experiment are also shown in </table></figure>

			<note place="foot" n="1"> We use four-layer LSTMs with embedding and hidden sizes of 1,024, which we train for 30 epochs.</note>

			<note place="foot" n="4"> Validation cross-entropy converges after 10-12 epochs, never reaching the scores of the complete bitext.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded in part by NWO under project numbers 639.022.213 and 639.021.646. We thank Ke Tran for providing the NMT system, and the reviewers for their valuable comments.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Mari Ostendorf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
	<note>Data selection with fewer words</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural versus phrasebased machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Antonio Jimeno Yepes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<imprint>
			<pubPlace>Philipp Koehn, Varvara Logacheva</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Christof Monz</orgName>
		</respStmt>
	</monogr>
	<note>et al. 2016. Findings of the 2016 conference on machine translation (WMT16)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Proceedings of the first conference on machine translation (WMT16)</title>
		<meeting>the first conference on machine translation (WMT16)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wit 3 : Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 16 th Conference of the European Association for Machine Translation (EAMT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional networks for translation adaptation with tiny amount of in-domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical comparison of simple domain adaptation methods for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SYSTRAN&apos;s pure neural machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anabel</forename><surname>Rebollo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Akhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Brunelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Coquard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptation data selection using neural language models: Experiments in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="678" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low cost portability for statistical machine translation based on n-gram frequency and TF-IDF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 International Workshop on Spoken Language Translation</title>
		<meeting>the 2005 International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Survey of data-selection methods in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sauleh</forename><surname>Eetemadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayder</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="189" to="223" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural vs. phrase-based machine translation in a multi-domain scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amin Farajian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="280" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06897</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward a unified approach to statistical language modeling for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="33" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Does more data always yield better translations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Gascó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha-Alicia</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Sanchis-Trilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Andrés-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="152" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<pubPlace>Christine Moran</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03872</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dramatically reducing training data size through vocabulary saturation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sauleh</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eetemadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Statistical Machine Translation</title>
		<meeting>the 8th Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="281" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the 10th International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Spoken Language Translation</title>
		<meeting>the 12th International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data selection for compact adapted SMT models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Mirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the 11th Conference of the Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="301" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to select data for transfer learning with bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">News from OPUS-a collection of multilingual parallel corpora with tools and interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent advances in natural language processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
