<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Clova AI</orgName>
								<address>
									<settlement>NAVER</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Clova AI</orgName>
								<address>
									<settlement>NAVER</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">XNOR.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Clova AI</orgName>
								<address>
									<settlement>NAVER</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Language</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="559" to="564"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>559</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We formalize a new modular variant of current question answering tasks by enforcing com</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extractive question answering (QA) is the task of selecting an answer phrase (span) to a question given an evidence document. Due to the easi- ness of evaluation (compared to generative QA) and the fine-grainess of the answer (compared to sentence-level QA), it has become one of the most popular QA tasks, driven by massive new datasets such as SQuAD ( <ref type="bibr" target="#b22">Rajpurkar et al., 2016)</ref> and Triv- iaQA ( <ref type="bibr" target="#b13">Joshi et al., 2017)</ref>. Current QA models heavily rely on explicitly learning the interaction between the evidence document and the question using neural attention mechanisms ( <ref type="bibr">Wang and Jiang, 2017;</ref><ref type="bibr">Xiong et al., 2017;</ref><ref type="bibr" target="#b24">Seo et al., 2017;</ref><ref type="bibr">Lee et al., 2016, inter alia)</ref>, in which the model is fully aware of the question before or as it reads the document. As a result, despite significant ad- vances, they have not led to the standalone repre- sentation of document discourse which is never- * Most work done during internship with Google AI. theless a key goal of research in reading compre- hension. Furthermore, QA models that condition the document representation on a question have the practical scalability downside that the entire model should be re-applied on the same document for every question.</p><p>In this paper, we formalize a modular variant of the QA task, Phrase Indexed Question Answer- ing (PIQA), that enforces complete independence between document encoder and question encoder <ref type="figure">(Figure 1</ref>). In PIQA, all documents are processed independently of any question to generate phrase index vectors (blue nodes in the figure) for each answer candidate (left boxes in the <ref type="figure">figure)</ref>. Sim- ilarly, the questions are independently mapped to query vectors (red nodes in <ref type="figure">figure)</ref>. Then, at in- ference time, the answer is obtained by retrieving the nearest indexed phrase vector to the query vec- tor. Hence the algorithms aimed at tackling PIQA have the inherent benefit of modularity and scala- bility compared to current QA systems.</p><p>The task setup is analogous to how documents or sentences are retrieved in modern search en- gines via similarity search algorithms <ref type="bibr" target="#b26">(Shrivastava and Li, 2015)</ref>. Nevertheless, there is a key dis- tinction that search engines index each document by its content, while PIQA requires one to index each phrase in documents by its context.</p><p>We formally define the PIQA problem and provide baseline models for the new task. Our experiments show that the constraint introduced by PIQA leads to meaningful standalone docu- ment representations and practical scalability ad- vantage, demonstrating the significance of the new task. Moreover, there is still a large gap between the baselines and the unconstrained state of the art, showing that the task is yet far from being solved. We have set up a leaderboard 1 for PIQA challenge and invite the research community to participate. We currently support SQuAD and plan to expand to other datasets as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Reading comprehension. Massive reading comprehension question answering datasets <ref type="bibr" target="#b7">(Hermann et al., 2015;</ref><ref type="bibr" target="#b8">Hill et al., 2016;</ref><ref type="bibr" target="#b5">Dhingra et al., 2017;</ref><ref type="bibr" target="#b6">Dunn et al., 2017</ref>) have driven a large number of successful neural approaches ( <ref type="bibr" target="#b14">Kadlec et al., 2016;</ref><ref type="bibr" target="#b10">Hu et al., 2017</ref>, inter alia). ; <ref type="bibr" target="#b1">Chen et al. (2017)</ref>; <ref type="bibr" target="#b3">Clark and Gardner (2017)</ref>; <ref type="bibr" target="#b18">Min et al. (2018)</ref> tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer. <ref type="bibr" target="#b23">Salant and Berant (2018)</ref> proposed to minimize (but not prohibit) the influence of question when modeling the document. Similarly to ours, <ref type="bibr" target="#b17">Lee et al. (2016)</ref> proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question. Sentence retrieval. A closely related task to ours is that of retrieving a sentence/paragraph in a corpus that answers the question ( <ref type="bibr" target="#b27">Tay et al., 2017)</ref>. A comprehensive survey for neural ap- proaches in information retrieval literature is dis- cussed in <ref type="bibr" target="#b19">Mitra and Craswell (2017)</ref>. We note that our problem is focused on phrasal answer extrac- tion, which presents a unique challenge over sen- tence retrieval-the need for context-based repre- sentation as opposed to the content-based repre- sentation in the sentence-retrieval literature. ral network is applied. Our proposed problem shares similar traits but has a stronger constraint that only inner product comparison is allowed and one needs to model phrases instead of complete sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Phrase-Indexed Question Answering</head><p>Extractive question answering is the task of ob- taining the answerâanswerˆanswerâ to a question Q = {q 1 . . . q n } given an evidence document D = {d 1 . . . d m }, where the answerâanswerˆanswerâ = (s, e) indicates the start and end of a span in the document. The task is often formulated as learning the probabilistic dis- tribution of the answer given the question and the document. In existing literature (Section 2), the distribution is mainly featurized by</p><formula xml:id="formula_0">Pr(a|Q, D) ∝ exp(F θ (Q, D, a))</formula><p>where F θ could be any real- valued scoring function parameterized by θ. Once θ is learned, the predictionâpredictionˆpredictionâ is obtained byâ</p><formula xml:id="formula_1">byˆbyâ = argmax a F θ (Q, D, a).<label>(1)</label></formula><p>So far, most competitive designs of F θ (Q, D, a) make use of attention connections between the words in Q and D. As a result, these models can- not yield a query independent representation of the document D. It is subsequently not possible to independently assess the document understanding capability of the model. Furthermore, F θ (Q, D, a) needs to be re-computed for the entire document for every new question. We believe that this inef- ficiency precludes all current models as the candi- dates for end-to-end QA systems. We propose a new task-Phrase-Indexed Ques- tion Answering (PIQA)-that addresses these is- sues. We enforce the decomposability of F θ into two exclusive functions <ref type="figure">a)</ref>), where • is the inner product. The prediction is obtained byâ</p><formula xml:id="formula_2">G θ (Q), H θ (D, a) ∈ R k . The answer distribution is then modeled by Pr(a|Q, D) ∝ exp(G θ (Q) • H θ (D,</formula><formula xml:id="formula_3">byˆbyâ = argmax a G θ (Q) • H θ (D, a).<label>(2)</label></formula><p>In this setting, the document encoder H θ learns models the document independently of the ques- tion. Successful question answering models that follow the structure of PIQA will have two im- portant advantages over current QA models: full document comprehension and scalablity.</p><p>Full document comprehension. Language un- derstanding ability is widely associated with learn- ing a good standalone representation of text (or its components such as phrases) independent of the end task ( <ref type="bibr" target="#b0">Bowman et al., 2015)</ref>. Under PIQA con- straints, the document encoder H θ learns the rep- resentation of the answer candidate phrases a in the document D independent of the question. In order to correctly answer questions, these phrase representations (index vectors) need to correctly encode their meaning with respect to their con- text. Therefore, PIQA constraint enforces eval- uating research in document comprehension and phrase representation learning.</p><p>Scalability. Models that adhere to the PIQA constraint only need to be run once for each docu- ment, regardless of the number of questions asked.</p><p>To answer a question, the model then just needs to encode the question and compare it to each of the answer candidates via the inner product in Equa- tion 2. Implemented naively, computing a single inner product for each answer candidate is more efficient than building a new document encoding; after the documents are pre-encoded, Equation 2 is O(k) time per word where k is the vector size (most neural models require O(k 2 ) per word for matrix multiplications). More importantly, PIQA also permits an ap- proximate solution in sublinear time using asym- metric locality-sensitive hashing (aLSH) <ref type="bibr">Li, 2014, 2015)</ref>, through which Equa- tion 2 can be approximated for N answer candi- dates with O(kN ρ log N ) time, where ρ &lt; 1 is a function of the approximation factor and the prop- erties of the hash functions. We argue that this type of approach will be essential for the develop- ment of real world QA systems, where the number of potential answers N is extremely large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baseline Models</head><p>We introduce several baselines for PIQA that are motivated by related literature.</p><p>For all (neural) baselines, we represent the words in D and Q with one of three embed- ding mechanisms: CharCNN (Kim, 2014) + GloVe ( <ref type="bibr" target="#b20">Pennington et al., 2014)</ref>, and ELMo (Pe- ters et al., 2018).</p><p>We follow the majority of the related literature and apply bidirectional LSTMs <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref> </p><formula xml:id="formula_4">d i , q i ∈ R 2k</formula><p>where k is the hidden state size of LSTMs.</p><p>PIQA disallows cross-attention between docu- ment and question. However, we can still bene- fit from self-attention, which has become crucial for machine translation ( <ref type="bibr">Vaswani et al., 2017</ref>) and QA ( <ref type="bibr" target="#b11">Huang et al., 2018;</ref><ref type="bibr">Yu et al., 2018)</ref>. In all of our baselines, each variable-length question is collapsed into a fixed length vector via the sum q SA = i u i q i where u = {u 1 . . . u n } is a vec- tor containing a single weight for each word in the question. Similarly, we experiment with doc- ument side self attention to represent each docu- ment word d j as a weighted sum of itself and all neighboring words</p><formula xml:id="formula_5">d SA j = i h j i d j .</formula><p>The weight vectors u and h j are calculated as</p><formula xml:id="formula_6">u = softmax i (w q i ) h j = softmax i (R θ (D, j) K θ (D, i))</formula><p>where R θ , and K θ are trainable neural networks with the same ouptut size, and w ∈ R 2k is a train- able weight vector. We use independent BiLSTMs with hidden state size k (i.e. the output size is 2k) to model both R θ and K θ . That is, R θ (D, j) is the j-th output of BiLSTM on top of D, and we similarly define K θ with unshared parameters.</p><p>For all (neural) baselines, the question is rep- resented using the concatenation of two copies of q SA , one that should have high inner product with the vector for the answer's start span and an- other that should have high inner product with the vector for the answer's end. Thus, Equation 2's G θ (Q) = [q SA s , q SA e ] where the subscripts s (start) and e (end) imply that different sets of parameters were used. Now we define several baselines.</p><p>LSTM baseline. An answer candidate a = (s, e) is represented using the LSTM outputs at its endpoints: from Equation 2,</p><formula xml:id="formula_7">H θ (D, (s, e)) = [d s , d e ] ∈ R 4k and G θ (Q) = [q SA s , q SA e ] ∈ R 4k</formula><p>. LSTM+SA baseline. The LSTM outputs are augmented with the endpoint representations that come out of the document's self-attention (SA):</p><formula xml:id="formula_8">H θ (D, (s, e)) = [d s , d SA s , d e , d SA e ] ∈ R 8k and G θ (Q) = [q SA s1 , q SA s2 , q SA e1 , q SA e2 ] ∈ R 8k</formula><p>. TF-IDF. We lastly include a purely TF-IDF- based model, where each answer candidate phrase is associated with a bag of neighbor words within a distance of 7. Then the BOW vector is normal- ized via TF-IDF and indexed. When the query comes in, its TF-IDF vector is queried on the in- dexed phrases to yield the answer.   <ref type="formula" target="#formula_1">(2018)</ref> 89.3 82.5 <ref type="table">Table 1</ref>: Performance on SQuAD dev set with the PIQA constraint (top), and without the constraint (bot- tom). See Section 4 for the description of the terms.</p><p>For training the (neural) models, we minimize the negative log probability of getting the cor- rect answer: the loss function for each example</p><formula xml:id="formula_9">(D, Q, a * ) is L(θ) = − log Pr(a * |D, Q)</formula><p>where a * is the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We impose the independence restrictions from PIQA on the Stanford Question Answering Dataset 2 . We only consider answer spans with length ≤ 7. We use the hidden state size (k) of 128, which results in a 512D (4k) and 1024D (8k) vector for each phrase in LSTM and LSTM+SA, respectively. The default embedding model is CharCNN concatenated with 200D GloVe, with an option to append ELMo vectors following the same setup for SQuAD experiments discussed in <ref type="bibr" target="#b21">Peters et al. (2018)</ref>. We use a batch size of 64 and train for 20 epochs with the default Adam op- timizer ( <ref type="bibr" target="#b16">Kingma and Ba, 2015)</ref>, and take the best model on the validation set during training.</p><p>Results. <ref type="table">Table 1</ref> shows the results for the PIQA baselines (top) and the unconstrained state of the art (bottom). First, the TF-IDF model performs poorly, which signifies the limitations of tradi- tional document retrieval models for the task. Sec- ond, we note that the addition of self-attention makes a significant impact on results, improving F1 by 2.6%. Next, we see that adding ELMo gives 3.7% and 2.9% improvement on F1 for LSTM and LSTM+SA models, respectively. Lastly, the best PIQA baseline model is 11.7% higher than the first (unconstrained) baseline model ( <ref type="bibr" target="#b22">Rajpurkar et al., 2016</ref>) and 26.6% lower than the state of the art ( <ref type="bibr">Yu et al., 2018)</ref>. This gives us a reasonable starting point of the new task and a significant gap 2 PIQA paradigm can be also extended to other extractive QA datasets. -According to the American Library Association, this makes. . . -. . . tasked with drafing a European Charter of Human Rights,. . . -The LM engines were successfully test-fired and restarted, . . . . -Steam turbines were extensively applied. . . -. . . primarily accomplished through the ductile stretching and thinning. -. . . directly derived from the homogeneity or symmetry of space. . . to close for future work.</p><p>Phrase representations. Since PIQA models encode all answer candidates into the same space, we expect similar answer candidates to have high inner products with one another. <ref type="table" target="#tab_1">Table 2</ref> shows pairs of answer candidates that come from differ- ent documents in SQuAD, but that have similar encodings (high inner product). We observe that phrase representations learned through the PIQA task capture different interesting characteristics of the phrases. In all three rows, we can see that the phrase pairs seem to fit into natural categories: na- tional, or multi-national organizational constructs; mechanical engines; and mechanical properties, respectively. This suggests that the model has learned interesting typing information above the word level. The second and third rows also indi- cate that the model has learned a rich representa- tion of context. This is particularly obvious in the third row where the two phrases are lexically dis- similar, but preceded by the similar contexts 'pri- marily accomplished through' and 'directly de- rived from'. We believe that this analysis, while not complete, points toward exciting future lines of work in learning highly contextualized phrase representations through question answering.</p><p>Scalability. PIQA can also gain massive execu- tion time speedups once the documents are pre- encoded: in our simple benchmark on a consumer- grade CPU and NumPy (for LSTM+SA model, 1024D vectors), one can easily perform exact search over 1 million document words per second. BiDAF ( <ref type="bibr" target="#b24">Seo et al., 2017)</ref>, an open-sourced and rel- atively light QA model reaching 77.5% F1 (66.5% EM), can process less than 1k document words per second with an equivalent computing power (after pre-encoding the document as much as possible), which is more than 1,000x slower. It is also important to consider the memory cost for storing a vector representation of each of the answer candidates. We train an indepen- dent single-layer perceptron classifier that pre- dicts whether the phrase encoding is likely to be a good one. By varying a threshold on the score assigned by this classifier, we can filter answer candidates prior to storage. <ref type="figure" target="#fig_4">Figure 2</ref> illustrates the trade-off between accuracy and memory (mea- sured in mean number of vectors per document word) resulting from this filtering procedure for the LSTM+SA model. We observe that 1.3 vectors (candidates) per word on average reaches &gt; 98% of the model's F1 accuracy. This is equivalent to 5.2 KB per word with 1024D (4 KB) float vectors, or around 15 TB for the entire English Wikipedia (3 billion words). Future work will also involve creating a better classifier (i.e. improving the trade-off curve in <ref type="figure" target="#fig_4">Figure 2</ref>) for determining which phrase vectors to store.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We introduced Phrase-Indexed Question Answer- ing (PIQA), a new variant of the extractive ques- tion answering task that requires documents and question encoded completely independently and that they only interact each other via inner prod- uct. We argued that building a question-agnostic document encoder for question answering should be an important consideration for those in the QA community with the research goal of learning a model that reads and comprehends documents. Furthermore, the imposed constraint of the task implies a sublinear scalability benefit. Given that SQuAD models have recently outperformed hu- mans, PIQA formulation motivates a new chal- lenge for which we hope that the community's effort gradually closes the gap between our con- strained baselines and the unconstrained models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: PIQA task for a short context sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Language representation.</head><label></label><figDesc>Recently there has been a growing interest in developing natural language representations that can be transferred across tasks (Vendrov et al., 2016; Wieting et al., 2016; Conneau et al., 2017, inter alia). In particular, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) encourage archi- tectures that first encode the hypothesis and the premise independently before a comparator neu- 1 nlp.cs.washington.edu/piqa</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) to these embeddings to build the context-aware rep- resentations of the document D = {d 1 . . . d m } and question Q = {q 1 . . . q n }, where the forward &amp; backward LSTM outputs are concatenated to get a single word representation, i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Constraint</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F1 score versus number of vectors per word for LSTM+SA. Answer candidates have been filtered with varying threshold on an independent classifier learned on the candidate representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Most similar phrase pairs from disjoint sets of 
documents. Bold print is the phrase, and non-bold is its 
context. 

</table></figure>

			<note place="foot" n="3"> The difference will be even higher with a dedicated similarity search package such as Faiss (Johnson et al., 2017) or approximate search (Section 3).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by ONR (N00014-18-1-2826), NSF (IIS 1616112), Allen Dis-tinguished Investigator Award, and gifts from Google, Allen Institute for AI, Amazon, and Bloomberg. We thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarse-to-fine question answering for long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10723</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quasar: Datasets for question answering by search and reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fusionnet: Fusing via fullyaware attention with application to machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsin-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient and robust question answering from minimal context over documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01509</idno>
		<title level="m">Neural models for information retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextualized word representations for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved asymmetric locality sensitive hashing (alsh) for maximum inner product search (mips)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hyperqa: Hyperbolic embeddings for fast and efficient ranking of question answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07847</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
