<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Reinforcement Learning Based Image Captioning with Natural Language Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tszhang</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mobile Internet Group</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<email>shiyu.chang@ibm.com, yum@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mobile Internet Group</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Reinforcement Learning Based Image Captioning with Natural Language Prior</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="751" to="756"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>751</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However , this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image captioning <ref type="bibr" target="#b3">(Farhadi et al., 2010;</ref><ref type="bibr" target="#b9">Kulkarni et al., 2011;</ref><ref type="bibr">Yao et al., 2017;</ref><ref type="bibr" target="#b13">Lu et al., 2016;</ref><ref type="bibr" target="#b2">Dai et al., 2017;</ref><ref type="bibr" target="#b11">Li et al., 2017</ref>) aims at generating nat- ural language descriptions of images. Advanced by recent developments of deep learning, many captioning models rely on an encoder-decoder based paradigm ( <ref type="bibr">Vinyals et al., 2015)</ref>, where the input image is encoded into hidden representations using a Convolutional Neural Network (CNN) fol- lowed by a Recurrent Neural Network (RNN) de- coder to generate a word sequence as the caption. Further, the decoder RNN can be equipped with spatial attention mechanisms ( <ref type="bibr">Xu et al., 2015</ref>) to incorporate precise visual contexts, which often yields performance improvements empirically.</p><p>Although the encoder-decoder framework can be effectively trained with maximum likelihood estimation (MLE) <ref type="bibr">(Salakhutdinov, 2010)</ref>, recent <ref type="bibr">1</ref> https://github.com/tgGuo15/PriorImageCaption research ( <ref type="bibr" target="#b16">Ranzato et al., 2015</ref>) have pointed out that the MLE based approaches suffer from the so-called exposure bias problem. To address this problem, ( <ref type="bibr" target="#b16">Ranzato et al., 2015)</ref> proposed a Re- inforcement Learning (RL) based training frame- work. The method, developed on top of the RE- INFORCE algorithm <ref type="bibr">(Williams, 1992)</ref>, directly optimizes the non-differentiable test metric (e.g. BLEU ( <ref type="bibr" target="#b14">Papineni et al., 2002</ref>), CIDEr <ref type="bibr" target="#b1">(Vedantam et al., 2015)</ref>, METEOR (Banerjee and <ref type="bibr" target="#b0">Lavie, 2005</ref>) etc.), and achieves promising improve- ments. However, learning with RL is a notoriously difficult task due to the high-variance of gradient estimation. Actor-critic <ref type="bibr">(Sutton and Barto, 1998</ref>) methods are often adopted, which involves train- ing an additional value network to predict the ex- pected reward. On the other hand, ( <ref type="bibr">Rennie et al., 2017</ref>) designed a self-critical method that utilizes the output of its own test-time inference algorithm as the baseline to normalize the rewards, which leads to further performance gains.</p><p>Beside to the high-variance problem, we notice that there are two other drawbacks of RL-based captioning methods that are often overlooked in the literature. First, while these methods can di- rectly optimize the non-differentiable rewards and achieve high test scores, the generated captions contain many repeated trivial patterns, especially at the end of the sequence. <ref type="table" target="#tab_0">Table 1</ref> shows ex- amples of bad-endings generated by a self-critical based RL algorithm (model details refer to Sec- tion 4). Specifically, 46.44% generated captions end with phrases as "with a", "on a", "of a", etc.</p><p>(for detailed statistics see Appendix A), on the MSCOCO ( <ref type="bibr" target="#b1">Chen et al., 2015)</ref> validation set with the standard data splitting by <ref type="bibr" target="#b6">(Karpathy and Li, 2015)</ref>. The reason is that the shaped reward func- tion biases the learning. In <ref type="figure" target="#fig_0">Figure 1</ref>, we see these additive patterns at the end of captions, although make no sense to humans, yield to a higher re- <ref type="table" target="#tab_0">Image ID  Generated sentence  CIDEr  262262 a tall building with a clock tower with a 160.1  262148 a man doing a trick on a skateboard on a 146.5  52413  a person holding a cell phone in a  132.4  393225</ref> a bow of soup with carrots and a 118.5 ward. Empirically, removing these endings re- sults in a huge performance drop of around 6%.</p><p>( <ref type="bibr" target="#b15">Paulus et al., 2017</ref>) has also reported that in ab- stractive summarization, using RL only achieves high ROUGE <ref type="bibr" target="#b12">(Lin, 2004)</ref> score, yet the human- readability is very poor. The second drawback is that RL-based text generation is sample-inefficient due to the large action space. Specifically, the search space is of size O(|V| T ), where V is a set of words, T is the sentence length, and | · | denotes the cardinality of a set. This often makes training unstable and converge slowly. In this work, to tackle these two issues, we pro- pose a simple yet effective solution by introducing coherent language constraints on local action se- lections in RL. Specifically, we first obtain word- level n-gram <ref type="bibr" target="#b8">(Kneser and Ney, 1995</ref>) model from the training set and then use it as an effective prior. During the action sampling step in RL, we reduce the search space of actions based on the constitu- tion of the previous word contexts as well as our n-gram model. To further promote samples with high rewards, we sample multiple sentences dur- ing the training and update the policy based on the best-rewarded one. Such simple treatments prevent the appearance of bad endings and expe- dite the convergence while maintaining compara- ble performance to the pure RL counterpart. In ad- dition, the proposed framework is generic, which can be applied to many different kinds of neural structures and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Architecture</head><p>Encoder-Decoder Model: We adopt a similar structure as GNIC ( <ref type="bibr">Vinyals et al., 2015)</ref>, which first encodes an image I to a dense vector h I by CNN. The vector h I is then fed as the input to an LSTM-based <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) language model decoder. At each step t, the LSTM receives the previous output w t−1 as the in- put; computes the hidden state h t ; and predicts the next word w t as below:</p><formula xml:id="formula_0">h t = LSTM(h t−1 , w t−1 ), l t = W l h t w t ∼ softmax(l t ),<label>(1)</label></formula><p>where w 0 = h I and h 0 and c 0 are initialized to zero. The generation ends if a special token *end* is predicted.</p><p>Attention Model: Instead of utilizing a static representation of the image, attention mechanism dynamically reweights the spatial features from CNN to focus on the different region of the im- age at each word generation. We specifically con- sider the standard architecture used in ( <ref type="bibr">Xu et al., 2015)</ref>, where A = {a 1 , a 2 , ..., a L } is the spatial feature set and each a i ∈ R D corresponds to fea- tures extracted at different image locations. Then the hidden states of the LSTM is computed as</p><formula xml:id="formula_1">e ti = f att (a i , h t−1 ), β ti = exp(e ti ) L k=1 exp(e tk ) , z t = L k=1 β tk a k , h t = LSTM([h t−1 , z t ], w t−1 ),<label>(2)</label></formula><p>where f att is an attention model, which we use a single fully connected layer conditioned on the previous hidden state. Once h t is obtained, the word generation is same as equation <ref type="formula" target="#formula_0">(1)</ref>.</p><p>Sequence Generation with RL: We follow the training procedure of ( <ref type="bibr">Rennie et al., 2017</ref>). The decoder LSTM can be viewed as a "policy" de- noted by p θ , where θ is the set of parameters of the network. At each time step t, the policy chooses an action by generating a word w t and obtains a new "state" (i.e. hidden states of LSTM, attention weights, etc.). Once the end token is generated, a "reward" r is given based on the score (e.g. CIDEr or BLEU) of the predicted sentence. The goal is to maximize the expected reward as</p><formula xml:id="formula_2">L(θ) = E w s ∼p θ [r(w s )],<label>(3)</label></formula><p>where w s = {w s 1 , w s 2 , ..., w s T } are sampled words at every time step. The REINFORCE algorithm <ref type="bibr">(Williams, 1992)</ref> provides unbiased gradient esti- mation of θ as</p><formula xml:id="formula_3">θ L(θ) ≈ r(w s ) θ log p θ (w s ),<label>(4)</label></formula><p>using a single sequence.</p><p>Variance Reduction with Self-Critical: We re- duce the variance of the gradient estimator by us- ing the self-critical approach as</p><formula xml:id="formula_4">θ L(θ) ≈ (r(w s ) − r( ¯ w)) θ log p θ (w s ),<label>(5)</label></formula><p>where ¯ w t is the baseline reward calculated by the current model under the inference algorithm used at test time defined as</p><formula xml:id="formula_5">¯ w t = arg max wt p θ (w t |h t ).<label>(6)</label></formula><p>Then, sequences have rewards higher than ¯ w will be increased in probability, while samples result in lower reward will be suppressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prior Language Constraint with N -Gram Model</head><p>Method: We collect all n-grams (n=3 or 4 in our experiments) from a corpus of captions. We use the training set from MSCOCO to avoid the usage of the additional resource. Thus, a fair comparison to previous methods is guaranteed. Then, we fil- ter the n-grams with frequencies lower than five. The set of remaining ones is denoted as F. Dur- ing training, given the previous tokens predicted by the decoder, we constraint the sample space the current prediction by</p><formula xml:id="formula_6">w t ∼ softmax(p θ (w s t ) · α t ),<label>(7)</label></formula><p>where α i is an indicator vector whose length is the vocabulary size |V| and its elements are non-zero only if the corresponding word and the previous (n − 1)-gram constitute a valid n-gram in F as Discussion:</p><formula xml:id="formula_7">αt[k] = 1 if {w s t−n+1 , · · · , w s t−1 , k} ∈ F 0 otherwise .<label>(8)</label></formula><p>The key motivation for applying the above constraint is two-fold: (1) this ensures gen- erated captions always formed by valid n-grams, which provides us a direct way of eliminating the repeated common phrases and bad-endings like the ones in <ref type="table" target="#tab_0">Table 1</ref>; and (2) this shrinks the size of action space, which makes the training con- verges much faster. For MSCOCO, action space is changed from more than 9,000 to 56 on average.  gram. We directly compare with our counterparts that have the same structures but no n-gram mod- ules. Specifically, they are encoder-decoder based self-critical (ED-SC), and the one with attention (Att-SC). In addition, since our experimental setup is almost identical to many existing works, we also include their reported results, which include ( <ref type="bibr" target="#b6">Karpathy and Li, 2015;</ref><ref type="bibr">Xu et al., 2015;</ref><ref type="bibr" target="#b16">Ranzato et al., 2015;</ref><ref type="bibr">Ren et al., 2017)</ref>. At last, we also in- clude the performance of our warm-start models - the models trained by MLE ( <ref type="bibr">Vinyals et al., 2015</ref>) using cross entropy (ED-XE and Att-XE) -as a reference.</p><p>Evaluation Metric and Performance Adjust- ment: We report performance on FIVE metrics: BLEU4, METEOR, ROUGE-L,CIDEr and Bad Ending Rate. For the self-critical baselines, we report two sets of performances: 1) the captions directly generated by the model; and 2) the se- quences of removing bad endings of the generated captions, based on the distribution in Appendix A.</p><p>Results: <ref type="table" target="#tab_2">Table 2</ref> summarizes the performances of our models compared with other baselines. We see that without performance adjustments, the self-critical RL with attention performs the best. However, since it contains many bad endings, our method achieves supreme results after these re- peated patterns are removed. We also provide some qualitative comparison between our atten- tion model and self-critical in Appendix C.</p><p>Efficient Training: We show that constraining the action space leads to a more efficient RL train- ing in <ref type="figure" target="#fig_1">Figure 2</ref>. CIDEr score is calculated after removing bad endings. We plot three curves using architectures with/without attentions. The Green curve is the self-critical, the blue one is with prior- itized sampling, and the red one is our final model with 4-gram constraint. We observe that we can speed up almost twice than its counterpart.</p><p>Online Evaluation: We also evaluate our atten- tion model on COCO online server <ref type="bibr">4</ref> and results are reported in <ref type="table" target="#tab_4">Table 3</ref>. Att-SC gets a higher score than ours in the online test, however, with a lot of bad endings where the bad ending ratio is 72.7%.</p><p>Human Evaluation: We also implement human evaluation on the results generated by our Att-4- gram compared with Att-SC. We randomly select 200 images from the test set. Each time, one image with two captions generated by two different mod- els are shown to the volunteer and three choices are provided: (1) the first one is better; (2) both are the same level; (3) the second one is better. See more details in Appendix D. In <ref type="table" target="#tab_6">Table 5</ref>, our model wins 400 times and performs more closely to human than Att-SC.</p><p>Evaluating Captions Diversity: To further evaluate the quality of the caption model, we fol- low ( <ref type="bibr">Shetty et al., 2017)</ref> to measure the diversity of the generated captions. We compute the nov- elty score of our 4-gram model, which is defined as whether a particular caption has been observed in the training set. When two models have the same level predictive performances (e.g. CIDEr), a higher novelty score usually indicates more di- verse generations. We conduct the experiment five times and report the averaged novelty score of our 4-gram model and the Att-SC, which are 77.83% and 59.28% respectively. As the reference, the METEOR and novelty scores reported in (Shetty et al., 2017) are 23.6, and 79.84%, respectively.    LSTM Language Model: Given a word series {w 0 , w 1 , ..., w T }, the target of a neural language model is to maximize the log-likelihood as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIDEr BLEU4 METEOR ROUGE-L BadEnd-Rate</head><formula xml:id="formula_8">max θ log p θ (w 0 , w 1 , ..., w T ).<label>(9)</label></formula><p>We model p θ (w 0 , w 1 , ..., w T ) by an LSTM unit:</p><formula xml:id="formula_9">log p θ (w 0 , ..., w T ) = T t=1 log p θ LM (w t |h t−1 ) h t = LSTM LM (h t−1 , w t−1 ),<label>(10)</label></formula><p>where w 0 is set to a *start* token for all sentences. h 0 and c 0 are initialized to zero. After obtain- ing the optimized θ * LM , we can use it to constrain the action space similar to the N-gram language model. Specifically, given previous t − 1 sampled words from current caption model, we compute p θ * LM (w t |w 0 , w 1 , ..., w t−1 ), which is the probabil- ity of the next word over the entire vocabulary. We then apply a simple thresholding rule to form a subset of valid words for the captioning model.</p><formula xml:id="formula_10">α t [k] = 1 if {k} ∈ F 0 otherwise ,<label>where</label></formula><formula xml:id="formula_11">F ={w t |p θ * LM (w t |w 0 , w 1 , ..., w t−1 ) ≥ η}.<label>(11)</label></formula><p>η is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Experiments</head><p>The word embedding size and hidden dimension of θ LM are set to 256 for this experiment. We use Adam optimizer for training language model and the learning rate is set to 0.001. The batch size of language model training and REINFORCE training are both set to 20 in the experiments. η is set to 0.00005 for the first word and increases by a factor of two for ev- ery timestep. We report our results in two settings, which include the combination of with/without at- tention for the caption model (termed ED-LSTM- LM and Att-LSTM-LM). We use the same warm- start models as in the N-gram experiments. The performances are summarized in <ref type="table" target="#tab_1">Table 4</ref> and Ta- ble 3. We see that the neural language model provides further performance gains compared to the N-gram model without introducing any bad- endings. This is because that the LSTM language model covers a larger context than N-gram, which helps to generate more accurate captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a simple but efficient ap- proach to RL-based image caption by consider- ing n-gram language prior to constrain the action space. Our method converges faster and achieves better results than self-critical setting after remov- ing bad endings in the generated captions. In ad- dition, captions generated by our models are more human readable and graceful. We further extend our ideas using neural language model. The re- sults demonstrate that the captioning models are more beneficial from the neural language model than the N-gram model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A demonstration of the sequence with bad ending has higher BLEU and CIDEr scores compared to the one without.</figDesc><graphic url="image-1.png" coords="2,307.56,62.80,217.69,165.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training time of models with (right) and without (left) spatial attention.</figDesc><graphic url="image-2.png" coords="3,307.28,62.81,226.77,81.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Examples of bad sequences generated by a 
self-critical based RL baseline. Blue color indicates 
the bad ending. Sequences with bad endings have high 
CIDEr scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>4 Experiments</head><label>4</label><figDesc></figDesc><table>Dataset: We perform both quantitative and qual-
itative evaluations on MSCOCO dataset. The 
dataset contains 123,287 images and each image 
has at least five human captions. To seek fair com-
parison to others, we use the publicly available 
splits, which contains 82,783 training, 5,000 vali-
dation and 5,000 testing images. 

Implementation Details: Our implementations 
are based on the publicly project. 2 We use an Ima-
geNet pre-trained 101-layered ResNet 3 (He et al., 
2016) to extract visual features. We consider 
two types (see Section 2) of architectural train-
ing with RL: (1) the plain encoder-decoder, and 
(2) the encoder-decoder with attention. For the 
former one, we represent each image by a 2,048-
dimension vector by extracting the features from 
the last convolutional layer with average pooling. 
For the attention model, we apply spatial adaptive 
max pooling and the output feature map has the 
size of 14 × 14 × 2, 048. At each time step, the 
attention model produces weights over 196 spatial 
locations. The size of word embeddings and the 
hidden dimension of the LSTM are set to 512 for 
all experiments. More details are in Appendix B. 

Compared Methods: We report our results in 
four different settings, which include the combina-
tions of with/without attention and using tri-/four-Methods 
CIDEr 
BLEU4 
ROUGE-L METEOR BadEnd-Rate 
Published 
(Karpathy and Li, 2015) 
66.0 
23.0 
--
19.5 
0.0 
(Xu et al., 2015) 
--
25.0 
--
23.0 
0.0 
MIXER (Ranzato et al., 2015) 
--
29.1 
--
--
--
(Ren et al., 2017) 
93.7 
30.4 
52.5 
25.1 
--

Implemented 

ED-XE 
89.8 
28.0 
51.7 
24.2 
0.0 
Att-XE 
95.1 
29.2 
52.8 
24.8 
0.0 
ED-SC (Rennie et al., 2017) 
101.8 / 96.1 
31.2 / 30.3 
53.1 / 52.9 
24.6 / 23.9 
46.4% / 0.0 
Att-SC (Rennie et al., 2017) 
105.7 / 100.8 32.3 / 30.8 
53.8 / 53.1 
25.2 / 24.1 
43.7% / 0.0 
Ours-ED-4-gram 
96.7 
29.1 
51.4 
23.9 
0.0 
Ours-Att-4-gram 
102.0 
30.2 
53.6 
25.6 
0.0 
Ours-ED-tri-gram 
95.1 
29.8 
52.4 
24.1 
0.0 
Ours-Att-tri-gram 
100.4 
28.7 
51.8 
25.0 
0.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Quantitative evaluation of our method compared to baselines on MSCOCO. Blue text indicates the per-
formance after adjustments and red text indicates the best performance. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : Quantitative results on online server (C40 test).Red text indicates the best performance.</head><label>3</label><figDesc></figDesc><table>Methods 
CIDEr 
BLEU4 
ROUGE-L METEOR BadEnd-Rate 
Att-SC (Rennie et al., 2017) 105.7 / 100.8 32.3 / 30.8 
53.8 / 53.1 
25.2 / 24.1 
43.7% / 0.0 
Ours-ED-4-gram 
96.7 
29.1 
51.4 
23.9 
0.0 
Ours-Att-4-gram 
102.0 
30.2 
53.6 
25.6 
0.0 
ED-LSTM-LM 
99.4 
30.9 
52.7 
24.6 
0.0 
Att-LSTM-LM 
105.9 
32.8 
54.1 
25.4 
0.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Quantitative evaluation with our extension methods on MSCOCO. Blue text indicates the performance 
after adjustments and red text indicates the best performance. 

Methods 
4-gram win Same level 4-gram lose 
4-gram VS SC 
400 
349 
251 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Human evaluation results for attention models 

5 Neural Language Models Extension 

Inspired by the paper reviews, we extend our 
model by adopting another language prior to eval-
uating the effectiveness of constraining action 
space during REINFORCE training. We train our 
neural language model based on the MSCOCO 
caption corpus with an LSTM unit. 

</table></figure>

			<note place="foot" n="2"> https://github.com/ruotianluo/self-critical.pytorch 3 https://github.com/KaimingHe/deep-residual-networks</note>

			<note place="foot" n="4"> https://competitions.codalab.org/competitions/3221</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACLworkshop</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Every picture tells a story: generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gans for sequences of discrete elements with the gumbel-softmax distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos</forename><forename type="middle">Miguel</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hernndezlobato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1270" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3242" to="3250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
