<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributed Representations for Unsupervised Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
							<email>k.woodsend@ed.ac.uk, mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distributed Representations for Unsupervised Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new approach for unsuper-vised semantic role labeling that leverages distributed representations. We induce embeddings to represent a predicate , its arguments and their complex interdependence. Argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments , while predicate embeddings are learned from argument contexts. The induced representations are clustered into roles using a linear programming formulation of hierarchical clustering, where we can model task-specific knowledge. Experiments show improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, an increasing body of work has been devoted to learning distributed word repre- sentations and their successful usage in numerous tasks and real-world applications. Examples in- clude language modeling <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b17">Mikolov et al., 2013c;</ref><ref type="bibr" target="#b19">Mnih and Kavukcuoglu, 2013)</ref>, paraphrase detection ( <ref type="bibr" target="#b25">Socher et al., 2011a</ref>), sentiment analysis ( <ref type="bibr" target="#b26">Socher et al., 2011b;</ref><ref type="bibr" target="#b10">Kalchbrenner et al., 2014)</ref>, and most notably machine translation <ref type="bibr" target="#b9">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b4">Cho et al., 2014;</ref><ref type="bibr" target="#b0">Auli et al., 2013</ref>). Distributed word representations (also known as word embed- dings) are trained by predicting the contexts in which the words or phrases occur.</p><p>In this paper, we present a new approach for unsupervised semantic role labeling that leverages distributed representations. The goal of semantic role labeling is to discover the relations that hold between a predicate and its arguments in a given input sentence (e.g., "who" did "what" to "whom", "when", "where", and "how"). In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the physical object affected by the breaking event) and V determines the boundaries of the predicate. The semantic roles in the example are labeled in the style of <ref type="bibr">PropBank (Palmer et al., 2005</ref>), a broad-coverage human-annotated corpus of se- mantic roles and their syntactic realizations. In the unsupervised case, the model must induce such la- bels from data without access to a predefined set of semantic roles.</p><p>Role induction is commonly treated as a cluster- ing problem <ref type="bibr" target="#b30">(Titov and Klementiev, 2012;</ref><ref type="bibr" target="#b12">Lang and Lapata, 2014)</ref>. The input to the model are instances of arguments (e.g., window, the burglar in sentence (1)) and the output is a grouping of these instances into clusters such that each cluster contains arguments corresponding to a specific se- mantic role and each role corresponds to exactly one cluster. In other words, the syntactic repre- sentations of verbal predicates, and argument po- sitions are observable, whereas the associated se- mantic roles are latent and need to be inferred.</p><p>The task is challenging due to its unsupervised nature -it is difficult to define a learning objec- tive function whose optimization will yield an ac- curate model -but also because each predicate can allow several alternate mappings or linkings between its semantic roles and their syntactic real- ization. Despite occupying different syntactic po- sitions (subject in sentence (1) and object in sen- tence (2)), the noun phrase the window expresses the same role in both sentences. To learn such linkings, previous work has made use of syntac- tic and semantic features (e.g., whether two argu- ments are in the same position in the parse tree, whether they have the same POS-tags, whether they are lexically similar). These features are typ- ically defined on argument instances, without tak- ing the predicate into account, and do not interact but instead are sequentially applied.</p><p>In this work we propose to learn these features and their complex interactions (e.g., selectional restrictions) automatically from data. Specifi- cally, we induce embeddings to represent a pred- icate and its arguments. Argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments. Anal- ogously, predicate embeddings are learned from contexts representing their arguments. Our model learns a rich feature space which can serve as input to any clustering algorithm. We use a linear pro- gramming formulation of hierarchical clustering which is advantageous for two reasons. Firstly, ex- pressing clustering as a global optimization prob- lem with an explicit objective function can po- tentially yield higher quality output compared to greedy algorithms (such as agglomerative cluster- ing). Secondly, through the use of constraints, we can model task-specific knowledge (e.g., seman- tic roles are unique within a frame). Experimen- tal results show improved performance over both previous unsupervised semantic role labeling ap- proaches and other distributed word representation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our model is inspired by recent work in learning distributed representations of words ( <ref type="bibr" target="#b1">Bengio et al., 2006;</ref><ref type="bibr" target="#b18">Mnih and Hinton, 2008;</ref><ref type="bibr" target="#b5">Collobert et al., 2011;</ref><ref type="bibr" target="#b31">Turian et al., 2010;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref>. In this framework, a neural network is used to pre- dict a word taking into account its context. Words are represented by vectors which are concatenated or averaged in order to form a representation of the context. We induce vector representations to rep- resent each predicate and its argument. As a learn- ing objective, vectors are required to contribute to a prediction task about the target argument in the sentence, given the predicate and a small win- dow of surrounding arguments. Similarly, predi- cate vectors are learned from the contexts of pre- ceding arguments, and are required to contribute to the prediction of upcoming arguments. Our vectors encode the semantics of arguments, predi- cates, and their interdependence.</p><p>Approaches to unsupervised semantic role la- beling follow two main modeling paradigms. Un- der the the first variant, semantic roles are mod- eled as latent variables in a (directed) graphical model that relates a verb, its semantic roles, and their possible syntactic realizations ( <ref type="bibr" target="#b8">Grenager and Manning, 2006;</ref><ref type="bibr" target="#b11">Lang and Lapata, 2010;</ref><ref type="bibr" target="#b6">Garg and Henderson, 2012)</ref>. Role induction here corre- sponds to inferring the state of the latent variables representing the semantic roles of arguments. The second approach is similarity-driven and based on clustering. For instance, <ref type="bibr" target="#b12">Lang and Lapata (2014)</ref> induce semantic roles via graph partition- ing: each vertex in a graph corresponds to an ar- gument instance of a predicate and edges repre- sent features expressing syntactic or semantic sim- ilarity. The graph partitioning problem is solved using task-specific adaptations of label propaga- tion and agglomerative clustering. Titov and Kle- mentiev (2012) propose a Bayesian clustering al- gorithm based on the Chinese Restaurant Pro- cess. Their model encourages similar verbs to have similar linking preferences using a distance- dependent Chinese Restaurant Process prior.</p><p>More recently, <ref type="bibr" target="#b29">Titov and Khoddam (2015)</ref> pro- pose a reconstruction-error minimization framer- work for unsupervised semantic role induction. Their model consists of two componenets: the encoder (implemented as a log-linear model) predicts roles given syntactic and lexical fea- tures, whereas the reconstruction component (im- plemented as a probabilistic tensor factorization model) recovers argument fillers based on the role predictions, the predicate and other arguments. The two components are estimated jointly to min- imize errors in argument reconstruction.</p><p>Our work follows the similarity-driven model- ing paradigm. Rather than engineering relevant features, we learn them using a neural network and a task-appropriate training objective. We are thus able to model complex interactions between argu- ments and their predicates without making simpli- fying assumptions (e.g., that arguments are condi- tionally independent of each other given the pred- icate). Our embeddings are largely independent of the clustering algorithm used to induce the seman- tic roles. We advocate the use of linear program- ming, which supports the incorporation of linguis- tic and structural constraints during cluster forma- tion. ILP techniques have been previously applied to several supervised NLP tasks, including seman- tic role labeling ( <ref type="bibr" target="#b22">Punyakanok et al., 2008)</ref>, how-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>START Yesterday Kristina hit Scott with a baseball END a 1 a 2 a 3 a 4 a 5 a 6 predicate</head><p>Identification arg t−1 arg t arg t+1 Window 1 arg t−1 arg t arg t+1 Window 2 arg t−1 arg t arg t+1 Window 3 arg t−1 arg t arg t+1 Window 4</p><p>Figure 1: Symmetric context window from the list of arguments ever their application to unsupervised role induc- tion is novel to our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Unsupervised role induction is commonly mod- eled after supervised semantic role labeling <ref type="bibr">(M` arquez et al., 2008</ref>) and follows a two-stage ap- proach. Given a sentence and a designated verb, the goal is to identify the arguments of the verbal predicate (argument identification) and label them with semantic roles (role induction). The model is first given a syntactically analyzed sentence (e.g., in the form of a dependency parse) with the aim of determining all constitutents that fill a se- mantic role. Argument identification is performed heuristically using a small number of rules which take into account syntactic relations encountered when traversing the dependency tree from predi- cate to argument <ref type="bibr" target="#b12">(Lang and Lapata, 2014;</ref><ref type="bibr" target="#b30">Titov and Klementiev, 2012</ref>). An alternative which we follow here is to use a supervised classifier trained on a small amount of data using non-lexicalized features.</p><p>As mentioned earlier, we treat role induction as a type-level clustering problem: argument in- stances are assigned to clusters such that these rep- resent semantic roles. We induce a separate set of clusters for each verb, and each cluster thus repre- sents a verb-specific role. Clustering algorithms commonly take a matrix of pairwise similarity scores between instances as input and produce a set of output clusters, often satisfying some op- timality criterion. In our case, instances are type- level arguments represented by embeddings whose similarity is quantified using a distance measure such as cosine (see Section 3.1) and clusters are formed using a linear programming formulation of hierarchical clustering (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predicate and Argument Embeddings</head><p>Our approach for learning predicate and argu- ment vectors is inspired by recent methods aimed at learning high-quality vector representations of words from large amounts of unstructured text data ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref>. In this framework, vectors of the surrounding words within a fixed- sized window (the context) are summed into a sin- gle vector v c , which is useful in predicting the output vector v 0 representing the current or target word. Longer-range context information can also be captured ( <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>), specifically words within the current paragraph but outside of the target word context window.</p><p>In contrast to previous word-based approaches, our model induces vector representations for each predicate and its semantic arguments. As a learn- ing objective, vectors are required to contribute to a prediction task about the target argument in the sentence, given the predicate and a small win- dow of surrounding arguments. So despite the fact that the argument vectors and weightings are ini- tialized randomly, they can eventually capture se- mantics as an indirect result of the prediction task. Similarly, predicate vectors are learned from the many contexts sampled from sentences involving that predicate, and are required to contribute to the prediction task of the next argument. One way to consider the role of the predicate token is as an- other argument. It acts as a memory (similar to the paragraph memory of <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>) that remembers what is missing from the current context, and so captures something of the core na- ture of the predicate. <ref type="figure">Figure 1</ref> illustrates our approach for building the context, for the example sentence Yesterday, Kristina hit Scott with a baseball. As a prepro- cessing step, (verbal) predicates and arguments are identified based on a dependency parse, to give a full list of arguments for the sentence. Boxes show the span of each argument. In our model, contexts are symmetric and of fixed length (c = 1 in the <ref type="figure">Figure)</ref>, sampled from a sliding window over the argument list. To enable the first and last argu- ments within the sentence to be predicted from the context, we augment the argument list with START and END arguments. Meanwhile, the predicate is associated with all contexts generated from the sliding window approach.</p><p>More formally, given a training set compris- ing a predicate b and a sequence of its semantic arguments a 1 , a 2 , a 3 , . . . , a T , the objective of the model is to maximize the average log probability:</p><formula xml:id="formula_0">1 T T ∑ t=1 log p (a t |b, a t+ j , −c ≤ j ≤ c, j = 0) (1)</formula><p>where c is the size of the training context around the center argument a t . We define probability us- ing the softmax function:</p><formula xml:id="formula_1">p (a t | b, a context ) ∝ exp v T c v 0 ,<label>(2)</label></formula><p>where v 0 is the target argument vector and v c the context vector formed from predicate and con- text arguments vectors. Vectors are trained using stochastic gradient descent where the gradient is obtained via back-propagation. After the training converges, predicates and arguments with similar meaning are mapped to a similar position in the vector space. Every predicate is mapped to a unique vec- tor v pred , with the vocabulary of vectors shared across the data set. For the arguments, we generate feature vectors f −1 and f +1 from syntactic infor- mation (dependency relations and POS-tags), con- catenated with a distributional vector to represent the head word token in each argument. The repre- sentation vectors v −1 and v +1 are calculated from the feature vectors using v j = W context f j , where the matrix W context is also updated as part of the learning process. W context is common for all ar- guments. In a similar manner, the representation vector v 0 for the target argument is calculated us- ing v 0 = W argument f 0 . The predicate and argument vectors are concatenated to predict the middle ar- gument in a context. Other ways of dividing the argument window between context and predicted argument, and of combining context vectors, are possible. As the full list of arguments in a sentence is known, we use a symmetric window. The ad- vantage of concatenating the vectors is that infor- mation on the sequence of arguments is preserved. An illustration of our model is given in <ref type="figure" target="#fig_3">Figure 2</ref>. arg t arg t+1 predicate  Through a context window of arguments rather than neighboring tokens, our model captures a semantic representation of each verbal predicate. Furthermore, the arguments themselves are posi- tioned in vector space as a result of the selectional preferences of the predicates. In the next section, we use the induced semantic space to cluster argu- ments into semantic roles.</p><formula xml:id="formula_2">f −1 f +1 v −1 v +1 v pred ×W context ×W context v c v 0 f 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Argument Clustering</head><p>Hierarchical clustering is a method of clustering which seeks to build a hierarchy of clusters, often presented in a dendrogram. In such a represen- tation, all possible pairs of clusters are merged at some level. It is typically implemented as a greedy heuristic algorithm with no explicit objective func- tion. Instead, it requires a measure of dissimilar- ity between sets of observations, typically through a measure of distance between pairs of observa- tions. An example is the agglomerative clustering technique used in <ref type="bibr" target="#b12">Lang and Lapata (2014)</ref>. Their algorithm starts from seed clusters based on shared syntactic information, and then repeatedly merges pairs of clusters "bottom up" to form a hierarchy.</p><p>It is possible to formalize hierarchical clustering as an integer linear programming (ILP) problem with the dendrogram properties enforced as linear constraints ( <ref type="bibr" target="#b7">Gilpin et al., 2013)</ref>. Although exact solvers exists for ILP, their performance is highly dependent on the number of variables involved, and we found it necessary to develop a linear pro- gramming (LP) relaxation to provide approximate solutions faster. Dynamic programming is an al-ternative approximation technique that could be explored; it has recently been used successfully in the context of supervised semantic role labeling <ref type="bibr">(Täckström et al., 2015)</ref>.</p><p>But first, we consider the exact formalization of agglomerative clustering as an ILP. In order to generate a legal dendrogram, it is necessary for the model to enforce the following partition proper- ties:</p><p>Reflexivity A seed cluster is always in the same merged cluster as itself.</p><p>Symmetry If seed cluster a is merged into the same cluster as seed cluster b, then b is also in the same cluster as a.</p><p>Transitivity If a and b are merged at a certain level, and b and c are also merged at the same level, then a is in the same cluster as c at that level.</p><p>To model hierarchical clustering as an ILP prob- lem, we consider all pairs of clusters a and b, and introduce variables M ab to represent the merge level between clusters a and b. Reflexivity is en- forced by the constraint:</p><formula xml:id="formula_3">M aa = 0,<label>(3)</label></formula><p>Meanwhile the symmetry requirement is captured by the constraint:</p><formula xml:id="formula_4">M ab = M ba .<label>(4)</label></formula><p>The transitivity requirement and the objective to find the hierarchy that minimizes pair-wise dis- tances are modeled in the objective of the ILP using auxiliary variables O abc that represent the merge order of pairs (a, b) and (a, c), and coeffi- cients w abc that are set equal to the difference be- tween distance metrics D between those pairs: arg max</p><formula xml:id="formula_5">M , O ∑ a,b,c∈Instances w abc O abc subject to:</formula><p>M ab is a merge function</p><formula xml:id="formula_6">O abc = 1 if M ab &lt; M ac 0 otherwise w abc = D(a, c) − D(a, b).</formula><p>Although exact solutions can be found using ILP solvers, for the problems we consider there are typically over 100 seed clusters. This generates in the order of 10 6 transitivity constraints, and it is this in particular that results in combinatorial com- plexity from off-the-shelf ILP solvers. An LP relaxation provides approximate solu- tions faster. A maximum merge level L is first de- fined as a parameter, although as this is not an inte- ger problem and fractional levels are possible, this does not represent the number of levels. Auxiliary variables Z ab≥ac capture the merge hierarchy, and T abc rewards transitivity by a factor α:</p><formula xml:id="formula_7">arg max M , O,Z ∑ a,b,c∈Instances w abc O abc + αT abc subject to: 0 ≤ T ≤ 1 0 ≤ O ≤ 1 0 ≤ Z ≤ 1 0 ≤ M ≤ L −L ≤ M ac − M ab − (L + 1)O abc ≤ 0 −L ≤ M ab − M ac − (L + 1)Z ab≥ac + 1 ≤ 0 −L ≤ M bc − M ac − (L + 1)Z bc≥ac + 1 ≤ 0 Z ab≥ac + Z bc≥ac ≥ T abc (5)</formula><p>To capture the linguistic principles involved in semantic role labeling <ref type="bibr" target="#b12">(Lang and Lapata, 2014</ref>), our formulation includes additional constraints. These are expressed explicitly through the con- struction of the linear programme:</p><p>Role Uniqueness Semantic roles are unique within a particular frame. This principle is cap- tured by constraining the merge level of two seed clusters a and b to be at the top level L of the hi- erarchy, where a and b are roles that occur within the same frame, with the constraint:</p><formula xml:id="formula_8">M ab = L ∀(a, b) in frame.<label>(6)</label></formula><p>Syntactic Position Arguments occurring in a specific syntactic position within a specific link- ing all bear the same semantic role. This is han- dled by construction of the problem, where all ar- guments of a particular predicate occurring in a specific syntactic position are collected into a seed cluster at the beginning of the merging problem.</p><p>Argument Head Distribution The distribution over argument heads is the same for two clusters that represent the same semantic role. The distri- bution of arguments is captured in vector space by the model described in Section 3.1. We calculate centroid vectors from the instances in each clus- ter. To measure similarity between clusters a and b, we use cosine similarity between centroids:</p><formula xml:id="formula_9">D(a, b) = v T a v b v a v b<label>(7)</label></formula><p>Equations <ref type="formula" target="#formula_3">(3)</ref>- <ref type="formula" target="#formula_9">(7)</ref> comprise the LP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section we present our experimental setup for assessing the performance of the model pre- sented above. We explain how it was trained and tested, and also briefly introduce the models used for comparison with our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>To obtain distributed representations, we used text from a subset of the English Gigaword corpus <ref type="bibr" target="#b21">(Parker et al., 2011</ref>), comprising almost 64 mil- lion tokens (2.7 million sentences). The training corpus was pre-processed using MATE <ref type="bibr" target="#b2">(Björkelund et al., 2009</ref>) to lemmatize the words, provide POS-tags and a dependency parse, identify verbal predicates and the position of arguments. The neural network model described in Sec- tion 3.1 was trained using Matlab. We restricted the predicate vocabulary to use the 5,000 most fre- quent verbs in the training corpus, and the verbal predicates found in the CoNLL-2008 shared task data set ( <ref type="bibr" target="#b27">Surdeanu et al., 2008)</ref>. Predicates were represented as vectors of size 80, while vectors of length 50 were used for arguments. We used a symmetric context window of size c = 1. As the mechanism to prevent all vectors from hav- ing the same value, we used "negative-sampling" <ref type="bibr" target="#b16">(Mikolov et al., 2013b)</ref>, where there are k = 5 ran- domly sampled negative examples of (context, tar- get) pairs for each data sample. This technique has the advantage that we do not need to provide numerical probabilities for the noise distribution. Model parameters were updated during training using stochastic gradient descent over 5 epochs, decreasing the update step size at each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Argument clustering</head><p>Following common practice in unsupervised role induction <ref type="bibr" target="#b30">(Titov and Klementiev, 2012;</ref><ref type="bibr" target="#b12">Lang and Lapata, 2014)</ref>, we evaluated our model on the complete CoNLL-2008 shared task data set. We used the clustering metrics of purity, collocation and their harmonic mean F1. In addition, we used V-measure <ref type="bibr" target="#b23">(Rosenberg and Hirschberg, 2007)</ref>, an entropy-based measure which explicitly evaluates how successfully the criteria of homogeneity and completeness have been satisfied.</p><p>In previous work on unsupervised role induc- tion, the results for each predicate were weighted in proportion to the number of times the predicate appeared in the CoNLL-2008 test set. In addi- tion to this measure, we evaluate clustering where predicates are uniformly weighted. In a data set where the top 10 predicates account for almost 20% of the samples, these metrics give a view of performance on the other 3,000-plus predicates where less predicate-specific data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Models</head><p>We compared our model against a baseline that assigns arguments to clusters based on their syn- tactic functions (SYNTF; <ref type="bibr" target="#b12">Lang and Lapata, 2014</ref>). Specifically, the baseline forms clusters from the syntactic position of an argument using four cues: the verb's voice, the argument's position relative to the predicate, its syntactic relation, and any re- alizing preposition. <ref type="bibr">1</ref> To assess whether our argument-based model has any advantages over other word-based dis- tributed representations we compared the follow- ing variants: (a) the arg2vec model presented in Section 3.1 trained on the subset of Gigaword; (b) the continuous bag-of-words model trained using word2vec on the same Gigaword corpus; and (c) 300-dimensional vectors pre-trained on part of the Google News dataset 2 (about 100 billion words), again using word2vec. In all three instances, we performed argument cluster- ing using the LP of Section 3.2. We also com- pare against Agglomerative-cosine (AGGLOM), the best performing model of <ref type="bibr" target="#b12">Lang and Lapata (2014)</ref>. <ref type="bibr">3</ref> Where applicable, we also refer to the models presented in <ref type="bibr" target="#b30">Titov and Klementiev (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our results on the semantic role induction task are summarized in <ref type="table" target="#tab_0">Tables 1 and 2. Table 1</ref> presents results using the gold standard parses and argu-  <ref type="table" target="#tab_0">Weighted  Unweighted  PU  CO  F1  PU  CO  F1  HO  CO  V1  HO  CO  V1  SYNTF</ref> 81.6 78.1 79.8 90.0 86.8 87.8 71.7 66.2 68.8 85.5 81.7 82.1 AGGLOM 87.4 75.3 80.9 95.1 80.7 86.5 79.2 65.5 71.7 93.1 78.1 84.0 word2vec-GIGAWORD 82.8 77.9 80.3 91.4 86.3 88.2 78.8 63.7 70.4 90.2 81.1 84.4 word2vec-GOOGLENEWS 83.4 76.2 79.7 91.6 85.7 87.9 78.7 63.7 70.4 90.2 80.7 84.1 arg2vec-GIGAWORD 87.9 74.7 80.8 94.2 85.4 88.9 86.1 64.6 73.8 94.6 80.9 86.2   <ref type="table" target="#tab_1">Table 2</ref> uses au- tomatic parses with automatically identified argu- ments which is a more realistic evaluation setting.</p><p>As can be seen in <ref type="table" target="#tab_0">Table 1</ref>, when gold standard information is used the syntactic function baseline (SYNTF) is very effective. When considering F1 (weighted by the number of instances), arg2vec performs on the same par with graph-based ag- glomerative clustering (AGGLOM). Interestingly, word2vec performs worse when trained either on Gigaword or the Google News corpora. Ac- cording to (weighted) V1, arg2vec outperforms all other comparison models. When predicates are weighted uniformly, arg2vec is the best per- forming model using F1 or the more information- centric V-measure. This suggests that our model performs well on the the less-frequent predicates and rarer semantic roles. The results also show that our model captures semantic information use- ful for this task more successfully than the word- based distributional models. Both word2vec mod- els have similar performance, despite significant differences in the size of their training data. <ref type="table" target="#tab_1">Table 2</ref> shows similar trends. The poorer per- formance of SYNTF and AGGLOM can partly be ascribed to the heuristics used for argument iden- tification: DEPREL+MATE gives the baseline per- formance of our dependency parser and argu- ment identification. Nevertheless, when compar- ing systems that have access to the same prepro- cessing, our arg2vec model gives the best per- formance particularly in the information-centric V-measures. Also note, that it seems robust to noise incurred by the automatic parsing and argu- ment identification procedures.</p><p>Titov and Klementiev (2012) report a (weighted) F1 of 83.0 on the gold standard CoNLL-2008 dataset, using a coupled model where parameters are shared across verbs and a form of smoothing which replaces argument fillers by lexical cluster ids stemming from <ref type="bibr">Brown et al.'s (1992)</ref> algorithm (trained on the RCV1 corpus, about 63 millions words). Our model would presumably benefit from a similar coupling mechanism which we could enforce as a constraint in the ILP. However, we leave this to future work. When tested on automatic parses and gold arguments, their model yields a weighted F1 of 78.8. For comparison, arg2vec obtains an F1 of 80.6 on automatic parses and arguments. <ref type="figure" target="#fig_6">Figure 4</ref> shows visualizations of the argument semantic space as captured by the arg2vec- GIGAWORD model, for the predicates eat and win. Dimensionality reduction was performed by the t-SNE library. <ref type="bibr">4</ref> The visualization suggests that the model learns similarities beyond simple word contexts.</p><p>The evaluation presented so far assesses the quality of the argument representations learned by our model. We also wanted to see whether the predicate embeddings capture meaningful se- mantic content. of the predicate semantic space as captured by arg2vec when it is trained on the Gigaword cor- pus. It shows a projection of the 100 most frequent verbs in <ref type="bibr">CoNLL-2008</ref>, with dimensionality reduc- tion again performed by t-SNE. The visualization suggests that the model captures non-trivial predi- cate similarities. Verbs relating to buying and sell- ing lie close together (e.g., offer, buy, receive, pay, sell). Verbs denoting growth or decrease are also grouped together (drop, fall, increase, grow, re- duce, cut). Interestingly, verbs with similar argu- ment structure share regions of the space (e.g., say, estimate, or believe, think or seem, appear). Use- fully, verbs are represented in a continuous space rather than discrete clusters (e.g., acquire is some- where between buy and own).</p><p>In order to quantitatively evaluate the qual- ity of the predicate representations induced by our model, we compared the cosine distances between vectors to the hierarchy of VerbNet ( <ref type="bibr" target="#b24">Schuler, 2005)</ref>. VerbNet is a hierarchical domain- independent broad-coverage verb lexicon for En- glish, organizing verbs into classes. The evalua- tion task was, for all pairs of predicates, to predict whether they would be in the same cluster at the top layer of the hierarchy of VerbNet. To form the top layer of VerbNet, we took the first inte- ger of each VerbNet class number. As an exam- ple, the verbs believe (VN class conjecture-29.5), think (consider-29.9), expect (conjecture-29.5-1), and adopt (appoint-29.1) would all be in the same class 29. According to this reduction of VerbNet, there are 101 classes. The prediction was based on whether the cosine distance between the pair of vectors was above a threshold value. We mea- sured area under the precision-recall curve (AUC) which captures performance at all thresholds, and F1-score at the best threshold. arg2vec does bet- ter in both measures than a baseline of random vectors of the same dimension, scoring 0.637 for AUC compared to a baseline of 0.505, and 29.5 against 22.9 for F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we presented a new approach for learning distributed representations for predicates and their arguments which we show is useful for unsupervised semantic role labeling. Rather than creating a task-specific algorithm for role induc- tion, we learn a task-specific representation. We thus decouple feature learning from clustering in- ference, which results in a conceptually simpler model. Through a formulation of the clustering problem as a linear programme, we are able to per- form clustering efficiently and incorporate task- specific constraints. In the future, we would like to investigate how our approach generalizes across languages and tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>. [The burglar] A0 [broke] V [the window] A1 . 2. [The window] A1 [broke] V .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distributional model for learning representations of predicates and semantic arguments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 showsFigure 3 :</head><label>33</label><figDesc>Figure 3: 2-D representation of the induced predicate space for the 100 most frequent predicates in CoNLL-2008.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 2-D representation of the induced argument space for the predicates eat (top) and win (bottom). In both representations, A0 arguments are clustered bottom left, while A1 arguments are found top right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Purity, collocation and F1 measures (left), and homogeneity, completeness and V1 measures 
(right) for CoNLL-2008 data set, using gold syntax information. 

Weighted 
Unweighted 
Weighted 
Unweighted 
PU 
CO 
F1 
PU 
CO 
F1 
HO 
CO 
V1 
HO 
CO 
V1 
SYNTF 
68.3 72.1 70.1 80.6 81.3 80.3 
55.2 54.9 55.0 74.4 74.3 73.2 
AGGLOM 
75.5 69.5 72.4 89.3 77.9 82.4 
64.9 55.7 60.0 86.1 74.6 78.8 
DEPREL+MATE 
81.4 77.7 79.5 88.7 84.8 86.2 
71.5 65.7 68.5 83.7 79.2 80.3 
word2vec-GIGAWORD 
83.3 76.1 79.5 91.3 85.7 87.8 
78.4 63.4 70.1 89.9 80.3 83.9 
word2vec-GOOGLENEWS 82.9 77.4 80.1 91.3 86.0 88.0 
78.3 63.6 70.2 89.9 80.9 84.2 
arg2vec-GIGAWORD 
87.7 74.6 80.6 93.9 85.3 88.8 
85.7 64.4 73.5 94.4 80.8 86.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Purity, collocation and F1 measures, and homogeneity, completeness and V1 measures for 
CoNLL-2008 data set using automatic parse syntax information. 

ments available in the CoNLL 2008 data set. No-
tice that our embeddings are still learned using au-
tomatically identified arguments. </table></figure>

			<note place="foot" n="1"> Differences in the results compared to Lang and Lapata (2014) are due to our re-implementation of the predicate labeling stage, to be consistent with the preprocessing we used for the other comparison systems. 2 http://code.google.com/p/word2vec/ 3 Differences from published results are again due to changes at the predicate labeling stage.</note>

			<note place="foot" n="4"> http://lvdmaaten.github.io/tsne/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<title level="m">Neural Probabilistic Language Models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilingual Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Love</forename><surname>Hafdell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
	<note>Shared Task</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised Semantic Role Induction with Global Role Ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="145" to="149" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Formalizing Hierarchical Clustering as Integer Linear Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegried</forename><surname>Nijssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 27th AAAI Conference on Artificial Intelligence<address><addrLine>Bellevue, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="372" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of a statistical verb lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Convolutional Neural Network for Modelling Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised induction of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="939" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Similaritydriven semantic role induction via graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="133" to="164" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic Role Labeling: An Introduction to the Special Issue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<meeting><address><addrLine>October</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An Annotated Corpus of Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">English Gigaword Fifth Edition. LDC2011T07. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="287" />
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VMeasure: A Conditional Entropy-Based External Cluster Evaluation Measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">VerbNet: A broadcoverage, comprehensive verb lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting><address><addrLine>Manchester, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="159" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient inference and structured learning for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised induction of semantic roles within a reconstructionerror minimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Khoddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Denver</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Bayesian approach to unsupervised semantic role induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
