<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Structured Prediction with Neural CRF Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Information Science and Technology</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Structured Prediction with Neural CRF Autoencoder</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1701" to="1711"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of sequential structured prediction problems. Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to reconstruct the input. Our model has a unified structure with different loss functions for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages , show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), charac- terized by highly structured tasks, promising re- sults were obtained by models that combine deep learning methods with traditional structured learn- ing algorithms <ref type="bibr" target="#b4">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b7">Durrett and Klein, 2015;</ref><ref type="bibr" target="#b1">Andor et al., 2016;</ref><ref type="bibr" target="#b33">Wiseman and Rush, 2016</ref>). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to com- bine the local decisions into a coherent global de- cision. Among these models, neural variants of the Conditional Random Fields (CRF) model <ref type="bibr" target="#b13">(Lafferty et al., 2001</ref>) are especially popular. By re- placing the linear potentials with non-linear poten- tial using neural networks these models were able to improve performance in several structured pre- diction tasks <ref type="bibr" target="#b1">(Andor et al., 2016;</ref><ref type="bibr" target="#b23">Peng and Dredze, 2016;</ref><ref type="bibr" target="#b14">Lample et al., 2016;</ref><ref type="bibr" target="#b16">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b7">Durrett and Klein, 2015)</ref>.</p><p>Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously sus- ceptible to over-fitting unless large amounts of training data are available. This problem is exacer- bated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task.</p><p>In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning on sequence labeling problems.</p><p>An autoencoder is a special type of neural net, modeling the conditional probability P ( ˆ X|X), where X is the original input to the model andˆX andˆ andˆX is the reconstructed input <ref type="bibr" target="#b8">(Hinton and Zemel, 1994)</ref>. Autoencoders consist of two parts, an en- coder projecting the input to a hidden space, and a decoder reconstructing the input from it.</p><p>Traditionally, autoencoders are used for gener- ating a compressed representation of the input by projecting it into a dense low dimensional space. In our setting the hidden space consists of discrete variables that comprise the output structure. These generalized settings are described in <ref type="figure">Figure 1a</ref>. By definition, it is easy to see that the encoder (lower half in <ref type="figure">Figure 1a)</ref> can be modeled by a discrimina- tive model describing P (Y |X) directly, while the decoder (upper half in <ref type="figure">Figure 1a</ref>) naturally fits as a generative model, describing P ( ˆ X|Y ), where Y is the label. In our model, illustrated in <ref type="figure">Figure 1b</ref>, the encoder is a CRF model with neural networks as its potential extractors, while the decoder is a generative model, trying to reconstruct the input.</p><p>Our model carries the merit of autoencoders, which can exploit valuable information from unla- beled data. Recent works ( <ref type="bibr" target="#b0">Ammar et al., 2014;</ref><ref type="bibr" target="#b15">Lin et al., 2015)</ref> suggested using an autoencoder with a CRF model as an encoder in an unsupervised set- ting. We significantly expand on these works and suggest the following contributions:</p><p>1. We propose a unified model seamlessly ac- commodating both unlabeled and labeled data. While past work focused on unsupervised struc- tured prediction, neglecting the discriminative power of such models, our model easily sup- ports learning in both fully supervised and semi- supervised settings. We developed a variation of the Expectation-Maximization (EM) algorithm, used for optimizing the encoder and the decoder of our model simultaneously.</p><p>2. We increase the expressivity of the traditional CRF autoencoder model using neural networks as the potential extractors, thus avoiding the heavy feature engineering necessary in previous works. Interestingly, our model's predictions, which unify the discriminative neural CRF encoder and the generative decoder, have led to an improved performance over the highly optimized neural CRF (NCRF) model alone, even when trained in the supervised settings over the same data.</p><p>3. We demonstrate the advantages of our model empirically, focusing on the well-known Part- of-Speech (POS) tagging problem over 8 differ- ent languages, including low resource languages. In the supervised setting, our NCRF-AE outper- formed the highly optimized NCRF. In the semi- supervised setting, our model was able to suc- cessfully utilize unlabeled data, improving on the performance obtained when only using the la- beled data, and outperforming competing semi- supervised learning algorithms.</p><p>Furthermore, our newly proposed algorithm is directly applicable to other sequential learn- ing tasks in NLP, and can be easily adapted to other structured tasks such as dependency parsing or constituent parsing by replacing the forward- backward algorithm with the inside-outside algo- rithm. All of these tasks can benefit from semi- supervised learning algorithms. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Neural networks were successfully applied to many NLP tasks, including tagging ( <ref type="bibr" target="#b16">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b20">Mesnil et al., 2015;</ref><ref type="bibr" target="#b14">Lample et al., 2016)</ref>, parsing <ref type="bibr" target="#b4">(Chen and Manning, 2014</ref>), text generation <ref type="bibr" target="#b29">(Sutskever et al., 2011</ref>), machine trans- lation ( <ref type="bibr" target="#b3">Bahdanau et al., 2015)</ref>, sentiment anal- ysis <ref type="bibr" target="#b11">(Kim, 2014)</ref> and question answering <ref type="bibr" target="#b2">(Andreas et al., 2016)</ref>. Most relevant to this work are structured prediction models capturing dependen- cies between decisions, either by modeling the de- pendencies between the hidden representations of connected decisions using RNN/LSTM ( <ref type="bibr" target="#b10">Katiyar and Cardie, 2016)</ref>, by explic- itly modeling the structural dependencies between output predictions <ref type="bibr" target="#b7">(Durrett and Klein, 2015;</ref><ref type="bibr" target="#b14">Lample et al., 2016;</ref><ref type="bibr" target="#b1">Andor et al., 2016)</ref>, or by combin- ing the two approaches <ref type="bibr" target="#b25">(Socher et al., 2013;</ref><ref type="bibr" target="#b33">Wiseman and Rush, 2016)</ref>.</p><p>In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in ( <ref type="bibr" target="#b24">Quattoni et al., 2007)</ref>, which utilize additional la- tent variables to infer for supervised structure pre- diction, we do not presume any additional latent variables in our NCRF-AE model in both super- vised and semi-supervised setting.</p><p>The difficulty of providing sufficient super- vision has motivated work on semi-supervised and unsupervised learning for many of these tasks <ref type="bibr" target="#b18">(McClosky et al., 2006;</ref><ref type="bibr" target="#b26">Spitkovsky et al., 2010;</ref><ref type="bibr" target="#b28">Subramanya et al., 2010;</ref><ref type="bibr" target="#b27">Stratos and Collins, 2015;</ref><ref type="bibr" target="#b17">Marinho et al., 2016;</ref><ref type="bibr" target="#b30">Tran et al., 2016)</ref>, including several that also used autoen- coders ( <ref type="bibr" target="#b0">Ammar et al., 2014;</ref><ref type="bibr" target="#b15">Lin et al., 2015;</ref><ref type="bibr" target="#b21">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b12">Kocisk´yKocisk´y et al., 2016;</ref><ref type="bibr" target="#b5">Cheng et al., 2017)</ref>. In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural CRF Autoencoder</head><p>In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoen- coders offer a convenient way of dealing with both types of data in a unified fashion.</p><p>A generalized autoencoder <ref type="figure">(Figure 1a</ref>) tries to reconstruct the inputˆXinputˆ inputˆX given the original input X, aiming to maximize the log probability P ( ˆ X|X) without knowing the latent variable Y explicitly. Since we focus on sequential structured prediction problems, the encoding and decoding processes are no longer for a single data point (x, y) (x if unlabeled), but for the whole input instance and output sequence (x, y) (x if unlabeled). Addition- ally, as our main purpose in this study is to recon- struct the input with precision, ˆ x is just a copy of x.</p><formula xml:id="formula_0">Encoder Decoderˆx Decoderˆ Decoderˆx t1ˆx t1ˆ t1ˆx t+1ˆx t+1ˆ t+1ˆx t y t y t1 y t+1 X Y ˆ X x (a) A generalized autoencoder. Encoder Decoderˆx Decoderˆ Decoderˆx t1ˆx t1ˆ t1ˆx t+1ˆx t+1ˆ t+1ˆx t y t y t1 y t+1 X Y ˆ X x (b)</formula><p>The neural CRF autoencoder model in this work.</p><p>Figure 1: On the left is a generalized autoencoder, of which the lower half is the encoder and the up- per half is the decoder. On the right is an illus- tration of the graphical model of our NCRF-AE model. The yellow squares are interactive poten- tials among labels, and the green squares represent the unary potentials generated by the neural net- works.</p><p>As shown in <ref type="figure">Figure 1b</ref>, our NCRF-AE model consists of two parts: the encoder (the lower half) is a discriminative CRF model enhanced by deep neural networks as its potential extractors with en- coding parameters Λ, describing the probability of a predicted sequence of labels given the input; the decoder (the upper half) is a generative model with reconstruction parameters Θ, modeling the probability of reconstructing the input given a se- quence of labels. Accordingly, we present our model mathematically as follows:</p><formula xml:id="formula_1">P Θ,Λ (ˆ x|x) = y P Θ,Λ (ˆ x, y|x) = y P Θ (ˆ x|y)P Λ (y|x),</formula><p>where P Λ (y|x) is the probability given by the neural CRF encoder, and P Θ (ˆ x|y) is the proba- bility produced by the generative decoder.</p><p>When making a prediction, the model tries to find the most probable output sequence by per- forming the following inference procedure using the Viterbi algorithm:</p><formula xml:id="formula_2">y * = arg max y P Θ,Λ (ˆ x, y|x).</formula><p>To clarify, as we focus on POS tagging prob- lems in this study, in the unsupervised setting where the true POS tags are unknown, the labels used for reconstruction are actually the POS tags being induced. The labels induced here are core- spoding to the hidden nodes in a generalized au- toencoder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural CRF Encoder</head><p>In a CRF model, the probability of predicted labels y, given sequence x as input is modeled as</p><formula xml:id="formula_3">P Λ (y|x) = e Φ(x,y) Z ,</formula><p>where</p><formula xml:id="formula_4">Z = ˜ y e Φ(x, ˜ y)</formula><p>is the partition function that marginalize over all possible assignments to the predicted labels of the sequence, and Φ(x, y) is the scoring function, which is defined as:</p><formula xml:id="formula_5">Φ(x, y) = t φ(x, y t ) + ψ(y t−1 , y t ).</formula><p>The partition function Z can be computed effi- ciently via the forward-backward algorithm. The term φ(x, y t ) corresponds to the score of a par- ticular tag y t at position t in the sequence, and ψ(y t−1 , y t ) represents the score of transition from the tag at position t − 1 to the tag at position t.</p><p>In our NCRF-AE model, φ(x, y t ) is described by deep neural networks while ψ(y t−1 , y t ) by a tran- sition matrix. Such a structure allows for the use of distributed representations of the input, for in- stance, the word embeddings on a continuous vec- tor space ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>).</p><p>Typically in our work, φ(x, y t ) is modeled jointly by a multi-layer perceptron (MLP) that utilizes the word-level information, and a bi- directional long-short term memory (LSTM) neu- ral network (Hochreiter and Urgen Schmidhuber, 1997) that captures the character level information within each word. A bi-directional structure can extract character level information from both di- rections, with which we expect to catch the pre- fix and suffix information of words in an end- to-end system, rather than using hand-engineered features. The bi-directional LSTM neural network consumes character embeddings e c ∈ R k 1 as in- put, where k 1 is the dimensionality of the charac- ter embeddings. A normal LSTM can be denoted as:</p><formula xml:id="formula_6">i t = σ(W ei e ct + W hi h t−1 + b i ), f t = σ(W ef e ct + W hf h t−1 + b f ), o t = σ(W eo e ct + W ho h t−1 + b o ), g t = Relu(W ec e ct + W hc h t−1 + b c ), c t = f t c t−1 + i t g t , h t = o t tanh(c t ),</formula><p>where denotes element-wise multiplication. Then a bi-directional LSTM neural network ex- tends it as follows, by denoting the procedure of generating h t as H:</p><formula xml:id="formula_7">− → h t = H(W e − → h e ct + W− → h − → h − → h t−1 + b− → h ), ← − h t = H(W e ← − h e ct + W← − h ← − h ← − h t−1 + b← − h ),</formula><p>where e ct here is the character embedding for character c in position t in a word.</p><p>The inputs to the MLP are word embed- dings e v ∈ R k 2 for each word v, where k 2 is the dimensionality of the vector, concatenated with the final representation generated by the bi-directional LSTM over the characters of that word:</p><formula xml:id="formula_8">u = [e v ; − → h v ; ← − h v ].</formula><p>In order to leverage the capacity of the CRF model, we use a word and its context together to generate the unary potential. More specifically, we adopt a concatenation v t = [u t−(w−1)/2 ; · · · ; u t−1 ; u t ; u t+1 ; · · · ; u t+(w−1)/2 ] as the inputs to the MLP model, where t denotes the position in a sequence, and w being an odd number indicates the context size. Further, in order to enhance the generality of our model, we add a dropout layer on the input right before the MLP layer as a regularizer. Notice that different from a normal MLP, the activation function of the last layer is no more a softmax function, but a linear function generates the log-linear part φ t (x, y t ) of the CRF model:</p><formula xml:id="formula_9">h t = Relu(W v t + b) φ t = w y h t + b y .</formula><p>The transition score ψ(y t−1 , y t ) is a single scalar representing the interactive potential. We use a transition matrix Ψ to cover all the transi- tions between different labels, and Ψ is part of the encoder parameters Λ.</p><p>All the parameters in the neuralized encoder are updated when the loss function is minimized via error back-propagation through all the structures of the neural networks and the transition matrix.</p><p>The detailed structure of the neural CRF en- coder is demonstrated in <ref type="figure">Fig 2.</ref> Note that the MLP layer is also interchangeable with a recur- rent neural network (RNN) layer or LSTM layer. But in our pilot experiments, we found a single MLP structure yields better performance, which we conjecture is due to over-fitting caused by the high complexity of those alternatives.  <ref type="figure">Figure 2</ref>: A demonstration of the neural CRF en- coder. l t and r t are the output of the forward and backward character-level LSTM of the word at po- sition t in a sentence, and e t is the word-level em- bedding of that word. u t is the concatenation of e t , l t and r t , denoted by blue dashed arrows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Decoder</head><p>In our NCRF-AE, we assume the generative pro- cess follows several multinomial distributions: each label y has the probability θ y→x to recon- struct the corresponding word x, i.e., P (x|y) = θ y→x . This setting naturally leads to a constraint</p><formula xml:id="formula_10">x θ y→x = 1.</formula><p>The number of parameters of the decoder is |Y| × |X |. For a whole sequence, the reconstruction probability is P Θ (ˆ x|y) = t P (ˆ x t |y t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Unified Learning Framework</head><p>We first constructed two loss functions for labeled and unlabeled data using the same model. Our model is trained in an on-line fashion: given a labeled or unlabeled sentence, our NCRF-AE op- timizes the loss function by choosing the corre- sponding one. In an analogy to coordinate de- scent, we optimize the loss function of the NCRF-AE by alternatively updating the parameters Θ in the decoder and the parameters Λ in the encoder. The parameters Θ in the decoder are updated via a variation of the Expectation-Maximization (EM) algorithm, and the the parameters Λ in the encoder are updated through a gradient-based method due to the non-convexity of the neuralized CRF. In contrast to the early autoencoder models ( <ref type="bibr" target="#b0">Ammar et al., 2014;</ref><ref type="bibr" target="#b15">Lin et al., 2015)</ref>, our model has two distinctions: First, we have two loss functions to model labeled example and unlabeled examples; Second, we designed a variant of EM algorithm to alternatively learn the parameters of the encoder and the decoder at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unified Loss Functions for Labeled and unlabeled Data</head><p>For a sequential input with labels, the complete data likelihood given by our NCRF-AE is</p><formula xml:id="formula_11">P Θ,Λ (ˆ x, y|x) = P Θ (ˆ x|y)P Λ (y|x) = t P (ˆ x t |y t ) e Φ(x,y) Z = e t st(x,y) Z ,</formula><p>where s t (x, y) = log P (x t |y t ) + φ(x, y t ) + ψ(y t−1 , y t ).</p><p>If the input sequence is unlabeled, we can sim- ply marginalize over all the possible assignment to labels. The probability is formulated as Our formulation have two advantages. First, term U is different from but in a similar form as term Z, such that to calculate the probability P (ˆ x|x) for an unlabeled sequence, the forward- backward algorithm to compute the partition func- tion Z can also be applied to compute U effi- ciently. Second, our NCRF-AE highlights a uni- fied structure of different loss functions for labeled and unlabeled data with shared parameters. Thus during training, our model can address both la- beled and unlabeled data well by alternating the loss functions. Using negative log-likelihood as our loss function, if the data is labeled, the loss function is:</p><formula xml:id="formula_12">P Θ,Λ (ˆ x|x) = y P (ˆ x, y|x) = U Z ,</formula><formula xml:id="formula_13">loss l = − log P Θ,Λ (ˆ x, y|x) = −( t s t (x, y) − log Z)</formula><p>If the data is unlabeled, the loss function is:</p><formula xml:id="formula_14">loss u = − log P Θ,Λ (ˆ x|x) = −(log U − log Z).</formula><p>Thus, during training, based on whether the en- countered data is labeled or unlabeled, our model can select the appropriate loss function for learn- ing parameters. In practice, we found for labeled data, using a combination of loss l and loss u actu- ally yields better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mixed Expectation-Maximization Algorithm</head><p>The Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b6">(Dempster et al., 1977</ref>) was applied to a wide range of problems. Generally, it establishes a lower-bound of the objective function by using Jensen's Inequality. It first tries to find the pos- terior distribution of the latent variables, and then based on the posterior distribution of the latent variables, it maximizes the lower-bound. By al- ternating expectation (E) and maximization (M) steps, the algorithm iteratively improves the lower- bound of the objective function. In this section we describe the mixed Expectation-Maximization (EM) algorithm used in our study. Parameterized by the encoding parameters Λ and the reconstruction parameters Θ, our NCRF-AE consists of the encoder and the decoder, which together forms the log-likelihood a highly non-convex function. However, a careful observation shows that if we fix the encoder, the lower bound derived in the E step, is convex with respect to the reconstruction parameters Θ in the M step. Hence, in the M step we can analytically obtain the global optimum of Θ. In terms of the reconstruction parameters Θ by fixing Λ, we describe our EM algorithm in iteration t as follows:</p><p>In the E-step, we let Q(y i ) = P (y i |x i , ˆ x i ), and treat y i the latent variable as it is not observable in unlabeled data. We derive the lower-bound of the log-likelihood using Q(y i ):</p><formula xml:id="formula_15">i log P (ˆ x i |x i ) = i log y i Q(y i ) P (ˆ x i , y i |x i ) Q(y i ) ≥ i y i Q(y i ) log P (ˆ x i , y i |x i ) Q(y i ) ,</formula><p>where Q(y i ) is computed using parameters Θ (t−1) in the previous iteration t − 1.</p><p>In the M-step, we try to improve the aforemen- tioned lower-bound using all examples:</p><p>arg max</p><formula xml:id="formula_16">Θ (t) i y i Q(y i ) log P Θ (t) (ˆ x i |y i )P Λ (y i |x i ) Q(y i )</formula><p>arg max</p><formula xml:id="formula_17">Θ (t) i y i Q(y i ) log P Θ (t) (ˆ x i |y i ) + const arg max Θ (t) y→x log θ (t) y→x y Q(y)C(y, x)</formula><p>arg max</p><formula xml:id="formula_18">Θ (t) y→x log θ (t) y→x E y∼Q [C(y, x)] s.t. x θ (t) y→x = 1.</formula><p>In this formulation, const is a constant with re- spect to the parameters we are updating. Q(y) is the distribution of a label y at any position by marginalizing labels at all other positions in a se- quence. By denoting C(y, x) as the number of times that (x, y) co-occurs, E y∼Q Θ (t−1) [C(y, x)] is the expected count of a particular reconstruc- tion at any position, which can also be calculated using Baum-Welch algorithm <ref type="bibr" target="#b32">(Welch, 2003)</ref>, and can be summed over for all examples in the dataset (In the labeled data, it is just a real count). The al- gorithm we used to calculate the expected count is described in Algorithm 1. Therefore, it can be shown that the aforementioned global optimum can be calculated by simply normalizing the ex- pected counts. In terms of the encoder's parame- ters Λ, they are first updated via a gradient-based optimization before each EM iteration. Based on the above discussion, our Mixed EM Algorithm is presented in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Obtain Expected Count (T e )</head><p>Require: the expected count table T e 1: for an unlabeled data example x i do 2:</p><p>Compute the forward messages: α(y, t) ∀y, t.</p><p>t is the position in a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Compute the backward messages: β(y, t) ∀y, t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Calculate the expected count for each x in x i : P (y t |x t ) ∝ α(y, t) × β(y, t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>T e (x t , y t ) ← T e (x t , y t ) + P (y t |x t ) T e is the expected count Train the encoder on labeled data {x, y} l and unlabeled data {x} u to update Λ (t−1) to Λ (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Re-initialize expected count table T e with 0s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Use labeled data {x, y} l to calculate real counts and update T e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Use unlabeled data {x} u to compute the expected counts with parameters Λ (t) and Θ (t−1) and update T e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Obtain Θ (t) globally and analytically based on T e . 9: end for This mixed EM algorithm is a combination of the gradient-based approach to optimize the en- coder by minimizing the negative log-likelihood as the loss function, and the EM approach to up- date the decoder's parameters by improving the lower-bound of the log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Settings</head><p>Dataset We evaluated our model on the POS tagging task, in both the supervised and semi- supervised learning settings, over eight different languages from the UD (Universal Dependencies) 1.4 dataset ( <ref type="bibr" target="#b19">Mcdonald et al., 2013</ref>). The task is defined over 17 different POS tags, used across the different languages. We followed the original English French German Italian Russian <ref type="table" target="#tab_3">Spanish Indonesian Croatian  Tokens  254830 391107 293088 272913 99389  423346  121923  139023  Training  12543  14554  14118  12837  4029  14187  4477  5792  Development 2002  1596  799  489  502  1552  559  200  Testing  2077  298  977  489  499  274  297  297   Table 1</ref>: Statistics of different UD languages used in our experiments, including the number of tokens, and the number of sentences in training, development and testing set respectively.</p><p>UD division for training, development and test- ing in our experiments. The statistics of the data used in our experiments are described in table 1. The UD dataset includes several low-resource lan- guages which are of particular interest to our semi- supervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Representation and Neural Architecture</head><p>Our model uses word embeddings as input. In our pilot experiments, we compared the performance on the English dataset of the pre-trained embed- ding from Google News ( <ref type="bibr" target="#b22">Mikolov et al., 2013)</ref> and the embeddings we trained directly on the UD dataset using the skip-gram algorithm ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>). We found these two types of em- beddings yield very similar performance on the POS tagging task. So in our experiments, we used embeddings of different languages directly trained on the UD dataset as input to our model, whose dimension is 200. For the MLP neural network layer, the number of hidden nodes in the hidden layer is 20, which is the same for the hidden layer in the character-level LSTM. The dimension of the character-level embeddings sent into the LSTM layer is 15, which is randomly initialized. In or- der to incorporate the global information of the in- put sequence, we set the context window size to 3. The dropout rate for the dropout layer is set to 0.5.</p><p>Learning We used ADADELTA <ref type="bibr" target="#b34">(Zeiler, 2012)</ref> to update parameters Λ in the encoder, as ADADELTA dynamically adapts learning rate over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent (SGD). The au- thors of ADADELTA also argue this method ap- pears robust to noisy gradient information, dif- ferent model architecture choices, various data modalities and selection of hyper-parameters. We observed that ADADELTA indeed had faster con- vergence than vanilla SGD optimization. In our experiments, we include word embeddings and character embeddings as parameters as well. We used Theano to implement our algorithm, and all the experiments were run on NVIDIA GPUs. To prevent over-fitting, we used the "early-stop" strat- egy to determine the appropriate number of epochs during training. We did not take efforts to tune those hyper-parameters and they remained the same in both our supervised and semi-supervised learning experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Supervised Learning</head><p>In these settings our Neural CRF autoencoder model had access to the full amount of annotated training data in the UD dataset. As described in Section 5, the decoder's parameters Θ were esti- mated using real counts from the labeled data. We compared our model with existing sequence labeling models including HMM, CRF, LSTM and neural CRF (NCRF) on all the 8 languages. Among these models, the NCRF can be most di- rectly compared to our model, as it is used as the base of our model, but without the decoder (and as a result, can only be used for supervised learning).</p><p>The results, summarized in <ref type="table" target="#tab_3">Table 2</ref>, show that our NCRF-AE consistently outperformed all other systems, on all the 8 languages, including Russian, Indonesian and Croatian which had considerably less data compared to other languages. Interest- ingly, the NCRF consistently came second to our model, which demonstrates the efficacy of the ex- pressivity added to our model by the decoder, to- gether with an appropriate optimization approach.</p><p>To better understand the performance difference by different models, we performed error analy- sis, using an illustrative example, described in <ref type="figure" target="#fig_2">Fig- ure 3</ref>.</p><p>In this example, the LSTM incorrectly predicted the POS tag of the word "search" as a verb, instead of a noun (part of the NP "nice search engine"), while predicting correctly the preceding word, "nice", as an adjective. We attribute the error to LSTM lacking an explicit output transition scor- ing function, which would penalize the ungram- matical transition between "ADJ" and "VERB".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English French German Italian</head><p>Russian Spanish Indonesian Croatian     The NCRF, which does score such transitions, correctly predicted that word. However, it incor- rectly predicted "Google" as a noun rather than a proper-noun. This is a subtle mistake, as the two are grammatically and semantically similar. This mistake appeared consistently in the NCRF results, while NCRF-AE predictions were correct.</p><p>We attribute this success to the superior ex- pressivity of our model: The prediction is done jointly by the encoder and the decoder, as the re- construction decision is defined over all output sequences, picking the jointly optimal sequence. From another perspective, our NCRF-AE model is a combination of discriminative and generative models, in that sense the decoder can be regarded as a soft constraint that supplements the encoder. Such that, the decoder performs as a regularizer to check-balance the choices made by the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Semi-supervised Learning</head><p>In the semi-supervised settings we compared our models with other semi-supervised structured pre- diction models. In addition, we studied how vary- ing the amount of unlabeled data would change the performance of our model.</p><p>As described in Sec. 5, the decoder's parame- ters Θ are initialized by the labeled dataset using real counts and updated in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Varying Unlabeled Data Proportion</head><p>We first experimented with varying the proportion of unlabeled data, while fixing the amount of la- beled data. We conducted these experiments over two languages, English and low-resource language Croatian. We fixed the proportion of labeled data at 20%, and gradually added more unlabeled data from 0% to 20% (from full supervision to semi- supervision). The unlabeled data was sampled from the same dataset (without overlapping with the labeled data), with the labels removed. The results are shown in <ref type="figure">Figure 4</ref>.</p><p>The left most point of both sub-figures is the accuracy of fully supervised learning with 20% of the whole data. As we can observe, the tagging accuracy increased as the proportion of unlabeled data increased.  <ref type="figure">Figure 4</ref>: UD English and Croatian POS tag- ging accuracy versus increasing proportion of un- labeled sequences using 20% labeled data. The green straight line is the performance of the neural CRF, trained over the labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Semi-supervised POS Tagging on Multiple Languages</head><p>We compared our NCRF-AE model with other semi-supervised learning models, including the HMM-EM algorithm and the hard-EM version of our NCRF-AE. The hard EM version of our model can be considered as a variant of self-training, as it infers the missing labels using the current model in the E-step, and uses the real counts of these la- bels to update the model in the M-step. To contex- tualize the results, we also provide the results of the NCRF model and the supervised version our NCRF-AE model trained on 20% of the data. We set the proportion of labeled data to 20% for each language and set the proportion of unlabeled data to 50% of the dataset. There was no overlap be- tween labeled and unlabeled data.</p><p>The results are summarized in <ref type="table" target="#tab_4">Table 3</ref>. Simi- lar to the supervised experiments, the supervised version of our NCRF-AE, trained over 20% of the labeled data, outperforms the NCRF model. Our model was able to successfully use the unlabeled data, leading to improved performance in all lan- guages, over both the supervised version of our model, as well as the HMM-EM and Hard-EM models that were also trained over both the labeled and unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Varying Sizes of Labeled Data on English</head><p>As is known to all, semi-supervised approaches tend to work well when given a small size of la- beled training data. But with the increase of la- beled training data size, we might get diminishing effectiveness. To verify this conjecture, we con- ducted additional experiments to show how vary- ing sizes of labeled training data affect the effec- tiveness of our NCRF-AE model. In these exper- NCRF-AE-Semi NCRF-AE-Sup iments, we gradually increased the proportion of labeled data, and in accordance decreased the pro- portion of unlabeled data.</p><p>The results of these experiments are demon- strated in <ref type="figure" target="#fig_3">Figure 5</ref>. As we speculated, we ob- served diminishing effectiveness when increasing the proportion of labeled data in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed an end-to-end neural CRF autoen- coder (NCRF-AE) model for semi-supervised se- quence labeling. Our NCRF-AE is an integration of a discriminative model and generative model which extends the generalized autoencoder by us- ing a neural CRF model as its encoder and a gen- erative decoder built on top of it. We suggest a variant of the EM algorithm to learn the parame- ters of our NCRF-AE model. We evaluated our model in both supervised and semi-supervised scenarios over multiple lan- guages, and show it can outperform other super- vised and semi-supervised methods. Additional experiments suggest how varying sizes of labeled training data affect the effectiveness of our model.</p><p>These results demonstrate the strength of our model, as it was able to utilize the small amount of labeled data and exploit the hidden information from the large amount of unlabeled data, with- out additional feature engineering which is of- ten needed in order to get semi-supervised and weakly-supervised systems to perform well. The superior performance on the low resource lan- guage also suggests its potential in practical use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>HMM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example from the test set to compare the predicted results of our NCRF-AE model, the NCRF model and the LSTM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of the NCRF-AE model on different proportion of labeled and unlabeled data. The green line shows the results on only labeled data, and the red line on both labeled and unlabeled data. The difference between the red line and the green line are gradually vanishing.</figDesc><graphic url="image-39.png" coords="9,350.07,88.03,149.60,98.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Supervised learning accuracy of POS tagging on 8 UD languages using different models 

Models 
English French German Italian 
Russian Spanish Indonesian Croatian 

NCRF (OL) 

88.01% 93.38% 90.43% 91.75% 86.63% 91.22% 
88.35% 
86.11% 

NCRF-AE 

(OL) 

88.41% 93.69% 90.75% 92.17% 87.82% 91.70% 
89.06% 
87.92% 

HMM-EM 

79.92% 88.15% 77.01% 84.57% 72.96% 86.77% 
83.61% 
77.20% 

NCRF-AE 

(HEM) 

86.79% 92.83% 89.78% 90.68% 86.39% 91.30% 
88.86% 
86.55% 

NCRF-AE 

89.43% 93.89% 90.99% 92.85% 88.93% 92.17% 
89.41% 
89.14% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Semi-supervised learning accuracy of POS tagging on 8 UD languages. HEM means hard-EM, 
used as a self-training approach, and OL means only 20% of the labeled data is used and no unlabeled 
data is used. 

Text Google is a nice search engine . 

</table></figure>

			<note place="foot" n="1"> Our code and experimental set up will be available at https://github.com/cosmozhang/NCRF-AE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional random field autoencoders for unsupervised structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representation (ICLR)</title>
		<meeting>International Conference on Learning Representation (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods for Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum reconstruction estimation for generative latentvariable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>of the National Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Neural crf parsing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating lstms for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods for Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods for Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>John D Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando C N</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning (ICML)</title>
		<meeting>of the International Conference on Machine Learning (ICML)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised pos induction with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1311" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of sequence models with the method of moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods for Natural Language essing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Tckstrm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Bedini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nria</forename><surname>Bertomeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Castell Jungmee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using recurrent neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods for Natural Language essing (EMNLP)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distributed representations of words and phrases and their compositionality. The Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sybor</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From baby steps to leapfrog: How less is more in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple semisupervised pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient graph-based semisupervised learning of structured tagging models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods for Natural Language essing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning (ICML)</title>
		<meeting>of the International Conference on Machine Learning (ICML)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised neural hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supertagging with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hidden markov models and the baum-welch algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lloyd R Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Information Theory Society Newsletter</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="10" to="13" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
