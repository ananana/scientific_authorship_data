<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Inside-Outside Recursive Neural Network model for Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Logic</orgName>
								<orgName type="institution">Computation University of Amsterdam</orgName>
								<address>
									<settlement>Language</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
							<email>{p.le,zuidema}@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Logic</orgName>
								<orgName type="institution">Computation University of Amsterdam</orgName>
								<address>
									<settlement>Language</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Inside-Outside Recursive Neural Network model for Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="729" to="739"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose the first implementation of an infinite-order generative dependency model. The model is based on a new recursive neural network architecture, the Inside-Outside Recursive Neural Network. This architecture allows information to flow not only bottom-up, as in traditional recursive neural networks, but also top-down. This is achieved by computing content as well as context representations for any constituent, and letting these representations interact. Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a per-plexity seven times lower than the traditional third-order model using counting, and tends to choose more accurate parses in k-best lists. In addition, reranking with this model achieves state-of-the-art unla-belled attachment scores and unlabelled exact match scores.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating probability distributions is the core is- sue in modern, data-driven natural language pro- cessing methods. Because of the traditional defi- nition of discrete probability P r(A) ≡ the number of times A occurs the size of event space</p><p>counting has become a standard method to tackle the problem. When data are sparse, smoothing techniques are needed to adjust counts for non- observed or rare events. However, successful use of those techniques has turned out to be an art. For instance, much skill and expertise is required to create reasonable reduction lists for back-off, and to avoid impractically large count tables, which store events and their counts.</p><p>An alternative to counting for estimating prob- ability distributions is to use neural networks. Thanks to recent advances in deep learning, this approach has recently started to look very promis- ing again, with state-of-the-art results in senti- ment analysis <ref type="bibr" target="#b31">(Socher et al., 2013)</ref>, language mod- elling ( <ref type="bibr" target="#b22">Mikolov et al., 2010)</ref>, and other tasks. The <ref type="bibr" target="#b22">Mikolov et al. (2010)</ref> work, in particular, demon- strates the advantage of neural-network-based ap- proaches over counting-based approaches in lan- guage modelling: it shows that recurrent neu- ral networks are capable of capturing long histo- ries efficiently and surpass standard n-gram tech- niques (e.g., Kneser-Ney smoothed 5-gram).</p><p>In this paper, keeping in mind the success of these models, we compare the two approaches. Complementing recent work that focused on such a comparison for the case of finding appropriate word vectors ( <ref type="bibr" target="#b0">Baroni et al., 2014</ref>), we focus here on models that involve more complex, hierarchical structures. Starting with existing generative mod- els that use counting to estimate probability distri- butions over constituency and dependency parses (e.g., <ref type="bibr" target="#b9">Eisner (1996b)</ref>, <ref type="bibr" target="#b5">Collins (2003)</ref>), we develop an alternative based on recursive neural networks. This is a non-trivial task because, to our knowl- edge, no existing neural network architecture can be used in this way. For instance, classic recur- rent neural networks <ref type="bibr" target="#b10">(Elman, 1990)</ref> unfold to left- branching trees, and are not able to process ar- bitrarily shaped parse trees that the counting ap- proaches are applied to. Recursive neural net- works ( <ref type="bibr" target="#b28">Socher et al., 2010</ref>) and extensions <ref type="bibr" target="#b30">(Socher et al., 2012;</ref><ref type="bibr" target="#b17">Le et al., 2013</ref>), on the other hand, do work with trees of arbitrary shape, but pro- cess them in a bottom-up manner. The probabil- ities we need to estimate are, in contrast, defined by top-down generative models, or by models that require information flows in both directions (e.g., the probability of generating a node depends on the whole fragment rooted at its just-generated sis- ter).</p><p>To tackle this problem, we propose a new ar- chitecture: the Inside-Outside Recursive Neural Network (IORNN) in which information can flow not only bottom-up but also top-down, inward and outward. The crucial innovation in our architec- ture is that every node in a hierarchical structure is associated with two vectors: one vector, the in- ner representation, representing the content under that node, and another vector, the outer represen- tation, representing its context (see <ref type="figure" target="#fig_0">Figure 1</ref>). In- ner representations can be computed bottom-up; outer representations, in turn, can be computed top-down. This allows information to flow in any direction, depending on the application, and makes the IORNN a natural tool for estimating probabilities in tree-based generative models.</p><p>We demonstrate the use of the IORNN by ap- plying it to an ∞-order generative dependency model which is impractical for counting due to the problem of data sparsity. Counting, instead, is used to estimate a third-order generative model as in <ref type="bibr" target="#b26">Sangati et al. (2009)</ref> and <ref type="bibr" target="#b12">Hayashi et al. (2011)</ref>. Our experimental results show that our new model not only achieves a seven times lower perplex- ity than the third-order model, but also tends to choose more accurate candidates in k-best lists. In addition, reranking with this model achieves state- of-the-art scores on the task of supervised depen- dency parsing.</p><p>The outline of the paper is following. Firstly, we give an introduction to Eisner's generative model in Section 2. Then, we present the third-order model using counting in Section 3, and propose the IORNN in Section 4. Finally, in Section 5 we show our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Eisner's Generative Model</head><p>Eisner (1996b) proposed a generative model for dependency parsing. The generation process is top-down: starting at the ROOT, it generates left dependents and then right dependents for the ROOT. After that, it generates left dependents and right dependents for each of ROOT's dependents. The process recursively continues until there is no further dependent to generate. The whole process is captured in the following formula</p><formula xml:id="formula_0">P (T (H)) = L l=1 P H L l |C H L l P T (H L l ) × R r=1 P H R r |C H R r P T (H R r ) (1)</formula><p>where H is the current head, T (N ) is the fragment of the dependency parse rooted in N , and C N is the context in which N is generated. H L , H R are respectively H's left dependents and right depen- dents, plus EOC (End-Of-Children), a special to- ken to indicate that there are no more dependents to generate. Thus, P (T (ROOT )) is the proba- bility of generating the entire dependency struc-</p><formula xml:id="formula_1">ture T . We refer to H L l , C H L l , H R r , C H R r as "events", and C H L l , C H R r</formula><p>as "conditioning con- texts".</p><p>In order to avoid the problem of data sparsity, the conditioning context in which a dependent D is generated should capture only part of the frag- ment generated so far. Based on the amount of information that contexts hold, we can define the order of a generative model (see <ref type="bibr" target="#b12">Hayashi et al. (2011</ref> <ref type="table" target="#tab_0">, Table 3</ref>) for examples)</p><p>• first-order: C 1 D contains the head H, • second-order: C 2 D contains H and the just- generated sibling S, • third-order: C 3 D contains H, S, the sibling S before S (tri-sibling); or H, S and the grand- head G (the head of H) (grandsibling) (the fragment enclosed in the blue doted contour in <ref type="figure" target="#fig_1">Figure 2</ref>), • ∞-order: C ∞ D contains all of D's ancestors, theirs siblings, and its generated siblings (the fragment enclosed in the red dashed contour in <ref type="figure" target="#fig_1">Figure 2</ref>).</p><p>In the original models <ref type="bibr" target="#b8">(Eisner, 1996a)</ref>, each de- pendent D is a 4-tuple dist, w, c, t</p><p>• dist(H, D) the distance between D and its head H, represented as one of the four ranges 1, 2, 3-6, 7-∞. • word(D) the lowercase version of the word of D, • cap(D) the capitalisation feature of the word of D (all letters are lowercase, all letters are uppercase, the first letter is uppercase, the first letter is lowercase),</p><formula xml:id="formula_2">• tag(D) the POS-tag of D,</formula><p>Here, to make the dependency complete, deprel(D), the dependency relation of D (e.g., SBJ, DEP), is also taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Third-order Model with Counting</head><p>The third-order model we suggest is similar to the grandsibling model proposed by <ref type="bibr" target="#b26">Sangati et al. (2009)</ref> and <ref type="bibr" target="#b12">Hayashi et al. (2011)</ref>. It defines the probability of generating a dependent D = dist, d, w, c, t as the product of the distance- based probability and the probabilities of gener- ating each of its components (d, t, w, c, denoting dependency relation, POS-tag, word and capitali- sation feature, respectively). Each of these prob- abilities is smoothed using back-off according to the given reduction lists (as explained below).</p><formula xml:id="formula_3">P (D|CD) = P (dist(H, D), dwct(D)|H, S, G, dir) = P (d(D)|H, S, G, dir) reduction list: tw(H), tw(S), tw(G), dir tw(H), tw(S), t(G), dir tw(H), t(S), t(G), dir t(H), tw(S), t(G), dir t(H), t(S), t(G), dir × P (t(D)|d(D), H, S, G, dir) reduction list: d(D), dtw(H), t(S), dir d(D), d(H), t(S), dir d(D), d(D), dir × P (w(D)|dt(D), H, S, G, dir) reduction list: dtw(H), t(S), dir dt(H), t(S), dir × P (c(D)|dtw(D), H, S, G, dir) reduction list: tw(D), d(H), dir tw(D), dir × P (dist(H, D)|dtwc(D), H, S, G, dir) (2) reduction list: dtw(D), dt(H), t(S), dir dt(D), dt(H), t(S), dir</formula><p>The reason for generating the dependency rela- tion first is based on the similarity between rela- tion/dependent and role/filler: we generate a role and then choose a filler for that role.</p><p>Back-off The back-off parameters are identi- cal to <ref type="bibr" target="#b9">Eisner (1996b)</ref>. To estimate the proba- bility</p><formula xml:id="formula_4">P (A|context) given a reduction list L = (l 1 , l 2 , ..., l n ) of context, let p i = count(A,l i )+0.005 count(l i )+0.5 if i = n count(A,l i )+3p i+1 count(l i )+3</formula><p>otherwise then P (A|context) = p 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Inside-Outside Recursive Neural Network</head><p>In this section, we first describe the Recur- sive Neural Network architecture of <ref type="bibr" target="#b28">Socher et al. (2010)</ref> and then propose an extension we call the Inside-Outside Recursive Neural Network (IORNN). The IORNN is a general architecture for trees, which works with tree-based genera- tive models including those employed by Eisner (1996b) and <ref type="bibr" target="#b5">Collins (2003)</ref>. We then explain how to apply the IORNN to the ∞-order model. Note that for the present paper we are only concerned with the problem of computing the probability of a tree; we assume an independently given parser is available to assign a syntactic structure, or multi- ple candidate structures, to an input string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recursive Neural Network</head><p>The architecture we propose can best be under- stood as an extension of the Recursive Neural Net- works (RNNs) proposed by <ref type="bibr" target="#b28">Socher et al. (2010)</ref>, that we mentioned above. In order to see how an RNN works, consider the following example.</p><p>Assume that there is a constituent with parse tree (p 2 (p 1 x y) z) ( <ref type="figure" target="#fig_2">Figure 3</ref>), and that x, y, z ∈ R n are the (inner) representations of the three words x, y and z, respectively. We use a neural network which consists of a weight matrix W 1 ∈ R n×n for left children and a weight matrix W 2 ∈ R n×n for right children to compute the vector for a parent node in a bottom up manner. Thus, we compute p 1 as follows</p><formula xml:id="formula_5">p 1 = f (W 1 x + W 2 y + b)</formula><p>where b is a bias vector and f is an activation function (e.g., tanh or logistic). Having computed p 1 , we can then move one level up in the hierarchy and compute p 2 :</p><formula xml:id="formula_6">p 2 = f (W 1 p 1 + W 2 z + b)</formula><p>This process is continued until we reach the root node. The RNN thus computes a single vector for each node p in the tree, representing the con- tent under that node. It has in common with log- ical semantics that representations for compounds (here xyz) are computed by recursively applying a composition function to meaning representations of the parts. It is difficult to characterise the ex- pressivity of the resulting system in logical terms, but recent work suggests it is surprisingly power- ful (e.g., <ref type="bibr" target="#b15">Kanerva (2009)</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IORNN</head><p>We extend the RNN-architecture by adding a sec- ond vector to each node, representing the context of the node, shown as white rectangles in <ref type="figure" target="#fig_3">figure 4</ref>. The job of this second vector, the outer represen- tation, is to summarize all information about the context of node p so that we can either predict its content (i.e., predict an inner representation), or pass on this information to the daughters of p (i.e., compute outer representations of these daughters).</p><p>Outer representations thus allow information to flow top-down. We explain the operation of the resulting Inside- Outside Recursive Neural Network in terms of the same example parse tree (p 2 (p 1 x y) z) (see <ref type="figure" target="#fig_3">Fig- ure 4)</ref>. Each node u in the syntactic tree carries two vectors o u and i u , the outer representation and inner representation of the constituent that is cov- ered by the node.</p><p>Computing inner representations Inner repre- sentations are computed from the bottom up. We assume for every word w an inner representation i w ∈ R n . The inner representation of a non- terminal node, say p 1 , is given by</p><formula xml:id="formula_7">i p 1 = f (W i 1 i x + W i 2 i y + b i )</formula><p>where W i 1 , W i 2 are n × n real matrices, b i is a bias vector, and f is an activation function, e.g. tanh. (This is the same as the computation of non-terminal vectors in the RNNs.) The inner rep- resentation of a parent node is thus a function of the inner representations of its children.</p><p>Computing outer representations Outer repre- sentations are computed from the top down. For a node which is not the root, say p 1 , the outer repre-sentation is given by</p><formula xml:id="formula_8">o p 1 = g(W o 1 o p 2 + W o 2 i z + b o )</formula><p>where W o 1 , W o 2 are n × n real matrices, b o is a bias vector, and g is an activation function. The outer representation of a node is thus a function of the outer representation of its parent and the inner representation of its sisters.</p><p>If there is information about the external context of the utterance that is being processed, this infor- mation determines the outer representation of the root node o root . In our first experiments reported here, no such information was assumed to be avail- able. In this case, a random value o ∅ is chosen at initialisation and assigned to the root nodes of all utterances; this value is then adjusted by the learn- ing process discussed below.</p><p>Training Training the IORNN is to minimise an objective function J(θ) which depends on the pur- pose of usage where θ is the set of parameters. To do so, we compute the gradient ∂J/∂θ and ap- ply the gradient descent method. The gradient is effectively computed thanks to back-propagation through structure <ref type="bibr" target="#b11">(Goller and Küchler, 1996)</ref>. Fol- lowing <ref type="bibr" target="#b31">Socher et al. (2013)</ref>, we use AdaGrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) to update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The ∞-order Model with IORNN</head><p>The RNN and IORNN are defined for context- free trees. To apply the IORNN architecture to dependency parses we need to adapt the defini- tions somewhat. In particular, in the generative dependency model, every step in the generative story involves the decision to generate a specific word while the span of the subtree that this word will dominate only becomes clear when all depen- dents are generated. We therefore introduce par- tial outer representation as a representation of the current context of a word in the generative pro- cess, and compute the final outer representation only when all its siblings have been generated.</p><p>Consider an example of head h and its depen- dents x, y (we ignore directions for simplicity) in <ref type="figure" target="#fig_4">Figure 5</ref>. Assume that we are in the state in the generative process where the generation of h is complete, i.e. we know its inner and outer rep- resentations i h and o h . Now, when generating h's first dependent x (see <ref type="figure" target="#fig_4">Figure 5</ref>-a), we first com- pute x's partial outer representation (representing its context at this stage in the process), which is a function of the outer representation of the head (representing the head's context) and the inner rep- resentation of the head (representing the content of the head word):</p><formula xml:id="formula_9">¯ o 1 = f (W hi i h + W ho o h + b o )</formula><p>where W hi , W ho are n × n real matrices, b o is a bias vector, f is an activation function.</p><p>With the context of the first dependent deter- mined, we can proceed and generate its content. For this purpose, we assume a separate weight ma- trix W, trained (as explained below) to predict a specific word given a (partial) outer representa- tion. To compute a proper probability for word x, we use the softmax function:</p><formula xml:id="formula_10">sof tmax(x, ¯ o 1 ) = e u(x,¯ o 1 ) w∈V e u(w,¯ o 1 )</formula><p>where</p><formula xml:id="formula_11">u(w 1 , ¯ o 1 ), ..., u(w |V | , ¯ o 1 ) T = W¯ o 1 + b</formula><p>and V is the set of all possible dependents. Note that since o h , the outer representation of h, represents the entire dependency structure gen- erated up to that point, ¯ o 1 is a vectorial represen- tation of the ∞-order context generating the first dependent (like the fragment enclosed in the red dashed contour in <ref type="figure" target="#fig_1">Figure 2</ref>). The softmax func- tion thus estimates the probability P (D = x|C ∞ D ). The next step, now that x is generated, is to compute the partial outer representation for the second dependent (see <ref type="figure" target="#fig_4">Figure 5</ref>-b)</p><formula xml:id="formula_12">¯ o 2 = f (W hi i h + W ho o h + W dr(x) i x + b o )</formula><p>where W dr(x) is a n × n real matrix specific for the dependency relation of x with h.</p><p>Next y is generated (using the softmax function above), and the partial outer representation for the third dependent (see <ref type="figure" target="#fig_4">Figure 5</ref>-c) is computed:</p><formula xml:id="formula_13">¯ o 3 = f (W hi i h + W ho o h + 1 2 W dr(x) i x + W dr(y) i y + b o )</formula><p>Since the third dependent is the End-of- Children symbol (EOC), the process of generat- ing dependents for h stops. We can then return to x and y to replace the partial outer represen- tations with complete outer representations 1 (see <ref type="figure" target="#fig_4">Figure 5</ref>: Example of applying IORNN to dependency parsing. Black, grey, white boxes are respectively inner, partial outer, and outer representations. For simplicity, only links related to the current computation are drawn (see text). </p><formula xml:id="formula_14">o x = f (W hi i h + W ho o h + W dr(y) i y + b o ) o y = f (W hi i h + W ho o h + W dr(x) i x + b o )</formula><p>In general, if u is the first dependent of h then</p><formula xml:id="formula_15">¯ o u = f (W hi i h + W ho o h + b o ) otherwise ¯ o u = f (W hi i h + W ho o h + b o + 1 | ¯ S(u)| v∈ ¯ S(u) W dr(v) i v )</formula><p>where ¯ S(u) is the set of u's sisters generated be- fore it. And, if u is the only dependent of h (ig- noring EOC) then</p><formula xml:id="formula_16">o u = f (W hi i h + W ho o h + b o ) otherwise o u = f (W hi i h + W ho o h + b o + 1 |S(u)| v∈S(u) W dr(v) i v )</formula><p>where S(u) is the set of u's sisters.</p><p>We then continue this process to generate de- pendents for x and y until the process stops.</p><p>Inner Representations In the calculation of the probability of generating a word, described above, we assumed inner representations of all possible words to be given. These are, in fact, themselves a function of vector representations for the words (in our case, the word vectors are initially borrowed from Collobert et al. (2011)), the POS-tags and capitalisation features. That is, the inner represen- tation at a node h is given by:</p><formula xml:id="formula_17">i h = f (W w w h + W p p h + W c c h )</formula><p>where W w ∈ R n×dw , W p ∈ R n×dp , W c ∈ R n×dc , w h is the word vector of h, and p h , c h are respectively binary vectors representing the POS- tag and capitalisation feature of h.</p><p>Training Training this IORNN is to minimise the following objective function which is the reg- ularised cross-entropy</p><formula xml:id="formula_18">J(θ) = − 1 m T ∈D w∈T log(P (w|¯ o w )) + 1 2 λ W θ W 2 + λ L θ L 2</formula><p>where D is the set of training dependency parses, m is the number of dependents; θ W , θ L are the weight matrix set and the word embeddings</p><formula xml:id="formula_19">(θ = (θ W , θ L )); λ W , λ L are regularisation hyper- parameters.</formula><p>Implementation We decompose a dependent D into four features: dependency relation, POS-tag, lowercase version of word, capitalisation feature of word. We then factorise P (D|C ∞ D ) similarly to Section 3, where each component is estimated by a softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In our experiments, we convert the Penn Treebank to dependencies using the Universal dependency annotation <ref type="bibr" target="#b21">(McDonald et al., 2013)</ref>  <ref type="bibr">2</ref> ; this yields a dependency tree corpus we label PTB-U. In or- der to compare with other systems, we also ex- periment with an alternative conversion using the head rules of <ref type="bibr" target="#b34">Yamada and Matsumoto (2003)</ref>  <ref type="bibr">3</ref> ; this yields a dependency tree corpus we label PTB- YM. Sections 2-21 are used for training, section 22 for development, and section 23 for testing. For the PTB-U, the gold POS-tags are used. For the PTB-YM, the development and test sets are tagged by the Stanford POS-tagger 4 trained on the whole Perplexity 3rd-order model 1736.73 ∞-order model 236.58 <ref type="table">Table 1</ref>: Perplexities of the two models on PTB- U-22.</p><p>training data, whereas 10-way jackknifing is used to generate tags for the training set. The vocabulary for both models, the third-order model and the ∞-order model, is taken as a list of words occurring more than two times in the training data. All other words are labelled 'UN- KNOWN' and every digit is replaced by '0'. For the IORNN used by the ∞-order model, we set n = 200, and define f as the tanh activation func- tion. We initialise it with the 50-dim word embed- dings from <ref type="bibr" target="#b6">Collobert et al. (2011)</ref> and train it with the learning rate 0.1, λ W = 10 −4 , λ L = 10 −10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Perplexity</head><p>We firstly evaluate the two models on PTB-U-22 using the perplexity-per-word metric</p><formula xml:id="formula_20">ppl(P ) = 2 − 1 N T ∈D log 2 P (T )</formula><p>where D is a set of dependency parses, N is the total number of words. It is worth noting that, the better P estimates the true distribution P * of D, the lower its perplexity is. Because Eisner's model with the dist(H, D) feature <ref type="table">(Equation 2)</ref> is leaky (the model allocates some probability to events that can never legally arise), this feature is discarded (only in this experiment). <ref type="table">Table 1</ref> shows results. The perplexity of the third-order model is more than seven times higher than the ∞-order model. This reflects the fact that data sparsity is more problematic for counting than for the IORNN.</p><p>To investigate why the perplexity of the third- order model is so high, we compute the percent- ages of events extracted from the development set appearing more than twice in the training set. Events are grouped according to the reduction lists in Equation 2 (see <ref type="table">Table 2</ref>). We can see that re- ductions at level 0 (the finest) for dependency re- lations and words seriously suffer from data spar- sity: more than half of the events occur less than three times, or not at all, in the training data. We thus conclude that counting-based models heavily rely on carefully designed reduction lists for back- off. <ref type="table">back-off level  d  t  w  c  0</ref> 47.4 61.6 43.7 87.7 1 69.8 98.4 77.8 97.3 2 76.0, 89.5 99.7 3 97.9 total 76.1 86.6 60.7 92.5 <ref type="table">Table 2</ref>: Percentages of events extracted from PTB-U-22 appearing more than twice in the train- ing set. Events are grouped according to the reduc- tion lists in Equation 2. d, t, w, c stand for depen- dency relation, POS-tag, word, and capitalisation feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reranking</head><p>In the second experiment, we evaluate the two models in the reranking framework proposed by <ref type="bibr" target="#b26">Sangati et al. (2009)</ref> on PTB-U. We used the MST- Parser (with the 2nd-order feature mode) <ref type="bibr" target="#b19">(McDonald et al., 2005</ref>) to generate k-best lists. Two evaluation metrics are labelled attachment score (LAS) and unlabelled attachment score (UAS), in- cluding punctuation.</p><p>Rerankers Given D(S), a k-best list of parses of a sentence S, we define the generative reranker</p><formula xml:id="formula_21">T * = arg max T ∈D(S) P (T (ROOT ))</formula><p>which is identical to <ref type="bibr" target="#b26">Sangati et al. (2009)</ref>. Moreover, as in many mixture-model-based ap- proaches, we define the mixture reranker as a com- bination of the generative model and the MST dis- criminative model <ref type="bibr" target="#b12">(Hayashi et al., 2011</ref>)</p><formula xml:id="formula_22">T * = arg max T ∈D(S) α log P (T (ROOT ))+(1−α)s(S, T )</formula><p>where s(S, T ) is the score given by the MST- Parser, and α ∈ [0, 1].</p><p>Results <ref type="figure" target="#fig_5">Figure 6</ref> shows UASs of the generative reranker on the development set. The MSTParser achieves 92.32% and the Oracle achieve 96.23% when k = 10. With the third-order model, the generative reranker performs better than the MST- Parser when k &lt; 6 and the maximum improve- ment is 0.17%. Meanwhile, with the ∞-order model, the generative reranker always gains higher UASs than the MSTParser, and with k = 6, the difference reaches 0.7%. <ref type="figure" target="#fig_6">Figure 7</ref> shows UASs of the mixture reranker on the same set. α is opti- mised by searching with the step-size 0.005. Un- surprisingly, we observe improvements over the   generative reranker as the mixture reranker can combine the advantages of the two models. <ref type="table" target="#tab_0">Table 3</ref> shows scores of the two rerankers on the test set with the parameters tuned on the develop- ment set. Both the rerankers, either using third- order or ∞-order models, outperform the MST- Parser. The fact that both gain higher improve- ments with the ∞-order model suggests that the IORNN surpasses counting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with other systems</head><p>We first compare the mixture reranker using the ∞-order model against the state-of-the-art depen- dency parser TurboParser (with the full mode) <ref type="bibr" target="#b18">(Martins et al., 2013</ref>) on PTB-U-23. <ref type="table">Table 4</ref> shows LASs and UASs. When taking labels into account, the TurboParser outperforms the reranker. But without counting labels, the two systems perform comparably, and when ignoring punctuation the reranker even outperforms the TurboParser. This pattern is also observed when the exact match met- rics are used (see <ref type="table">Table 4</ref>). This is due to the fact that the TurboParser performs significantly better than the MSTParser, which generates k-best lists for the reranker, in labelling: the former achieves 96.03% label accuracy score whereas the latter achieves 94.92%.</p><p>One remarkable point is that reranking with the ∞-order model helps to improve the exact match scores 4% -6.4% (see <ref type="table">Table 4</ref>). Because the exact match scores correlate with the ability to handle global structures, we conclude that the IORNN is able to capture ∞-order contexts. <ref type="figure" target="#fig_8">Fig- ure 8</ref> shows distributions of correct-head accuracy over CPOS-tags and <ref type="figure" target="#fig_7">Figure 9</ref> shows F1-scores of binned HEAD distance. Reranking with the ∞- order model is clearly helpful for all CPOS-tags and dependent-to-head distances, except a minor decrease on PRT.</p><p>We compare the reranker against other systems on PTB-YM-23 using the UAS metric ignoring punctuation (as the standard evaluation for En- glish) (see <ref type="table" target="#tab_3">Table 5</ref>). Our system performs slightly better than many state-of-the-art systems such as <ref type="bibr" target="#b18">Martins et al. (2013)</ref> (a.k.a. TurboParser), <ref type="bibr" target="#b35">Zhang and McDonald (2012)</ref>, <ref type="bibr" target="#b16">Koo and Collins (2010)</ref>. It outperforms <ref type="bibr" target="#b12">Hayashi et al. (2011)</ref> which is a reranker using a combination of third-order gen- erative models with a variational model learnt  <ref type="table">Table 4</ref>: Comparison with the TurboParser on PTB-U-23. LEM and UEM are respectively the labelled exact match score and unlabelled exact match score metrics. The numbers in brackets are scores com- puted excluding punctuation. System UAS <ref type="bibr" target="#b14">Huang and Sagae (2010)</ref> 92.1 <ref type="bibr" target="#b16">Koo and Collins (2010)</ref> 93.04 <ref type="bibr" target="#b35">Zhang and McDonald (2012)</ref>   on the fly; performs equally with <ref type="bibr" target="#b13">Hayashi et al. (2013)</ref> which is a discriminative reranker using the stacked technique; and slightly worse than <ref type="bibr" target="#b1">Bohnet and Kuhn (2012)</ref>, who develop a hybrid transition- based and graphical-based approach.</p><formula xml:id="formula_23">LAS (w/o punc) UAS (w/o punc) LEM (w/o punc) UEM (w/o punc)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Using neural networks to process trees was first proposed by <ref type="bibr" target="#b24">Pollack (1990)</ref> in the Recursive Au- toassociative Memory model which was used for unsupervised learning. <ref type="bibr" target="#b28">Socher et al. (2010)</ref> later introduced the Recursive Neural Network archi- tecture for supervised learning tasks such as syn- tactic parsing and sentiment analysis <ref type="bibr" target="#b31">(Socher et al., 2013)</ref>. Our IORNN is an extension of the RNN: the former can process trees not only bottom-up like the latter but also top-down. Elman (1990) invented the simple recurrent neural network (SRNN) architecture which is ca- pable of capturing very long histories. <ref type="bibr" target="#b22">Mikolov et al. (2010)</ref> then applied it to language mod- elling and gained state-of-the-art results, outper- forming the the standard n-gram techniques such as Kneser-Ney smoothed 5-gram. Our IORNN architecture for dependency parsing bears a re- semblance to the SRNN in the sense that it can also capture long 'histories' in context represen- tations (i.e., outer representations in our terminol- ogy). Moreover, the IORNN can be seen as a gen- eralization of the SRNN since a left-branching tree is equivalent to a chain and vice versa.</p><p>The idea of letting parsing decisions depend on arbitrarily long derivation histories is also ex- plored in <ref type="bibr" target="#b2">Borensztajn and Zuidema (2011)</ref> and is related to parsing frameworks that allow arbi- trarily large elementary trees (e.g., <ref type="bibr" target="#b27">Scha (1990)</ref>, O' <ref type="bibr">Donnell et al. (2009)</ref>, <ref type="bibr" target="#b25">Sangati and Zuidema (2011)</ref>, and van <ref type="bibr" target="#b33">Cranenburgh and Bod (2013)</ref>). <ref type="bibr" target="#b32">Titov and Henderson (2007)</ref> were the first proposing to use deep networks for dependency parsing. They introduced a transition-based gen- erative dependency model using incremental sig- moid belief networks and applied beam pruning for searching best trees. Differing from them, our work uses the IORNN architecture to rescore k-best candidates generated by an independent graph-based parser, namely the MSTParser.</p><p>Reranking k-best lists was introduced by <ref type="bibr" target="#b4">Collins and Koo (2005)</ref> and <ref type="bibr" target="#b3">Charniak and Johnson (2005)</ref>. Their rerankers are discriminative and for constituent parsing. <ref type="bibr" target="#b26">Sangati et al. (2009)</ref> proposed to use a third-order generative model for reranking k-best lists of dependency parses. Hayashi et al. (2011) then followed this idea but combined gen- erative models with a variational model learnt on the fly to rerank forests. In this paper, we also followed <ref type="bibr" target="#b26">Sangati et al. (2009)</ref>'s idea but used an ∞-order generative model, which has never been used before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a new neural network architecture, the Inside-Outside Recursive Neural Network, that can process trees both bottom-up and top-down. The key idea is to extend the RNN such that every node in the tree has two vectors associated with it: an inner representation for its content, and an outer representation for its context. Inner and outer representations of any constituent can be computed simultaneously and interact with each other. This way, information can flow top- down, bottom-up, inward and outward. Thanks to this property, by applying the IORNN to depen- dency parses, we have shown that using an ∞- order generative model for dependency parsing, which has never been done before, is practical.</p><p>Our experimental results on the English section of the Universal Dependency Treebanks show that the ∞-order generative model approximates the true dependency distribution better than the tradi- tional third-order model using counting, and tends to choose more accurate parses in k-best lists. In addition, reranking with this model even out- performs the state-of-the-art TurboParser on unla- belled score metrics.</p><p>Our source code is available at: github. com/lephong/iornn-depparse.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Inner (i p ) and outer (o p ) representations at the node that covers constituent p. They are vectorial representations of p's content and context, respectively.</figDesc><graphic url="image-1.png" coords="2,106.34,62.81,149.60,79.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of different orders of context of "diversified". The blue dotted shape corresponds to the third-order outward context, while the red dashed shape corresponds to the ∞-order left-to-right context. The green dot-dashed shape corresponds to the context to compute the outer representation.</figDesc><graphic url="image-2.png" coords="3,94.68,62.81,408.18,121.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recursive Neural Network (RNN).</figDesc><graphic url="image-3.png" coords="4,95.14,62.81,172.00,105.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Inside-Outside Recursive Neural Network (IORNN). Black rectangles correspond to inner representations, white rectangles correspond to outer representations.</figDesc><graphic url="image-4.png" coords="4,336.41,62.81,160.00,105.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 -</head><label>5</label><figDesc>Figure 5-d,e):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of the generative reranker on PTB-U-22.</figDesc><graphic url="image-6.png" coords="8,90.43,62.81,181.41,129.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of the mixture reranker on PTB-U-22. For each k, α was optimized with the step-size 0.005.</figDesc><graphic url="image-7.png" coords="8,90.43,246.80,181.41,129.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: F1-scores of binned HEAD distance (PTB-U-23).</figDesc><graphic url="image-8.png" coords="8,337.04,62.81,158.74,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Distributions of correct-head accuracy over CPOS-tags (PTB-U-23).</figDesc><graphic url="image-9.png" coords="9,140.03,177.62,317.48,127.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison based on reranking on PTB-
U-23. The numbers in the brackets are improve-
ments over the MSTParser. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with other systems on PTB-
YM-23 (excluding punctuation). 

</table></figure>

			<note place="foot" n="1"> According to the IORNN architecture, to compute the outer representation of a node, the inner representations of the whole fragments rooting at its sisters must be taken into account. Here, we replace the inner representation of a fragment by the inner representation of its root since the meaning of a phrase is often dominated by the meaning of its head.</note>

			<note place="foot" n="2"> https://code.google.com/p/uni-dep-tb/ 3 http://stp.lingfil.uu.se/ ˜ nivre/ research/Penn2Malt.html 4 http://nlp.stanford.edu/software/ tagger.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Remko Scha and three anonymous re-viewers for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The best of both worlds: a graph-based completion model for transition-based parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Episodic grammar: a computational model of the interaction between episodic and semantic memory in language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Borensztajn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33d Annual Conference of the Cognitive Science Society (CogSci&apos;11)</title>
		<meeting>the 33d Annual Conference of the Cognitive Science Society (CogSci&apos;11)</meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="507" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="66" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical comparison of probability models for dependency grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania Institute for Research in Cognitive Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Küchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Third-order variational reranking on packed-shared dependency forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1479" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient stacked dependency parsing by forest reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pentti</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="159" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from errors: Using vector-based compositional semantics for parse reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ACL 2013)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd</title>
		<meeting>the 43rd</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fragment grammar: Exploring reuse in hierarchical generative processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J O&amp;apos;</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Noah D Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<idno>MIT-CSAIL-TR-2009- 013</idno>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">77105</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate parsing with compact tree-substitution grammars: Double-DOP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Sangati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMLNP&apos;11)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMLNP&apos;11)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A generative re-ranking model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Sangati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies</title>
		<meeting>the 11th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="238" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Taaltheorie en taaltechnologie; competence en performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</author>
		<ptr target="http://iaaa.nl/rs/LeerdamE.html" />
	</analytic>
	<monogr>
		<title level="m">Computertoepassingen in de Neerlandistiek</title>
		<editor>R. de Kort and G.L.J. Leerdam</editor>
		<meeting><address><addrLine>Almere, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="7" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A latent variable model for generative dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Parsing Technologies</title>
		<meeting>the 10th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="144" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discontinuous parsing with an efficient and accurate DOP model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Van Cranenburgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parsing Technologies (IWPT&apos;13)</title>
		<meeting>the International Conference on Parsing Technologies (IWPT&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Parsing Technologies (IWPT)</title>
		<meeting>International Conference on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized higher-order dependency parsing with cube pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
