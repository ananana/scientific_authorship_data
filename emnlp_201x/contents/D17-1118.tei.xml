<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Dubossarsky</surname></persName>
							<email>haim.dub@gmail.com, {eitan.grossman,daphna}@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Edmond and Lily Safra Center for Brain Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Grossman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1136" to="1145"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition , in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and pro-totypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasing availability of digitized histori- cal corpora, together with newly developed tools of computational analysis, make the quantitative study of language change possible on a larger scale than ever before. Thus, many important ques- tions may now be addressed using a variety of NLP tools that were originally developed to study synchronic similarities between words. This has catalyzed the evolution of an exciting new field of historical distributional semantics, which has yielded findings that inform our understanding of the dynamic structure of language ( <ref type="bibr" target="#b21">Sagi et al., 2009;</ref><ref type="bibr" target="#b23">Wijaya and Yeniterzi, 2011;</ref><ref type="bibr" target="#b17">Mitra et al., 2014;</ref><ref type="bibr" target="#b10">Hilpert and Perek, 2015;</ref><ref type="bibr" target="#b7">Frermann and Lapata, 2016;</ref><ref type="bibr" target="#b5">Dubossarsky et al., 2016)</ref>. Recent research has even proposed laws of change that predict the conditions under which the meaning of words is likely to change <ref type="bibr" target="#b4">(Dubossarsky et al., 2015;</ref><ref type="bibr" target="#b24">Xu and Kemp, 2015;</ref><ref type="bibr" target="#b9">Hamilton et al., 2016)</ref>. This is an important development, as traditional historical linguistics has generally been unable to provide predictive models of semantic change.</p><p>However, these preliminary results should be addressed with caution. To date, analyses of changes in words' meanings have relied on the comparison of word representations at different points in time. Thus any proposed change in meaning is contingent on a particular model of word representation and the method used to mea- sure change. Distributional semantic models typi- cally count words and their co-occurrence statis- tics (explicit models) or predict the embedding contexts of words (implicit models). In this paper, we show that the choice of model may introduce biases into the analysis. We therefore suggest that empirical findings may be used to support laws of semantic change only after a proper control can be shown to eliminate artefactual factors as the un- derlying cause of the empirical observations.</p><p>Regardless of the specific representation used, a frequent method of measuring the semantic change a word has undergone <ref type="bibr" target="#b8">(Gulordava and Baroni, 2011;</ref><ref type="bibr" target="#b11">Jatowt and Duh, 2014;</ref><ref type="bibr" target="#b12">Kim et al., 2014;</ref><ref type="bibr" target="#b4">Dubossarsky et al., 2015;</ref><ref type="bibr" target="#b13">Kulkarni et al., 2015;</ref><ref type="bibr" target="#b9">Hamilton et al., 2016</ref>) is to compare the word's vector representations between two points in time using the cosine distance:</p><p>cosDist(x, y) = 1 − x · y x 2 y 2</p><p>This choice naturally assumes that greater dis- tances correspond to greater semantic changes. However, this measure introduces biases that may affect our interpretation of meaning change.</p><p>We examine various representations of word meaning, in order to identify inherent confounds when meaning change is evaluated using the co- sine distance. In addition to the empirical evalua- tion, in Section 5 we provide an analytical account of the influence of word frequency on cosine dis- tance scores when using these representations.</p><p>In our empirical investigation, we highlight the critical role of control conditions in the validation of experimental findings. Specifically, we argue that every observation about a change of mean- ing over time should be subjected to a control test. The control condition described in Section 2.1 is based on the construction of an artificially gener- ated corpus, which resembles the historical corpus in most respects but where no change of mean- ing over time exists. In order to establish the va- lidity of an observation about meaning change - and even more importantly, the validity of a law- like generalization about meaning change -the re- sult obtained in a genuine experimental condition should be demonstrated to be lacking (or at least significantly diminished) in the control condition.</p><p>As we show in Section 4, some recently re- ported laws of historical meaning change do not survive this proposed test. In other words, sim- ilar results are obtained in the genuine and con- trol conditions. These include the correlation of meaning change with word frequency, polysemy (the number of different meanings a word has), and prototypicality (how representative a word is of its category). These factors lie at the basis of the following proposed laws of semantic change:</p><p>• The Law of Conformity, according to which frequency is negatively correlated with se- mantic change <ref type="bibr" target="#b9">(Hamilton et al., 2016</ref>).</p><p>• The Law of Innovation, according to which polysemy is positively correlated with se- mantic change <ref type="bibr" target="#b9">(Hamilton et al., 2016</ref>).</p><p>• The Law of Prototypicality, according to which prototypicality is negatively correlated with semantic change ( <ref type="bibr" target="#b4">Dubossarsky et al., 2015</ref>).</p><p>Our analysis shows that these laws have only residual effects, suggesting that frequency and prototypicality may play a smaller role in semantic change than previously claimed. The main artefact underlying the emergence of the first two laws in both the genuine and control conditions may be due to the SVD step used for the embedding of the PPMI word representation (see Section 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The historical corpus used here is Google Books 5-grams of English fiction. Equally sized sam- ples of 10 million 5-grams per year were ran- domly sampled for the period of <ref type="bibr">1900</ref><ref type="bibr">-1999</ref><ref type="bibr" target="#b12">(Kim et al., 2014</ref>) to prevent the more prolific publi- cation years from biasing the results, and were grouped into ten-year bins. Uncommon words were removed, keeping the 100,000 most frequent words as the vocabulary for subsequent model learning. All words were lowercased and stripped of punctuation.</p><p>This corpus served as the genuine condition, and was used to replicate and evaluate findings from previous studies. In this corpus, words are expected to change their meaning between decadal bins, as they do in a truly random sample of texts. According to the distributional hypothesis <ref type="bibr" target="#b6">(Firth, 1957)</ref>, one can extract a word's meaning from the contexts in which it appears. Therefore, if words' meanings change over time, as has been argued at least since Reisig (1839), it follows that the words' contexts should change accordingly, and this change should be detected by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Control condition setup</head><p>Complementary to the genuine condition, a con- trol condition was created where no change of meaning is expected. Therefore, any observed change in a word's meaning in the control con- dition can only stem from random "noise", while changes in meaning in the genuine condition are attributed to "real" semantic change in addition to "noise". Two methods were used to construct the corpus in the control condition:</p><p>Chronologically shuffled corpus (shuffle): 5- grams were randomly shuffled between decadal bins, so that each bin contained 5-grams from all the decades evenly. This was chosen as a control condition for two reasons. First, this condition re- sembles the genuine condition in size of the vocab- ulary, size of the corpus, overall variance in words' usage, and size of the decadal bins. Second and crucially, words are not expected to show any ap- parent change in their meaning between decades in the control condition, because their various us- age contexts are shuffled across decades.</p><p>One synchronous corpus (subsample): All 5- grams of the year 1999, which amount to 250 mil- lion 5-grams, were selected from Google Books English fiction. 10 million 5-grams were ran- domly subsampled from this selection, and this process was repeated 30 times. This is suggested as an additional control condition since the under- lying assumption is always that words in the same year do not change their meaning. Again, unlike in the genuine condition, any changes that are ob- served based on these 30 subsamples can be at- tributed only to "noise" that stems from random sampling, rather than real change in meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Measures of interest</head><p>Meaning change: Meaning change was evalu- ated as the cosine distance between vector rep- resentations of the same word in consecutive decades. This was done separately for each pro- cessing stage (see Section 2.5). For the subsample condition, this was defined as the average cosine distance between the vectors in all 30 samples.</p><p>Frequency: Words' frequencies were computed separately for each decadal bin as the number of times a word appeared divided by the total number of words in that decade. For the subsample control condition, it was computed as the number of times a word appeared among the 250 million 5-grams, divided by the total number of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Construct validity</head><p>To establish the adequacy of our control condition, we compared the meaning change scores (before log-transformation and standardization) between the genuine and the shuffled control conditions. Change scores were obtained by taking the aver- age meaning change over all words in each decade using the representation of the final processing stage (SVD). An adequate control condition will exhibit a lower degree of change compared to the genuine condition, and is expected to show a fixed rate of change across decades (see 3a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Statistical analysis</head><p>Following common practice ( <ref type="bibr" target="#b9">Hamilton et al., 2016)</ref>, the 10k most frequent words, as measured by their average decadal bin frequencies, were used for the analysis of semantic change. Change scores and frequencies were log-transformed, and all variables were subsequently standardized.</p><p>A linear mixed effects model was used to evalu- ate meaning change in both the genuine and shuf- fled control conditions. Frequency was set as a fixed effect while random intercepts were set per word. The model attempts to account for semantic change scores using frequency, while controlling for the variability between words by assuming that each word's behavior is strongly correlated across decades and independent across words as follows:</p><formula xml:id="formula_1">∆w (t) i = β 0 + β f f req (t) w i + z w i + ε (t) w i<label>(2)</label></formula><p>Here ∆w</p><formula xml:id="formula_2">(t)</formula><p>i is the semantic change score of the i'th word measured between two specific consec- utive decades, β 0 is the model's intercept, β f is the fixed-effect predictor coefficient for frequency, z w i ∼ N (0, σ) is a random intercept for the i'th word, and ε</p><formula xml:id="formula_3">(t)</formula><p>w i is an error term associated with the i'th word. We report the predictor coefficient as well as the proportion of variance explained 1 by each model. Only statistically significant results (p &lt; .01) are reported. All statistical tests are per- formed in R (lme4 and MuMln packages).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Word meaning representation</head><p>We used a cascade of processing stages based on the explicit meaning representation of words (i.e., word counts, PPMI, SVD, as explained be- low) as commonly practiced ( <ref type="bibr" target="#b0">Baroni et al., 2014;</ref><ref type="bibr" target="#b15">Levy et al., 2015)</ref>. For each of these stages, we sought to evaluate the relationship between word frequency and meaning change, by computing the corresponding correlations between these two fac- tors in the subsample control condition.</p><p>Counts: Co-occurrence counts were collected for all the words in the vocabulary per decade.</p><p>PPMI: Sparse square matrices of vocabulary size containing positive pointwise mutual infor- mation (PPMI) scores were constructed for each decade based on the co-occurrence counts. We used the context distribution smoothing parameter α = 0.75, as recommended by ( <ref type="bibr" target="#b15">Levy et al., 2015)</ref>, using the following procedure: SVD: Each PPMI matrix was approximated by a truncated singular value decomposition as de- scribed in ( <ref type="bibr" target="#b15">Levy et al., 2015)</ref>. This embedding was shown to improve results on downstream tasks ( <ref type="bibr" target="#b0">Baroni et al., 2014;</ref><ref type="bibr" target="#b3">Bullinaria and Levy, 2012;</ref><ref type="bibr" target="#b22">Turney and Pantel, 2010)</ref>. Specifically, the top 300 elements of the diagonal matrix of singular values Σ, denoted Σ d , were retained to represent a new, dense embedding of the word vectors, using the truncated left hand orthonormal matrix U d :</p><formula xml:id="formula_4">P P M I α (w, c) = max logˆP logˆ logˆP (w, c) ˆ P (w) ˆ P α (c) , 0<label>(a</label></formula><formula xml:id="formula_5">W SV D i = (U d · Σ d ) i<label>(3)</label></formula><p>These representations were subsequently aligned with the orthogonal Procrustes method following <ref type="bibr" target="#b9">(Hamilton et al., 2016</ref>).</p><p>Relation to other models: (Levy and Gold- berg) have shown that the Skip-Gram with Neg- ative Sampling (SGNS) embedding model, e.g. word2vec ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) -perhaps the most popular model of word meaning representa- tion, implicitly factorizes the values of the word- context PMI matrix. Hence, the optimization goal and the sources of information available to SGNS and our model are in fact very similar. We there- fore hypothesize that conclusions similar to those reported below can be drawn for SGNS models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Confound of frequency</head><p>There are many factors that may confound the measurement of meaning change. Here we focus on frequency, and investigate the existence of an artefactual relation between frequency and mean- ing change. This is done by evaluating this re- lation in the subsample control condition. Any changes observed in this condition must be the consequence of inherent noise, since this con- trol condition contains random samples from the same year (and the baseline assumption is that no change can be observed within the same year).</p><p>We first plotted the change scores that use the representation based on word count vs. word fre- quency. This resulted in a robust correlation (r = −0.915) between the two variables, as shown in <ref type="figure" target="#fig_0">Fig. 1a</ref> (see the analytical account in Section 5). We repeated the same procedure using the PPMI representation, which showed a much weaker cor- relation with frequency (r = −0.295), see <ref type="figure" target="#fig_0">Fig. 1b</ref>.</p><p>Finally, we repeated the same procedure us- ing the final explicit representation after SVD em- bedding 2 , see <ref type="figure" target="#fig_0">Fig. 1c</ref>. Surprisingly, the negative correlation with frequency was reinstated (r = −0.793). To investigate how this came about, we computed the change in the PPMI vectors be- fore and after the low-rank SVD embedding using the cosine-distance. As apparent from <ref type="figure" target="#fig_1">Fig. 2</ref>, it turns out that the SVD procedure distorts data in an uneven manner -frequent words are distorted less than infrequent words. Thus we demonstrate that this reinstatement of correlation between fre- quency and change scores is merely an artefactual consequence of the truncated SVD factorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Construct validity</head><p>Potential confounding factors can be addressed by comparing any experimental finding to a validated control condition. Here we validate the use of the shuffled condition as a proper control. To this end, the average change scores of words per decade in both the genuine and shuffled conditions are com- pared within each processing stage. In the genuine condition, words appear in different usage con- texts between decades, while in the shuffled condi- tion they do not, because the random shuffling cre- ates a homogeneous corpus. Therefore, the valid- ity of the control condition is established if: (a) the change scores are diminished as compared to the genuine condition; (b) change scores are uniform across decades (since decades are shuffled); (c) the variance of change scores is smaller that in the genuine condition. As seen in <ref type="figure" target="#fig_2">Fig. 3a</ref>, all these re- quirements are met by the control condition. Note that the change scores in the shuffled condition are all significantly positive, namely, meaning change allegedly exists in this control condition. This sup- ports the claim that any measurement is signifi- cantly affected by unrelated noise. Thus, we have established that the shuffled con- dition is a suitable control for meaning change.</p><p>While validity was established for each of the pro- cessing stages, the most robust effect was seen for the PPMI representation, following by SVD and word counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Accounting for the frequency confound</head><p>In Section 3.1 we used the subsample control con- dition to establish the confounding effect of fre- quency on meaning change. We now examine the extent to which this frequency confound exists in a historical corpus. We do so by comparing the fre- quency confound between the genuine historical corpus and the shuffled historical corpus.</p><p>To visualize the frequency confound in a man- ner comparable to the analysis presented in Sec- tion 3.1, we again plot change scores vs. fre- quency, ignoring the time dimension of the data. <ref type="figure" target="#fig_2">Fig. 3b</ref> presents this plot for the genuine condi- tion. The same analysis is repeated in the shuffled condition, see <ref type="figure" target="#fig_2">Fig. 3c</ref>.</p><p>Both plots reveal a highly significant correla- tion between change scores and frequency. Fur- thermore, the fact that the correlation coefficients are virtually identical in the genuine and shuffled conditions, with r = −0.748 and r = −0.747 re- spectively, suggests that they are due to artefactual factors in both conditions and not to true change of meaning over time. In fact, this pattern of re- sults is reminiscent of the spurious pattern we see in <ref type="figure" target="#fig_0">Fig. 1c</ref>.</p><p>The relation between frequency and meaning change can also be represented by a linear mixed effect model, with the benefit that this model en- ables the addition of more explanatory variables to the data. The regression model found frequency to have a negative influence on change scores, with β f =-0.91 and β f =-0.75, for the genuine and shuffled conditions respectively. Importantly, fre- quency accounted for 67% of the variance in the change scores in the genuine condition, and was only slightly diminished in the shuffled condition, accounting for 56% of the variance. Similar re- sults were obtained for the PPMI representation (see <ref type="table">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Revisiting previous studies</head><p>We replicated three recent results that were af- fected by this frequency effect, since they all de- fine change as the word's cosine distance relative to itself at two time points. These studies report laws of semantic change that measure the role of frequency in semantic change either directly (Law of Conformity), or indirectly through another lin- guistic variable that is dependent on frequency (Laws of Innovation and Prototypicality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Laws of conformity and innovation</head><p>Continuing the work described in Section 3.1, we replicated the model and analysis procedure de- scribed in <ref type="bibr" target="#b9">(Hamilton et al., 2016)</ref>, where two pre- dictors were used together to explain the change scores: frequency and polysemy. Polysemy, which describes the number of different senses a word has, naturally differs among words, where some words are more polysemous than others (com- pare bank and date to wine). Following (Hamil- ton et al., 2016), we defined polysemy as the words' secondary connections patterns -the con- nections between each word's co-occurring words (using the entries in the PPMI representation for that word). The more interconnected these sec- ondary connections are, the less polysemic a word is, and vice versa. Polysemy scores were com- puted using the authors' provided code <ref type="bibr">3</ref> . We then log-transformed and standardized the polysemy scores. Next, frequency and polysemy were set as two fixed effect predictors in a linear mixed effect model, like the one described in Section 2.4. Thus we were able to replicate the results in the genuine condition as reported in <ref type="bibr" target="#b9">(Hamilton et al., 2016)</ref>. Interestingly, the same pattern of results emerged, again, in the shuffled condition (see <ref type="table">Table 1</ref>). Importantly, the difference in ef- fect size between conditions, as evaluated by the explained variance of frequency and polysemy to- gether, showed a modest effect of 8% over the shuffled condition, pointing to the conclusion that the putative effects may indeed be real, but to a far lesser extent than had been claimed. We conclude that adding polysemy to the analysis contributed very little to the model's predictive power.</p><p>Since the PPMI representation (the explicit rep- resentation without dimensionality reduction with SVD) seems much less affected by spurious ef- fects correlated with frequency (see <ref type="figure" target="#fig_0">Fig. 1b)</ref>, we repeated the analysis of frequency described here and in Section 3.1 while using this representation. The results are listed in <ref type="table">Table 1</ref>, showing a similar pattern of rather small frequency effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prototypicality</head><p>Prototypicality is the degree to which a word is representative of the category of which it is a member (a robin is a more prototypical bird than a parrot). According to the proposed Law of Pro- totypicality, words with more prototypical mean- ings will show less semantic change, and vice versa. Following ( <ref type="bibr" target="#b4">Dubossarsky et al., 2015)</ref>, we computed words' prototypicality scores for each decade as the cos-distance between a word's vec-tor and its k-means cluster's centroid, and ex- tended the analysis to encompass the entire 20th century. The previous regression model assumed independence between words, and therefore as- signed words to a random effect variable. How- ever, when modeling prototypicality, this assump- tion is invalid as relations between words are what inherently define prototypicality. We therefore designed a model in which decades, rather than words, are the random effect variable.</p><p>With this analysis the prototypicality effect seems to be substantiated in two ways. First, the addition of prototypicality explains an additional 5% of the variance. Second, the effect of proto- typicality meets the more stringent requirement of being diminished in the shuffle condition (see <ref type="table">Ta- ble 1</ref>). Nevertheless, here too the effect originally reported was found to be drastically reduced after being compared with the proper control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical analysis</head><p>We show in Section 5.1 that the average cosine dis- tance between two vectors representing the same word is equivalent to the variance of the popula- tion of vectors representing the same word in inde- pendent samples, and is therefore always positive. This is true for any word vector representation.</p><p>In Sections 5.2-5.3 we prove that the average cosines distance between two count vectors rep- resenting the same word is negatively correlated with the frequency of the word, and positively cor- related with the polysemy score of the word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sampling variability and the cos distance</head><p>Lemma 1. Assume two random variables x, y of length x 2 = y 2 = 1, distributed iid with ex- pected value µ and covariance matrix Σ. The ex- pected value of the cosine distance between them is equal to the sum of the diagonal elements of Σ.</p><p>Proof.</p><formula xml:id="formula_6">E(x − y) 2 =E(x − µ) 2 + E(y − µ) 2 + 2E(x − µ)(y − µ) =2 E(x i − µ i ) 2 = 2 V ar(x i ) E(x − y) 2 =E(x 2 ) + E(y 2 ) − 2E(x · y) =2 − 2E x · y x 2 y 2 =2E(cosDist(x, y)) It follows that E(cosDist(x, y)) = V ar(x i ) (4)</formula><p>Implication: The average cosine distance be- tween two samples of the same random variables is directly related to the variance of the variable, or the sampling noise. This variance should be measured empirically whenever cosine distance is used, since only distances that are larger than the empirical variance can be relied upon to support significant observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cos distance of count vectors: frequency</head><p>Next, we analyze the cosine distance between 2 iid samples from a normalized multinomial ran- dom variable. This distribution models the dis- tribution of the count vector representation. Let k i , 1 ≤ i ≤ m denote the number of times word i appeared in the context of word w, and let m denote the size of the dictionary not including w. Let n = k i denote the number of words in the count vector of w; n determines the word's fre- quency score. Assume that the counts are sampled from the distribution Multinomial(n, p), namely</p><formula xml:id="formula_7">P rob(k 1 , · · · , k m ) = n k 1 · · · , k m p k 1 1 · · · p km m</formula><p>Lemma 2. The expected value of the cosine dis- tance between two count vectors x, y sampled iid from this distribution is monotonically decreasing with n.</p><p>Proof. By definition, 1 − E[cosDist(x, y)] equals</p><formula xml:id="formula_8">E x · y x 2 y 2 = i E x i x 2 2 = i E 2 i (5)</formula><p>We compute the expected value of E i directly:</p><formula xml:id="formula_9">E i = (k 1 ,··· ,km) k i j k 2 j n k 1 · · · , k m p k 1 1 · · · p km m</formula><p>Using Taylor expansion:</p><formula xml:id="formula_10">k i j k 2 j = k i n ( j k j n ) 2 − l =j k j k l n 2 = k i n 1 1 − l =j k j k l n 2 = k i n 1 + ε 2 + O(ε 2 )<label>(6)</label></formula><p>With some algebraic manipulations, it can be shown that M (1) &gt; M (2) if the following holds:</p><formula xml:id="formula_11">(ϕ 0 + ϕ 1 ) 2 ϕ 2 + (ψ 0 + ψ 1 )ϕ 2 2 (8) +2(ψ 0 + ψ 1 )(ϕ 0 + ϕ 1 )ϕ 2 + (ψ 0 + ψ 1 )ϕ 2 +(ϕ 0 + ϕ 1 )(ϕ 2 2 − ψ 2 ) &gt; ψ 2 (ϕ 0 + ϕ 1 ) 2</formula><p>Thus when (8) holds, the average cosine distance between two samples of a certain word w gets larger as w acquires more meanings.</p><p>(8) readily holds under reasonable conditions, e.g., when the prior counts for each meaning are similar (as a set) and much bigger than the prior counts of the joint context words (i.e., ϕ 0 = ψ 0 = ε, ϕ 1 = ϕ 2 , ψ 1 = ψ 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and discussion</head><p>In this article we have shown that some reported laws of semantic change are largely spurious re- sults of the word representation models on which they are based. While identifying such laws is probably within the reach of NLP analyses of mas- sive digital corpora, we argued that a more strin- gent standard of proof is necessary in order to put them on a firm footing. Specifically, it is nec- essary to demonstrate that any proposed law of change has to be observable in the genuine con- dition, but to be diminished or absent in a control condition. We replicated previous studies claim- ing to establish such laws, which propose that se- mantic change is negatively correlated with fre- quency and prototypicality, and positively corre- lated with polysemy. None of these laws -at least in their strong versions -survived the more strin- gent standard of proof, since the observed correla- tions were found in the control conditions.</p><p>In our analysis, the Law of Conformity, which claims a negative correlation between word fre- quency and meaning change, was shown to have a much smaller effect size than previously claimed. This indicates that word frequency probably does play a role -but a small one -in semantic change. According to the Law of Innovation, polysemy was claimed to correlate positively with meaning change. However, our analysis showed that pol- ysemy is highly collinear with frequency, and as such, did not demonstrate independent contribu- tion to semantic change. For similar reasons, the alleged role of prototypicality was diminished.</p><p>These results may be more consonant than pre- vious ones with the findings of historical linguis- tics, as it is commonly assumed that the factors leading to semantic change are more diverse than purely distributional factors. For example, socio- cultural, political, and technological changes are known to impact semantic change ( <ref type="bibr" target="#b2">Bochkarev et al., 2014;</ref><ref type="bibr" target="#b20">Newman, 2015)</ref>. Furthermore, some regularities of semantic change have been imputed to 'channel bias', inherent biases of utterance pro- duction and interpretation on the part of speakers and listeners, e.g., <ref type="bibr" target="#b18">(Moreton, 2008)</ref>. As such, it would be surprising if word frequency, polysemy, and prototypicality were to capture too high a de- gree of variance. In other words, since semantic change may result from the interaction of many factors, small effects may be a priori more credi- ble than large ones.</p><p>The results of our empirical analysis showed that the spurious effects of frequency were much weaker for the explicit PPMI representa- tion unaugmented by SVD dimensionality reduc- tion. We therefore conclude that the artefactual frequency effects reported are inherent to the type of word representations upon which these analy- ses are based. As the analytical proof in Section 5 demonstrates, it is count vectors that introduce an artefactual dependence on word frequency.</p><p>Intuitively, one might expect that the average value for the cosine distance between a given word's vector in any two samples would be 0. However, Lemma 1 above shows that this is not the case, and the average distance is the vari- ance of the population of vectors representing the same word. This result is independent of the spe- cific method used to represent words as vectors. Lemma 2 proves that the average cosine distance between two samples of the same word, when us- ing count vector representations, is negatively cor- related with the word's frequency. Thus, the role of frequency cannot be evaluated as an indepen- dent predictor in any model based on count vector representations. It remains for future research to establish whether other approaches to word repre- sentation, e.g. ( <ref type="bibr" target="#b1">Blei et al., 2003;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013)</ref>, have inherent biases.</p><p>While our findings may seem to be mainly nega- tive, since they invalidate proposed laws of seman- tic change, we would like to point to the positive contribution made by articulating more stringent standards of proof and devising replicable control conditions for future research on language change based on distributional semantics representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Correlations in the control condition between change scores in the year 1999 and word frequency for three word representation types, based on: (a) Counts, (b) PPMI, (c) SVD. Correlation coefficients are reported above each subplot. LS regression lines are shown in dashed green.</figDesc><graphic url="image-4.png" coords="4,347.02,252.24,136.07,100.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cosine distances between PPMI and approximated PPMI representations (y-axis), plotted against frequency (x-axis). Correlation coefficient is reported above the plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Average change score per decade for the genuine and control conditions. Bars represent standard deviations. (b-c) Change scores (y-axis), relative to their frequency (x-axis): (b) genuine historical corpus, (c) chronologically shuffled historical corpus. LS regression lines are shown in dashed green.</figDesc><graphic url="image-6.png" coords="5,235.28,69.40,142.87,99.24" type="bitmap" /></figure>

			<note place="foot" n="1"> R 2 for mixed linear models (Nakagawa and Schielzeth, 2013)</note>

			<note place="foot" n="2"> Similar results were obtained for the implicit embedding (word2vec-SGNS) described in Section 2.5.</note>

			<note place="foot" n="3"> https://github.com/williamleif/histwords</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cos distance of count vectors: polysemy</head><p>We start our investigation of polysemy by mod- eling the distribution of the parameters of the multinomial distribution from which count vec- tors are sampled. A common prior distribution on the vector p w in m-simplex, which defines the multinomial distribution generating the context of word w, is the Dirichlet distribution f ( p w ;</p><p>α w is a sparse vector of prior counts on all the words in the dictionary, by which the co- occurrence context of word w is modeled. We divide the set of none-zero indices of α w into two subsets: i 1 , · · · , i m 0 correspond to the words which always appear in the context of w, while j 1 , · · · , i m 1 correspond to the words which appear in the context of w in one given meaning. If w is polysemous and has two meanings, then there is a third set of indices k 1 , · · · , k m 2 which correspond to the words appearing in the context of w in its second meaning. If w has more then two mean- ings, they can be modeled with additional sets of disjoint indices.</p><p>Lemma 3. Under certain conditions specified in the proof, given two count vectors x, y sampled iid from the above distribution of w, the expected value of the cosine distance between them in- creases with the number of sets of disjoint indices which represent different meanings of w.</p><p>Proof. We will prove that when w has two mean- ings, the expected value of the cosine distance is larger than in the case of a single meaning. The proof for the general case immediately follows.</p><p>Starting from (6) while keeping only the 0-order term in ε, it follows from the derivations in the proof of Lemma 2 that the expected cosine dis- tance between two count vector samples of w, to be denoted M , is 1− p 2 i . In our current model p is a random variable, and we shall compute the ex- pected value of this random variable under the two conditions, when w has either one or two mean- ings.</p><p>We start by observing that, given the definition of the Dirichlet distribution, it follows that</p><p>Considering the different sets of indices in iso- lation, let</p><p>α i , and</p><p>i , and ψ 2 = km 2 i=k 1 α 2 i . We rewrite (7) for the two conditions:</p><p>1. w has one meaning:</p><p>2. w has two meanings:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet Allocation. Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universals versus historical contingencies in lexical evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bochkarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valery</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Royal Society Interface</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. Behavior research methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph P</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="890" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A bottom up approach to category mapping and meaning change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Dubossarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NetWordS 2015 Word Knowledge and Word Usage</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Verbs change more than nouns: A bottom up computational approach to semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Dubossarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingue e Linguaggio</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5" to="25" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rupert Firth</surname></persName>
		</author>
		<title level="m">Papers in Linguistics 19341951</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Bayesian model of diachronic meaning change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Frermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="31" to="45" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="67" to="71" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diachronic word embeddings reveal statistical laws of semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hilpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics Vanguard</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="339" to="350" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A framework for analyzing semantic change of words across time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries</title>
		<meeting>the 14th ACM/IEEE-CS Joint Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal analysis of language through neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-I</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Hanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darshan</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistically significant detection of linguistic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">That&apos;s sick dude!: Automatic identification of word sense change across different timescales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunny</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1020" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analytic bias and phonological typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Moreton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phonology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="127" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A general and simple method for obtaining r2 from generalized linear mixed-effects models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schielzeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="142" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Routledge Handbook of Semantics</title>
		<editor>Nick Rimer</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="266" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic density analysis: Comparing word meaning across time and phonetic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Geometrical Models of Natural Language Semantics</title>
		<meeting>the Workshop on Geometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding semantic change of words over centuries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanti</forename><surname>Derry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reyyan</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeniterzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web</title>
		<meeting>the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A computational evaluation of two laws of semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In CogSci</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
