<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Features for Essay Scoring-An Empirical Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
							<email>fei dong@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>yue zhang@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Features for Essay Scoring-An Empirical Study</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1072" to="1077"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Essay scoring is a complicated processing requiring analyzing, summarizing and judging expertise. Traditional work on essay scoring focused on automatic handcrafted features, which are expensive yet sparse. Neural models offer a way to learn syntactic and semantic features automatically, which can potentially improve upon discrete features. In this paper , we employ convolutional neural network (CNN) for the effect of automatically learning features, and compare the result with the state-of-art discrete baselines. For in-domain and domain-adaptation essay scoring tasks, our neural model empirically outperforms discrete models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic essay scoring (AES) is the task of build- ing a computer-based grading system, with the aim of reducing the involvement of human raters as far as possible. AES is challenging since it relies not only on grammars, but also on semantics, discourse and pragmatics. Traditional approaches treat AES as a classification <ref type="bibr" target="#b10">(Larkey, 1998;</ref><ref type="bibr" target="#b13">Rudner and Liang, 2002</ref>), regression ( <ref type="bibr" target="#b1">Attali and Burstein, 2004;</ref><ref type="bibr" target="#b12">Phandi et al., 2015)</ref>, or ranking classification problem <ref type="bibr" target="#b16">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b2">Chen and He, 2013)</ref>, ad- dressing AES by supervised learning. Features are typically bag-of-words, spelling errors and lengths, such word length, sentence length and essay length, etc. Some grammatical features are considered to assess the quality of essays <ref type="bibr" target="#b16">(Yannakoudakis et al., 2011)</ref>. A drawback is feature engineering, which can be time-consuming, since features need to be carefully handcrafted and selected to fit the appro- riate model. A further drawback of manual feature templates is that they are sparse, instantiated by dis- crete pattern-matching. As a result, parsers and se- mantic analyzers are necessary as a preprocessing step to offer syntactic and semantic patterns for fea- ture extraction. Given variable qualities of student essays, such analyzers can be highly unreliable.</p><p>Neural network approaches have been shown to be capable of inducing dense syntactic and semantic features automatcially, giving competitive results to manually designed features for several tasks <ref type="bibr" target="#b7">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b6">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b4">dos Santos and Gatti, 2014)</ref>. In this paper, we empir- ically investigate a neural network method to learn features automatically for AES, without the need of external pre-processing. In particular, we build a hi- erarchical CNN model, with one lower layer repre- senting sentence structures and one upper layer rep- resenting essay structure based on sentence repre- sentations. We compare automatically-induced fea- tures by the model with state-of-art baseline hand- crafted features. Empirical results show that neural features learned by CNN are very effective in essay scoring task, covering more high-level and abstract information compared to manual feature templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Following the first AES system Project Essay Grade (PEG) been developed in 1966 <ref type="bibr" target="#b11">(Page, 1994)</ref>, a num- ber of commercial systems have come out, such as IntelliMetric 2, Intelligent Essay Assessor (IEA) ( <ref type="bibr" target="#b5">Foltz et al., 1999</ref>) and e-rater system ( <ref type="bibr" target="#b1">Attali and Burstein, 2004</ref>). The e-rater system now plays a second human rater's role in the Test of English as a Foreign Language (TOEFL) and Graduate Record Examination (GRE). The e-rater extracts a number of complex features, such as grammatical error and lexical complexity, and uses stepwise linear regres- sion. IEA uses Latent Semantic Analysis (LSA) <ref type="bibr" target="#b9">(Landauer et al., 1998</ref>) to create semantic vectors for essays and measure the semantic similarity between the vectors.</p><p>In the research literature, <ref type="bibr" target="#b10">Larkey (1998)</ref> and <ref type="bibr" target="#b13">Rudner and Liang (2002)</ref> treat AES as classification us- ing bag-of-words features. Other recent work for- mulates the task as a preference ranking problem <ref type="bibr" target="#b16">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b2">Chen and He, 2013)</ref>. <ref type="bibr" target="#b16">Yannakoudakis et al. (2011)</ref> formulated AES as a pairwise ranking problem by ranking the order of pair essays based on their quality. Features consist of word, POS n-grams features, complex grammati- cal features and so on. <ref type="bibr" target="#b2">Chen and He (2013)</ref> formu- lated AES into a listwise ranking problem by con- sidering the order relation among the whole essays and features contain syntactical features, grammar and fluency features as well as content and prompt- specific features. <ref type="bibr" target="#b12">Phandi et al. (2015)</ref> use correlated Bayesian Linear Ridge Regression (cBLRR) focus- ing on domain-adaptation tasks. All these previous methods use discrete handcrafted features.</p><p>Recently, <ref type="bibr" target="#b0">Alikaniotis et al. (2016)</ref> also employ a neural model to learn features for essay scor- ing automatically, which leverages a score-specific word embedding (SSWE) for word representations and a two-layer bidirectional long-short term mem- ory network (LSTM) to learn essay representations. <ref type="bibr" target="#b0">Alikaniotis et al. (2016)</ref> show that by combining SSWE, LSTM outperforms traditional SVM model. On the other hand, using LSTM alone does not give significantly more accuracies compared to SVM. This conforms to our preliminary experiments with the LSTM structure. Here, we use CNN without any specific embeddings and show that our neural models could outperform baseline discrete models on both in-domain and cross-domain senarios.</p><p>CNN has been used in many NLP applications, such as sequence labeling <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>) , sentences modeling ( <ref type="bibr" target="#b7">Kalchbrenner et al., 2014</ref>), sen- tences classification <ref type="bibr" target="#b8">(Kim, 2014</ref>), text categorization <ref type="bibr" target="#b6">(Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b17">Zhang et al., 2015)</ref> and sentimental analysis <ref type="bibr" target="#b4">(dos Santos and Gatti, 2014</ref>), Relative and absolute number of words and their synonyms in the essay appearing in the prompt Bag-of-words Count of useful unigrams and bigrams (unstemmed, stemmed and spell corrected)  <ref type="table" target="#tab_1">Table 1</ref>. "Use- ful n-grams" are determined using the Fisher test to separate the good scoring essays and bad scoring es- says. Good essays are essays with a score greater than or equal to the average score, and the remainder are considered as bad scoring essays. The top 201 n- grams with the highest Fisher values are chosen as the bag of features and these top 201 n-grams consti- tute useful n-grams. Correct POS tags are generated using grammatically correct texts, which is done by EASE. The POS tags that are not included in the correct POS tags are treated as bad POS tags, and these bad POS tags make up the "bad POS n-grams" features.</p><p>The features tend to be highly useful for the in-domain task since the discrete features of same prompt data share the similar statistics. However, for different prompts, features statistics vary signif- icantly. This raises challenges for discrete feature patterns.</p><p>ML-ρ ( <ref type="bibr" target="#b12">Phandi et al., 2015</ref>) was proposed to ad- dress this issue. It is based on feature augmentation, incorporating explicit correlation into augmented feature spaces. In particular, it expands baseline fea- ture vector x to be Φ s (x) = (ρx, (1 − ρ 2 ) 1/2 x) and Φ t (x) = (x, 0 p ) for source and target domain data in R 2p respectively, with ρ being the correlation be- tween source and target domain data. Then BLRR and maximum likelihood estimation are used to the optimize correlation. All the baseline models re- quire POS-tagging as a pre-processing step, extract- ing syntactic features based on POS-tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>Word Representations We use word embedding with an embedding matrix E w ∈ R dw×Vw where d w is the embedding dimension, and V w represents words vocabulary size. A word vector z i is repre- sented by z i = E w w i where w i is the i-th word in a sentence. In contrast to the baseline models, our CNN model does not rely on POS-tagging or other pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Model</head><p>We take essay scoring as a regression task and employ a two-layer CNN model, in which one convolutional layer is used to extract sentences representations, and the other is stacked on sentence vectors to learn essays representations. The archi- tecture is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Given an input sen- tence z 1 , z 2 , ..., z n , a convolution layer with a filter w ∈ R h×k is applied to a window of h words to produce n-grams features. For instance, a feature c i is generated from a window of words z i:i+h−1 by c i = f (w · z i:i+h−1 + b) , b ∈ R is the bias term and f is the non-linear activation function rectified linear unit (ReLU).</p><p>The filter is applied to the all possible win- dows in a sentence to produce a feature map c = [c 1 , c 2 , ..., c m−h+1 ]. For c j of the j-th sentence in an essay, max-pooling and average pooling func- tion are used to produce the sentence vector s j =  max{c j } ⊕ avg{c j }. The second convolutional layer takes s 1 , s 2 ,..., s n as inputs, followed by pool- ing layer (max-pooling and average-pooling) and a fully-connected hidden layer. The hidden layer di- rectly connects to output layer which generates a score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Data We use the Automated Student Assessment Prize (ASAP) 2 dataset as evaluation data for our task, which contains 8 prompts of different genres as listed in   Hyper-parameters We use Adagrad for optimiza- tion. Word embeddings are randomly initialized and the hyper-parameter settings are listed in <ref type="table" target="#tab_5">Table 3</ref>.</p><note type="other">Parameter Parameter Name Value d w Word embedding dimension 100 h wrd Word context window size 5 h sent Sentence context window size 3 k wrd Word convolution units 50 k sent Sentence convolution units 50 p Hidden size 50 drop rate Dropout rate 0.5 batch size Batch size 4 λ Learning rate 0.01</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In-domain The in-domain results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The average values of all 8 prompt sets are listed in <ref type="table">Table 4</ref>. For the in-domain task, CNN outperforms the baseline model SVR on all prompts of essay sets, and is competitive to BLRR. For the statistical significance, neural model is significantly better than baseline models with the p-value less than 10 −5 at the confidence level of 95%. The av- erage kappa value over 8 prompts is close to that of human raters.</p><p>Cross-domain The domain-adaptation results are shown in  <ref type="table">Table 4</ref>: Indomain average kappa value and standard de- viation over all 8 prompts.  model outperforms ML-ρ on almost all pairs of adaptation experiments. ML-ρ domain-adaptation method's performance improves as the size of tar- get domain training data increases. However, com- pared to ML-ρ, target training data size has less im- pact on our neural model. Even if the target train- ing size is small, the neural model still gives strong performance. This results from the fact that neu- ral model could learn more high-level and abstract features compared to traditional models with hand- crafted discrete features. We plot the confusion ma- trix between truth and model prediction on test data in <ref type="figure">Figure 4</ref>, which shows that prediction scores of neural model tend to be closer to true values, which is very important in our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Feature Analysis</head><p>To visualize the features learned by our model, we use t-distributed stochastic neighbor embedding (t- SNE) (Van der Maaten and Hinton, 2008), pro- jecting 50-dimensional features into 2-dimensional space. We take two domain pairs 3→4 and 5→6 as examples on the cross-domain task, extracting fully-connected hidden-layer features for target do- main data using model trained on source domain data. The results are showed in <ref type="figure">Figure 3</ref>. The base- line discrete features are more concentrated, which shows that patterns on source prompt are weak in differentiating target prompt essays. By using ML-ρ and leveraging 100 target prompt training examples, the discrete features patterns are more scattered, in- creasing the differentiating power. In contrast, CNN features trained on source prompt are sparse when used directly on the target prompt. This shows that neural features learned by the CNN model can better differentiate essays of different qualities. Without manual templates, such features automatically cap- ture subtle and complex information that is relevant to the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We empirically investigated a hierarchical CNN model for automatic essay scoring, showing au- tomatically learned features competitive to dis- crete handcrafted features for both in-domain and domain-adaptation tasks. The results demonstrate large potential for deep learning in AES.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hierarchical CNN structure</figDesc><graphic url="image-1.png" coords="3,72.00,29.48,468.00,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In-domain results</figDesc><graphic url="image-2.png" coords="4,91.80,199.89,187.19,112.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Visualization of discrete and neural features using t-SNE (each value represents an essay of the corresponding score). Top: Set 4 (3→4), Bottom: Set 6 (5→6). (a) discrete features; (b) ML-ρ features, n t = 100; (c) neural features; (d) discrete features; (e) ML-ρ features, n t = 100; (f) neural features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Feature description used by EASE. 

etc. In this paper, we explore CNN representa-
tion ability for AES tasks on both in-domain and 
domain-adaptation settings. 

3 Baseline 

Bayesian Linear Ridge Regression (BLRR) and 
Support Vector Regression (SVR) (Smola and Vap-
nik, 1997) are chosen as state-of-art baselines. Fea-
ture templates follow (Phandi et al., 2015), extracted 
by EASE 1 , which are briefly listed in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Details of the ASAP data; the last two columns are score range and median scores. For genre, ARG spec- ifies argumentative essays, RES means response essays and NAR denotes narrative essays.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>The essay scores are scaled into 
the range from 0 to 1. The settings of data prepara-
tion follow (Phandi et al., 2015). We use quadratic 
weighted kappa (QWK) as the metric. For domain-
adaptation (cross-domain) experiments, we follow 
(Phandi et al., 2015), picking four pairs of essay 
prompts, namely, 1→2, 3→4, 5→6 and 7→8, where 
1→2 denotes prompt 1 as source domain and prompt </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Neural Model Hyper-parameters 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 . It can be seen that our CNN</head><label>5</label><figDesc></figDesc><table>Model BLRR SVR 
CNN 
Human 
Avg 
0.725 
0.682 
0.734 
0.754 
Std dev 0.0025 0.0033 0.0029 -

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 : Cross-domain results.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/edx/ease</note>

			<note place="foot" n="2"> https://www.kaggle.com/c/asap-aes/data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their con-structive comments, which helped to improve the paper. This work is supported by NSFC61572245 and T2MOE201301 from Singapore Ministry of Ed-ucation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04289</idno>
		<title level="m">Automatic text scoring using neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-rater R v. 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Report Series</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated essay scoring by maximizing human-machine agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1741" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated essay scoring: Applications to educational technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Peter W Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas K</forename><surname>Laham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of EdMedia</title>
		<meeting>EdMedia</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="40" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An introduction to latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas K Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discourse processes</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic essay grading using text categorization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leah S Larkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Computer grading of student prose, using modern concepts and software. The Journal of experimental education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Page</forename><surname>Ellis Batten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="127" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated essay scoring using bayes&apos; theorem. The Journal of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahung</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and Assessment</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
