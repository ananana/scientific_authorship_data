<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Neural Representation for CLIR with Adversarial Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<email>libo@mail.ccnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science Central</orgName>
								<orgName type="institution">China Normal University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science Central</orgName>
								<orgName type="institution">China Normal University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Neural Representation for CLIR with Adversarial Framework</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1861" to="1870"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The existing studies in cross-language information retrieval (CLIR) mostly rely on general text representation models (e.g., vector space model or latent semantic analysis). These models are not optimized for the target retrieval task. In this paper, we follow the success of neural representation in natural language processing (NLP) and develop a novel text representation model based on adversarial learning, which seeks a task-specific embedding space for CLIR. Adversarial learning is implemented as an interplay between the generator process and the discriminator process. In order to adapt adversarial learning to CLIR, we design three constraints to direct representation learning, which are (1) a matching constraint capturing essential characteristics of cross-language ranking, (2) a translation constraint bridging language gaps, and (3) an ad-versarial constraint forcing both language and source invariant to be reached more efficiently and effectively. Through the joint exploitation of these constraints in an adversarial manner , the underlying cross-language semantics relevant to retrieval tasks are better preserved in the embedding space. Standard CLIR experiments show that our model significantly outperforms state-of-the-art continuous space models and approaches the strong machine translation and monolingual baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text representation is a crucial problem in most natural language processing (NLP) and informa- tion retrieval (IR) tasks. In monolingual IR, early research works mostly use vector space models for query-document semantic matching ( <ref type="bibr" target="#b26">Salton et al., 1975)</ref>, which suffer from the problem of syn- onymy and polysemy. In order to bridge the lexical gaps, latent semantic models such as latent seman- tic analysis (LSA) <ref type="bibr" target="#b4">(Deerwester et al., 1990</ref>) have been proposed to abstract away from surface text forms to approximate semantics. More recently, text representation learned with neural networks is attracting increasing attention of the IR commu- nity ( <ref type="bibr" target="#b19">Mitra and Craswell, 2017)</ref> and positive re- sults have been reported on various evaluation data sets <ref type="bibr" target="#b6">(Fan et al., 2018</ref>).</p><p>Compared to the prosperity in monolingual IR, there have been less advancements in CLIR where documents are written in a language different from that of queries. In addition to document ranking, CLIR models need to cross the language barri- ers, which makes the task intuitively more difficult than monolingual IR. Traditional approaches re- duce CLIR to its monolingual counterpart via per- forming some way of translation on queries or/and documents. The typical translation process is per- formed with either off-the-shelf machine trans- lation (MT) systems or multilingual dictionaries <ref type="bibr" target="#b20">(Nie, 2010)</ref>. However, MT based approaches are far from being a suitable solution for solving CLIR problems (refer to detailed analysis in ( <ref type="bibr" target="#b38">Zhou et al., 2012)</ref>). Dictionary-based approaches suffer from the same problem of lexical gaps as in the mono- lingual case <ref type="bibr" target="#b9">(Gupta et al., 2017</ref>). An efficient cross-language representation is in need for CLIR, which is expected to be able to cross both the lan- guage and lexical gaps.</p><p>The most intuitive idea one can have so as to represent text in cross-language settings is to ex- tend those models in monolingual environment. For instance, we note studies such as the exten- sion of LSA in ( <ref type="bibr" target="#b14">Littman et al., 1998</ref>), the ex- tension of principle component analysis (PCA) in <ref type="bibr" target="#b24">(Platt et al., 2010)</ref>, the extension of autoencoder model in ( <ref type="bibr" target="#b3">Chandar et al., 2014)</ref>, and the extension of word2vec ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) in <ref type="bibr" target="#b33">(Vuli´cVuli´c and Moens, 2015)</ref>. These approaches construct cross- language and semantic-rich representation of text, which can be applied to CLIR directly. However, all the models listed here aim to learn general text representation where the objective is to capture term proximity rather than relevance that is essen- tial for retrieval task . A recent work <ref type="bibr" target="#b9">(Gupta et al., 2017)</ref> tries to learn task- specific representation for CLIR. However, their model only captures ranking signals in monolin- gual settings, which does not necessarily general- ize well in CLIR.</p><p>In this paper, we propose to learn task-specific text representation for CLIR via a novel adversar- ial learning framework. Following the convention in generative adversarial networks (GAN) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, our representation learning model is realized as an interplay between two pro- cesses, an embedding generator (G) and an ad- versarial discriminator (D), conducted as a min- max game. With the GAN framework, we de- sign three constraints to direct the representation learning process. CLIR is essentially a rank- ing problem and we develop a matching con- straint to make sure that documents can be ranked in the right order given a query in another lan- guage. The matching constraint considers both cross-language and monolingual pairwise rank- ing signals, which is superior to previous studies (e.g., <ref type="bibr" target="#b9">(Gupta et al., 2017)</ref>) only considering mono- lingual matching signals. Meanwhile, a transla- tion constraint is imposed on the latent representa- tion to bridge the language gaps. These two con- straints direct the encoding networks to generate a language-invariant and task-specific representa- tion in the embedding space. Lastly, an adversarial constraint is proposed to force both language and source invariant to be reached more efficiently and effectively. Through the joint exploitation of these constraints in an adversarial manner, the embed- ding space being optimal for CLIR will then result through the convergence of this process. Compre- hensive CLIR experiments reveal that our model is superior to state-of-the-art continuous space mod- els and approaches the machine translation and monolingual baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Text representation has been a long-standing re- search question in IR. Classic methods such as vector space model are not able to deal with lexi- cal gaps between queries and documents, resulting in inferior retrieval performance. Latent semantic approaches such as LSA <ref type="bibr" target="#b4">(Deerwester et al., 1990)</ref> and latent dirichlet allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) abstract away from surface text forms to al- leviate sparsity and approximate semantics. More recently, learning based approaches with neural networks have gained great success in NLP ( <ref type="bibr" target="#b0">Baroni et al., 2014</ref>) and started to attract increasing interests of the IR community. In terms of word level embedding, word2vec <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref> and <ref type="bibr">Glove (Pennington et al., 2014</ref>) are two mod- els that have been cited frequently in recent litera- ture. These two models provide semantic-rich rep- resentations to bridge lexical gaps between queries and documents, which have been used broadly in neural IR studies ( <ref type="bibr" target="#b7">Ganguly et al., 2015;</ref><ref type="bibr" target="#b37">Zheng and Callan, 2015;</ref><ref type="bibr" target="#b35">Zamani and Croft, 2016)</ref>.</p><p>The above studies deal with monolingual text representation, which are related to the cross- language models presented below. As for CLIR, typical approaches reduce CLIR to its monolin- gual counterparts via performing some way of translation. Machine translation systems such as Google translator <ref type="bibr">1</ref> have been widely used to trans- late queries or documents, which serve as a de- fault and convenient translation option in CLIR. It is however far from being a suitable solution for solving CLIR problems (a detailed analysis can be found in ( <ref type="bibr" target="#b38">Zhou et al., 2012)</ref>). An alter- native solution is to rely on multilingual dictio- naries to perform lexicon-level translation, which is mostly in combination with either language modeling strategy ( <ref type="bibr" target="#b13">Kraaij et al., 2003</ref>) or query structuring framework <ref type="bibr" target="#b23">(Pirkola, 1998)</ref>. However, dictionary-based methods still suffer from the lex- ical gap problem which reduces their performance in CLIR.</p><p>In fact, researchers have extended the models in monolingual settings and developed continu- ous space models for cross-language tasks to cap- ture rich semantics. These cross-language exten- sions can be applied to CLIR directly. For in- stance, <ref type="bibr" target="#b14">Littman et al. (1998)</ref> extend LSA to its cross-language version CL-LSA by concatenating document-term matrix of parallel data which acts as dual-language documents to be learned by LSA. Such a methodology leads to a dual-language se- mantic space in which terms from both languages are represented. <ref type="bibr" target="#b31">Vinokourov et al. (2002)</ref> use parallel data to find most likely correlations be- tween projected vectors based on canonical com- ponent analysis technique. The OPCA model <ref type="bibr" target="#b24">(Platt et al., 2010)</ref> starts with the basic model PCA that is then made discriminative by encour- aging comparable document pairs to have simi- lar vector representation. Compared to CL-LSA, OPCA avoids the use of artificial concatenated documents. More recently, neural models have been employed to learn cross-language represen- tations. For instance, autoencoder is extended to a bilingual version BAE in ( <ref type="bibr" target="#b3">Chandar et al., 2014</ref>) which learns vectorial word representations from aligned sentences. <ref type="bibr" target="#b34">Yih et al. (2011)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>develop S2Net</head><p>to learn a projection matrix to map the correspond- ing term vectors into a latent space where simi- lar documents are close. S2Net is implemented with Siamese neural network framework. Vuli´cVuli´c and Moens (2015) first merge two documents from the aligned document pair in a comparable corpus and then train word2vec on the pseudo-bilingual document to obtain cross-language embeddings. The above approaches learn general text represen- tation that captures term proximity rather than rel- evance which is important for retrieval task . A recent work (Gupta et al., 2017) tries to learn task-specific embeddings for CLIR. However, it learns ranking signals by preserving pairwise ranking in monolingual set- tings prior to a transfer learning process to another language, which does not necessarily generalize well in CLIR.</p><p>One can find from above analysis that, most existing approaches, either based on neural net- works or not, learn general embeddings irrelevant to CLIR. We argue that task-specific embeddings are superior, a fact that is inspired by monolin- gual IR studies and that will actually be validated by CLIR experiments in this paper. To this end, we will learn cross-language and task-specific em- beddings for CLIR via a novel text representation model based on adversarial learning ( <ref type="bibr" target="#b8">Goodfellow et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representation learning framework</head><p>We will present in this section a neural represen- tation learning framework for CLIR. As discussed before, the framework is realized based on adver- sarial learning as an interplay between the genera- tor process and the discriminator process. We will develop three constraints, namely a matching con- straint, a translation constraint and an adversarial constraint, to direct the learning of cross-language and target-specific text embeddings. For ease of presentation, let us assume in CLIR we have a  source language query q s and a target language document d t . The translation of q s in the target language is q t . The learning framework is illus- trated in <ref type="figure" target="#fig_0">figure 1</ref>, which consists of an adversar- ial network N N adv , three dimension adaptation networks N N dim and three encoding networks re- spectively for q t , d t and q s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text representation networks</head><p>There have been various approaches one can use to encode sentences/documents into dense vectors.</p><p>For instance, models based on convolutional neu- ral networks <ref type="bibr" target="#b12">(Kalchbrenner et al., 2014</ref>) and mod- els based on recurrent neural networks ( <ref type="bibr" target="#b15">Liu et al., 2016</ref>) have been popular choices.</p><p>In order to map queries and documents into the embedding space, we make use of recurrent neural network with the long short-term memory (LSTM) architecture that can deal with vanishing and exploring gradient problems <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>). We present here derivation details of LSTM for clarification sake. The LSTM framework consists of several gates to control the cell state in the network. Firstly, a forget gate f (a sigmoid layer) functions according to:</p><formula xml:id="formula_0">f τ = σ(W f · [h τ −1 , x τ ] + b f )</formula><p>Then, an input gate i (a sigmoid layer) and a tanh layer work together as follows:</p><formula xml:id="formula_1">i τ = σ(W i · [h τ −1 , x τ ] + b i ) C τ = tanh(W c · [h τ −1 , x τ ] + b c )</formula><p>With the forget gate f , the input gate i and the new value C, one can update the cell state C as:</p><formula xml:id="formula_2">C τ = f τ * C τ −1 + i τ * C τ</formula><p>Lastly, an output gate o (a sigmoid layer) outputs:</p><formula xml:id="formula_3">o τ = σ(W o · [h τ −1 , x τ ] + b o ) h τ = o τ * tanh(C τ )</formula><p>In above equations, x τ is the input at time step τ . h τ and h τ −1 denote the hidden states at time steps τ and τ − 1. All W and b are parameters. For brevity, we can write the update process as:</p><formula xml:id="formula_4">h τ = LST M (h τ −1 , x τ )</formula><p>Given a text sequence x = (x 1 , x 2 , . . . , x l ), typical methods take the output h l of LSTM at the last time step l as the concentrated represen- tation of the whole sequence x ( <ref type="bibr" target="#b28">Sutskever et al., 2014</ref>). Since queries in IR tasks tend to be short and noisy, we make use of Bidirectional LSTM with pooling ( <ref type="bibr" target="#b29">Tan et al., 2015)</ref> to obtain a more effective text representation from all the hidden states h 1:l . The sequence x is fed from left to right into LST M a and from right to left into LST M b . The new hidden state h τ ab at time step τ is obtained by concatenating the hidden states of LST M a and LST M b at their respective time step τ . Since max-pooling has been proven to be efficient in similar tasks ( <ref type="bibr" target="#b29">Tan et al., 2015)</ref>, the latent repre- sentation z x of x can be formulated as:</p><formula xml:id="formula_5">z x = N N dim (M axP ooling(h 1:l ab ))</formula><p>where x can be q s , q t or d t . N N dim is designed to adapt the output dimension and to allow further flexibility for representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching constraint and Translation constraint</head><p>Document ranking is the central problem in both monolingual IR and CLIR tasks. CLIR differs it- self from its monolingual counterpart in that the language gap needs to be crossed prior to the re- trieval process. Since the choice of translation strategies (query, document or both) affects the de- sign of other components in our model, we will discuss the translation constraint in section 3.2.1 prior to matching constraints in sections 3.2.2 and 3.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Translation constraint</head><p>The translation constraint is developed to mini- mize the differences between a pair of parallel texts, which serves as a basic requirement in the translation scenario. Such a constraint directs the learning of language-invariant text representation for CLIR. We follow the arguments in previous studies ( <ref type="bibr" target="#b30">Vilares et al., 2016</ref>) and choose to trans- late queries in our model, since it is computation- ally expensive to translate large-scale document collections in practice. In this paper, we directly employ Google translator to translate queries, which is a popular choice for machine transla- tion that leads to state-of-the-art translation per- formance. The translation constraint is then im- posed on the embedding vectors z qs and z qt of the queries q s and q t . The translation loss L tra on a set QP of query pairs can be defined with the squared L2 norm, which is:</p><formula xml:id="formula_6">L tra = (qs,qt)∈QP z qs − z qt 2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cross-language matching constraint</head><p>The matching constraint captures essential charac- teristics of cross-language ranking. Following the practice in learning to rank <ref type="bibr" target="#b17">(Liu, 2009)</ref>, we model document ranking in the pairwise style where the relevance information is in the form of preferences between pairs of documents with respect to in- dividual queries. In the model for CLIR, since we have matching signals from both monolingual text pairs and cross-language text pairs, the model can benefit from complementary knowledge from two resources. The monolingual pairwise match- ing constraint will be introduced in section 3.2.3. Similar to neural models in monolingual set- tings ( <ref type="bibr" target="#b11">Huang et al., 2013)</ref>, the cross-language pair- wise matching constraint is placed on top of the embedding vectors of source language query and target language documents. In <ref type="figure" target="#fig_0">figure 1</ref>, let us as- sume x qs has a relevant document x d t+ and an irrelevant document x d t− according to annotated text pairs. In training, the positive sample x d t+ for x qs can be chosen as the most relevant texts according to annotation, and the negative sample x d t− is picked randomly from the data collection. The cross-language matching constraint encour- ages the hidden representation of x d t+ to be near to the hidden representation of x qs in the semantic- rich embedding space. Meanwhile, it asks the hid- den representation of x d t− to be far from that of x qs . We follow typical neural IR models and make use of cosine as the distance measure of hidden vectors. The probability that d t+ is ranked higher than d t− given q s can be derived as:</p><formula xml:id="formula_7">ˆ P (q s ) = σ[β c · (cos(z qs , z d t+ ) − cos(z qs , z d t− ))]</formula><p>where σ is the sigmoid function with a hyper- parameter β c controlling its shape. The cross- language matching loss L matc on cross-language triplet set QD c can be defined with cross-entropy loss as:</p><formula xml:id="formula_8">L matc = (qs,d t+ ,d t− )∈QDc CE[P (q s ), ˆ P (q s )]</formula><p>where CE denotes the cross-entropy operator be- tween two distributions and P (q s ) is the actual counterpart ofˆPofˆ ofˆP (q s ) estimated from annotation with a strategy similar to that in (Dehghani et al., 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Monolingual matching constraint</head><p>The monolingual matching constraint L matm can be built in a way similar to that of L matc . L matm is imposed on a set QD m of monolingual triplet (q t , d t+ , d t− ) as:</p><formula xml:id="formula_9">L matm = (qt,d t+ ,d t− )∈QDm CE[P (q t ), ˜ P (q t )]</formula><p>where P (q t ) is the actual counterpart of˜Pof˜ of˜P (q t ) es- timated from annotation. ˜ P (q t ) denotes the prob- ability that d t+ is ranked higher than d t− given q t . It can be computed with the sigmoid function as:</p><formula xml:id="formula_10">˜ P (q t ) = σ[β m · (cos(z qt , z d t+ ) − cos(z qt , z d t− ))]</formula><p>where β m is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Embedding generator constraint</head><p>Since our model is implemented with adversar- ial framework, we propose to model the repre- sentation generator G, which embodies the pro- cess of language-invariant and task-specific em- bedding of queries and documents into a latent subspace, under a combination of three constraints introduced above. The translation constraint aims to guarantee language invariant when translating queries. The cross-language matching constraint explicitly captures cross-language ranking signals from cross-language text pairs. The monolin- gual matching constraint takes monolingual rank- ing into account so as to complement the cross- language ranking signals.</p><p>Combing the three constraints above, we obtain a comprehensive constraint that should be obeyed by the embedding generator process. With the regularization term L reg equaling to the sum of Frobenius norms of all weight matrices in the text embedding phase, we can write the embedding generator constraint L G as:</p><formula xml:id="formula_11">L G (θ G ) = γ 1 ·L tra +γ 2 ·L matc +γ 3 ·L matm +L reg</formula><p>where θ G denotes the set of parameters in the generator networks, and γ 1 , γ 2 , γ 3 are hyper- parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial constraint</head><p>We will introduce the adversarial constraint in this part. <ref type="bibr">GAN (Goodfellow et al., 2014</ref>) simultane- ously trains a generative model G and a disrim- inative model D in a competing way. G gener- ates samples from a source of noise w that satisfies w ∼ P n (w) and tries to capture the real data dis- tribution P r . D learns to distinguish between the generated samples from G and the true data sam- pled from P r (in practice, from training data). The training procedure for G is to try its best to fool D. Let us assume that G generates samples satis- fying the distribution P g that is implicitly decided by G(w). The GAN value function V (G, D) on which D and G play the minmax game can be writ- ten as:</p><formula xml:id="formula_12">min G max D V (D, G) =E x∼Pr [log D(x)]<label>(1)</label></formula><formula xml:id="formula_13">+ E x∼Pg [log(1 − D(x))]</formula><p>Theoretical analysis has indicated that playing the minmax game as above amounts to minimizing the Jensen-Shannon divergence between P g and P r . We follow the general idea of GAN and de- velop an adversarial component on top of the em- bedding space in <ref type="figure" target="#fig_0">figure 1</ref>. We note that GAN has been used in representation learning in a sim- ilar way as in ( <ref type="bibr" target="#b2">Bousmalis et al., 2016;</ref><ref type="bibr" target="#b16">Liu et al., 2017</ref>). In our model in <ref type="figure" target="#fig_0">figure 1</ref>, the adversar- ial component N N adv acts as the discriminator D which tries its best to detect whether the embed- ding vector z is encoded from x qt , x dt or x qs . In this paper, N N adv is implemented as a neural net- work with a softmax output layer. The output of N N adv then corresponds to a probability distri- bution vector over the input sources. Let us de- note the ground truth label of the current input z to N N adv as l z which indicates the source that z is encoded from. We can adjust equation 1 to our settings and obtain the adversarial loss L adv on a query set Q t and a document set D t in the target language, as well as a query set Q s in the source language, which can be written as:</p><formula xml:id="formula_14">L adv = min G max D x∈Qt,Dt,Qs log N N adv (z x ) • l zx</formula><p>where • is the inner product operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training procedure</head><p>Following the training convention of GAN <ref type="bibr" target="#b8">(Goodfellow et al., 2014</ref>), the process of learning the language-invariant and task-specific text represen- tation for CLIR should be conducted by jointly minimizing the generator constraint L G and the adversarial loss L adv , which leads us to the com- bined objective function L as:</p><formula xml:id="formula_15">L = L G + L adv</formula><p>According to the rule of playing the minmax game in GAN, G tries its best to maximize the probability that D makes a mistake and D tries its best to distinguish between real data and gen- erated data (in our case, various input sources). The theoretical requirement behind GAN that D is maintained near its optimal solution as long as G changes slowly enough motivates us to update the discriminator part k steps per update of the gen- erator part in the iterative optimization process. Based on these discussions, the minmax optimiza- tion process can be derived as:</p><formula xml:id="formula_16">1. Optimize D when fixing G through: ˆ θ D = arg max θ D L( ˆ θ G , θ D )</formula><p>2. Optimize G when fixing D through:</p><formula xml:id="formula_17">ˆ θ G = arg min θ G L(θ G , ˆ θ D )</formula><p>The optimization can be implemented with mini-batch gradient ascent (for θ D ) and descent (for θ G ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head><p>In this section, we conduct CLIR experiments so as to compare our text representation model with several other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">CLIR evaluation sets</head><p>To perform CLIR experiments, we rely on broadly used data sets released in the bilingual tasks of the cross-language evaluation forum (CLEF) 2 . We choose to use the data from the year 2000 to 2004. <ref type="table" target="#tab_1">Table 1</ref> lists the characteristics of the data set, which include number of documents (N d ), num- ber of distinct words (N w ), the average document length (DL avg ) and the number of queries (N q ) in each task. We use source language queries in French (Fr), German (De) and Italian (It) to re- trieve target language documents in English (En). Queries from year 2000 to 2002 are combined to a single task in table 1 since they have the same target set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training set</head><p>In order to train the representation learning model, we need to construct a data set consisting of anno- tated text pairs. We combine AOL queries ( <ref type="bibr" target="#b21">Pass et al., 2006</ref>) and a set of news titles downloaded from the news sites 3 to constitute training query set of diversity. Following the previous work (Gupta et al., 2017), we sample a balanced subset (1M) from such query set and use these queries to retrieve the data collection with BM25. For each training query, we take the top retrieved texts as positive samples, and the negative samples are se- lected randomly from the data collection. In addi- tion to the pseudo-labeled text pairs of low quality, we combine the LETOR4.0 dataset (Qin and Liu, 2013) that is developed for evaluating learning to rank models. The LETOR4.0 dataset consists of relevance judgments of higher quality compared to pseudo-labeled data. The two data resources can complement each other in the training process.</p><p>In our experiments, the pseudo-labeled data is used to train the whole model and the LETOR dataset is employed to fine tune the parameters rel- evant to the source queries and target documents which are more important for the cross-language retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experimental setup</head><p>The terms are initialized as the 512d word2vec vectors trained on Wikipedia dump corpus 4 . The term embeddings are fed into the LSTM model of which the hidden unit number is chosen from {64, 128, 256, 512}. The adversarial network N N adv is as a three-layer feed-forward network with softmax on top of the last layer. N N dim is implemented as a feed-forward network with layer dimension chosen from {32, 64, 128, 256} and hidden layer number chosen from {1, 2}. The values of hyper-parameters γ 1 , γ 2 and γ 3 are cho- sen from {0.01, 0.1, 1, 10, 100}. The learning rate is selected from {10 −1 , 10 −2 , 10 −3 , 10 −4 , 10 −5 }. Those hyper-parameters are tuned on the valida- tion set which is 20% of the training queries ran- domly selected.</p><p>For evaluation, we present results in terms of mean average precision (MAP). Statistically sig- nificant differences between various models are determined using the paired t-test with p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Baseline approaches</head><p>We make use of three categories of baselines for CLIR experiments.</p><p>1. Monolingual run (MON): a baseline with tar- get language queries that are strictly parallel to source language queries.</p><p>2. Machine translation (MT): a baseline with target-language queries translated by machine translation system from source- language queries.</p><p>3. Cross-language text representation models: baselines that rely on continuous space mod- els for cross-language text representation.</p><p>We make use here of S2Net (Yih et al., 2011), BAE ( <ref type="bibr" target="#b3">Chandar et al., 2014)</ref>, and XCNN (Gupta et al., 2017) for the CLIR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and analysis</head><p>4.3.1 Comparisons to state-of-the-art <ref type="table">Table 2</ref> lists the experimental results on CLEF dataset for our model (the column OURS) and all baseline models. There are three data collec- tions and three language pairs, amounting to nine cross-language retrieval tasks. Except the strong baselines MON and MT, our model shows the best overall performance among all CLIR strate- gies. Indeed, our model outperforms all continu- ous space baselines (i.e., S2Net, BAE and XCNN) with statistical significance in almost all cases. Our model decreases slightly from the strong MT baseline in most retrieval tasks with only one degradation being significant on 03(De-En). Fur- thermore, one can find that our model approaches the monolingual baseline very much in all re- trieval tasks with all MAP ratios around or over 90%. In our experiments, we have not performed comparisons to CL-LSA ( <ref type="bibr" target="#b14">Littman et al., 1998)</ref> and its variant OPCA ( <ref type="bibr" target="#b24">Platt et al., 2010</ref>), because they have been consistently outperformed by other CLIR strategies with a large margin <ref type="bibr" target="#b27">(Schauble and Sheridan, 1997;</ref><ref type="bibr" target="#b20">Nie, 2010;</ref><ref type="bibr" target="#b32">Vuli´cVuli´c et al., 2011</ref>). Among all continuous space baselines, the most recent model XCNN shows the best perfor- mance. XCNN always outperforms linear projec- tion methods S2Net with significance. It also sig- nificantly outperforms the non-linear model BAE in all cases. This is coincident with previous conclusions in ( <ref type="bibr" target="#b9">Gupta et al., 2017</ref>) due to the fact that XCNN learns target-specific representa- tion for CLIR but the other models do not. Our model also tries to learn task-specific represen- tation for CLIR, which significantly outperforms XCNN in most cases according to the results in table 2. The reasons might be that (1) our method is modeled in a more effective adversarial learn- ing framework. (2) we explicitly capture cross- language ranking signals in embedding genera- tor in addition to monolingual ranking signals used in XCNN. (3) our model can jointly capture the translation knowledge and document ranking knowledge in a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Variant of our model</head><p>Our model can be customized easily by altering the constraints to direct the representation learn- ing process. Since the specificity of our model comes from the adversarial learning framework that has never been investigated in CLIR, we re- <ref type="table">Table 2</ref>: Retrieval performance (MAP scores) of all models on CLEF collections. +(m/x) or −(m/x) indicates that the improvements or degradations with respect to MT/XCNN are statistically significant. The highest value in each row (except the MON and MT baselines) is marked in bold. The percentages in the last column denote the MAP ratio of our model with respect to the MON baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Lang move the constraint L adv from the original model M and obtain the variant M adv . In this case, M adv can be optimized with standard mini-batch gradi- ent descent approach, without playing the minmax game. We redo above CLIR experiments with the same settings as above and obtain the retrieval re- sults of M adv in table 3.  From the results one can find that when remov- ing the adversarial component from the original model, M adv decreases from the original model M in all retrieval tasks. The differences that are significant appear in 5 out of 9 retrieval tasks. The results demonstrate that learning generator and discriminator in a competing style within the adversarial learning framework leads to represen- tation of higher quality, which eventually supports efficient CLIR. If we compare the variant M adv with the XCNN model in table 2, we find that M adv still performs better than XCNN in most cases. Such a comparison implicitly indicates that the joint exploitation of monolingual match- ing constraint, cross-language matching constraint and translation constraint in a single model is more efficient than using them separately as in the XCNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel text representa- tion approach for CLIR based on the adversarial learning framework. The learning framework is implemented as an interplay between an embed- ding generator process and an adversarial discrim- inator process, which leads to an optimal represen- tation that is both language invariant and domain specific. The embedding generator is learned such that it explicitly considers both cross-language and monolingual pairwise ranking signals. In this way, it can ensure that the learned embeddings bene- fit from both sources and are directly optimized for CLIR. To the best of our knowledge, it is the first time adversarial learning has been applied to CLIR. Experiments on various language pairs in CLEF data collection show that our model is sig- nificantly better than other latent semantic models for CLIR. Indeed, our model approaches the per- formance of machine translation and monolingual baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Text representation learning model for CLIR with adversarial framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fr-En 0 .</head><label>0</label><figDesc>424 0.412 − De-En 0.435 0.418 − It-En 0.426 0.424 03 Fr-En 0.456 0.440 − De-En 0.439 0.435 It-En 0.467 0.448 − 04 Fr-En 0.470 0.453 − De-En 0.473 0.465 It-En 0.481 0.469</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>CLIR dataset statistics (k = thousand). 

Dataset 
N d 
N w DL avg N q 
CLEF00-02 113k 173k 
311 
140 
CLEF03 
169k 233k 
284 
60 
CLEF04 
56k 120k 
231 
50 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Retrieval performance (MAP scores) of the 
variant M adv on CLEF collections. + or − indicates 
that the improvements or degradations with respect to 
our original model M are statistically significant. The 
higher value in each row is marked in bold. 

Data 
Lang 
M 
M adv 

00-02 

</table></figure>

			<note place="foot" n="1"> https://translate.google.com</note>

			<note place="foot" n="2"> http://www.clef-initiative.eu 3 We fetch 2.8M web pages from several news websites such as ChinaDaily (www.chinadaily.com.cn) and XinhuaNews (www.xinhuanet.com).</note>

			<note place="foot" n="4"> https://dumps.wikimedia.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able comments. This work was supported by the Fundamental Research Funds for Central Univer-sities of CCNU (No. CCNU15A05062).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural ranking models with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling diverse relevance patterns in ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word embedding based generalized language model for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debasis</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwaipayan</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="795" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Continuous space models for clir. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parth</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information and Knowledge Management, CIKM</title>
		<meeting>the 22nd ACM International Conference on Information and Knowledge Management, CIKM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embedding web-based statistical translation models in cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="419" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic cross-language information retrieval using latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cross-Language Information Retrieval</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep fusion lstms for text semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1034" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural models for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<idno>abs/1705.01509</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A picture of search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdur</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cayley</forename><surname>Torgeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Scalable Information Systems</title>
		<meeting>the 1st International Conference on Scalable Information Systems</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>InfoScale</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Pirkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Translingual document representations from discriminative projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Introducing LETOR 4.0 datasets. CoRR, abs/1306.2597</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crosslanguage information retrieval (clir) track overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schauble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paraic</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth Text REtrieval Conference</title>
		<meeting>the sixth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Lstmbased deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the feasibility of character n-grams pseudo-translation for cross-language information retrieval tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Oakes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="136" to="164" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inferring a semantic representation of text via cross-language correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Vinokourov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS</title>
		<meeting>the 15th International Conference on Neural Information Processing Systems, NIPS</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1497" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-language information retrieval with latent topic models trained on a comparable corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Asia Conference on Information Retrieval Technology</title>
		<meeting>the 7th Asia Conference on Information Retrieval Technology</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning, CoNLL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Embedding-based query language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval, ICTIR</title>
		<meeting>the 2016 ACM International Conference on the Theory of Information Retrieval, ICTIR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relevancebased word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to reweight terms with distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Translation techniques in cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Truran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brailsford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Ashman</surname></persName>
		</author>
		<idno>1:1-1:44</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
