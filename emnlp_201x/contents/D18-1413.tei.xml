<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Quantized Representations for Script Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Weber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leena</forename><surname>Shekhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
							<email>nchamber@usna.edu</email>
							<affiliation key="aff1">
								<orgName type="department">United States</orgName>
								<orgName type="institution">Naval Academy</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Quantized Representations for Script Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3783" to="3792"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3783</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Scripts define knowledge about how everyday scenarios (such as going to a restaurant) are expected to unfold. One of the challenges to learning scripts is the hierarchical nature of the knowledge. For example, a suspect arrested might plead innocent or guilty, and a very different track of events is then expected to happen. To capture this type of information , we propose an autoencoder model with a latent space defined by a hierarchy of categorical variables. We utilize a recently proposed vector quantization based approach, which allows continuous embeddings to be associated with each latent variable value. This permits the decoder to softly decide what portions of the latent hierarchy to condition on by attending over the value embeddings for a given setting. Our model effectively encodes and generates scripts, outperforming a recent language modeling-based method on several standard tasks, and allowing the autoencoder model to achieve substantially lower perplexity scores compared to the previous language modeling-based method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scripts were originally proposed by <ref type="bibr" target="#b27">Schank and Abelson (1977)</ref> as "structures that describe the appropriate sequence of events in a particular context". These event sequences define expec- tations for how common scenarios (such as go- ing to a restaurant) should unfold, thus enabling better language understanding. Although scripts represented many other factors (roles, entry con- ditions, outcomes) recent work in script induc- tion ( <ref type="bibr" target="#b25">Rudinger et al., 2015;</ref><ref type="bibr" target="#b23">Pichotta and Mooney, 2016;</ref><ref type="bibr" target="#b21">Peng and Roth, 2016</ref>) has focused on lan- guage modeling (LM) approaches where the "ap- propriate sequence of events" is the textual or- der of events (tuples of event predicates and their arguments). Modeling a distribution of text se- quences gives the intuitive interpretation of appro- priate event sequences being roughly equivalent to probable textual sequences. We continue with an LM approach, but we tackle two very impor- tant LM problems that have not yet been addressed with regards to event sequence modeling.</p><p>The first problem to address is that language models tend towards local coherency. Count based models are restricted by window size and sparse counts, while neural language models are known to rely on the local context for predictions. Since scripts are meant to describe longer coherent scenarios, this is a major issue. For example, con- tradictory events like (he denied charges) and (he pleads guilty) are given high probability in a typ- ical language model. Our model instead captures these variations with learned latent variables.</p><p>The second problem with recent work is that the hierarchical nature of scripts is not explicitly cap- tured. A high level script (like a suspect getting arrested) can branch off into many possible varia- tions. These variations are called the "tracks" of a script. <ref type="figure" target="#fig_0">Figure 1</ref> shows a script with two tracks learned by our model. LM-based approaches of- ten fail to explicitly capture this structure, instead throwing it all into one big distribution. This mud- dies the water for language understanding, making it difficult to tease apart differences like going to a fancy restaurant or a casual restaurant.</p><p>To remedy these problems, we propose a model that captures hierarchical structure via global la- tent variables. The latent variables are categorical (representing the various types of scripts and thier possible tracks and variations) and form a tree (or more generally, a DAG) 1 , thus capturing hierarchi- cal structure with the top (or bottom) levels of the tree representing high (or low) level features of the script. The top might control for large differences like restaurant vs crime, while the bottom selects between fancy and casual dining.</p><p>The overall model, which we describe below, takes the form of an autoencoder, with an encoder network inferring values of the latents and a de- coder conditioned on the latents generating scripts. We show the usefulness of these latent representa- tions against a prior RNN language model based system (Pichotta and Mooney, 2016) on several tasks. We additionally evaluate the perplexity of the system against the RNN language model, a task that autoencoder models have typically strug- gled with <ref type="bibr" target="#b3">(Bowman et al., 2016)</ref>. We find that the latent tree reduces model perplexity by a signifi- cant amount, possibly indicating the usefulness of the model in a more general sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Variational Autoencoders</head><p>Variational Autoencoders <ref type="bibr">(VAEs, Kingma and Welling (2014)</ref>) are generative models which learn latent codes z for the data x by maximizing a lower bound on the data likelihood:</p><formula xml:id="formula_0">log(p(x)) ≥ E q(z|x) [p(x|z)] − KL[q(z|x)||p(z)]</formula><p>VAEs consist of two components: an encoder which parameterizes the latent posterior q(z|x) and a decoder which parameterizes p(x|z). The objective function can be made completely differ- entiable via the reparameterization trick, with the full model resembling an autoencoder and the KL term acting as a regularizer.</p><p>While VAEs have been useful in continuous domains, they have been less successful in gen- erating discrete domains whose outputs have lo- 1 In this work we only look at linear chains of categorical variables, which is enough to encode trees (such as the one in <ref type="figure" target="#fig_0">Figure 1)</ref> cal syntactic regularities. Part of this is due to the "posterior collapse" problem ( <ref type="bibr" target="#b3">Bowman et al., 2016)</ref>; when VAEs are equipped with powerful au- toregressive decoders, they tend to ignore the la- tent, collapsing the posterior q(z|x) to the (usually zero-mean Gaussian) prior p(z). By doing this, the model takes no penalty from the KL term, but effectively ignores its encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vector Quantized Variational Autoencoders</head><p>Vector Quantized VAEs (VQ-VAEs, van den Oord et al. <ref type="formula">(2017)</ref>) are a recently proposed class of mod- els which both alleviates the posterior collapse problem and allows the model to use a latent space of discrete values. In VQ-VAEs the latent z is rep- resented as a categorical variable that can take on K values. Each of these values k ∈ {1, ..., K} has associated with it a vector embedding e k . The posterior of VQ-VAEs are discrete, deterministic, and parameterized as follows:</p><formula xml:id="formula_1">q(z = k|x) = 1 k=argmin i ||f (x) − e i || 2 0 elsewise</formula><p>where f (x) is a function defined by an encoder network. The decoding portion of the network is similar to VAEs, where a decoder parameterizes a distribution p(x|z = k) = g(e k ), where g is the decoder network, and e k is the corresponding em- bedding, which is fed as input to the decoder. This process can be seen as a "quantization" operation mapping the continuous encoder output to the la- tent embedding it falls closest to, and then feeding this latent embedding (in lieu of the encoder out- put) to the decoder. The quantization operator is not differentiable, thus during training, the gradient of the loss with respect to the decoder input is used as an estima- tion to the gradient of the loss with respect to the encoder output. If one assumes a uniform prior over the latents (we do so here), then the KL term in the VAE objective becomes constant and may be ignored. In practice, multiple latent variables z may by used, each with their own (or shared) embeddings space. </p><formula xml:id="formula_2">q(z|x) = q 0 (z 0 |x) M −1 i=1 q i (z i |pr(z i ), x)</formula><p>where pr(z i ) denotes the parent of z i in the tree. Since the latent variables are meant to capture the hierarchical categorization of the script, we make the assumption that when a higher level script cat- egory (for example, z 0 ) is observed with the ac- tual sequence of events (x), determining the im- mediate lower level category (z 1 ) is a determinis- tic operation. Thus, similar to VQ-VAEs, we pa- rameterize the individual factors of the posterior, q i (z i = k|pr(z i ), x), as:</p><formula xml:id="formula_3">1 k=argmin j ||f i (x, pr(z i )) − e ij || 2 0 elsewise</formula><p>where f i (x, pr(z i )) is an encoding function spe- cific to latent z i and e ij is the jth value em- beddings for z i . The distribution p(x|z) is simi- larly parameterized by an decoder function g(z e ), where z e is the set of corresponding value embed- ding for each latent z i . We describe the forms of the encoder and decoder in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HAQAE Encoder and Decoder</head><p>During the encoding process, certain parts of the input may provide more evidence towards differ- ent parts of the hierarchy. For example, the event (he ate food) gives evidence to the high level cate- gory of a restaurant script, while the more specific event (he drank wine) gives more evidence to the lower level category fancy restaurant. Thus dur- ing encoding, it makes sense to allow each latent to decide which parts of the input to take into con- sideration, based on its parent latents. This is ac- complished by parameterizing the encoding func- tion for latent z i as an attention over the input x, with the parent of z i (more specifically, the embed- ding for the parent's current value) acting as the 'query' vector. As is standard when using atten- tion, the input sequence of events,</p><formula xml:id="formula_4">x = (x 1 , ...x n ),</formula><p>is first encoded into a sequence of hidden states</p><formula xml:id="formula_5">h x = (h 1 , ..., h n ) via a RNN encoder.</formula><p>The full encoding function for latent z i can thus be written as:</p><formula xml:id="formula_6">f i (x, pr(z i )) = attn(h x , pr(z i ))</formula><p>Though any attention formulation is possible, we use the bi-linear attention proposed in <ref type="bibr" target="#b13">Luong et al. (2015)</ref> in our implementations. For the root of the latent tree (z 0 ), which has no parents, we use the averaged value of the encoder vectors h x as the query vector for its attention. We can define the decoder in a similar fashion. As is usually done, the distribution p(x|z e ) can be defined in an autoregressive manner using a RNN decoder network. Like the encoding process, dif- ferent parts of the hierarchy may affect the gen- eration of different parts of the input. We thus also allow the decoder network g(z e ) to be a RNN with a standard attention mechanism over the la- tent value embeddings, z e . Since the latent root z 0 is supposed to capture the highest level informa- tion about the script, we use its embedding value, (passed through an affine transformation and tanh activation) to initialize the hidden state of the de- coder. Both encoder and decoder can be trained end to end using the same gradient estimation used for VQ-VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Objective</head><p>The training objective for HAQAE is nearly the same as the VQ-VAE objective proposed in van den <ref type="bibr">Oord et al. (2017)</ref>. For a single training example, x i the objective can be written as:</p><formula xml:id="formula_7">L = − log p(x i |z) + 1 M M j L R j + 1 M M j L C j</formula><p>where L R j and L C j are the reconstruct and commit loss for the jth latent variable. As in van den <ref type="bibr">Oord et al. (2017)</ref>, we let sg(·) stand for a stop gradient operator, such that any term passed to it is treated as a constant. The reconstruction loss is defined as:</p><formula xml:id="formula_8">L R j = ||sg(f j (x, pr(z j ))) − e * j || 2 2</formula><p>where e * j is the argmin value embedding for z j (for the given input). The reconstruct loss is how the value embeddings for z are learned, and pushes the value embeddings to be closer to the output of the f i . The commit loss is defined as:</p><formula xml:id="formula_9">L C j = β||f j (x, pr(z j )) − sg(e * j )|| 2 2</formula><p>which forces the encoder to push its output closer to some embedding, preventing a situation in which the encoder maps inputs far away from all embeddings. β is a hyperparameter that weighs the commit loss 2 . Note that the commitment loss may be propogated all the way up through the hier- archy of latent nodes. We allow the latent embed- dings to receive updates only from the reconstruct and commit loss (not from the NLL loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Preprocessing</head><p>Dataset We use the New York Times Gigaword Corpus as our dataset. The corpus contains a to- tal of around 1.8 million articles. We hold out 4000 articles from the corpus to construct our de- velopment (dev) set for hyperparameter tuning and 6000 articles for the test set. The input and output of the model is in the form of an event sequence. Each event is defined as a 4-tuple, (v, s, o, p), con- taining the verb, subject, object and preposition. Events without prepositions are given a null token in their preposition slot. The components of the events (the verb, subject, etc.) are all taken to be individual tokens, and can thus be treated more or <ref type="bibr">2</ref> In our implementations we set β = 0.25 less like normal text. For example, the events (he played harp), (he touched moon), would be tok- enized and given to the model as: played he harp null tup touched he moon null, where null is the null preposition token and tup is a special separa- tion token between events. We extract event tuples using Open Information Extraction system Ollie ( <ref type="bibr" target="#b14">Mausam et al., 2012</ref>). We then group together event tuples for 4 subse- quent sentences to create a single event sequence. We also ignore tuples with common (is, are, be, ...) and repeating predicates. Finally we have 7123097, 19425, and 28667 event sequences for training, dev, and test dataset respectively. For all the experiments we fix the minimum and maxi- mum sequence lengths to be 8 and 50 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">HAQAE Model Details</head><p>The HAQAE model we use across all evaluations uses 5 discrete latent variables, structured in the form of a linear chain (thus no variable has more than one child or parent). Each variable can ini- tially take on K = 512 values, with all latents having an embeddings dimension of 256. The en- coder RNN that performs the initial encoding of the event sequence is a bidirectional, single layer RNN with GRU cell ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>) with a hidden dimension of 512. The inputs to this en- coder are word embeddings derived from the one- hot encodings of the tokens in the event sequence. The embeddings size is 300. We find initializ- ing the embeddings with pretrained GloVe <ref type="bibr" target="#b22">(Pennington et al., 2014</ref>) vectors to be useful. The de- coder RNN is also a single layer RNN with GRU cells with a hidden dimension of 512 and 300 dimensional (initialized) word embeddings as in- puts. For all experiments we use a vocabulary size of 50k. We train the model using Adam <ref type="bibr" target="#b10">(Kingma and Ba, 2014</ref>) with a learning rate of 0.0005, and gradient clipping at 5.0. We find that the training converges around 1.5 epochs on our dataset. Fur- ther details can be found in our implementation <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare the performance of our proposed model against three previous baselines and a mod- ification of our HAQAE model that removes ex- plicit dependencies between latents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN Language Model</head><p>For our first baseline system we train a RNN sequence model. This model is 2 layered GRU cells with hidden size 512 and embedding size 300. We use Adam with a learning rate 0.001. To prevent the problem of ex- ploding gradients, we clip the gradients at 10. We use uniform distribution [-0.1, 0.1] for random ini- tialization and biases initialized to zero. We also use a dropout of 0.15 on the input and output em- bedding layers but none on the recurrent layers. We initialize the word embedding layer with pre- trained Glove vectors as it improved the perfor- mance and makes the system directly comparable to HAQAE. We refer to this model as RNNLM in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNNLM + Role Embeddings</head><p>We also repro- duced the model from <ref type="bibr" target="#b23">Pichotta and Mooney (2016)</ref> for comparison. This model is similar to the one above except that at each time step the model has an additional role marker input going into it. The marker guides the language model further by indicating what type of input is being currently fed to it: a subject, object, or predicate. These role embeddings are learned during training itself. Hyperparameters are exactly the same as the RNNLN except that the role embeddings have a dimension of 300. We will refer to this model as RNNLM+Role. We perform hyperparameter tun- ing of both the models using the development set. We use a vocabulary size of 50k. We trained both the baseline models for 2 epochs on the training set.</p><p>VAE We report results using a vanilla VAE model similar to the one used in <ref type="bibr" target="#b3">Bowman et al. (2016)</ref>. The encoder/decoder for the VAE baseline has the same specs as the encoder/decoder for the HAQAE model, with a latent dimension of 300. We use linear KL annealing for the first 15000 steps and 0.5 as the word dropout rate.</p><p>Hierarchyless HAQAE (NOHIER) In order to test the effect of explicitly having a hierarchy in the latent variables, we additionally train another HAQAE model with no explicit hierarchical latent space. The model still has 5 discrete latent vari- ables like our original model, however each of the variables are independent of each other (given the input). All five variables are have an attention over the input and take the average of encoder vectors h x as the query vector (as done with the latent root z 0 in the original model). We additionally desig- nate one of the variables to be used to initialize the hidden state of the decoder. We found the same training hyperparmaters used in the training of the original model to work well here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Modeling: Perplexity</head><p>As our proposed models are essentially language models, it is natural to evaluate their perplexity scores, which can be viewed as an indirect mea- sure of how well the models can identify scripts. We compute per-word perplexity and per-word negative log likelihood on the validation and test sets. We compute these values without consid- ering the end-of-sentence (EOS) token. <ref type="table">Table 1</ref> gives these results. A good language model should assign low perplexity (high probability) to the val- idation and test sets. We observe that HAQAE achieves the minimum negative log likelihood and perplexity scores on both the validation and test sets as compared to the previous RNN-based mod- els. The result is particularly interesting as au- toencoders usually perform worse or comparable to other RNN language models in terms of per- plexity (negative log likelihood) as is in the case of the vanilla VAE here; similar observations have also been made in <ref type="bibr" target="#b3">Bowman et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inverse Multiple Choice Narrative Cloze</head><p>Narrative cloze evaluations of event based lan- guage models (LMs) start with a sequence of events as input and test whether the LMs cor- rectly predict a single held-out event. The standard narrative cloze task has various issues <ref type="bibr">(Chambers, 2017</ref>). In our evaluations we opt instead for the multiple choice variant proposed in Granroth- <ref type="bibr" target="#b8">Wilding and Clark (2016</ref>  <ref type="table">Table 2</ref>: Inverse narrative cloze accuracy(%) on randomly selected 2k validation and test set. Mod- els scored on whether they assign a higher proba- bility to legitimate event sequence over detractor event sequences. Higher is better.</p><p>verse narrative cloze task. Instead of being given an event sequence and predicting a single event to go with it, we instead are given only one event and the model must identify the rest of the event sequence. The model is identifying sequences of events, not single events. We chose this setup be- cause sequences is what we ultimately want, but also because identifying single events resulted in very high scores (around 98% accuracy). This task proved to be more challenging as an evaluation.</p><p>We thus score a model based on the probabil- ity it assigns to event sequences that begin with a single input event. A legitimate event sequence should have high probability compared to an event sequence that is stitched together using two ran- dom event sequences. We create legitimate event sequences of a fixed length (six) by extracting ac- tual event sequences observed in documents. For every legitimate event sequence, we use the first event in the sequence as a seed event. Then, we construct detractor event sequences for this seed by appending a different sequence of events (num- ber of events being five) from a randomly cho- sen document. We create five such detractor se- quences for every legitimate sequence. We rank the six sequences based on the probabilities as- signed by the model and then evaluate the ac- curacy of the top ranked sequence. A random model will uniformly choose one among the six sequences and thus will score 16 = 16.60% on the task. We report results averaged over 2000 sets of legitimate and detractor sequences.</p><p>Results in <ref type="table">Table 2</ref> show that the HAQAE is sub- stantially better than both RNN LMs and vanilla VAE and similar to the NOHIER model, which shows the usefulness of the quantized embeddings overall as a global representation. The comparable results of the NOHIER model on this task might also indicate that explicitly modeling the hierar- chical structure may not be completely necessary if ones only aim is to capture global coherence.</p><p>The results on the perplexity task do indicate that overall, modeling the hierarchical structure is use- ful for better prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparing HAQAE and NOHIER</head><p>Both HAQAE and NOHIER models achieve the best results across all tasks. The HAQAE model does better on the perplexity task, while results of the two models on the cloze task are nearly the same. One clear benefit of explicitly connecting the latent variables together appears to be in the efficiency of the learning. The HAQAE model performs comparable or better than the NOHIER model despite (in this case at least) having fewer parameters 4 . The HAQAE model also appears to learn much faster than the NOHIER model. We show this in <ref type="figure" target="#fig_2">Figure 3</ref>, which shows the per-word cross en- tropy error on the validation set as training pro- gresses. We observe that the cross entropy error drops much faster in the latter model than the for- mer one. Also, the error is always lower for the HAQAE model.</p><p>One possibility is that the NOHIER model learns similar information as the HAQAE model, but due to its lack of explicit inductive bias, takes a longer time to learn this. We leave it as future work to confirm whether this is the case through an in depth study into the properties of the learned discrete latents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluating Event Schemas</head><p>So far, we've evaluated how well the models rec- ognize real textual events (perplexity) and how well the models predict events in scripts (narrative cloze). This section evaluates the script genera- tion ability of the model, and specifically its abil- ity to capture hierarchical information with differ- ent tracks in scripts (e.g., pleading guilty causes different events to occur than does pleading inno- cent). In many respects, this section illustrates best the power of HAQAE even though the results are partly subjective.</p><p>While we presented two automatic evaluations above, we shift to human judgment to evaluate the scripts themselves. We believe this complements the empirical gains already presented. The scripts generated by the models were shown to human judges and scored on several metrics.</p><p>Most previous work on script induction starts with a seed event and then grows the script based on measures of event proximity or from sampling the distribution with the seed as context. While ef- fective in generating a bag of events, a major prob- lem in all previous work is that conflicting events are included (sentenced and acquitted). While the events are related and part of the same high-level script, they should never appear together in an ac- tual instance of a script.</p><p>In order to evaluate our model for this type of knowledge, we instead defined a seed as 2 events: the first event sets the general topic, and the sec- ond event starts a specific track in that topic. For instance, below are two seeds that are intended to generate two tracks for the same script: "people reported fire" "fire spread in neighborhood" "people reported fire" "fire spread to forest" For each seed (2 events in one seed) we select the first 3 events generated by a model conditioned on the seed as context. The 2 events in a seed thus initialize the latent variable values, which then inform the decoder to generate more events (we choose the first 3). The strength of our model is that the second event helps select the more spe- cific script track, and to ignore conflicting events in other tracks.</p><p>While generating for both RNNLM+Role and HAQAE models, we additionally enforce a con- straint that restricts models from generating events that have a predicate that has already been gener- ated, as well as events whose subject and object are the same.</p><p>We evaluated the RNNLM+Role model from Pichotta and Mooney (2016) against our proposed HAQAE. Each model was given 40 seeds (20 first event each with 2 contrasting second seed) and thus generated 40 scripts. The annotators were also shown the seeds (2 events), and then asked to rate each three-event sequence for various metrics described below. Non-Sensical (Sense): Binary, is each event itself non-sensical or understandable? Event Relevance (Rel): Binary, each event was scored for being relevant or not to the script topic. This ignored whether it was consistent with the seed's branch. Coherency with Branch (Coh): 0-2, each event was scored for being coherent with the seed's spe- cific branch (the second event). 0 means not at all, 1 means somewhat, and 2 means yes. Branching Uniqueness (BranchU): 0-2, each pair of scripts (both branches of the same topic) were scored for overlap of events. 0 means similar events generated for both, 1 means some similar events, and 2 means distinct. This score is impor- tant because some RNN decoders might ignore the second event and focus on the general topic only. Branching Quality (BranchQ): 0-2, each gen- erated branch was scored for branch quality. 0 means the generated events are not specific to the branch, 1 means some are specific, and 2 means most/all events are specific. This is the most im- portant score in measuring how well a model cap- tures hierarchical structure and script tracks.</p><p>Two expert annotators evaluated the generated event sequences. In case of disagreements in scores, we also involved a third annotator to re- solve these conflicts. Results for this task are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Both RNNLM and HAQAE produce sensical events, but the HAQAE model outperforms on all other metrics. It produces more relevant and co- herent events for the topic at hand (relevance and coherency). But most important to the goals of this paper, it doubles the RNNLM scores on branch- ing uniqueness and quality. This is because an RNNLM mostly generates from a bag of events after encoding the seed, but the HAQAE utilizes its latent space to produce branch-specific tracks of event sequences. <ref type="table">Tables 4 show a few such</ref>   RNN+Role bomb found in backpack, bomb failed to detonate bomb killed people, bomb detonated in blast bomb found in backpack, bomb detonated explosion killed people, people injured in blast people reported fire, fire spread to forest fire destroyed building people reported fire, fire spread to neighborhood fire damaged building HAQAE bomb found in backpack, bomb failed to detonate they found evidence, explosive hidden in luggage bomb found in backpack, bomb detonated blast left crater, blast killed people people reported fire, fire spread to forest fire burned acres people reported fire, fire spread to neighborhood fire destroyed building <ref type="table">Table 4</ref>: Sample outputs from the baseline and our proposed system. The seeds (what is given to the system) are shown in the left column while the outputs are on the right. HAQAE is able to distinguish between the contrasting seeds. Red highlights the lack of branching quality in the baseline model and Blue highlights the correct behavior as exhibited by HAQAE.</p><formula xml:id="formula_10">ex-System Sense(%) Rel(%) Coh(0-2) BranchU(0-2) BranchQ(0-2)</formula><p>person denied charges, lawsuit filed by person, judge dismissed lawsuit person denied charges, they accused person, person resigned in january clinton carried promises, clinton began in 1988, clinton made changes campaign carried promises, campaign began on september, campaign made effort campaign carried promises, campaign began on september, campaign made effort team carried for championship, team played in philadelphia, they won champoinship amples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Observations about the Latent Variables</head><p>We also look at how changing the values of var- ious latent variables change the resulting output, in order to get a small idea as to what properties the variables capture. We find that the root level variable z 0 has the largest effect on the output, and typically corresponds to the domain that the se- quence of events belong to. The non root variables generally change the output on a smaller scale, however we find no correspondence between the level of the variable and the amount of output that is affected upon changing its value. One reason for the difficulty of interpreting the variables is that the model conditions on them through attention, thus changing the value of one does not necessarily need to have any effect.</p><p>We do find that changing the lower level latents generally leads to the ending/beginning of the se- quence changing or the entities of the sequence changing (but still remaining in the same topi- cal domain). We additionally find that changing the top level latent may often preserve the overall form of the event sequence, and only transform the topic. We provide examples of these output by our system in <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Scripts were originally proposed by <ref type="bibr" target="#b26">Schank and Abelson (1975)</ref> and further expanded upon in <ref type="bibr" target="#b27">Schank and Abelson (1977)</ref>. The notion of hier- archies in scripts has been studied in the works of <ref type="bibr" target="#b0">Abbott et al. (1985)</ref> and <ref type="bibr" target="#b2">Bower et al. (1979)</ref>. <ref type="bibr" target="#b18">Mooney and DeJong (1985)</ref> present an early non probabilistic system for extracting scripts from text. A highly related work by <ref type="bibr" target="#b15">Miikkulainen (1990)</ref> provides an early example of a system explicitly designed to take advantage of the hi- erarchical nature of scripts, creating a model of scripts based on self organizing maps <ref type="bibr" target="#b12">(Kohonen, 1982)</ref>. Interestingly, self organizing maps also uti- lize vector quantization during learning (albeit in a different way than done here).</p><p>Recent work starting from Chambers and Ju- rafsky (2008) has focused on learning scripts as prototypical sequences of events using event co- occurrence. Further work has framed this task as a language modeling problem <ref type="bibr" target="#b23">(Pichotta and Mooney, 2016;</ref><ref type="bibr" target="#b25">Rudinger et al., 2015;</ref><ref type="bibr" target="#b21">Peng and Roth, 2016)</ref>. Other work has looked at learn- ing more structured forms of script knowledge called schemas <ref type="bibr" target="#b4">(Chambers, 2013;</ref><ref type="bibr" target="#b1">Balasubramanian et al., 2013;</ref><ref type="bibr" target="#b19">Nguyen et al., 2015</ref>) which fo- cuses on additionally inducing script specific roles to be filled by entities. In this work we treat event components as separate tokens, though work has also looked into methods for composing this com- ponents into a single distributed event representa- tion ( <ref type="bibr" target="#b17">Modi and Titov, 2014;</ref><ref type="bibr" target="#b16">Modi, 2016;</ref><ref type="bibr" target="#b30">Weber et al., 2018</ref>). We leave this as possible future work.</p><p>The hierarchical structure of our proposed model is similar to structure of the latent space in other VAE variants <ref type="bibr" target="#b29">(Sonderby et al., 2016;</ref><ref type="bibr" target="#b31">Zhao et al., 2017)</ref>, with the discrete variables and at- tentions in our model being the major differences. <ref type="bibr" target="#b9">Hu et al. (2017)</ref> present a VAE based model for controllable text generation, with different latents controlling different aspects of the generated text, but requiring labels for semi-supervision. Other methods using discrete variables for VAEs have also been proposed <ref type="bibr" target="#b24">(Rolfe, 2017)</ref>, as have varia- tions in the VQ-VAE learning process ( <ref type="bibr" target="#b28">Sonderby et al., 2017)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a new model, HAQAE, for script learning and generation that is one of the first to model the hierarchy that is inherent in this type of real-world knowledge. Previous work has fo- cused on modeling event sequences with language models, while ignoring the problem of contradic- tory events and different tracks being jumbled to- gether. The hierarchical latent space of HAQAE instead attends to the choice points in event se- quences, and is able to provide some discrimina- tion between tracks of events.</p><p>While HAQAE is motivated by the specific need for hierarchies in scripts, it can also be seen as a general event language model. As a language model HAQAE has a substantially lower perplex- ity on our test set than previous RNN models de- spite HAQAE's decoder having fewer parameters.</p><p>We also presented a new inverse narrative cloze task that is a multiple-choice selection of event sequences. It proved to be a very difficult task with systems producing accuracies in the mid 20% range. HAQAE and NOHEIR were the only sys- tems to break 30 with a top accuracy of 34.0%. This further illustrates that using a latent space to capture script differences helps identify relevant sequences.</p><p>To our knowledge, all previous work on script induction has focused on learning single event se- quences or bags of events. We view our proposed model as a new step toward learning different de- tails about scripts, such as tracks and hierarchies. Though the proposed model works well empiri- cally, understanding exactly what is learned in the latent variables is non trivial, and is a possible di- rection for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Acknowledgements</head><p>This work is supported in part by the National Sci- ence Foundation under Grant IIS-1617969.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An automatically learned multi-track script. The left track is a dismissed case, and the right is a convicted suspect. Our model generated both tracks through a latent hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical Quantized Autoencoder Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Cross entropy error on dev set of NOHIER (red) and HAQAE (blue) models as training progresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). One of our goals is to test if our generative model can produce globally coherent scripts by evaluating their ability to generate coherent event sequences. To evaluate this we create a new in-</figDesc><table>System 
validation 
test 
RNNLM 
25.30 
26.30 
RNNLM+Role 
24.60 
26.35 
VAE 
26.54 
28.01 
NOHIER 
31.68 
34.00 
HAQAE 
31.80 
33.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Human evaluation of schemas generated for the seed events. HAQAE consistently performs better than the baseline model. Higher is better for all metrics.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results of changing a single latent variable while keeping others fixed. Lower level latents 
typically change ending/beginnings or entity names (Rows 1 and 2). The top level latent changes the 
topic and may occasionally preserve the form (Row 3) 

</table></figure>

			<note place="foot" n="3"> Hierarchical Quantized Autoencoder Our goal is to build a model that can generate globally coherent multi-track scripts which allow us to account for the different ways in which a script can unfold. The main idea behind our approach</note>

			<note place="foot" n="3"> github.com/StonyBrookNLP/HAQAE</note>

			<note place="foot" n="4"> NOHIER has more parameters in our case due to each latent taking a bidirectional encoder state as a query vector, as opposed to taking the parent latent vector as the query</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The representation of scripts in memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerie</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="199" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating coherent event schemas at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1721" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scripts in memory for text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Bower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="220" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<editor>Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Event schema induction with a probabilistic entity-driven model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Behind the scenes of an evolving event cloze test</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What happens next? event prediction using a compositional neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Granroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Wilding</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2727" to="2733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-organized formation of topologically correct feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teuvo Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Script recognition with hierarchical feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Risto Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="101" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event embeddings for semantic script modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inducing neural models of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning schemata for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Dejong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative event schema induction with entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiem-Hieu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Besançon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<title level="m">Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural discrete representation learning. NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two discourse driven language models for semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-712" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning statistical scripts with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2800" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Discrete variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolfe</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Script induction as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1681" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scripts, plans, and knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert P Abelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scripts, plans, goals and understanding: An inquiry into human knowledge structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert P Abelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mhwah</title>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continuous relaxation training of discrete latent variable image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casper Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae Sonderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Snderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<title level="m">Ladder variational autoencoders. NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Event representations with tensor-based compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<idno>abs/1711.07611</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning hierarchical features from deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="4091" to="4099" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
