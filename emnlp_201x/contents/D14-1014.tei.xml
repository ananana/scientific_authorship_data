<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Submodularity for Data Selection in Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
							<email>bilmes@u.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Submodularity for Data Selection in Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="131" to="141"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce submodular optimization to the problem of training data subset selection for statistical machine translation (SMT). By explicitly formulating data selection as a submodular program, we obtain fast scalable selection algorithms with mathematical performance guarantees, resulting in a unified framework that clarifies existing approaches and also makes both new and many previous approaches easily accessible. We present a new class of submodular functions designed specifically for SMT and evaluate them on two different translation tasks. Our results show that our best submodular method significantly outperforms several baseline methods, including the widely-used cross-entropy based data selection method. In addition, our approach easily scales to large data sets and is applicable to other data selection problems in natural language processing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>SMT has made significant progress over the last decade, not least due to the availability of increas- ingly larger data sets. Large-scale SMT systems are now routinely trained on millions of sentences of parallel data, and billions of words of mono- lingual data for language modeling. Large data sets are often beneficial, but they do create certain other problems. First, they place higher demands on computational resources (storage and compute). Hence, existing software infrastructure may need to be adapted and optimized to handle such large data sets. Second, experimental turn-around time is increased as well, making it more difficult to quickly train, fine-tune, and evaluate novel model- ing approaches. Most importantly, however, SMT performance does not increase linearly with the training data size but levels off after a certain point. This is because the additional training data may be noisy, irrelevant to the task at hand, or inherently redundant. Thus, a linear increase in the amount of training data typically leads to a sublinear increase in performance, an effect known as diminishing returns. Several recent papers <ref type="bibr" target="#b4">(Bloodgood and Callison-Burch, 2010;</ref><ref type="bibr" target="#b42">Turchi et al., 2012a;</ref><ref type="bibr" target="#b43">Turchi et al., 2012b</ref>) have amply demonstrated this effect.</p><p>A way to counteract this is to perform data sub- set selection, i.e., choose a subset of the available training data to optimize a particular quality cri- terion. One scheme is to select a subset that ex- presses as much of the information in the original data set as possible -i.e., the data set should be "summarized" by excluding redundant information. Another scheme, popular in the context of SMT, is to subselect the original training set to match the properties of a particular test set.</p><p>In this paper, we introduce submodularity for subselecting SMT training data, a methodology that follows both of the above schemes. 1 Sub- modular functions <ref type="bibr" target="#b13">(Fujishige, 2005</ref>) are a class of discrete set functions having the property of di- minishing returns. They occur naturally in a wide range of problems in a diverse set of fields includ- ing economics, game theory, operations research, circuit theory, and more recently machine learn- ing. Submodular functions share certain properties with convexity (e.g., naturalness and mathematical tractability) although submodularity is still quite distinct from convexity.</p><p>We present a novel class of submodular func- tions particularly suited for SMT subselection and evaluate it against state-of-the-art baseline meth- ods on two different translation tasks, showing that our method outperforms them significantly in most cases. While many approaches to SMT data se- lection have been developed previously (a detailed overview is provided in Section 3), many of them are heuristic and do not offer performance guaran- tees. Certain previous approaches, however, have inadvertently made use of submodular methods. This, in addition to our own positive results, pro- vides strong evidence that submodularity is a natu- ral and practical framework for data subset selec- tion in SMT and related fields.</p><p>An additional advantage of this framework is that many submodular programs (e.g., the greedy procedure reviewed in Section 2) are fast and scalable to large data sets. By contrast, trying to solve a submodular problem using, say, an integer-linear programming (ILP) procedure, would lead to impenetrable scalability problems.</p><p>Initial value f(X) = 2 colors in urn. Updated value f(X∪{v}) = 3 with added blue ball.</p><p>Initial value f(Y) = 3 colors in urn. Updated value f(Y∪{v}) = 3 with added blue ball.</p><p>X Y v v This paper makes several contributions: First, we present a brief overview of submodular functions (Section 2) and their potential application to natural language processing (NLP). Next we review pre- vious approaches to MT data selection <ref type="bibr">(Section 3)</ref> and analyze them with respect to their submodular properties. We find that some previous approaches are submodular in nature although this connection was not heretofore made explicit. Section 4 details our new approach. We discuss desirable properties of an SMT data selection objective and present a new class of submodular functions tailored towards this problem. Section 5 presents the data and systems used for the experiments, and results are reported in Section 6. Section 7 then concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Submodular Functions/Optimization</head><p>Submodular functions <ref type="bibr" target="#b11">(Edmonds, 1970;</ref><ref type="bibr" target="#b13">Fujishige, 2005)</ref>, are widely used in mathematics, economics, circuit theory <ref type="bibr" target="#b38">(Narayanan, 1997)</ref>, and operations research. More recently, they have attracted much interest in machine learning (e.g., <ref type="bibr" target="#b37">(Narasimhan and Bilmes, 2004;</ref><ref type="bibr" target="#b24">Kolmogorov and Zabih, 2004;</ref><ref type="bibr" target="#b26">Krause et al., 2008;</ref><ref type="bibr" target="#b25">Krause and Guestrin, 2011;</ref><ref type="bibr" target="#b20">Jegelka and Bilmes, 2011;</ref><ref type="bibr" target="#b19">Iyer and Bilmes, 2013)</ref>), where they have been applied to a variety of prob- lems. In natural language and speech processing, they have been applied to document summariza- tion ( <ref type="bibr" target="#b29">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b30">Lin and Bilmes, 2012)</ref> and speech data selection <ref type="bibr" target="#b44">(Wei et al., 2013)</ref>.</p><p>We are given a finite size-n set of objects V (i.e., |V | = n). A valuation function f : 2 V → R + is de- fined that returns a non-negative real value for any subset X ⊆ V . The function f is said to be submodu- lar if it satisfies the property of diminishing returns: namely, for all X ⊆ Y and v / ∈ Y , we must have:</p><formula xml:id="formula_0">f (X ∪ {v}) − f (X) ≥ f (Y ∪ {v}) − f (Y ). (1)</formula><p>This means that the incremental value (or gain) of element v decreases when the context in which v is considered grows from X to Y ⊇ X. We define the "gain" as f (v|X) f (X ∪ {v}) − f (X). Hence, f is submodular if f (v|X) ≥ f (v|Y ). We note that a function m : 2 V → R + is said to be modular if it satisfies the above with equality, meaning</p><formula xml:id="formula_1">m(v|X) = m(v|Y ) for all X ⊆ Y ⊆ V \ {v}.</formula><p>If m is modular and m( / 0) = 0, it can be written as m(X) = ∑ x∈X m(x) and, moreover, is seen simply as a n-dimensional vector m ∈ R V .</p><p>As an example, suppose we have a set V of balls and f (X) counts the number of colors present in any subset X ⊆ V . In <ref type="figure" target="#fig_0">Figure 1</ref>, |X| = 5 and f (X) = 2, |Y | = 7 and f (Y ) = 3, and X ⊂ Y . Adding v (a blue ball) to X has a unity gain f (v|X) = 1 but since a blue ball exists in Y , we have</p><formula xml:id="formula_2">f (v|Y ) = 0 &lt; f (v|X) = 1.</formula><p>Submodularity is a natural model for data subset selection in SMT. In this case, each v ∈ V is a distinct training data sentence and V corresponds to a training set. An important characteristic of any good model for this problem is that we wish to decrease the "value" of a sentence v ∈ V based on how much that sentence has in common with those sentences, say X, that have already been chosen. The value f (v|X) of a given sentence v in a context of previously chosen sentences X ⊆ V further diminishes as the context grows Y ⊇ X. When, for example, a sentence's value is represented as the value of its set of features (e.g., n-grams), it is natural for those features' values to be discounted based on how much representation of those features already exists in a previously chosen subset. This corresponds to submodularity, which can easily be expressed mathematically by functions such as Eqn. <ref type="formula" target="#formula_11">(4)</ref> below.</p><p>Not only are submodular functions natural for SMT subset selection, they can also be optimized efficiently and scalably such that the result has mathematical performance guarantees. In the re- mainder of this paper we will assume that f is not only submodular, but also non-negative ( f (X) ≥ 0 for all X), and monotone non-decreasing ( f (X) ≤ f (Y ) for all X ⊆ Y ). Such functions are trivial to uselessly maximize, since f (V ) is the largest possi- ble valuation. Typically, however, we wish to have Algorithm 1: The Greedy Algorithm 1 Input: Submodular function f : 2 V → R + , cost vector m, budget b, finite set V .</p><p>2 Output: X k where k is the number of iterations.</p><formula xml:id="formula_3">3 Set X 0 ← / 0 ; i ← 0 ; 4 while m(X i ) &lt; b do 5</formula><p>Choose v i as follows:</p><formula xml:id="formula_4">v i ∈ argmax v∈V \X i f ({v}|X i ) m(v) ; 6 X i+1 ← X i ∪ {v i } ; i ← i + 1 ;</formula><p>a valuable subset of bounded and small cost, where cost is measured based on a modular function m(X). For example, the cost m(v) of a sentence v ∈ V might be its length, so m(X) = ∑ x∈X m(x) is a sum of sentence lengths. This leads to the following optimization problem:</p><formula xml:id="formula_5">X * ∈ argmax X⊆V,m(X)≤b f (X),<label>(2)</label></formula><p>where b is a known budget. Solving this problem exactly is NP-complete <ref type="bibr" target="#b12">(Feige, 1998)</ref>, and express- ing it as an ILP procedure renders it impractical for large data sizes. When f is submodular the cost is just size (m(X) = |X|), then the simple greedy algo- rithm (detailed below) will have a worst-case guar-</p><formula xml:id="formula_6">antee of f ( ˜ X * ) ≥ (1 − 1/e) f (X opt ) ≈ 0.63 f (X opt )</formula><p>where X opt is the optimal and˜Xand˜ and˜X * is the greedy so- lution ( <ref type="bibr" target="#b39">Nemhauser et al., 1978)</ref>.</p><p>This constant factor guarantee has practical im- portance. First, a constant factor guarantee stays the same as n grows, so the relative worst-case qual- ity of the solution is the same for small and for big problem instances. Second, the worst-case result is achieved only by very contrived and unrealistic function instances -the typical case is almost al- ways much better. Third, the worst-case guarantee improves depending on the "curvature" κ ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> of the submodular function <ref type="bibr">(Conforti and Cornuejols, 1984)</ref>. When the submodular function is not fully curved (κ &lt; 1, something true of the func- tions used in this paper), the worst case guarantee is better, namely 1 κ (1−e −κ ) (e.g., a function f with κ = 0.2 has a worst-case guarantee of 0.91). Lastly, when the cost m is not just cardinality but an arbi- trary non-negative modular function, a greedy al- gorithm has similar guarantees <ref type="bibr" target="#b41">(Sviridenko, 2004)</ref>, and a scalable variant has a worst-case guarantee of 1 − 1/ √ e (Lin and Bilmes, 2010). The basic greedy algorithm has a very simple form. Starting with X ← / 0, we repeat the operation</p><formula xml:id="formula_7">X ← X ∪ argmax v∈V \X f (v|X)</formula><p>m(v) until the budget is exceeded (m(X) &gt; b) and then backoff to the previous iteration (complete details are given in Algorithm 1). While the algorithm has complexity O(n 2 ), there is an accelerated instance of this algorithm <ref type="bibr" target="#b34">(Minoux, 1978;</ref><ref type="bibr" target="#b27">Leskovec et al., 2007</ref>) that has empirical computational complexity of O(n log n) where n = |V |. The greedy algorithm, therefore, scales practically to very large n. Recently, still much faster ( <ref type="bibr" target="#b45">Wei et al., 2014</ref>) and also parallel distributed <ref type="bibr" target="#b35">(Mirzasoleiman et al., 2013</ref>) greedy procedures have been advanced offering still better scalability.</p><p>There are many submodular functions that are appropriate for subset selection ( <ref type="bibr" target="#b29">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b30">Lin and Bilmes, 2012</ref>). Some of them are graph-based, where we are given a non-negative weighted graph G = (V, E, w) and w : E → R + is a set of edge weights (i.e., w(x, y) is a non-negative similarity score between sentences x and y). A submodular function is obtained via a graph cut function f (X) = ∑ x∈X,y∈V \X w(x, y) or via a monotone truncated graph cut function</p><formula xml:id="formula_8">f (X) = ∑ v∈V min(C v (X), αC v (V )) where α ∈ (0, 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) is a scalar parameter and</head><formula xml:id="formula_9">C v (X) = ∑ x∈X w(v, x)</formula><p>is a v-specific modular function. Alternatively, the class of facility loca- tion functions f (X) = ∑ v∈V max x∈X w(x, v) have been widely and successfully used in the field of operations research, and are also applicable here.</p><p>In the worse case, the required graph construc- tion has a worst-case complexity of O(n 2 ). While sparse graphs can be used, this can be prohibitive when n = |V | gets large. Another class of sub- modular functions that does not have this prob- lem is based on a weighted bipartite graph G = (V,U, E, w) where V are the left vertices, U are the right vertices, E ⊆ V × U is a set of edges, and w : U → R + is a set of non-negative weights on the vertices U. For X ⊆ V , the bipartite neighborhood function is defined as:</p><formula xml:id="formula_10">f (X) = w({u ∈ U : ∃x ∈ X with (x, u) ∈ E}) (3)</formula><p>This function is interesting for NLP applications since U can be seen as a set of "features" of the ele- ments v ∈ V (i.e., if V is a set of sentences, U can be the collective set of n-grams for multiple values of n, and f (X) is the weight of the n-grams contained collectively in sentences X). 2 Given a set X ⊆ V , we get value from the features of the elements x ∈ X, but we get credit for each feature only one time -once a given object x ∈ X has a given fea- ture u ∈ U, any additions to X by elements also hav- ing feature u offer no further credit via that feature.</p><p>Another interesting class of submodular func- tions, allowing additional credit from an element even when its features already exist in X, are what we call feature-based submodular functions. They involve sums of non-decreasing concave functions applied to modular functions ( <ref type="bibr" target="#b40">Stobbe and Krause, 2010)</ref> and take the following form:</p><formula xml:id="formula_11">f (X) = ∑ u∈U w u φ u (m u (X))<label>(4)</label></formula><p>where</p><formula xml:id="formula_12">w u &gt; 0 is a feature weight, m u (X) = ∑ x∈X m u (x)</formula><p>is a non-negative modular function specific to feature u, m u (x) is a relevance score (a non-negative scalar score indicating the relevance of feature u in object x), and φ u is a u-specific non-negative non-decreasing concave function.</p><formula xml:id="formula_13">The gain is f (v|X) = ∑ u∈U φ (m u (X ∪ {v})) − φ (m u (X))</formula><p>, and thanks to φ u 's concavity, the term φ (m u (X ∪ {v})) − φ (m u (X)) for each feature u ∈ U is decaying as X grows. The rate of decay, and hence the degree of diminishing returns and ultimately the measure of redundancy of the information provided by the feature, is controlled by the concave function. The rate of decay is also related to the curvature κ of the submodular function (c.f. §2), with more aggressive decay having higher curvature (and a worse worst-case guarantee). The decay is a modeling choice that should be decided based on a given application.</p><p>Feature-based functions have the advantage that they do not require the construction of a pairwise graph; they have a cost of only O(n|U|), which is linear in the data size and therefore scalable to large data set sizes.</p><p>We utilize this class for our subset selection ex- periments described in Section 4, where we use one global concave function φ u = φ for all u ∈ U. In this work we chose one particular set of features U. However, given the large body of research into NLP feature engineering ( <ref type="bibr">Jurafsky and Martin, 2009)</ref>, this class is extensible beyond just this set, which makes it suitable for many other NLP applications.</p><p>Before describing our SMT-specific functions in detail, we review previous work on subset selection for SMT in the context of submodularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Previous Approaches</head><p>There have been many previous approaches to data subset selection in SMT. In this section, we show that some of them in fact correspond to submodular methods, thus introducing a connection between submodularity and the practical problem of SMT data selection. The fact that submodularity is implicitly and unintentionally used in previous work suggests that it is natural for this problem.</p><p>A currently widely-used data selection method in SMT (which we also use as a baseline in Section 6) uses the cross-entropy between two language mod- els ( <ref type="bibr" target="#b36">Moore and Lewis, 2010)</ref>, one trained on the test set of interest, and another trained on a large set of generic or out-of-domain training data. We call this the cross-entropy method. This method trains a test-set specific (or in-domain) language model, LM in , and a generic (out-of-or mixed-domain) lan- guage model, LM out . Each sentence x ∈ V in the training data is given a probability score with both language models and then ranked in descending order based on the log ratio</p><formula xml:id="formula_14">m ce (x) = 1 (x) log[Pr(x|LM in )/Pr(x|LM out )] (5)</formula><p>where (x) is the length of sentence x. Finally, the top N sentences are chosen. In <ref type="bibr" target="#b1">(Axelrod et al., 2011</ref>) this method is extended to take both sides of the parallel corpus into account rather than just the source side. The cross-entropy approach values each sentence individually, without regard to any in- teraction with already selected sentences. This ap- proach, therefore, is modular (a special case of sub- modular) and values a set X via m(X) = ∑ x∈X m(x). Moreover, the thresholding method for choosing a subset corresponds exactly to the optimization problem in Eqn. (2) where f ← m and the budget b is set to the sum of the top N sentence scores. Thanks to modularity, the problem is no longer NP- complete, and the threshold method solves Eqn. <ref type="formula" target="#formula_5">(2)</ref> exactly. On the other hand, a modular function does not have the diminishing returns property, and thus has no chance to represent interaction or re- dundancy between sentences. The chosen subset, therefore, might have an enormous overrepresenta- tion of one aspect of the training data while having minimal or no representation of another aspect, a major vulnerability of this approach. Other methods use information retrieval <ref type="bibr" target="#b17">(Hildebrand et al., 2005;</ref><ref type="bibr" target="#b32">Lü et al., 2007</ref>) which can also be described as modular function optimization (e.g., take the top k scoring sentences). Duplicate sentence removal is easily represented by a feature- based submodular function, Equation <ref type="formula" target="#formula_11">(4)</ref>, where there is one sentence-specific feature per sentence and where φ u (m u (X)) = min(|X ∩ {u}|, 1) -once a sentence is chosen, its contribution is saturated so any duplicate sentence has a gain of zero. Also, the unseen n-gram function of <ref type="bibr" target="#b4">Bloodgood and Callison-Burch, 2010</ref></p><note type="other">) corresponds to a bipartite neighborhood submodular function, with a weight function defined based on n-gram counts. Moreover their functions are optimized using the greedy algorithm; hence they in fact have a 1 − 1/e guarantee. Other methods have noted and dealt with the existence of redundancy in phrase-based systems (Ittycheriah and Roukos, 2007) by limiting the set of phrases -submodular optimization inherently removes redundancy. Also, (Callison-Burch et al., 2005; Lopez, 2007) involve modular functions but where selection is over subsets of phrases (rather than sentences as in our current work) and where multiple selections occur, each specific to an individual test set sentence rather than the entire test set.</note><p>In the feature-decay method, presented in <ref type="bibr" target="#b2">Biçici and Yuret, 2011;</ref><ref type="bibr" target="#b3">Biçici, 2013)</ref>, the value of a sentence is based on its decomposition into a set of feature values. As sentences are added to a set, the feature decay approach in general di- minishes the value of each feature depending on how much of that feature has already been covered by those sentences previously chosen -the pa- pers define a set of feature decay functions for this purpose.</p><p>Our analysis of <ref type="bibr" target="#b2">Biçici and Yuret, 2011;</ref><ref type="bibr" target="#b3">Biçici, 2013)</ref>, from the perspective of sub- modularity, has revealed an interesting connection. The feature decay functions used in these papers turn out to be derivatives of non-decreasing con- cave functions. For example, in one case φ (a) = 1/(1 + a) which is the derivative of the concave function φ (a) = ln(1 + a). We are given a constant initialization w u for feature u ∈ U -in the papers, they set either w u ← 1, or w u ← log(m(V )/m u (V )), or w u ← log(m(V )/(1 + m u (V ))), where m(V ) = ∑ u m u (V ), and where m u (X) = ∑ x∈X m u (x) is the count of feature u within the set of sentences X ⊆ V . This yields the submodular feature function f u (X) = w u φ (m u (X)). The value of sentence v as measured by feature u in the context of X is the gain f u (v|X), which is a discrete derivative correspond- ing to w u /(1 + m u <ref type="figure">(X ∪ {v})</ref>). An alternative decay function they define is given as φ (a) = 1/(1 + b a ) for a base b (they set b ← 2) which is the derivative of the following non-decreasing concave function:</p><formula xml:id="formula_15">φ (a) = 1 − 1 ln(b) ln 1 + exp −a ln(b)<label>(6)</label></formula><p>We note that this function is saturating, meaning that it quickly reaches its asymptote at its maxi- mum possible value. We can, once again, define a function specific for feature u ∈ U as f u (X) = w u φ (m u (X)) with a gain f u (v|X) being a discrete derivative corresponding to w u /(1 + b m u (X∪{v}) ).</p><p>The connection between this work and submod- ularity is not complete, however, without consider- ing the method used for optimization. In fact, Algo- rithm 1 of (Biçici and Yuret, 2011) is precisely the accelerated greedy algorithm of <ref type="bibr" target="#b34">(Minoux, 1978)</ref> applied to the submodular function corresponding to f (X) = ∑ u∈U f u (X), and Algorithm 1 of <ref type="bibr" target="#b3">(Biçici, 2013</ref>) is the cost-normalized variant of this greedy algorithm corresponding to a knapsack constraint <ref type="bibr" target="#b41">(Sviridenko, 2004</ref>). Thus, our analysis shows that these methods also have a 1 − 1/e performance guarantee and also the O(n log n) empirical com- plexity mentioned in Section 2. This is an impor- tant connection, as it furthers the evidence that submodularity is natural for the problem of SMT subset selection. This also increases the accessibil- ity of this method since we may view it as a special case of Equation (4).</p><p>Another class of approaches focuses on active learning. In ( <ref type="bibr" target="#b16">Haffari et al., 2009</ref>) a large corpus of noisy parallel data is created automatically; a smaller set of samples is then selected from this set that receive human translations. A combination of several "informativeness" scores is computed on a sentence-level basis, and samples are selected via hierarchical adaptive sampling <ref type="bibr" target="#b9">(Dasgupta and Hsu, 2008</ref>). In ( <ref type="bibr" target="#b33">Mandal et al., 2008</ref>) a measure of disagreement between different MT systems, as well as an entropy-based criterion are used to select additional data for annotation. In <ref type="bibr" target="#b4">(Bloodgood and Callison-Burch, 2010)</ref> and ( <ref type="bibr" target="#b0">Ambati et al., 2010)</ref>, active learning is combined with crowd-sourced an- notations to produce large, human-translated data sets that are as informative as possible. In <ref type="bibr" target="#b7">(Cao and Khudanpur, 2012</ref>), samples are selected for discriminative training of an MT system accord- ing to a greedy algorithm that tries to maximize overall quality. These methods address a differ- ent scenario (data selection for annotation or dis- criminative training) than the one considered here; however, we also note that the actual selection tech- niques employed in these papers do not appear to be submodular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>135</head><p>In this section, we design a parameterized class of submodular functions useful for SMT training data subset selection. By staying within the realm of submodularity, we retain the advantages of the greedy algorithm, its theoretical performance as- surances, and its scalability properties. At the same time this opens the door to a general framework for quickly exploring a much larger class of functions (with the same desirable properties) than before.</p><p>It is important to note that we are using sub- modularity as a "model" of the selection process, and the submodular objective acts as a surrogate for the actual SMT objective function. Thus, the mathematical guarantee we have is in terms of the surrogate objective rather than the true SMT ob- jective. Evaluating one point of the actual SMT objective would require the complete training and testing of an SMT system, so even an algorithm as efficient as Algorithm 1 would be infeasible, even on small data. It is therefore important to design a natural and scalable surrogate objective.</p><p>We do not consider the graph-based functions discussed in Section 2 here since they require a pairwise similarity matrix over all training sen- tences and thus have O(n 2 ) worst-case complexity. For large tasks with millions or even billions of sentences, this eventually becomes impractical. Instead we focus on feature-based functions of the type presented in <ref type="figure">Eqn. (4)</ref>, where each sentence is represented as a set of features rather than as a vertex in a graph. In this function there are four components to specify: 1) U, the linguistic feature set; 2) m u (x), the relevance scores for each feature u and sentence x; 3) w u , the feature weights; and 4) φ , the concave function (we use one concave function, so φ u = φ for all u ∈ U). Feature set: U is the set of n-grams from either the source language U src , or from both the source and target language U src ∪ U tgt (see Section 6); since we are interested in selecting a training set that matches a given test set, we use the set of n- grams that occur both in the training set and in the development/test data (for target features, only development set features are used). I.e., U src = (U src dev ∪U src test ) ∩U src train and U tgt = U tgt dev ∩U tgt train . Relevance scores: A feature u within a sentence x should be valued based on how salient that fea- ture is within the "document" in which it occurs; here, the "document" is the set of training sen- tences. This is a task well suited to TFIDF. As an alternative to raw feature counts we thus also consider scores of the form m u (x) ← tfidf(u, x) = tf(u, x) × idf trn (u), where tf(u, x) and idf trn (u) are defined as usual.</p><p>Feature weights: We wish to select those training samples that contain features occurring frequently in the test data while avoiding the over-selection of features that are very frequent in the training data because those are likely to be translated correctly anyway. This is similar to the approach in <ref type="bibr" target="#b36">(Moore and Lewis, 2010)</ref> (see Equation <ref type="formula">(5)</ref>), where a log-probability ratio of in-domain to out-of-domain language model is utilized. In the present case, we need a value that is specific to feature u ∈ U; a natural approach is to use the ratio of counts c tst (u)/c trn (u) where c tst (u) is the raw count of u in the development/test data, and c trn (u) is its raw count in the training data (note that c trn (u) is never zero due to the way U is defined). As an additional factor we allow feature length to have an influence. In general, longer n-grams might be considered more valuable since they typically lead to better translations and are more relevant for BLEU. Thus, we include a reward term for longer n-grams in the form of β |u| where β ≥ 1 and |u| is the length of feature u. This gives greater weight to longer n-grams when β &gt; 1. Concave function: It is imperative to find the right form of concave function since, as described in Sec- tion 2, the concave shape determines the degree to which redundancy and diminishing returns are rep- resented. Intuitively, when the shape of the concave function for a feature becomes "flat" rapidly, that feature quickly looses its ability to provide addi- tional value to a candidate subset. Many different concave functions were tested for φ , including one of the two functions implicit in <ref type="bibr" target="#b2">(Biçici and Yuret, 2011</ref>) and derived in Section 3, and a variety of roots of the form φ (a) = a α for 0 &lt; α &lt; 1. In <ref type="table">Table 2</ref>, for example, we find evidence that the simple square root φ (a) = √ a performs slightly better than the log function. The square root is much less curved and decays much more gradually than either of the two functions implicit in <ref type="bibr" target="#b2">(Biçici and Yuret, 2011</ref>), of which one is a log form and the other is even more curved and quickly satu- rates (see §3). The square root function yields a less curved submodular function, in the sense of <ref type="bibr">(Conforti and Cornuejols, 1984)</ref>, resulting in better worst-case guarantees. Indeed, <ref type="table">Table 1</ref> in <ref type="bibr" target="#b2">(Biçici and Yuret, 2011</ref>) corroborates by showing that the more curved saturating function does worse than the less curved log function. Four Components Together: Different instantia-tions of the four components discussed above will result in different submodular functions of the gen- eral class defined in Eqn. (4). Particular settings of these general parameters produce the methods considered in <ref type="bibr" target="#b2">(Biçici and Yuret, 2011)</ref>, thus mak- ing that approach easily accessible once the general submodular framework is set up. As a very special case, this is also true of the cross-entropy method <ref type="bibr" target="#b36">(Moore and Lewis, 2010)</ref>, where |U| = 1, m u (x) ← exp(m ce (x)) of Equation <ref type="formula">(5)</ref> 3 , w u ← 1, and φ (a) = a is the identity function. In Section 6, we specify the parameter settings used in our experiments. <ref type="table">Task  Train  Dev  Test  LM  NIST  189M  48k  49k 2</ref>.5B Europarl 52.8M 57.7k 58.1k 53M <ref type="table">Table 1</ref>: Data set sizes (number of source-side words) for MT tasks. LM = language model data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data and Systems</head><p>We evaluate our approach on the NIST Arabic- English translation task, using the NIST 2006 set for development and the NIST 2009 set for eval- uation. The training data consists of all Modern Standard Arabic-English parallel LDC corpora per- mitted in the NIST evaluations (minus the restricted time periods). Together these sets form a mixed- domain training set containing relevant in-domain data similar to the NIST data sets but also less rele- vant data (e.g., the UN parallel corpora); we thus expect data selection to work well on this task. Ad- ditional English language modeling data was drawn from several other LDC corpora (English Giga- word, AQUAINT, HARD, ANC/DCI and the Amer- ican National Corpus). Preprocessing included con- version of the Arabic data to Buckwalter format, tokenization, spelling normalization, and morpho- logical segmentation using MADA ( <ref type="bibr" target="#b15">Habash et al., 2009</ref>). Numbers and URLs were replaced with variables. The English data was tokenized and lowercased. Postprocessing involved recasing the translation output, replacing variable names with their original corresponding tokens, and normal- izing spelling and stray punctuation marks. The recasing model is an SMT system without reorder- ing, trained on parallel cased and lowercased ver- sions of the training data. The recasing model re- mains fixed for all experiments and is not retrained for different sizes of the training data. Evalua- tion follows the NIST guidelines and was done by computing BLEU scores using the official NIST evaluation tool mteval-v13a.pl with the −c flag for case-sensitive scoring. In addition to the NIST task we also applied our method to the Europarl German-English translation task. The training data comes from the Europarl-v7 collection <ref type="bibr">4</ref> ; the devel- opment set is the 2006 dev set, and the test set is the 2007 test set. The number of reference translations is 1. The German data was preprocessed by tok- enization, lower-casing, splitting noun compounds and lemmatization to address morphological vari- ation in German. The English side was tokenized and lowercased. Evaluation was done by comput- ing BLEU on the lowercased versions of the data. Since test and training data for this task come from largely the same domain we expect the training data to be less redundant or irrelevant; nevertheless it will be interesting to see how much different data selection methods can contribute even to in-domain translation tasks. The sizes of the various data sets are shown in <ref type="table">Table 1</ref>.</p><p>All translation systems were trained using the GIZA++/Moses infrastructure ( <ref type="bibr" target="#b22">Koehn et al., 2007</ref>). The translation model is a standard phrase-based model with a maximum phrase length of 7. Since a large number of experiments had to be run for this study, more complex hierarchical or syntax-based translation models were deliberately excluded in order to limit the experimental turn-around time needed for each experiment. The reordering model is a hierarchical model according to <ref type="bibr" target="#b14">(Galley and Manning, 2008)</ref>. The feature weights in the log- linear function were optimized on the development set BLEU score using minimum error-rate training. The language models for the NIST task (5-grams) were trained on three different data sources (Gi- gaword, GALE data, and all remaining corpora), which were then interpolated into a single model. The interpolation weights were optimized sepa- rately for the two different genres present in the NIST task (newswire and web text). All models used Witten-Bell discounting and interpolation of higher-order and lower-order models. Language models remain fixed for all experiments, i.e., the language model training data is not subselected since we were interested in the effect of data subset selection on the translation model only. The lan- guage model for the Europarl system was a 5-gram trained on Europarl data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Data Subset Sizes 10% 20% 30% 40% Rand 0.3991 (± 0.004) 0.4142 (± 0.003) 0.4205 (± 0.002) 0.4220 (± 0.002) Xent 0.4235 (± 0.004) 0.4292 (± 0.002) 0.4290 (± 0.003) 0.4292 (± 0.001) SM-1 0.4309 (± 0.000) 0.4367 (± 0.001) 0.4330 (± 0.004) 0.4351 (± 0.002) SM-2 0.4330 * (± 0.001) 0.4395 * (± 0.003) 0.4333 (± 0.001) 0.4366 * (± 0.003) SM-3 0.4313 * (± 0.002) 0.4338 (± 0.002) 0.4361 * (± 0.002) 0.4351 (± 0.003) SM-4 0.4276 (± 0.003) 0.4303 (± 0.002) 0.4324 (± 0.002) 0.4329 (± 0.000) SM-5 0.4285 (± 0.004) 0.4356 (± 0.002) 0.4333 (± 0.003) 0.4324 (± 0.002) SM-6 0.4302 * (± 0.004) 0.4334 (± 0.003) 0.4371 * (± 0.002) 0.4349 (± 0.003) SM-7 0.4295 (± 0.002) 0.4374 (± 0.002) 0.4344 (± 0.001) 0.4314 (± 0.0004) SM-8 0.4304 * (± 0.002) 0.4323 (± 0.000) 0.4358 (± 0.003) 0.4337 (± 0.001) 100%</p><p>0.4257 <ref type="table">Table 2</ref>: BLEU scores (standard deviations) on the NIST 2009 (Ara-En) test set for random (Rand), cross-entropy (Xent), and submodular (SM) data selection methods defined in <ref type="table">Table 4</ref>. 100% = system using all of the training data. Boldface numbers indicate a statistically significant improvement (p ≤ 0.05) over the median Xent system. Starred scores are also significantly better than SM-5.  <ref type="table">Table 3</ref>: BLEU scores (standard deviation) on the Europarl translation task for random (Rand), cross- entropy (Xent), and submodular (SM) data selection methods. 100% = system using all of the training data. Boldface numbers indicate a statistically significant improvement (p ≤ 0.05) over the median Xent system. Starred scores are significantly better than SM-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Function parameters <ref type="table">Table 4</ref>: Different instantiations of the general sub- modular function in Eq. 4 (β = 1.5 in all cases).</p><formula xml:id="formula_16">w(u) φ (a) m u (x) U SM-1 c tst (u) c trn (u) β |u| √ a tfidf(u,x) U src SM-2 c tst (u) c trn (u) β |u| √ a tfidf(u,x) U src ∪U tgt SM-3 c tst (u) c trn (u) β |u| √ a c(u,x) U src SM-4 c tst (u) √ a tfidf(u,x) U src SM-5 1 ln(1 + a) c(u,x) U src SM-6 c tst (u) c trn (u) √ a tfidf(u,x) U src SM-7 c tst (u) c trn (u) (a) tfidf(u,x) U src ∪U tgt SM-8 c tst (u) c trn (u) ln(1 + a) tfidf(u,x) U src ∪U tgt</formula><p>We first trained a baseline system on 100% of the training data. Different data selection methods were then used to select subsets of 10%, 20%, 30% and 40% of the data. While not reported in the tables, above 40%, the performance slowly drops to the 100% performance.</p><p>The first baseline selection method utilizes ran- dom data selection, for which 3 different data sets of the specified size were drawn randomly from the training data. Individual systems were trained on all random subsets of the same size, and their scores were averaged. The second baseline is the cross-entropy method by <ref type="bibr" target="#b36">(Moore and Lewis, 2010)</ref>. In-domain language models were trained on the combined development and test data, and out- of-domain models were trained on an equivalent amount of data drawn randomly from the training set. Sentences were ranked by the function in Eq. 5, and the top k percent were chosen. The order of the n-gram models was optimized on the development set and was found to be 3. Larger model orders resulted in worse performance, possibly due to the limited size of the data used for their training. Since this method also involves random data selection, we report the average BLEU score over 5 different trials. For the submodular selection method, <ref type="table">Ta- ble 4</ref> shows the different values that were tested for the four components listed in Section 4. The combi- nation was optimized on the development set. The selection algorithm (Alg. 1) runs within a few min- utes on our complete training set of 189M words.</p><p>Results on the NIST 2009 test set are shown in <ref type="table">Table 2</ref>. The scores for the submodular systems are averages over 3 different runs of MERT tuning. Random data subset selection (Row 1) falls short of the baseline system using 100% of the training data. The cross-entropy method (Row 2) surpasses the performance of the baseline system at about 20% of the data, demonstrating that data subset selection is a suitable technique for such mixed- domain translation tasks. The following rows show results for the various submodular functions shown in <ref type="table">Table 4</ref>. Out of these, SM-5 corresponds to the best approach in ( <ref type="bibr" target="#b2">Biçici and Yuret, 2011</ref>). SM-6 is our own best-performing function, beating the cross-entropy method by a statistically significant margin (p ≤ 0.05) under all conditions. 5 SM-6 is also significantly better than SM-5 in two cases. Finally, it surpasses the performance of the all-data system at only 10% of the training data; possibly, even smaller training data sets could be used but this option was not investigated. While the bilingual submodular functions SM-2 and SM-7) yield an improvement of up to 0.015 BLEU points on the dev set (not shown in the table), they do not consistently outperform the monolingual functions on the test set. Since test set target features cannot be used in our scenario, bilingual features are only helpful to the extent that the development set closely matches the test set. However, target fea- tures should be quite helpful when selecting data from an out-of-domain set to match an in-domain training set (as in e.g. <ref type="figure" target="#fig_0">(Axelrod et al., 2011)</ref>). We found no gain from the length reward β |u| .</p><p>The Europarl results <ref type="table">(Table 3)</ref> show a similar pattern. Although the differences in BLEU scores are smaller overall (as expected on an in-domain translation task), data subset selection improves over the all-data baseline system in this case as well. The cross-entropy method again outperforms random data selection. On this task we only tested our submodular function that worked best on the <ref type="bibr">5</ref> Statistical significance was measured using the paired bootstrap resampling test of <ref type="bibr" target="#b23">(Koehn, 2004</ref>), applied to the systems with the median BLEU scores. NIST task; again we find that it outperforms the cross-entropy method. In two conditions (10% and 30%) these differences are statistically significant. 10% of the training data suffices to outperform the all-data system, and up to a full BLEU point can be gained on this task using 20-30% of the data and a submodular data selection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have introduced submodularity to SMT data subset selection, generalizing previous approaches to this problem. Our method has theoretical perfor- mance guarantees, comes with scalable algorithms, and significantly improves over current, widely- used data selection methods on two different trans- lation tasks. There are many possible extensions to this work. One strategy would be to extend the feature set U with features representing different types of linguistic information -e.g., when using a syntax-based system it might be advantageous to select training data that covers the set of syn- tactic structures seen in the test data. Secondly, the selected data was test data specific. In some contexts, it is not possible to train test data spe- cific systems dynamically; in that case, different submodular functions could be designed to select a representative "summary" of the training data. Finally, the use of submodular functions for subset selection is applicable to other data sets that can be represented as features or as a pairwise similar- ity graph. Submodularity thus can be applied to a wide range of problems in NLP beyond machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: f (Y ) measures the number of distinct colors in the set of balls Y , and hence is submodular.</figDesc></figure>

			<note place="foot" n="1"> As far as we know, submodularity has not before been explicitly utilized for SMT subset selection.</note>

			<note place="foot" n="2"> To be consistent with standard notation in previous literature, we overload the use of n in &quot;n-grams&quot; and the size of our set &quot;n = |V |&quot;, even though the two ns have no relationship with each other.</note>

			<note place="foot" n="3"> Due to modularity, any monotone increasing transformation from m ce (x) to m u (x) that ensures m u (x) ≥ 0 is equivalent.</note>

			<note place="foot" n="4"> http://http://www.statmt.org/europarl/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based on research sponsored by Intelligence Advanced Research Projects Activity (IARPA) under agreement number FA8650-12-2-7263, and is also supported by the National Science Foundation under Grant No. (IIS-1162606), and by a Google, a Microsoft, and an Intel research award. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of In-telligence Advanced Research Projects Activity (IARPA) or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning and crowd-sourcing for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [ambati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2169" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Axelrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instance selection for machine translation using feature decay algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Biçici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">E</forename><surname>Yuret2011</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Biçici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Statistical Machine Translation</title>
		<meeting>the 6th Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature decay algorithms for fast deployment of accurate statistical machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Biçici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Statistical Machine Translation</title>
		<meeting>the 8th Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="78" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bucking the trend: large-scale cost-focused active learning for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Biçici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; M</forename><surname>Bloodgood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="854" to="864" />
		</imprint>
		<respStmt>
			<orgName>KOÇ University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>The Regression Model of Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chris Callison-Burch</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling phrase-based statistical machine translation to larger corpora and longer phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sample selection for large-scale MT discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">Y</forename><surname>Khudanpur2012</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Submodular set functions, matroids and the greedy algorithm: tight worst-case bounds and some generalizations of the Rado-Edmonds theorem</title>
	</analytic>
	<monogr>
		<title level="m">Cornuejols</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="251" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical sampling for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">S</forename><surname>Hsu2008</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low cost portability for statistical machine translation based on n-gram frequency and tf-idf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Machine Translation Summit X</title>
		<meeting>the 10th Machine Translation Summit X</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Combinatorial Structures and their Applications, chapter Submodular functions, matroids and certain polyhedra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Gordon and Breach</publisher>
			<biblScope unit="page" from="69" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A threshold of ln n for approximating set cover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Feige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="634" to="652" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Submodular functions and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujishige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annals of Discrete Mathematics</title>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple and effective hierarchical phrase reordering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">M</forename><surname>Manning2008</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MEDAR conference</title>
		<meeting>the MEDAR conference</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active learning for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT</title>
		<meeting>HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptation of the translation model for statistical machine translation based on information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hildebrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EAMT</title>
		<meeting>EAMT</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Direct translation model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">5764</biblScope>
		</imprint>
	</monogr>
	<note>Ittycheriah and Roukos2007</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Submodular optimization with submodular cover and submodular knapsack constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilmes2013] R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Society (NIPS)</title>
		<meeting><address><addrLine>Lake Tahoe, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Submodularity beyond submodular energies: coupling edges in graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Bilmes2011</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">D</forename><surname>Martin2009</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
	<note>Martin. 2nd edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">V</forename><surname>Zabih2004</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="159" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Submodularity and its applications in optimized information gathering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">A</forename><surname>Guestrin2011</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust submodular observation selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2761" to="2801" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cost-effective outbreak detection in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilmes2010] H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2761" to="2801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilmes2011] H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning mixtures of submodular shells with application to document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilmes2012] H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artifical Intelligence (UAI)</title>
		<meeting><address><addrLine>Catalina Island, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical phrasebased translation with suffix arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="976" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation performance by training data selection and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient data selection for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Spoken Language Technology Workshop</title>
		<meeting>the Spoken Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="261" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerated greedy algorithms for maximizing submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Control and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="234" to="243" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed submodular maximization: Identifying representative elements in massive data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirzasoleiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis2010] R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PAC-learning bounded tree-width graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilmes2004] M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence: Proceedings of the Twentieth Conference (UAI-2004)</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Submodular functions and electrical networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nemhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient minimization of decomposable submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krause2010] P</forename><surname>Stobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A note on maximizing a submodular set function subject to a knapsack constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sviridenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="43" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to translate: A statistical and computational analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Artificial Intellligence</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning machine translation from in-domain and out-of-domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EAMT</title>
		<meeting>EAMT<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using document summarization techniques for speech data subset selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<publisher>Georgia</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="721" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast multi-stage submodular maximization</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
