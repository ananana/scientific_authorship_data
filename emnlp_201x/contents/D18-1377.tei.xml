<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological Univerisity</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="middle">Shad</forename><surname>Akhtar</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Patna</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological Univerisity</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Patna</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3402" to="3411"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3402</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sentiment analysis has immense implications in modern businesses through user-feedback mining. Large product-based enterprises like Samsung and Apple make crucial business decisions based on the large quantity of user reviews and suggestions available in different e-commerce websites and social media platforms like Amazon and Facebook. Sentiment analysis caters to these needs by summarizing user sentiment behind a particular object. In this paper, we present a novel approach of incorporating the neighboring aspects related information into the sentiment classification of the target aspect using memory networks. Our method outperforms the state of the art by 1.6% on average in two distinct domains.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis plays a huge role in user- feedback extraction from different popular e- commerce websites like Amazon, eBay, etc. Large enterprises are not only interested in the overall user sentiment about a given product, but the sen- timent behind the finer aspects of a product is also very important to them. Companies allocate their resources to research, development, and marketing based on these factors. Aspect-based sentiment analysis (ABSA) caters to these needs.</p><p>Users tend to express their opinion on differ- ent aspects of a given product. For example, the sentence "Everything is so easy to use, Mac soft- ware is just so much simpler than Microsoft soft- ware." expresses sentiment behind three aspects: "use", "Mac software", and "Microsoft software" to be positive, positive, and negative respectively. This leads to two tasks to be solved: aspect extrac- tion ( <ref type="bibr" target="#b9">Shu et al., 2017</ref>) and aspect sentiment polar- ity detection ( <ref type="bibr" target="#b14">Wang et al., 2016)</ref>. In this paper, we tackle the latter problem by modeling the relation among different aspects in a sentence.</p><p>Recent works on ABSA does not consider the neighboring aspects in a sentence during classifi- cation. For instance, in the sentence "The menu is very limited -I think we counted 4 or 5 entries.", the sub-sentence "I think ... entries" does not re- flect the true sentiment behind containing aspect "entries", unless the other aspect "menu" is con- sidered. Here, the negative sentiment of "menu" induces "entries" to have the same sentiment. We hypothesize that our architecture iteratively mod- els the influence from the other aspects to generate accurate target aspect representation.</p><p>In sentences containing multiple aspects, the main challenge an Aspect-Based-Sentiment- Analysis (ABSA) classifier faces is to correctly connect an aspect to the corresponding sentiment- bearing phrase (typically adjective).</p><p>Let us consider this sentence "Coffee is a better deal than overpriced cosi sandwiches". Here, we find two aspects: "coffee" and "cosi sandwiches". It is clear in this sentence that the sentiment of "cof- fee" is expressed by the sentimentally charged word "better"; on the other hand, "overpriced" carries the sentiment of "cosi sandwiches". The aim of the ABSA classifier is to learn these con- nections between the aspects and their sentiment bearing phrases.</p><p>In this work, we argue that during sentiment prediction of an aspect (say "coffee" in this case), the knowledge of the existence and representation of the other aspects ("cosi sandwiches") in the sentence is beneficial. The sentiment of an aspect in a sentence can influence the succeeding aspects due to the presence of conjunctions. In particular, for sentences containing conjunctions like and, not only, also, but, however, though, etc., aspects tend to share their sentiments. In the sentence "Food is usually very good, though I wonder about fresh- ness of raw vegetables", the aspect "raw vegeta- bles" does not have any trivial sentiment marker linked to it. However, the positive sentiment of "food", due to the word ""good", and presence of conjunction "though" determines the sentiment of "raw vegetables" to be negative. Thus, aspects when arranged as a sequence, reveal high correla- tion and interplay of sentiments.</p><p>To model these scenarios, firstly, following <ref type="bibr" target="#b14">Wang et al. (2016)</ref>, we independently generate aspect-aware sentence representations for all the aspects using gated recurrent unit (GRU) ( <ref type="bibr" target="#b0">Chung et al., 2014</ref>) and attention mechanism ( <ref type="bibr" target="#b5">Luong et al., 2015)</ref>. Then, we employ memory net- works ( <ref type="bibr" target="#b10">Sukhbaatar et al., 2015</ref>) to repeatedly match the target aspect representation with the other aspects to generate more accurate represen- tation of the target aspect. This refined repre- sentation is fed to a softmax classifier for final classification. We empirically show below that our method outperforms the current state of the art ( <ref type="bibr" target="#b6">Ma et al., 2017</ref>) by 1.6% on average in two distinct domains: restaurant and laptop.</p><p>The rest of the paper structured as follows: Sec- tion 2 discusses previous works; Section 3 delves into the method we present; Section 4 mentions the dataset, baselines, and experimental settings; Section 5 presents and analyzes the results; finally, Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Sentiment analysis is becoming increasingly im- portant due to the rise of the need to process textual data in wikis, micro-blogs, and other so- cial media platforms. Sentiment analysis requires solving several related NLP problems, like aspect extraction ( <ref type="bibr" target="#b8">Poria et al., 2016)</ref>. Aspect based sen- timent analysis (ABSA) is a key task of sentiment analysis which focuses on classifying sentiment of each aspect in the sentences.</p><p>In this paper, we focus on ABSA, which is a key task of sentiment analysis that aims to classify sen- timent of each aspect individually in a sentence. In recent days, thanks to the increasing progress of deep neural network research ( <ref type="bibr" target="#b17">Young et al., 2018)</ref>, novel frameworks have been proposed, achieving notable performance improvement in aspect-based sentiment analysis.</p><p>The common way of doing ABSA is feeding the aspect-aware sentence representation to the neural network for classification. This was first proposed by <ref type="bibr" target="#b14">Wang et al. (2016)</ref> where they appended as- pect embeddings with the each word embeddings of the sentence to generate aspect-aware sentence representation. This representation was further fed to an attention layer followed by softmax for final classification. More recently, <ref type="bibr" target="#b6">Ma et al. (2017)</ref> proposed a model where both context and aspect representa- tions interact with each other's attention mecha- nism to generate the overall representation. <ref type="bibr" target="#b13">Tay et al. (2017)</ref> proposed word-aspect associations using circular correlation as an improvement over <ref type="bibr" target="#b14">Wang et al. (2016)</ref>'s work. Also, <ref type="bibr" target="#b4">Li et al. (2018)</ref> used transformer networks for target-oriented sen- timent classification.</p><p>ABSA has also been researched from a question-answering perspective where deep mem- ory networks have played a major role ( <ref type="bibr" target="#b12">Tang et al., 2016b;</ref>. However, unlike our pro- posed method, none of these methods have tried to model the inter-aspect relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we formalize the task and present our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Input We are given a sentence S = [w 1 , w 2 , . . . , w L ], where w i are the words and L is the maximum number of words in a sentence. Also, the given aspect-terms for sentence S are A 1 , A 2 , . . . , A M , where</p><formula xml:id="formula_0">A i = [w k , . . . , w k+m−1 ], 1 ≤ k ≤ L, 0 &lt; m ≤ L − k + 1,</formula><p>and M is the maximum number of aspects in a sentence.</p><p>Output Sentiment polarity (1 for positive, 0 for negative, and 2 for neutral) for each aspect-term A i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>The primary distinction between our model and the literature is the consideration of the neighbor- ing aspects in a sentence with the target aspect. We assume that our inter-aspect relation modeling (IARM) architecture 1 models the relation between the target aspect and surrounding aspects, while filtering out irrelevant information. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overview</head><p>Our IARM model can be summarized with the fol- lowing steps:</p><p>Input Representation We replace the words in the input sentences and aspect-terms with pre-trained Glove word embeddings ( <ref type="bibr" target="#b7">Pennington et al., 2014</ref>). For multi-worded aspect-terms, we take the mean of constituent word embeddings as aspect representation.</p><p>Aspect-Aware Sentence Representation Fol- lowing <ref type="bibr" target="#b14">Wang et al. (2016)</ref>, all the words in a sen- tence are concatenated with the given aspect repre- sentation. These modified sequence of words are fed to a gated recurrent unit (GRU) 2 for context propagation, followed by an attention layer to ob- tain the aspect-aware sentence representation; we obtain for all the aspects in a sentence.</p><p>Inter-Aspect Dependency Modeling We em- ploy memory network ( <ref type="bibr" target="#b10">Sukhbaatar et al., 2015)</ref> to model the dependency of the target aspect with the other aspects in the sentence. This is achieved through matching target-aspect-aware sentence representation with aspect-aware sen- tence representation of the other aspects. After a certain number of iterations of the memory net- work, we obtain a refined representation of the sentence that is relevant to the sentiment classifi- cation of the target aspect. Further, this represen- tation is passed to a softmax layer for final classi- fication. The following subsections discuss these steps in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Input Representation</head><p>The words (w i ) in the sentences are represented with 300 (D) dimensional Glove word embed- dings ( <ref type="bibr" target="#b7">Pennington et al., 2014</ref>), resulting sentence S ∈ R L×D .</p><p>Similarly, aspect terms are represented with word embeddings. Multi-worded aspect terms are averaged over the constituent words. This results aspect representation a i ∈ R D for i th aspect term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Aspect-Aware Sentence Representation</head><p>It would be fair to assume that not all the words in a sentence carry sentimental information of a par- ticular aspect (e.g., stop words have no impact). This warrants a sentence representation that re- flects the sentiment of the given aspect. To achieve</p><formula xml:id="formula_1">S a i = [w 1 ⊕ a i , w 2 ⊕ a i , . . . , w L ⊕ a i ] ∈ R L×2D .</formula><p>(1)</p><p>In order to propagate the context information within the sentence, we feed S a i to a Gated Recur- rent Unit (GRU) with output size D s (kindly refer to <ref type="table">Table 1</ref> for the value). We denote this GRU as GRU s . GRU . is described as follows:</p><formula xml:id="formula_2">z = σ(x t U z . + s t−1 W z . ), (2) r = σ(x t U r . + s t−1 W r . ),<label>(3)</label></formula><formula xml:id="formula_3">h t = tanh(x t U h . + (s t−1 * r)W h . ), (4) s t = (1 − z) * h t + z * s t−1 ,<label>(5)</label></formula><p>where h t and s t are the hidden outputs and the cell states respectively at time t. We obtain</p><formula xml:id="formula_4">R a i = GRU s (S a i ),</formula><p>where R a i ∈ R L×Ds and the GRU s has the following parameters:</p><formula xml:id="formula_5">U z s ∈ R 2D×Ds , W z s ∈ R Ds×Ds , U r s ∈ R 2D×Ds , W r s ∈ R Ds×Ds , U h s ∈ R 2D×Ds , W h s ∈ R Ds×Ds .</formula><p>To amplify the sentimentally relevant words to aspect a i , we employ an attention layer to obtain the aspect-aware sentence representation (it is ef- fectively a refined aspect representation) r a i :</p><formula xml:id="formula_6">z = R a i W s + b s , (6) α = softmax(z),<label>(7)</label></formula><formula xml:id="formula_7">r a i = α T R a i ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">z = [z 1 , z 2 , . . . , z L ] ∈ R L×1 , softmax(x) = [e x 1 ∑ j e x j , e x 2 ∑ j e x j , . . . ], α = [α 1 , α 2 , . . . , α L ] ∈ R L×1 , r a i ∈ R Ds , W s ∈ R Ds×1</formula><p>, and b s is a scalar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Inter-Aspect Dependency Modeling</head><p>We feed R = [r a 1 , r a 2 , . . . , r a M ] ∈ R M ×Ds to a GRU (GRU a ) of size D o (kindly refer to <ref type="table">Table 1</ref> for the value) to propagate aspect information among the aspect-aware sentence representations and obtain Q = GRU a (R), where Q ∈ R M ×Do and GRU a has the following parameters:</p><formula xml:id="formula_9">U z a ∈ R Ds×Do , W z a ∈ R Do×Do , U r a ∈ R Ds×Do , W r a ∈ R Do×Do , U h a ∈ R Ds×Do , W h a ∈ R Do×Do .</formula><p>This par- tially helps to model the dependency among as- pects in a sentence.</p><p>After this, in order to further inter-aspect de- pendency modeling, we employ memory net- works ( <ref type="bibr" target="#b10">Sukhbaatar et al., 2015)</ref>, where the target- aspect representation (target-aspect-aware senti- ment representation) r at is supplied as the query.  r at is transformed into internal query state (q) with a fully connected layer as</p><formula xml:id="formula_10">AASR S a 1 AASR S a 2 ∑ GRU m GRU a Q Q′ Q (τ+1) = Q′ (τ) q (τ+1) = q (τ) + o Classification Negative Neutral</formula><formula xml:id="formula_11">q = tanh(r at W T + b T ),<label>(9)</label></formula><p>where q ∈ R Do , W T ∈ R Ds×Do , and b T ∈ R Do .</p><p>Input Memory Representation All the aspects in the sentence are stored in memory. Each aspect is represented by their corresponding aspect-aware sentence representation in Q. An attention mecha- nism is used to read these memories from Q (We- ston et al., 2014). We compute the match between the query q and the memory slots in Q with inner product:</p><formula xml:id="formula_12">z = qQ T ,<label>(10)</label></formula><formula xml:id="formula_13">β = softmax(z),<label>(11)</label></formula><p>where</p><formula xml:id="formula_14">z = [z 1 , z 2 , . . . , z M ] ∈ R M ×1 , β = [β 1 , β 2 , . . . , β M ] ∈ R M ×1 .</formula><p>Here, β i is the measure of relatedness between target aspect and aspect i i.e., the attention score.</p><p>Output Memory Representation We choose the output memory vectors (Q ′ ) to be a refined ver- sion of the input memory vectors (Q), obtained by applying a GRU of size</p><formula xml:id="formula_15">D o (named GRU m ) on Q.</formula><p>Hence,</p><formula xml:id="formula_16">Q ′ = GRU m (Q),<label>(12)</label></formula><p>where GRU m has the following parameters:</p><formula xml:id="formula_17">U z m ∈ R Do×Do , W z m ∈ R Do×Do , U r m ∈ R Do×Do , W r m ∈ R Do×Do , U h m ∈ R Do×Do , W h m ∈ R Do×Do .</formula><p>The response vector o is obtained by summing output vectors in Q ′ , weighted by the relatedness measures in β:</p><formula xml:id="formula_18">o = β T Q ′ ,<label>(13)</label></formula><p>where o ∈ R Do .</p><p>Final Classification (Single Hop) In the case of single hop, target aspect representation q is added with memory output o to generate refined target aspect representative. This sum is passed to a soft- max classifier of size C (C = 3 due to the classes of sentiment polarity):</p><formula xml:id="formula_19">P = softmax((q + o)W smax + b smax ),<label>(14)</label></formula><formula xml:id="formula_20">ˆ y = argmax i (P[i]),<label>(15)</label></formula><p>where W smax ∈ R Do×C , b smax ∈ R C , andˆyandˆ andˆy is the estimated sentiment polarity (0 for negative, 1 for positive, and 2 for neutral).</p><p>Multiple Hops We use total H (kindly refer to <ref type="table">Table 1</ref> for the value) number of hops in our model. Each hop generates a finer aspect repre- sentation q. Hence, we formulate the hops in the following way:</p><p>• Query (q) at the end of hop τ is updated as</p><formula xml:id="formula_21">q (τ +1) = q (τ ) + o.<label>(16)</label></formula><p>• Output memory vectors of hop τ , Q ′(τ ) , is updated as the input memory vectors of hop τ + 1:</p><formula xml:id="formula_22">Q (τ +1) = Q ′(τ ) .<label>(17)</label></formula><p>After H hops, q (H) becomes the target-aspect- aware sentence representation vector for the final classification:</p><formula xml:id="formula_23">P = softmax(q (H+1) W smax + b smax ),<label>(18)</label></formula><formula xml:id="formula_24">ˆ y = argmax i (P[i]),<label>(19)</label></formula><p>where W smax ∈ R Do×C , b smax ∈ R C , andˆyandˆ andˆy is the estimated sentiment polarity (0 for negative, 1 for positive, and 2 for neutral). The whole algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We train the network for 30 epochs using categori- cal cross entropy with L2-regularizer as loss func- tion (L):</p><formula xml:id="formula_25">L = − 1 N N i=1 C−1 k=0 y ik log P[k] + λ θ 2 ,<label>(20)</label></formula><p>where N is the number of samples, i is the sample index, k is the class value, λ is the regularization weight (we set it to 10 −4 ),</p><formula xml:id="formula_26">y ik = 1, if expected class value of sample i is k, 0, otherwise,<label>(21)</label></formula><p>and θ is the set of parameters to be trained, where</p><formula xml:id="formula_27">θ = {U z {s,a,m} , W z {s,a,m} , U r {s,a,m} , W r {s,a,m} , U h {s,a,m} , W h {s,a,m} , W s , b s , W T , b T , W smax , b smax }.</formula><p>As optimization algorithm, Stochastic Gradient Descent (SGD)-based ADAM algorithm ( <ref type="bibr" target="#b2">Kingma and Ba, 2014</ref>) is used with learning-rate 0.001 due to its parameter-wise adaptive learning scheme.</p><p>Hyper-Parameters We employed grid-search to obtain the best hyper-parameter values. <ref type="table">Table 1</ref> shows the best choice of these values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 IARM algorithm</head><p>1: procedure TRAINANDTESTMODEL(U , V ) U = train set, V = test set 2: Aspect-aware sentence representation extrac- tion: 3:</p><p>for i: <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we discuss the dataset used and dif- ferent experimental settings devised for the evalu- ation of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Details</head><p>We evaluate our model with SemEval-2014 ABSA dataset <ref type="bibr">3</ref> . It contains samples from two different domains: Restaurant and Laptop. <ref type="table">Table 2</ref> shows the distribution of these samples by class labels. Also, <ref type="table" target="#tab_4">Table 3</ref> shows the count of the samples with single aspect sentence and multi-aspect sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We compare our method against the following baseline methods:</p><p>LSTM Following <ref type="bibr" target="#b14">Wang et al. (2016)</ref>, the sen- tence is fed to a long short-term memory (LSTM) network to propagate context among the con- stituent words. The mean of all the hidden out- puts from the LSTM is taken as the sentence rep- resentation, which is fed to a softmax classifier. Aspect-terms have no participation in the classifi- cation process. ATAE-LSTM Following <ref type="bibr" target="#b14">Wang et al. (2016)</ref>, ATAE-LSTM is identical to AE-LSTM, except the LSTM is fed with the concatenation of aspect-term representation and word representation.</p><p>IAN Following <ref type="bibr" target="#b6">Ma et al. (2017)</ref>, target-aspect and its context are sent to two distinct LSTMs and the means of the hidden outputs are taken as inter- mediate aspect representation and context repre- sentation respectively. Attention scores are gen- erated from the hidden outputs of both LSTMs which is used to generate final aspect and con- text representation. The concatenation of these two vectors are sent to a softmax classifier for final classification.</p><p>Hyper  <ref type="table" target="#tab_3">Neutral  Train Test Train Test Train Test  Restaurant 2164 728  805  196  633  196  Laptop  987  341  866  128  460  169   Table 2</ref>: Distribution of the samples by class labels in SemEval 2014 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Settings</head><p>In order to draw a comprehensive comparison be- tween our IARM model and the baseline methods, we performed the following experiments:</p><p>Overall Comparison IARM is compared with the baseline methods for both of the domains.</p><p>Single Aspect and Multi Aspect Scenarios In this setup, samples with single aspect and multi aspect sentences are tested independently on the trained model. For IAN, we ran our own experi- ments for this scenario.</p><p>Cross-Domain Evaluation Here, the model trained for restaurant domain is tested with the test set for laptop domain and vice versa. For IAN, we ran our own experiments for this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We discuss the results of different experiments be- low:</p><p>Overall Comparison We present the overall performance of our model against the baseline methods in <ref type="table" target="#tab_5">Table 4</ref>. It is evident from the results that our IARM model outperforms all the baseline models, in- cluding the state of the art, in both of the do- mains. We obtained bigger improvement in lap- top domain, of 1.7%, compared to restaurant do- main, of 1.4%. This shows that the inclusion of the neighboring aspect information and memory net- work has an overall positive impact on the classi- fication process.</p><p>Single Aspect and Multi-Aspect Scenarios Following     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study</head><p>We analyze and compare IARM and IAN with sin- gle aspect and multi-aspect samples from the Se- mEval 2014 dataset.</p><p>Single Aspect Case It is evident from <ref type="table" target="#tab_3">Table 5</ref>, that IARM outperforms IAN in single-aspect sce- nario. For example, the sentence "I recommend any of their salmon dishes......" having aspect "salmon dishes", with positive sentiment, fails to be correctly classified by IAN as the attention net- work focuses on the incorrect word "salmon", as shown in <ref type="figure" target="#fig_3">Fig. 2a</ref>. Since, "salmon" does not carry any sentimental charge, the network generates a ineffective aspect-aware sentiment representation, which leads to misclassification. On the other hand, IARM succeeds in this case, because the word-level attention network gener- ates correct attention value as α in Eq. <ref type="formula" target="#formula_6">(7)</ref>. α for this case is depicted in <ref type="figure" target="#fig_3">Fig. 2b</ref>, where it is clear that the network emphasizes the correct sentiment- bearing word "recommended". This leads to effec- tive aspect-aware sentence representation by the network, making correct final classification.</p><p>Multi-Aspect Case IARM also outperforms IAN in multi-aspect scenario, which can be ob- served in <ref type="table" target="#tab_3">Table 5</ref>. We suspect that the presence of multiple aspects in sentence makes IAN network perplexed as to the connection between aspect and the corresponding sentiment-bearing word in the sentence. For example, the sentence "Coffee is a better deal than overpriced cosi sandwiches" con- tains two aspects: "coffee" and "better". Clearly, the sentiment behind aspect "coffee" comes from the word "better" and the same for aspect "cosi sandwiches" comes from "overpriced". However, IAN fails to make this association for the as- pect "cosi sandwiches", evident from the attention weights of IAN shown in <ref type="figure" target="#fig_4">Fig. 3a</ref> where the empha- sis is on "better". This leads to imperfect aspect- aware sentence representation generation, result- ing misclassification of the target aspect to be pos- itive.</p><p>However, IARM resolves this issue with the combination of word-level aspect aware attention (α) and the memory network. Since, the memory network compares the target-aspect-aware sen- tence representation with the sentence represen- tations for the other aspects repeatedly, eventu- ally the correct representation for the target aspect emerges from the memory network.</p><p>Also, the consideration of surrounding as- pects forces the network to better distinguish the sentiment-bearing words for a particular as- pect. These points are reflected in the α attention weights of the aspects "coffee" and "cosi sand-   wiches", shown in <ref type="figure" target="#fig_4">Fig. 3b</ref> and <ref type="figure" target="#fig_4">Fig. 3c</ref> respec- tively, where the network emphasizes the correct sentiment-bearing words for each aspect, "better" and "overpriced", respectively. Again, the mem- ory network compares the target aspect-aware sen- tence representation for "cosi sandwiches" with the same for "coffee" and incorporates relevant in- formation into the target-aspect representation q in Eq. (16) along several hops. This phenomenon is indicated in <ref type="figure" target="#fig_6">Fig. 4a</ref>, where the degree of incorporation of the aspect terms is measured by the attention weights β in Eq. <ref type="figure" target="#fig_0">(11)</ref>. Here, the network is incorporating information from aspect "coffee" into aspect "cosi sandwiches" over three hops. We surmise that this information is related to the sentiment-bearing word "better" of the aspect "coffee", because a comparison using the word "better" implies the presence of a good ("coffee") and a bad ("cosi sandwiches") object. However, this semantics is misconstrued by IAN, which leads to aspect misclassification.</p><p>IARM performs considerably well when con- junction plays a vital role in understanding the sentence structure and meaning for sentiment analysis. For example, "my favs here are the tacos pastor and the tostada de tinga" where the aspects "tacos pastor" and "tostada de tinga" are con- nected using conjunction "and" and both rely on the sentiment bearing word favs. Such complex relation between the aspects and the correspond- ing sentiment-bearing word is grasped by IARM as shown in <ref type="figure" target="#fig_6">Fig. 4b</ref>. Another example where the inter-aspect relation is necessary for the correct classification is shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, where the aspects "atmosphere" and "service" both rely on the sen- timent bearing word "good", due to the conjunc- tion "and".</p><p>(a) Memory network atten- tion weights for the sen- tence "Coffee is a better deal than overpriced cosi sand- wiches.".    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Error Analysis</head><p>IARM also fails to correctly classify in some cases, e.g., in the sentence "They bring a sauce cart up to your table and offer you 7 or 8 choices of sauces for your steak (I tried them ALL).", the aspect "choices of sauces" is misclassified by our network as neutral. This happened due to the IARM's inability to correctly interpret the positive sentiment behind "7 or 8 choices of sauces" .</p><p>Again, the IARM could not correctly classify aspect the "breads" to be positive in the sentence "Try the homemade breads.". This happened, be- cause the word "try" itself is not sentimentally charged, but can carry sentimental meaning given the right context. This context was not recognized by IARM, which led to misclassification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hop-Performance Relation</head><p>In our experiments, we tried different hop counts of the memory network. We observed that the net- work performs best with three hops for restaurant domain and ten hops for laptop domain, which is shown in the hop count -performance plot in <ref type="figure" target="#fig_8">Fig- ure Fig. 6</ref>. It can be observed that the plot for restaurant domain is smoother than the plot for laptop domain. We assume that this is due to the restaurant domain having higher number of sam- ples than laptop domain, as shown in <ref type="table">Table 2</ref>.</p><p>Also, the plot for restaurant domain shows a downward trend over the increasing number of hops, with spikes in hop 3, hop 10. This sug- gests a irregular cyclic nature of the memory net- work where those certain hop counts yields higher quality representations than their neighbor. The same cannot be said for laptop domain as the plot presents a zig-zag pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented a new framework, termed IARM, for aspect-based sentiment analy- sis. IARM leverages recurrent memory networks with multihop attention mechanism. We empiri- cally illustrate that an aspect in a sentence is influ- enced by its neighboring aspects. We exploit this property to obtain state-of-the-art performance in aspect-based sentiment analysis in two distinct do- mains: restaurant and laptop. However, there remains plenty of room for improvement in the memory network, e.g., for generation of better aspect-aware representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed IARM architecture; AASR stands for Aspect-Aware Sentence Representation.</figDesc><graphic url="image-4.png" coords="4,386.70,145.46,123.43,128.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>TD-LSTM Following Tang et al. (2016a), se- quence of words preceding (left context) and suc- ceeding (right context) target aspect term are fed to two different LSTMs. Mean of the hidden outputs of the LSTMs are concatenated and fed to softmax classifier. AE-LSTM Following Wang et al. (2016), the sentence is fed to an LSTM for context propaga- tion. Then, the hidden outputs are concatenated with target-aspect representation, from which at- tention scores are calculated. Hidden outputs are pooled based on the attention scores to generate intermediate aspect representation. Final repre- sentation is generated as the sum of the affine transformations of intermediate representation and final LSTM hidden output. This representation is fed to softmax classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>a) Attention weight for aspect "salmon dishes" for IAN. (b) Attention weight for aspect "salmon dishes" for IARM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention weights for IAN and IARM for "I recommend any of their salmon dishes".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention weights for IAN and IARM for the sentence "Coffee is a better deal than overpriced cosi sandwiches".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>b) Memory network atten- tion weights for the sentence "my favs here are the tacos pastor and the tostada de tinga.".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Memory network attention weights for IARM.</figDesc><graphic url="image-13.png" coords="8,422.07,567.23,101.18,102.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Memory network attention weights for IARM for the sentence "service was good and so was the atmosphere." The word importance heatmap is for the aspect "atmosphere".</figDesc><graphic url="image-17.png" coords="9,94.96,227.87,172.36,129.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hop-Accuracy plot for both domains.</figDesc><graphic url="image-18.png" coords="9,94.96,358.45,172.34,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>= β T Q′ q H+1 q output memory Input : GRU s w 1 a w 2 S : w L α r a AASR S a Aspect-aware Sentence Representation</head><label></label><figDesc></figDesc><table>Positive 

H o p s 

Memory Network 

input memory 

AASR 

S a t 

o </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 , our IARM model beats the</head><label>5</label><figDesc></figDesc><table>Domain 
Train 
Test 
SA 
MA 
SA MA 
Restaurant 1007 2595 285 835 
Laptop 
917 
1396 259 379 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Distribution of the samples by single 
aspect/multi aspect sentence criteria in SemEval 
2014 (SA: Single Aspect, MA: Multi Aspect). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Domain-wise accuracy (%) of the dis-
cussed models. Best accuracy for each domain is 
marked with bold font. 

state of the art in both single aspect and multi-
aspect scenarios in both of the domains. It is in-
teresting that both model perform better in multi-
aspect scenario for restaurant domain. However, 
for laptop domain IAN performs better in sin-
gle aspect scenario, even though there are more 
multi-aspect samples than single aspect samples 
(shown in Table 3). This indicates the failure of 
IAN model to learn multi-aspect scenario, where 
IARM model performs significantly better. 

Model 
Restaurant 
Laptop 
SA 
MA 
SA 
MA 
IAN (SoA) 75.4 
77.7 
72.5 71.6 
IARM 
78.6 80.48 73.4 74.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy of the models for single as-
pect and multi aspect scenario; SA: Single Aspect, 
MA: Multi Aspect. 

Cross-Domain Evaluation Following Table 6, 
IARM outperforms the state of the art IAN by 
2% in both cross-domain scenarios. This indicates 
the ability of IARM in learning general domain-
independent semantic structures from the training 
data. 

Model 
Rest → Lap Lap → Rest 
IAN (SoA) 
64.6 
72.0 
IARM 
66.7 
74.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracy for cross-domain evaluation; 
Rest: Restaurant domain, Lap: Laptop domain; A 
→ B signifies train-set is the train-set of domain A 
and test-set is the test-set of domain B. 

</table></figure>

			<note place="foot" n="1"> Implementation available on http://github. com/senticnet/IARM</note>

			<note place="foot" n="2"> LSTM (Hochreiter and Schmidhuber, 1997) yields similar performance, but requires training more parameters this, we first concatenate aspect a i to all the words in the sentence S:</note>

			<note place="foot" n="3"> http://alt.qcri.org/semeval2014/task4</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep memory networks for attitude identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive Attention Networks for Aspect-Level Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Aspect extraction for opinion mining with a deep convolutional neural network. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lifelong learning crf for supervised aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00251</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective LSTMs for Target-Dependent Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08900</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05403</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xiaoyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recent trends in deep learning based natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="55" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Soujanya Poria, and Erik Cambria</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
