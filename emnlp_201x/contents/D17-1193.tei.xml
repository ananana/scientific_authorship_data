<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Vector Spaces for Semantic Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kata</forename><surname>Gábor</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris 13</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haïfa</forename><surname>Zargayouna</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris 13</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Tellier</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">LaTTiCe</orgName>
								<orgName type="laboratory" key="lab2">UMR 8094)</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">ENS Paris</orgName>
								<orgName type="institution" key="instit3">Université Sorbonne Nouvelle</orgName>
								<orgName type="institution" key="instit4">PSL Research University</orgName>
								<address>
									<country>SPC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris 13</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Charnois</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">LIPN</orgName>
								<orgName type="laboratory" key="lab2">UMR 7030</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris 13</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Vector Spaces for Semantic Relations</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1814" to="1823"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings are used with success for a variety of tasks involving lexical semantic similarities between individual words. Using unsupervised methods and just cosine similarity, encouraging results were obtained for analogical similarities. In this paper , we explore the potential of pre-trained word embeddings to identify generic types of semantic relations in an unsupervised experiment. We propose a new relational similarity measure based on the combination of word2vec&apos;s CBOW input and output vectors which outperforms alternative vector representations, when used for unsuper-vised clustering on SemEval 2010 Relation Classification data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vector space word representations or word embed- dings, both 'count' models <ref type="bibr" target="#b31">(Turney and Pantel, 2010)</ref> and learned vectors ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b23">Pennington et al., 2014</ref>), were proven useful for a variety of semantic tasks <ref type="bibr" target="#b19">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b2">Baroni et al., 2014</ref>). Word vectors are used with success because they capture a notion of seman- tics directly extracted from corpora. Distributional representations allow to compute a functional or topical semantic similarity between two words or, more recently, bigger text units ( <ref type="bibr" target="#b11">Le and Mikolov, 2014</ref>). The more similar two entities are semanti- cally, the closer they are in the vector space (quan- tified usually, but not necessarily in terms of cosine similarity). Semantic similarity can be exploited for lexical substitution, synonym detection, sub- categorization learning etc. Recent studies sug- gest that neural word embeddings show higher per- formance than count models ( <ref type="bibr" target="#b2">Baroni et al., 2014;</ref><ref type="bibr" target="#b10">Krebs and Paperno, 2016</ref>) for most semantic tasks, although <ref type="bibr" target="#b13">Levy et al. (2015a)</ref> argue that this is only due to some specific hyperparameters that can be adapted to count vectors. In what follows, we will concentrate on exploring whether and how pre- trained, general-purpose word embeddings encode relational similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Relational analogies as vector offsets</head><p>Relation extraction and classification deal with identifying the semantic relation linking two en- tities or concepts based on different kinds of in- formation, such as their respective contexts, their co-occurrences in a corpus and their position in an ontology or other kind of semantic hierarchy. Whether the vector spaces of pre-trained word em- beddings are appropriate for discovering or iden- tifying relational similarities remains to be seen. <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref> claimed that the embed- dings created by a recursive neural network indeed encode a specific kind of relational similarities, i.e. analogies between pairs of words. He found that by using simple vector arithmetic, analogy questions in the form of "a 1 is to a 2 as b 1 is to b 2 " (man ~king :: woman ~queen) could be solved. Relationships are assumed to be present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset. Vector arithmetics give us the vector which fills the analogy, and we can search for the word b 2 whose embedding vector has the greatest similarity to it: <ref type="bibr" target="#b13">Levy et al. (2015a)</ref> suggested that instead of a vector offset method, this calculation can also be considered as a combination of similarities. Using cosine similarity for sim, equation 1 can be written as a combination of similarities ( <ref type="bibr" target="#b13">Levy et al., 2015a)</ref> as</p><formula xml:id="formula_0">argmax b 2 = sim(b 2 , (b 1 − a 1 + a 2 ))<label>(1)</label></formula><formula xml:id="formula_1">argmax b 2 = sim(b 2 , b 1 ) − sim(b 2 , a 1 )+ + sim(b 2 , a 2 ) (2)</formula><p>Analogy pairs, however, are a special case of relational similarity because not only a 1 (man) re- lates to a 2 (king) the same way that b 1 (woman) relates to b 2 (queen); the relation between a 1 (man) and b 1 (woman) is also parallel to the relation be- tween a 2 (king) and b 2 (queen.) This is not always the case: when it comes to different types of se- mantic relations, their instances may or may not be analogical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Criticism of the vector offset method</head><p>As precise as neural word embeddings combined with cosine similarity may be for calculating se- mantic proximity between individual words, recent results seem to suggest that their value in identi- fying relational analogies using vector arithmetics is limited. In fact, a big part of their merits is likely to come from the precise calculation of indi- vidual similarities instead of relational similarities. Hence, they can be approximated using relation- independent baselines. <ref type="bibr" target="#b15">Linzen (2016)</ref> remarks that currently used analogy tasks evaluate not only the consistency of the offsets a 1 − a 2 and b 1 − b 2 , but also the neighborhood structure of the words in the vector space. Concretely, "if a 1 and a 2 are very similar to each other (...) the nearest word to b 2 may simply be the nearest neighbor of b 1 (...) regardless of offset consistency" <ref type="bibr" target="#b15">(Linzen, 2016)</ref>. Moreover, some of the success obtained by the vector offset method on analogies can also be obtained by base- lines that ignore a 2 , or even both a 1 and a 2 . <ref type="bibr" target="#b14">Levy et al. (2015b)</ref> point out similar limitations: word embedding combinations in supervised learn- ing of taxonomical relations do not seem to learn the relations themselves, but individual properties of words. They tested previously suggested vec- tor compositions for supervised learning of infer- ence relations: concatenation, difference, compar- ing only the first or only the second element of the pairs. The study concludes that the classifiers only learn individual properties (e.g. a "category" type word is a good hypernym candidate), but not se- mantic relations between words. Altogether, these studies suggest that the semantic information ob- tained from word embeddings is correct for iden- tifying similar or related units, but is already self- contained and difficult to enrich in order to retrieve more specific semantic contents such as relational similarities or specific relations.</p><p>In this paper, we aim to challenge this conclusion within a large scale semantic relation classification experiment, and show that it is possible to achieve improvements compared to baselines and current methods. We apply known vector composition me- thods, and propose a new one, to unsupervised large-scale clustering of entity pairs categorized according to their semantic relation. While large scale semantic relation classification is a very diffi- cult task and the state of the art does not perform yet at human level, we expect that the experiment provides information to compare the potential of different vector/similarity combinations in a setting (i.e. clustering) that is more reliant on the global structure of the data instead of the close neighbor- hood structure of selected items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Semantic Relations in Vector Spaces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related work</head><p>Relation classification includes the task of finding the instances of the semantic relations, i.e. the entity tuples, and categorizing their relation ac- cording to an existing typology. In an unsupervised framework, relation types are inferred directly from the data. Supervised systems rely on a list of pre- defined relations and categorized examples, as de- scribed in the shared tasks of MUC, ACE or Sem- Eval campaigns <ref type="bibr" target="#b6">(Hobbs and Riloff, 2010;</ref><ref type="bibr" target="#b7">Jurgens et al., 2012;</ref><ref type="bibr" target="#b4">Hendrickx et al., 2010)</ref>. Competing systems extract different kinds of features eventu- ally combined with external knowledge sources, and build classifiers to categorize new relationship mentions ( <ref type="bibr" target="#b34">Zhou et al., 2005)</ref>. A commonly used method, initiated by <ref type="bibr" target="#b27">Turney (2005;</ref><ref type="bibr" target="#b28">2006)</ref>, is to rep- resent entity pairs by a pair-pattern matrix and cal- culate similarities over the distribution of the pairs. Another way of constructing a distributional vector space to represent quantifiable context features for relation extraction is to combine the vectors of the two entities. Different combinations were proposed to represent compositional meaning <ref type="bibr" target="#b20">(Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b3">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b0">Baroni et al., 2012</ref>). Popular methods include addition <ref type="bibr" target="#b20">(Mitchell and Lapata, 2010)</ref>, concatenating the two vectors ( <ref type="bibr" target="#b0">Baroni et al., 2012</ref>) or taking their differ- ence ( <ref type="bibr" target="#b32">Weeds et al., 2014;</ref><ref type="bibr" target="#b24">Roller et al., 2014</ref>). As of now, these vector combinations had two types of applications in semantic relation classification.</p><p>The first one aims to find specific types of semantic or functional analogies <ref type="bibr" target="#b5">(HerdaˇgdelenHerdaˇgdelen and Baroni, 2009;</ref><ref type="bibr">Makrai et al., 2013;</ref><ref type="bibr" target="#b13">Levy et al., 2015a</ref>). The second one tries to infer taxonomical relations, i.e. hypernymy, or lexical entailment, in supervised experiments ( <ref type="bibr" target="#b32">Weeds et al., 2014;</ref><ref type="bibr" target="#b30">Turney and Mohammad, 2014</ref>). For the hypernymy detection task, relation directionality can be captured by the inclu- sion of the hyponym's context in the broader term <ref type="bibr" target="#b9">(Kotlerman et al., 2010</ref>), as well as by measuring the informativeness of their contexts ( <ref type="bibr" target="#b25">Santus et al., 2014)</ref>.</p><p>A few experiments have been specifically tar- geted at combining different kinds of linguistic information for calculating relational similarities. Turney (2012) suggested a dual distributional fea- ture space, composed of a domain space and a syn- tactic function space, for supervised classification. HerdaˇgdelenHerdaˇgdelen and Baroni (2009) combine individual entity vectors with co-occurrence contexts in their vector space. These works either aim to identify very specific relation types (typically taxonomical relations) with a mixture of features and a super- vised classifier, or target analogy pairs: a task in which, as we have seen, relation-unaware baselines approximate relation-aware representations. How- ever, more recently, <ref type="bibr" target="#b26">Shwartz et al. (2016)</ref> achieved promising results on the hypernymy detection task by combining dependency path-based context rep- resentations with distributional vectors; this finding can be relevant for a broader range of semantic re- lations as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task definition</head><p>Whether we use the vector offset method or any pairwise similarity combination, finding the miss- ing word in an analogy depends on two factors:</p><p>1. Vector quality (do semantically close elements have a higher cosine similarity?);</p><p>2. Density and structure of the vector space.</p><p>If we adapt 1) above to the more generic relational similarity task, the question can be formulated as follows:</p><p>3. How much information about the semantic relation is actually in the text and how fit is the vector combination method to encode this information?</p><p>In accordance with Linzen (2016) and <ref type="bibr" target="#b14">Levy et al. (2015b)</ref>, we also think that analogy test sets are not optimal to answer this question. It was al- ready confirmed that word embeddings are precise in identifying closely related items, while it is an open question whether they are useful for inferring a global structure from potentially noisy data in a large scale experiment. We propose to study re- lational similarity using a more generic and large scale relation classification task ( <ref type="bibr" target="#b4">Hendrickx et al., 2010)</ref>, and clustering pairs according to semantic relations, instead of finding the one missing word in an analogy. This way, we rely less on the neigh- borhood structure and more on actual "linguistic regularities".</p><p>We evaluate different vector combination me- thods, and propose a new one, for calculating re- lational similarities. The evaluation concentrates on the aspects above. We test whether cosine si- milarity over these vector spaces is adapted for discovering groups and classifying individual in- stances. We report clustering results and compare the vector combinations by their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Motivation</head><p>The semantic relation classification task, super- vised or not, is a difficult one with a strong upper bound: relations vary considerably with respect to the way they are defined and expressed in the text. Some relation types are more lexical by nature: re- lations such as dog is an animal; a teacher works at a school; a car is kept at a parking lot, can be identified out of context. On the other hand, many relations are contextual; they are time-anchored or tied to extra-linguistic, situational context. Con- textual relations (e.g. "the accident was caused by the woman") tend to be expressed explicitly, but rarely, in a corpus. A similar distinction underlies the notion of classical vs non-classical lexical se- mantic relations coined by <ref type="bibr" target="#b21">Morris and Hirst (2004)</ref>; however, their distinction is made on the level of relation types, while different instances of the same relation type can also be different with respect to their lexical or contextual nature. Other types of relations are defined exclusively through examples or analogies (e.g. badge is to policeman as crown is to king).</p><p>Semantic relations can also be viewed as binary predicates, and such predicates have semantic con- straints on their arguments, similarly to verb sub- categorization. Indeed, the relations are often ex- pressed by verbs, and we expect specific arguments of relations to belong to a specific semantic type, e.g. Message (chapters in this book investigate issues), Instrument (telescope assists the eye), Col- lection (essays collected in this volume), Container (image is hidden in a carafe). We expect vector space models (VSMs) to capture such semantic groups through pairwise similarity combinations.</p><p>Semantic relation types and instances differ with respect to the degree of semantic constraint on their arguments, the analogical nature of the relation, and the lexical/contextual aspect. We expect distri- butional VSMs to contribute to capture analogical similarities, as well as the semantic types of the arguments and lexical or prototypical relation in- stances. Since their ability to capture contextual relations is limited, they need to be complemented with e.g. pattern-based or dependency-based ap- proaches when it comes to less typical examples.</p><p>In the scope of the current experiment, our pri- mary goal is to argue that vector combinations may encode lexical relational similarities in themselves. If a representation is more capable than others to group together word pairs according to relational similarities, this potential can further be exploited in unsupervised as well as in supervised experi- ments.</p><p>The current task requires a change of perspective compared to the analogical task: when we look for missing elements in an analogy, we know the word exists and we presume to know where it will be in the vector space. In unsupervised clustering, our aim is to infer a global structure from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semantic relation data</head><p>The SemEval 2010 Task 8 data we used <ref type="bibr" target="#b4">(Hendrickx et al., 2010</ref>) contains examples of relation instances for 9 relations with sufficiently broad coverage to be of general and practical interest <ref type="table">(Table 1)</ref>. There is no overlap between classes, but there are two groups of strongly related relations to assess models' ability to make fine-grained distinctions (CONTENT-CONTAINER, COMPONENT- WHOLE, MEMBER-COLLECTION and ENTITY-ORIGIN, ENTITY-DESTINATION). Human agreement rates, when annotated in context, range from 58.2% to 98.5% depending on the relation type ( <ref type="bibr" target="#b4">Hendrickx et al., 2010)</ref>. This data set is very challenging, not only because of the fine semantic distinctions, but also because semantic relations were annotated in context and contain many less typical relation instances. In the current experiment, the goal we set for ourselves is to explore models' abilities to capture the structure of the data, rather then in achieving a classification precision close to that of humans. We used 6637 pairs of single word instances from the training data. Contexts in the training data were discarded. Class bias is present: the most frequent relation has 979 instances, the least frequent has 486.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Vector combination methods</head><p>If</p><note type="other">a 1 , a 2 , b 1 , b 2 are entities (nouns or nominal compositions) from a corpus, each of them assigned a pre-trained word embedding, we would like to classify entity pairs a = (a 1 , a 2 ) and b = (b 1 , b 2 ) according to their semantic relation. This means that we are looking for an efficient combination of a 1 , a 2 and b 1 , b 2 vectors that encode their relational attributes. We aim to find effective methods to calculate a relational similarity sim(a, b) by combining entity vectors a 1 , a 2 and b 1 , b 2 .</note><p>Pairwise similarities build on the idea that if a 1 is semantically similar to b 1 and a 2 is similar to b 2 , the relation between a 1 and a 2 is similar to the relation between b 1 and b 2 . The recall of this ap- proach is expected to be limited: the same relation can hold between different types of entities. Analogical similarities presume that b 1 −b 2 shares the direction with a 1 − a 2 , ignoring the pairwise similarities. We adapt this measure, while aware that analogy pairs are a specific case of relational similarity in that analogies work both ways (man ~king :: woman ~queen and also man ~woman :: king ~queen). IN-OUT similarities: a new combination that builds on the integration of second order similari- ties. Only a 1 : In this baseline solution, the similarity between two pairs is calculated as the similarity between the first entity of each pair, the other pair being ignored.</p><formula xml:id="formula_2">sim(a, b) = sim(a 1 , b 1 )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pairwise similarities</head><p>Different combinations proposed in the literature were compared.</p><p>• concatenative : one vector for each entity pair is defined as the concatenation of the vec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instances in Typical examples Atypical examples training data</head><p>Cause-Effect 979 suicide − death, injury − discomf ort women − accident Component-Whole 978 claw − owl, walls − hospital image − photos Entity-Destination 789 solvent − f lask, hay − barn chair − corporation Product-Producer 775 industry − models, artist − design of f icer − oath Entity-Origin 762 relics − culture, plane − runway error − def inition Member-Collection 729 stable − hounds, ensemble − ladies mission − monkeys Message-Topic 622 pages − scene, speech − measures exhibition − glamour Instrument-Agency 517 user − console, eye − telescope companies − governments Content-Container 486 document − f older, pictures − box message − paper <ref type="table">Table 1</ref>: Semantic Relation Classification data tors of the two entities.</p><formula xml:id="formula_3">sim(a, b) = sim((a 1 ⊕ a 2 ), (b 1 ⊕ b 2 )) (4)</formula><p>• pairwise addition Pairwise similarities be- tween respective entities are added up. If we use cosine similarity, this is only slightly dif- ferent from the concatenative method. Vec- tor addition proved to work well as a compo- sitional representation <ref type="bibr" target="#b20">(Mitchell and Lapata, 2010)</ref>, despite the fact that word order is ig- nored.</p><formula xml:id="formula_4">sim(a, b) = sim(a 1 , b 1 ) + sim(a 2 , b 2 ) (5)</formula><p>A potential problem with this addition objective is that different properties of words are expressed on a different scale and, as a consequence, terms sharing these properties have a higher cosine simi- larity than terms that are similar with respect to a flatter property. It can be overcome by using multi- plication instead of addition ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014</ref>):</p><p>• pairwise multiplication</p><formula xml:id="formula_5">sim(a, b) = sim(a 1 , b 1 ) × sim(a 2 , b 2 ) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analogies</head><p>This is an adaptation of the measure proposed for queen = king -man + woman ( <ref type="bibr" target="#b19">Mikolov et al., 2013b</ref>). Vector arithmetics give us the vector which fills the analogy, and we can search for the word b 2 whose embedding vector has the greatest similarity to it:</p><formula xml:id="formula_6">argmax b 2 = sim(b 2 , (b 1 − a 1 + a 2 ))<label>(7)</label></formula><p>which, using cosine similarity for sim, can be writ- ten as a combination of similarities ( <ref type="bibr" target="#b13">Levy et al., 2015a</ref>), as</p><formula xml:id="formula_7">argmax b 2 = sim(b 2 , b 1 ) − sim(b 2 , a 1 ) + sim(b 2 , a 2 ) (8)</formula><p>Mikolov (2013b) notes that this measure is qualita- tively similar to the relational similarity model in <ref type="bibr" target="#b29">(Turney, 2012)</ref>, which predicts similarity between members of the word pairs (</p><formula xml:id="formula_8">x b , x d ), (x c , x d ) and dissimilarity for (x a , x d ).</formula><p>In the current context, we do not look for the miss- ing b 2 which maximizes the equation. Instead, we have different pairs a and b, and we aim to calcu- late sim(a, b) to quantify how much the analogy queen -woman = king -man holds.</p><p>• difference Focuses on the similarity of b 1 , b 2 and a 1 , a 2 , but does not take into account the pairwise distances between the individual entity vectors. <ref type="bibr" target="#b13">Levy et al. (2015a)</ref> propose a multiplicative ver- sion of the analogy formula. We tried to adapt it; however, this measure is not symmetrical (con- ceived to find b 2 which maximizes the form) and the adaptation produced bad results.</p><formula xml:id="formula_9">sim(a, b) = sim((a 1 − a 2 ), (b 1 − b 2 )) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IN-OUT similarities</head><p>This metric is a combination of first order and se- cond order similarities between the two entity pairs, adapted to relational similarity: a and b are similar if a 1 is similar to b 1 and also similar to the contexts of b 2 , the opposite entity in b.</p><p>In the current experiment, second order similarities are estimated using both input and output vectors generated by word2vec's CBOW model. In this model, the IN vectors of words get closer to the OUT vectors of other words that they co-occur with.</p><p>Words with a high input-output similarity tend to appear in the context of each other. This similarity combination was recently included for a few, dif- ferent tasks. It was shown to improve information retrieval <ref type="bibr" target="#b22">(Nalisnick et al., 2016)</ref>. <ref type="bibr" target="#b23">Pennington et al. (2014)</ref> propose to use second-order similarity to improve similarity calculation between words. Their proposed formula combines first and second order similarity, normalized by the reflective se- cond order similarity of the words with themselves. Finally, <ref type="bibr" target="#b17">Melamud et al. (2015)</ref> used a combination of input-output similarities for lexical substitution.</p><p>In these contexts, the use of second-order simi- larities 1 is based on the observation that words are similar if they tend to appear in similar contexts, or if they tend to appear in the contexts of each other. In our experiment, second order similarities are used in a different way and with a different purpose. Second-order similarities are calculated between opposite elements of the entity pairs. We combine those similarities by taking the in-in simi- larity between a 1 and b 1 , and the in-out similarities between a 1 and b 2 , and between a 2 and b 1 . Our mo- tivation is to add relational information in a form which also preserves pairwise similarity informa- tion, as both are relevant for calculating relational similarities. We do it by using a co-occurrence component which gives higher score between more prototypical example pairs. A pairwise similarity can be high even if the entities are similar, but their relation is not: an obvious example is ambiguity (when they are similar with respect to a meaning, but co-occur with their pair in an other meaning). If an entity is similar to one argument of a relation and is also likely to appear in the context of the other argument, it indicates a higher likelihood of being an instance of the same relation.</p><p>• additive in-out</p><formula xml:id="formula_10">sim(a, b) = sim(a 1 , b 1 ) + sim(a 2 , b 2 ) + sim 2 (a 1 , b 2 ) + sim 2 (a 2 , b 1 ) (10)</formula><p>where sim 2 designates the second order similarity and is calculated as follows:</p><formula xml:id="formula_11">sim 2 (x 1 , y 2 ) = sim(x in 1 , y out 2 ) + sim(x out 1 , y in 2 )<label>(11)</label></formula><p>• multiplicative in-out: The same as above, but addition is replaced by multiplication in sim and sim 2 .</p><formula xml:id="formula_12">sim(a, b) = sim(a 1 , b 1 ) * sim(a 2 , b 2 ) * sim 2 (a 1 , b 2 ) * sim 2 (a 2 , b 1 )<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">sim 2 (x 1 , y 2 ) = sim(x in 1 , y out 2 ) * sim(x out 1 , y in 2 )<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Clustering Experiments</head><p>For supervised classification tasks, it is desirable to adapt word2vec's hyperparameters to the task and the data at hand ( <ref type="bibr" target="#b13">Levy et al., 2015a</ref>). The interaction between hyperparameters is also to be considered ( <ref type="bibr" target="#b10">Krebs and Paperno, 2016)</ref>. However, our experiment is a clustering scenario aimed at exploratory analysis on a vector space created by pre-trained word embeddings; therefore, we set the parameters once and in advance.</p><p>We trained a word2vec CBOW model (Mikolov et al., 2013a) with negative sampling and a window size of 10 words on the ukWaC corpus ( , and extracted both input and output vectors of size = 400 to build the vector combina- tions above. This size corresponds to the best per- forming model in the comparative paper by <ref type="bibr" target="#b0">Baroni et al. (2012)</ref>. An adjacency matrix was constructed for each vector/similarity combination using cosine similarity. Clustering was implemented with Cluto's ( <ref type="bibr" target="#b33">Zhao et al., 2005</ref>) clustering function which takes the adjacency matrix as input. We used a hierarchi- cal agglomerative clustering with the unweighted average distance (UPGMA) criterion function 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation as classification</head><p>At first, we ran the clustering with 9 clusters (the number of classes in the standard) and tried to make one-to-one correspondences between the standard and the output. Every cluster is mapped to the stan- dard class that shares the more elements with. We then calculate precision and recall for each stan- dard class (zero if the class doesn't show up as a majority class in any cluster). Average class-based precision and recall is reported, as well as the num- ber of classes in the standard that could be assigned. These scores were published for the SemEval task participants, but ours are not comparable because we only consider one cluster for each class, and because we did the clustering on the training data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation as clustering</head><p>While the scores above can be indicative of the potential of different representations, they do not provide information on other aspects as cluster sta- bility, purity, the amount of post-processing needed. Above all, in a completely unsupervised setting, the number of classes in the standard is not known and cluster quality (precision) plays an important role with respect to interpretability: it is easier to unify two homogeneous clusters than to separate a noisy one. We ran complementary experiments with dif- ferent numbers of clusters. <ref type="table" target="#tab_2">Table 3</ref> indicates results for 20 and 30 clusters. The input-output combina- tion method still has an advantage, and concatena- tion and multiplication also perform well. However, the advantages over the baseline are less significant than when the number of clusters was identical to the standard.</p><p>In the next runs, we measure how stable the differ- ent clustering solutions are with settings that are structurally very different from the standard, i.e. have significantly more clusters. Class-based pre- cision and recall are less relevant measures in this setting, since they take the average over the nine standard classes and not over the produced clusters.</p><p>We therefore decided to use modified purity <ref type="bibr" target="#b8">(Korhonen et al., 2008</ref>  </p><formula xml:id="formula_14">P U R = |K| i=1 max j |w in k i ∩ w in c j | |K| i=1 w in k i (14)</formula><p>Modified purity is indicative of the quality and interpretability of the clusters. It favorizes small clusters, but singleton clusters were discarded. This measure corresponds to prediction accuracy in clas- sification if we assign the majority label to clusters.   We note that the baseline and the simple pairwise combinations have a high performance because they already capture arguments' semantic types successfully. This also lies behind previous suc- cess on the analogy dataset. Moreover, the nature of semantic spaces and of semantic datasets is such that they typically contain close or quasi-identical variants for the same phenomenon, that the base- lines identify easily. The additive input-output combination shows promising results, especially when it comes to cap- turing the structure: in the clustering setting with 9 clusters, it identifies 8 classes out of 9 in the standard. This indicates a good potential in differ- entiating between relation types, especially because the standard is conceived in a way that it contains strongly related classes. It outperforms every other measure until the number of clusters grows signifi- cantly above those in the standard <ref type="table" target="#tab_5">(Table 5)</ref>, when the concatenative measure catches up. The base- line performs well, but additive methods all beat it, while difference is especially weak. Pairwise multiplication is good at recognizing the structure (7 classes out of 9), but not good at assigning ele- ments.</p><p>Multiplicative methods show a fluctuating perfor- mance, especially the multiplicative input-output combination. This is due to the higher variance in similarities obtained by multiplication (in the case of input-output combination, 6 operands are multi- plied), combined with the agglomerative clustering, which is sensitive to chaining.</p><p>The very high precision of the baseline method with a large number of clusters <ref type="table" target="#tab_2">(Table 3)</ref> is note- worthy but not unexpected. Individual similarities have a strong precision for the easily identifiable clusters, while additional relational information is mostly expected to improve recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We presented an experiment to identify relational similarities in word embedding compositions at a large scale, using an unsupervised approach. On the one hand, our results confirm the recent find- ing that many of the success attributed to vector arithmetics for analogies come from similarities between individual elements. On the other hand, taking second order similarity into account, we can improve relational similarities and take a step to- ward a meaningful representation for entity couples in a semantic relation.</p><p>The baseline performs well and is difficult to enrich with relation-aware information. The results indicate that the vector offset method for analogies, which replaces the pairwise similarity, is the least efficient in capturing generic semantic relations at a large scale. The vector difference representation does not conserve pairwise similar- ities and the offsets do not prove to be constant enough for unsupervised clustering. Multiplicative methods do not scale up either, although to a lesser extent: they capture some of the relational information, but this happens at the expense of losing precision from individual similarities. Pairwise similarities can be better exploited in an additive or concatenative setting. Moreover, they can be meaningfully complemented by including second order similarities without losing too much information for precise classification. The input-output combination measure coherently outperformed the other combinations in almost every setting, indicating a better potential for unsupervised experiments.</p><p>Unsupervised relation classification is a very challenging task for several reasons. Some relation instances are lexical by nature and, therefore, can be expected to show up in the same cluster based on distributional cues. On the other hand, contex- tual relation instances tend to have relation-specific indicators when they co-occur, but their individual vectors will not reveal this information (unless they co-occur very often). Moreover, semantic relations differ with respect to the semantic constraints they put on their arguments. For instance, the second argument of the Content-Container relation tend to belong to a specific semantic class in the stan- dard (bag, box, trunk, case, drawer...), while both arguments of the Cause-Effect relation are much freer (gas, prices, pain, acts, species and pyrol- ysis, collapse, contraction, society, noise). Any future development towards an automated unsu- pervised classification needs to take these aspects into account and work towards a hybrid solution by separating relations with semantically constrained arguments from free ones, as well as adapting the clustering method to handle outliers. the French National Research Agency, ANR-10- LABX-0083 (Labex EFL). We are grateful to the anonymous reviewers for their valuable comments that helped to improve the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Class-based results for 9 clusters</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Class-based results for 20 and 30 clusters 

class c in their cluster k: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Cluster-based results, 10-50 clusters 

INPUT 
PUR 

a1(baseline) 0.3291 

add 
0.3578 
conc 
0.3737 
in-out.add 
0.3674 
mult 
0.3235 
in-out.mult 
0.2587 
diff 
0.3058 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Cluster-based results, 60-100 clusters 

1820 </table></figure>

			<note place="foot" n="1"> Note that we use the term &quot;second order similarity&quot; in the sense of word-to-context similarity, unlike Pennington et al. (2014).</note>

			<note place="foot" n="2"> We observed that these settings are sensitive to the chaining effect and there is probably room for improvement by experimenting with different task-specific clustering parameters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was generously supported by the pro-gram "Investissements d'Avenir" overseen by</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaela</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;12</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations</title>
		<meeting>the Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bagpack: A general framework to represent semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaç</forename><surname>Herdaˇgdelenherdaˇgdelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, GEMS &apos;09</title>
		<meeting>the Workshop on Geometrical Models of Natural Language Semantics, GEMS &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Natural Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 2: Measuring degrees of relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holyoak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations</title>
		<meeting>the Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The choice of features for classification of verbs in biomedical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Krymolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING&apos;08</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When hyperparameters help: Beneficial parameter combinations in distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Lexical and Computational Semantics (*SEM)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONLL&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Issues in evaluating semantic spaces using word analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RepEval Workshop, ACL&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applicative structure in vector space models</title>
	</analytic>
	<monogr>
		<title level="m">Workshop on Continuous Vector Space Models and their Compositionality</title>
		<editor>Márton Makrai, Dávid Nemeskey, and András Kornai</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple word embedding model for lexical substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Vector Space Modeling for NLP Workshop, NAACL</title>
		<meeting>the Vector Space Modeling for NLP Workshop, NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-classical lexical semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<idno>HLT-NAACL&apos;04</idno>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computational Lexical Semantics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving document ranking with dual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the WWW Conference</title>
		<meeting>the WWW Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine Schulte Im</forename><surname>Walde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Measuring semantic similarity by latent relational analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Similarity of semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<idno>abs/cs/0608100</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain and function: A dualspace model of semantic relations and compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Experiments with three approaches to recognizing lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<idno>abs/1003.1141</idno>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hierarchical clustering algorithms for document datasets. Data Mining for Knowledge Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usama</forename><surname>Fayyad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
