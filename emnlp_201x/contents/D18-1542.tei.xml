<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Input Uncertainty in Neural Network Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Van Der Goot</surname></persName>
							<email>r.van.der.goot@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Groningen</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
							<email>g.j.m.van.noord@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Groningen</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Input Uncertainty in Neural Network Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4984" to="4991"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4984</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently introduced neural network parsers allow for new approaches to circumvent data sparsity issues by modeling character level information and by exploiting raw data in a semi-supervised setting. Data sparsity is especially prevailing when transferring to non-standard domains. In this setting, lexical nor-malization has often been used in the past to circumvent data sparsity. In this paper, we investigate whether these new neural approaches provide similar functionality as lexical nor-malization, or whether they are complementary. We provide experimental results which show that a separate normalization component improves performance of a neural network parser even if it has access to character level information as well as external word embeddings. Further improvements are obtained by a straightforward but novel approach in which the top-N best candidates provided by the normalization component are available to the parser.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, neural network dependency parsers <ref type="bibr">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b1">Dyer et al., 2015;</ref><ref type="bibr">Kiperwasser and Goldberg, 2016</ref>) obtained state-of-the-art performance for dependency parsing. These parsers incorporate character level information (de <ref type="bibr">Lhoneux et al., 2017a;</ref><ref type="bibr" target="#b1">Ballesteros et al., 2015;</ref><ref type="bibr">Nguyen et al., 2017</ref>) and can more easily exploit raw text in a semi-supervised setup. These new methods are especially beneficial for words not occurring in the training data. In practice, such unseen words often are spelling mistakes, or alternative spellings of known words. In more classical parsing models, these unseen words were usually clustered using ad-hoc rules. For non-standard domains, the number of unseen words is much larger. To minimize the degra- dation in performance, lexical normalization is often used. Lexical normalization is the task of converting non-standard input to a more standard format. Previous work has shown that this is beneficial, in particular for parsing social media data <ref type="bibr">(Foster, 2010;</ref><ref type="bibr" target="#b8">Zhang et al., 2013;</ref><ref type="bibr">van der Goot and van Noord, 2017b)</ref>.</p><p>This leads to the question whether normaliza- tion is indeed no longer required for these mod- ern character-based neural network parsers, or whether normalization is capable of solving prob- lems beyond the scope of this type of neural net- work parsers.</p><p>Our main contributions are:</p><p>• We show that using normalization as pre- processing improves parser performance for non-standard language, even if pre-trained embeddings and character level information are used.</p><p>• We propose a novel technique to exploit the top-N candidates provided by the normaliza- tion component, and we show that this tech- nique leads to a further increase in parser per- formance.</p><p>• A treebank containing non-standard language is created to evaluate the effect of normal- ization on parser performance. The treebank consists of 10,005 tokens annotated with lex- ical normalization and Universal Dependen- cies ( <ref type="bibr" target="#b7">Nivre et al., 2017</ref>). The treebank has been made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early work on parser adaptation focused on relatively canonical domains, like biomedical data <ref type="bibr">(McClosky and Charniak, 2008)</ref>. More re- cently, there has been an increasing interest in parsing of the notoriously noisy domain of social media. A lot of previous work is orthogonal to our</p><formula xml:id="formula_0">Original word new pix comming tomoroe Cand. 1 (p1) new (0.95) pix (0.79) coming (0.57) tomorrow (0.54) Cand. 2 (p2) news (0.03) selfies (0.08) comming (0.43) tomoroe (0.39) Cand. 3 (p3)</formula><p>knew (0.01) pictures (0.06) combing (&lt;0.01) tomorrow's (0.02) approach, as it focuses on adaptation of the train- ing data <ref type="bibr">(Foster et al., 2011;</ref><ref type="bibr">Khan et al., 2013;</ref><ref type="bibr">Kong et al., 2014;</ref><ref type="bibr" target="#b2">Blodgett et al., 2018</ref>). In the remainder of this section we will shortly review work which evaluated the effect of normalization on dependency parsing. <ref type="bibr" target="#b8">Zhang et al. (2013)</ref> tune a normalization model for the parsing task, and show performance im- provement on a silver treebank obtained from manually normalized data. <ref type="bibr">Daiber and van der Goot (2016)</ref> use an existing normalization model as pre-processing for a graph-based dependency parser, and show a small but significant perfor- mance improvement. In the shared task of pars- ing the web ( <ref type="bibr" target="#b5">Petrov and McDonald, 2012)</ref> held at SANCL 2012, some teams used a simple rule- based normalization, but the effect on final perfor- mance remained untested.</p><p>Baldwin and Li (2015) examined the theoretical impact of different normalization actions on pars- ing performance. To this end they use manual nor- malization. They show that edits beyond the word level can also be crucial for parsing (e.g. inser- tion of copulas and subjects). However, these are difficult to obtain automatically.</p><p>Note that all this previous work, except for <ref type="bibr" target="#b2">Blodgett et al. (2018)</ref>, is based on traditional feature-based dependency parsers, whereas we fo- cus on neural network parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we will first shortly review the two models we will combine: a lexical normalization model and a neural network parser. Then we de- scribe how they can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Normalization</head><p>In this work we use an existing normalization model: MoNoise (van der Goot and van Noord, 2017a) 1 . This model is based on the observation that normalization requires a variety of different</p><formula xml:id="formula_1">1 https://bitbucket.org/robvanderg/ monoise word1 v 1 t 1 c 1 e 1 LSTM f LSTM b word2 v 2 t 2 c 2 e 2 LSTM f LSTM b word3 v 3 t 3 c 3 e 3 LSTM f LSTM b</formula><p>Figure 1: Overview of the conversion of input words to vectors which are used in the shift-reduce algorithm. replacement actions. For these different actions, different modules are used to generate candidates, including: the Aspell spell checker 2 , word embed- dings and a lookup list generated from the training data. Features from these generation modules are complemented with N-gram features from canon- ical data and non-canonical data. A random forest classifier is used to score and rank the candidates.</p><p>In this work, we use the top-N candidates and con- vert the confidence scores of the classifier to prob- abilities. An example of this output is shown in <ref type="table" target="#tab_0">Table 1</ref>. We train MoNoise on 2,577 tweets annotated with normalization by <ref type="bibr">Li and Liu (2014)</ref>, which only contains word-word replacements. In our ini- tal experiments, we noted that the normalization model wrongfully normalized some words due to the different tokenization in the treebank (e.g. "ca n't"), because these do not occur in the normal- ization data. We manually created a list of excep- tions, which are not considered for normalization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Network Parser</head><p>As a starting point, we use the shift-reduce UU- Parser 2.0 (de <ref type="bibr">Lhoneux et al., 2017b;</ref><ref type="bibr">Kiperwasser and Goldberg, 2016)</ref>. This parser uses the Arc-Hybrid Transition system ( <ref type="bibr">Kuhlmann et al., 2011</ref>). Words are first converted to continu-ous vectors, which are then processed through a Bidirectional Long-Short Term Memory network (BiLSTM) ( <ref type="bibr">Graves and Schmidhuber, 2005</ref>) be- fore they are passed on to the parsing algorithm. The decision whether to shift, reduce or swap is made by a multi-layer perceptron with one hid- den layer. The BiLSTM is trained jointly with the parsing objective, so that the vectors are optimized for the parsing task. <ref type="figure">Figure 1</ref> shows an overview of how the input words are converted to vectors which are used in the shift-reduce algorithm. We denote the vec- tor used as input to the BiLSTM for word i by v i . This vector is a concatenation of three vec- tors which are derived from the input word. t i is optimized on the training data, c i is the result of a separate BiLSTM ran over the characters of word i and e i is the external vector; it is obtained from external embeddings which are trained on huge amounts of raw texts. In this work we use the same word embeddings as used by the nor- malization model (van der Goot and van Noord, 2017a), which are trained on 760,744,676 tweets using word2vec <ref type="bibr">(Mikolov et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptation Strategy</head><p>Notation We use w 0 ... w n to represent the vec- tors of the original words of a sentence. The vectors of the normalization candidates are repre- sented by n ij , where i is the index of the original word in the sentence, and j is the rank of the can- didate. The corresponding probability as given by the normalization model is p ij . We use g i for the vector of the manual normalization of word i</p><p>Our baseline setup is to simply use the vector of the original word:</p><formula xml:id="formula_2">ORIG: v i = w i</formula><p>The most straightforward use of normalization is to use the best normalization sequence as input to the parser. In our setup, this means that we use the vector of the best normalization candidate for each position:</p><formula xml:id="formula_3">NORM: v i = n i0</formula><p>To give more information to the parser, we will exploit the top-n candidates of the normalization model. The vectors of the top-N candidates are merged using linear interpolation:</p><formula xml:id="formula_4">INTEGRATED: v i = n j=0 p ij * n ij</formula><p>An interesting property of this integration ap- proach is that it does not influence the size of the search space, so the effect on complexity of the parsing algorithm is negligible. The only extra runtime compared to ORIG originates from run- ning the normalization model.</p><p>Finally, we include a theoretical upperbound of the effect of normalization, which uses manually annotated normalization:</p><formula xml:id="formula_5">GOLD: v i = g i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>To test the effect of normalization, we need a tree- bank containing non-standard language, prefer- ably with a corresponding training treebank from a more standard domain. Since the existing tree- banks are not noisy enough <ref type="bibr">(Foster et al., 2011;</ref><ref type="bibr">Kaljahi et al., 2015)</ref>  <ref type="bibr">3</ref> or do not have a correspond- ing training treebank in the same annotation for- mat ( <ref type="bibr">Kong et al., 2014</ref>; Daiber and van der Goot, 2016) we annotate a small treebank for develop- ment and testing purposes <ref type="bibr">4</ref> . We choose to use the Universal Dependencies 2.1 annotation for- mat ( <ref type="bibr" target="#b7">Nivre et al., 2017)</ref>, since the annotation ef- forts on the the English Web Treebank ( <ref type="bibr" target="#b6">Silveira et al., 2014</ref>) provide suitable training data. This treebank already contains web specific phenomena like URL's, E-Mail addresses and emoticons, so we do not have to create special annotation guide- lines and the parser can learn these phenomena from the training data.</p><p>Our treebank consists of tweets, taken from Li and Liu <ref type="bibr">(2015)</ref>. The tweets in this dataset origi- nate from two sources: the LexNorm corpus (Han and Baldwin, 2011), which was originally anno- tated with normalization, and a corpus originally annotated with POS tags ( <ref type="bibr" target="#b4">Owoputi et al., 2013)</ref>. <ref type="bibr">Li and Liu (2015)</ref> complemented this annotation for both datasets, so that they both have a normal- ization layer and a POS layer. To avoid overfit- ting on a specific filtering or time-frame we use the data collected by <ref type="bibr" target="#b4">Owoputi et al. (2013)</ref> as devel- opment data and LexNorm as test data. We only keep the tweets which are still available on Twit- ter, resulting in a dataset of 305 development and 327 test tweets (10,005 tokens in total). It should be noted that these corpora were filtered to contain domain-specific phenomena and non-standard lan- guage, and thus provide an ideal testbed for our ex- periments but are not representative of the whole Twitter domain.</p><p>Tokenization</p><note type="other">and normalization are first re- annotated, because the Universal Dependencies format requires treebank specific tokenization. To avoid parser bias, dependency relations are anno- tated from scratch. For more details on annotation decisions for domain-specific structures, we refer to the appendix.</note><p>MoNoise reaches 90% accuracy on the word level for the normalization task for our develop- ment data. In this dataset, 18% of all words are in need of normalization, so a baseline which sim- ply copies the original words would reach an ac- curacy of 82%. The most common mistakes made by MoNoise are due to treebank specific normal- izations, like 'na' → go. However, these also oc- cur in the training treebank, so normalization is not crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we first use the development data to compare the effect of the different normalization settings with the use of character level information and external embeddings. Secondly, we confirm our main results on the test set. Thirdly, we test if our model is sensitive to over-normalization on standard data. Finally, we perform some analysis to examine why normalization is beneficial. All scores reported in this section are obtained using the CoNLL 2017 evaluation script ( <ref type="bibr" target="#b7">Zeman et al., 2017)</ref>. In Section 5.1 the results are the average over ten runs, using a different seed for the BiL- STM and the shuffling of the training data. In the remainder of this section, the best model is used to simplify interpretation. The parser is trained using default settings (de <ref type="bibr">Lhoneux et al., 2017b</ref>).</p><p>In our initial experiments, it became apparent that the parser often considered a username men- tion or retweet in the beginning of the tweet as root, resulting in a propagation of errors. Be- cause we want to exclude any influences from this simple construction, we added an heuristic to our parser which exclude usernames and retweets in the beginning of a tweet, and connects them to the root after parsing. We use this heuristic in all ex- periments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Normalization Strategies</head><p>The results of the different parser and normaliza- tion settings on the development data are plotted in <ref type="figure" target="#fig_0">Figure 2</ref>. Using external embeddings ( e) re- sults in a much bigger performance improvement compared to using character level information ( c).</p><p>Adding character level embeddings on top of ex- ternal embeddings only leads to a very minor im- provement. This can partly be explained by the coverage of 98.4% of the embeddings on the de- velopment data.</p><p>In the settings without external embeddings, the direct use of normalization (NORM) results in a improvement of approximately 3 LAS points. However, when external embeddings are included the improvement becomes more than twice as small, indicating that the approaches target some common issues, but are also complementary to each other. When external embeddings and nor- malization are already used, the character level embeddings slightly harm performance. Integra- tion of the normalization (INTEGRATED) consis- tently results in a slightly higher LAS compared to direct normalization. Interestingly, gold nor- malization still performs substantially better com- pared to automatic normalization. <ref type="table" target="#tab_2">Table 2</ref> shows the results of the parser with exter- nal embeddings and character embeddings (using the best seed from the development data), for the different normalization strategies on the test data. These results confirm the observations on the de- velopment data: normalization helps on top of ex-  ternal embeddings, and integrating normalization results in an even higher score. In contrast to the development data, the integrated approach almost reaches the theoretical upper bound of gold nor- malization on the test data. However, since this is only the case on the test data, not too strong con- clusions can be drawn from this result. The perfor- mance difference between the datasets is probably partly due to the differences in filtering <ref type="bibr">5</ref> . Interest- ingly, integrating normalization is especially ben- eficial for the LAS, meaning that it is most useful for choosing the type of relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Test Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Robustness</head><p>As stated in Section 4, our development and test data is filtered to be very non-standard. However, it is undesirable to have a parser that performs bad on more standard texts. Hence, we also tested per- formance on the English Web Treebank develop- ment set. This dataset also consists of data from the web, however, it contains much less words in need of normalization; MoNoise normalizes less than 0.5% of all words. We compared the per- formance using no normalization (ORIG) versus our INTEGRATED approach, which showed a very minor performance improvement from 81.42 to 81.43 LAS. This is a direct effect of the normaliza- tion model giving high probabilities to the original words on this more canonical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>To gain insights into which constructions are parsed better when using normalization, we com- pared the predictions of the vanilla parser with our NORM and INTEGRATED methods on the develop- ment data. Starting with NORM, the first observa- tion is that the incoming arcs of the words which are normalized are responsible for 44.1% of all improvements, whereas the outgoing arcs are re- sponsible for 17.6% of al improvements. So, the direct context of the normalized words is respon- sible for only 61.7% of all improvements. Consid- ering the type of syntactic constructions for which parsing improved, it is hard to identify trends, be- cause the improvements are based on the output of the normalization model, which normalizes a wide variety of words. One clearly influential effect of using normalization, was that the parser improved upon finding the root. When multiple unknown words occured in the beginning of a sentence, the vanilla parser often failed at identifying the root, which improved considerably after normalizing. For the INTEGRATED method, almost all the im- provements made by NORM remained. On top of these, some additional improvements where made. Manual inspection revealed that these im- provements often originated from a non-standard word, for which the correct normalization was ranked high. This then leads to improvements for the non-standard word as well as its context. In some cases, even incorrect normalization candi- dates lead to performance improvements. For ex- ample for 'Gma', where the normalization model ranked the original word first, but 'mom' sec- ond. Even though 'grandma' is the correct nor- malization, 'mom' occurs in similar contexts, and is much easier for the parser to process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We showed that normalization can improve perfor- mance of a neural network parser, even when mak- ing use of character level information and external word embeddings. Integrating multiple normal- ization candidates into the parser leads to an even larger performance increase. Normalization has shown to be complementary to external embed- dings, in contrast to character embeddings, which add no additional information. Our experiments revealed that our approach is robust, and it does not harm performance on more canonical data. However, when comparing our approach to the theoretical upperbound of using gold normaliza- tion, we saw that on different datasets the perfor- mance gain is of a different magnitude. Further- more, we release a dataset containing 636 tweets annotated with both normalization and Universal Dependencies. The data and all code to reproduce the results in this paper is available at: https:// bitbucket.org/robvanderg/normpar </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of normalization on LAS for the different parsing models on the development data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Output of the normalization model for the example sentence "new pix comming tomoroe"</head><label>1</label><figDesc></figDesc><table>including 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>LAS scores for the Twitter test data. 
* Statistically significant compared to the previous row 
at P &lt; 0.05 using a paired t-test. 

</table></figure>

			<note place="foot" n="2"> www.aspell.net</note>

			<note place="foot" n="3"> Kaljahi et al. (2015) only normalize 3.6%, and we manually normalized the development data from Foster et al. (2011), were even less words were in need of normalization. 4 It should be noted that two other suitable Twitter treebanks in the UD format where created in parallel to our treebank (Liu et al., 2018; Blodgett et al., 2018), which were released after submission of this paper.</note>

			<note place="foot" n="5"> Even when using the best seed on the development data, INTEGRATED results in two-thirds of the performance improvement compared to GOLD.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Gosse Bouma for help with the data annotation, and Antonio Toral, Barbara Plank and the anonymous reviewers for feedback on the paper. This work is part of the 'Parsing Algorithms for Uncertain Input' project sponsored by the Nuance Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An in-depth analysis of the effect of text normalization in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Twitter Universal Dependency parsing for African-American and mainstream American English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Su Lin Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojgan</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Seraji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuko</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Sichinava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katalin</forename><surname>Simionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mária</forename><surname>Simkó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Simková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Strnadová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Sulubacak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Szántó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Trosterud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Trukhina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsarfaty</surname></persName>
		</author>
		<title level="m">Faculty of Mathematics and Physics</title>
		<meeting><address><addrLine>Francis Tyers, Sumire Uematsu, Zdeňka Urešová, Larraitz Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van Niekerk, Gertjan van Noord, Viktor Varga, Eric Villemonte de la Clergerie, Veronika Vincze, Lars Wallin, Jonathan North Washington, Mats Wirén, Tak-sum Wong, Zhuoran Yu, Zdeněk Zabokrtsk´yZabokrtsk´y, Amir Zeldes, Daniel Zeman</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
	<note>and Hanzhi Zhu. 2017. Universal Dependencies 2.1. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (&apos;UFAL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overview of the 2012 shared task on parsing the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A gold standard dependency corpus for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">shared task: Multilingual parsing from raw text to Universal Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Memduh</forename><surname>Gokirmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Nedoluzhko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslava</forename><surname>Hlavacova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Václava</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdenka</forename><surname>Uresova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stina</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><surname>Depaiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Droganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gr C ¸ ¨ Oltekin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<editor>Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fernandez Alcalde, Jana Strnadová, Esha Banerjee, Ruli Manurung, Antonio Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo Mendonca, Tatiana Lando, Rattima Nitisaroj, and Josie Li</editor>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Umut Sulubacak, Hans Uszkoreit, Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marheinecke, Georg Rehm, Tolga; Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive parsercentric text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benny</forename><surname>Kimelfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
