<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Deeper Neural Machine Translation Models with Transparent Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><forename type="middle">Xu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Training Deeper Neural Machine Translation Models with Transparent Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3028" to="3033"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3028</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While current state-of-the-art NMT models, such as RNN seq2seq and Transformers, possess a large number of parameters, they are still shallow in comparison to convolutional models used for both text and vision applications. In this work we attempt to train significantly (2-3x) deeper Transformer and Bi-RNN encoders for machine translation. We propose a simple modification to the attention mechanism that eases the optimization of deeper models, and results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT&apos;14 English-German and WMT&apos;15 Czech-English tasks for both architectures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past few years have seen significant advances in the quality of machine translation systems, ow- ing to the advent of neural sequence to sequence models. While current state of the art models come in different flavours, including Transform- ers ( <ref type="bibr">Vaswani et al., 2017)</ref>, convolutional seq2seq models ( <ref type="bibr" target="#b7">Gehring et al., 2017)</ref> and <ref type="bibr">LSTMs (Chen et al., 2018</ref>), all of these models follow the seq2seq with attention ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) paradigm.</p><p>While revolutionary new architectures have contributed significantly to these quality improve- ments, the importance of larger model capacities cannot be downplayed. The first major improve- ment in NMT quality since the switch to neural models, amongst other factors, was brought about by a huge scale up in model capacity ( <ref type="bibr">Zhou et al., 2016;</ref><ref type="bibr">Wu et al., 2016)</ref>. While there are multi- ple approaches to increase capacity, deeper models have been shown to extract more expressive fea- tures ( <ref type="bibr" target="#b14">Mhaskar et al., 2016;</ref><ref type="bibr" target="#b24">Telgarsky, 2016;</ref><ref type="bibr" target="#b6">Eldan and Shamir, 2015)</ref>, and have resulted in signif- icant gains for vision tasks over the past few years ( <ref type="bibr" target="#b9">He et al., 2015;</ref><ref type="bibr" target="#b22">Srivastava et al., 2015)</ref>. * Equal contribution.</p><p>Despite this being an obvious avenue for im- provement, research in deeper models is often re- stricted by computational constraints. Addition- ally, deep models are often plagued by trainabil- ity concerns like vanishing or exploding gradi- ents ( <ref type="bibr" target="#b2">Bengio et al., 1994)</ref>. These issues have been studied in the context of capturing long range dependencies in recurrent architectures ( <ref type="bibr" target="#b16">Pascanu et al., 2012;</ref><ref type="bibr" target="#b10">Hochreiter et al., 2001</ref>), but resolv- ing these deficiencies in Transformers or LSTM seq2seq models deeper than 8 layers is unfortu- nately under-explored ( <ref type="bibr">Wang et al., 2017;</ref><ref type="bibr" target="#b1">Barone et al., 2017;</ref><ref type="bibr" target="#b5">Devlin, 2017)</ref>.</p><p>In this study we take the first step towards training extremely deep models for translation, by training deep encoders for Transformer and LSTM based models. As we increase the encoder depth the vanilla Transformer models completely fail to train. We also observe sub-optimal performance for LSTM models, which we believe is associ- ated with trainability issues. To ease optimiza- tion we propose an enhancement to the attention mechanism, which allows us to train deeper mod- els and results in consistent gains on the WMT'14 En→De and WMT'15 Cs→En tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transparent Attention</head><p>While the effect of attention on the forward pass is exalted with visualizations and linguistic inter- pretations, its influence on the gradient flow is of- ten forgotten. Consider the original seq2seq model without attention ( <ref type="bibr" target="#b23">Sutskever et al., 2014</ref>). To prop- agate the error signal from the last layer of the de- coder to the first layer of the encoder, it has to pass through multiple time-steps in the decoder, survive the encoder-decoder bottleneck, and pass through multiple time-steps in the encoder, before reach- ing the parameter to be updated. There is some loss of information at every step, especially in the early stages of training. Attention ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) creates a direct path from the de- coder to the topmost layer of the encoder, ensuring its efficient dispersal over time. This increase in inter-connectivity significantly shortens the credit- assignment path <ref type="bibr" target="#b3">(Britz et al., 2017)</ref>, making the network less susceptible to optimization patholo- gies like vanishing gradients.</p><p>For deeper networks the error signal also needs to traverse along the depth of the encoder. We propose an extension to the attention mechanism that behaves akin to creating weighted residual connections along the encoder depth, allowing the dispersal of error signal simultaneously over en- coder depth and time. Using trainable weights, this 'transparent' attention allows the model the flexibility to adjust the gradient flow to different layers in the encoder depending on its training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experimental Setup</head><p>We train our models on the standard WMT'14 En→De dataset. Each sentence is tokenized with the Moses tokenizer before breaking into sub- word units similar to ( <ref type="bibr" target="#b20">Sennrich et al., 2016</ref>). We use a shared vocabulary of 32k units for each lan- guage pair. We report all our results on newstest 2014, and use a combination of newstest 2012 and newstest 2013 for validation. To verify our re- sults, we also evaluate our models on WMT'15 Cs→En. Here we use newstest 2013 for valida- tion and newstest 2015 as the test set. To eval- uate the models we compute BLEU on the tok- enized, true-case output. We report the mean post- convergence score over a window of 21 check- points, obtained using dev performance, following <ref type="bibr" target="#b4">(Chen et al., 2018</ref>). Grad-norm ratio (r t ) vs training step (t) comparison for a 6 layer (blue) and 20 layer (red) RNMT+ model trained on WMT 14 En→De.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Baseline Experiments</head><p>We base our study on two architectures: Trans- former ( <ref type="bibr">Vaswani et al., 2017</ref>) and RNMT+ <ref type="bibr" target="#b4">(Chen et al., 2018</ref>). We choose a smaller version of each model to fit deep encoders with up to 20 layers on a single GPU. All our models are trained on eight P100 GPUs with synchronous training, and optimized using Adam ( <ref type="bibr" target="#b13">Kingma and Ba, 2014</ref>). For both architectures we train four models, with 6, 12, 16 and 20 encoder layers. We use 6 and 8 decoder layers for all our transformers and RNMT+ experiments respectively. We also re- port performance for the standard Transformer Big and RNMT+ setups, as described in <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, for comparison against higher capac- ity models.</p><p>Transformer: We use the latest version of the Transformer base model, using the implementa- tion from <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>. We modify the learning rate schedule to use a learning rate of 3.0 and 40, 000 warmup steps.</p><p>RNMT+: We implemented a smaller version of the En→De RNMT+ model based on the descrip- tion in <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, with 512 LSTM nodes in both encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Analysis</head><p>From <ref type="table" target="#tab_1">Tables 1 and 2</ref>, we notice that the deeper Transformer encoders completely fail to train.</p><p>To understand what goes wrong we keep track of the grad norm ratio r t =</p><formula xml:id="formula_0">h 1 L (t) h N L (t) , t = 1 . . . T , where</formula><formula xml:id="formula_1">L (t)</formula><p>is the loss at time step t, N is the number of layers in the encoder, h 1 is the output of the first encoder layer, h N is the output of the N -th encoder layer, and T is the total number of train- ing steps. We use r t as a diagnostic measure for two reasons: First, it indicates if training is suffer- ing from exploding or vanishing gradients. Sec- ond, when a network is properly trained the lowest layers usually converge quickly, whereas the top- most layers take longer ( <ref type="bibr" target="#b19">Raghu et al., 2017)</ref>. We therefore expect that, for a healthy training pro- cess, r t is relatively large during the early stages of training when updates to lower layers are larger than upper layers. We observe this in most suc- cessful Transformer and RNMT+ training runs. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the r t curves for the 6-layer and 20-layer Transformers. As expected, the shal- low model has a high r t value during early stages of training. For the deep model, however, r t re- mains flat at a much smaller value throughout training. We also observe that r t remains below 1.0 for both models, although the problem seems much less severe for the shallow model.</p><p>From <ref type="table" target="#tab_3">Tables 3 and 4</ref>, we also observe that the performance of deep RNMT+ encoders is not sig- nificantly impacted, reaching the level of the 6 layer model. This is supported by the RNMT+ r t curves in <ref type="figure" target="#fig_1">Figure 2</ref>, which indicate few differences in the learning dynamics of the shallow and deep models. This contrasts with the Transformer ex- periments, where increasing the depth leads to an unstable training process.</p><p>To gain further insights into the stability of the two architectures we completely remove the resid- ual connections from their encoders. Residual connections have been shown, in theory and prac- tice, to improve training stability and performance of deeper networks (see <ref type="bibr" target="#b9">(He et al., 2015;</ref><ref type="bibr" target="#b18">Philipp et al., 2017;</ref><ref type="bibr" target="#b8">Hardt and Ma, 2017;</ref><ref type="bibr" target="#b15">Orhan, 2017)</ref>). Removing residual connections leads to disastrous results for the Transformer, where the training pro- cess either does not converge or results in signifi- cantly worse results. On the other hand, the 6 layer RNMT+ converges with only a slight degradation in quality. Deeper versions of RNMT+ fail to train in the absence of residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Regulating Deep Encoder Gradients with Transparent Attention</head><p>Our baseline experiments reveal that mechanisms to regulate gradient flow can be critical to improv- ing the optimization of deeper encoders. Since the only difference between our shallow and deep models is the number of layers in the encoder, the trainability issues are likely to be associated with gradient flow through the encoder.</p><p>To improve gradient flow we let the decoder at- tend weighted combinations of all encoder layer outputs, instead of just the top encoder layer. Sim- ilar approaches have been found to be useful in deep convolutional networks, for example <ref type="bibr" target="#b21">(Shen and Zeng, 2016;</ref><ref type="bibr" target="#b11">Huang et al., 2016a;</ref><ref type="bibr" target="#b22">Srivastava et al., 2015;</ref><ref type="bibr" target="#b12">Huang et al., 2016b</ref>), but this remains un-investigated in sequence-to-sequence models. We formulate our proposal below.</p><p>Assume the model has N encoder layers and M encoder-decoder attention modules. For Trans- former models each decoder layer attends the en- coder, so M is equivalent to the number of decoder layers (M = 6). For RNMT+, attention is only applied in the first decoder layer, thus M = 1. Let the activations from the i-th encoder layer be {h i t |t = 1 . . . T }, and embeddings be layer 0. Then the traditional attention module attends to {h N t | t = 1 . . . T }. In transparent attention we evaluate M weighted combinations of the encoder outputs, one corresponding to each attention mod-  ule. We define a (N + 1) × M weight vector W , which is learned during training. <ref type="bibr">1</ref> We apply dropout to W since we empirically found it help- ful to stabilize training. We then compute softmax s to normalize the weights.</p><note type="other">En→De WMT 14 Transformer (Base) (Big) Encoder</note><formula xml:id="formula_2">s i,j = e W i,j Σ N k=0 e W k,j , j = 1 . . . M<label>(1)</label></formula><p>We now define</p><formula xml:id="formula_3">z j t = Σ N +1 i=1 s i,j h i t , t = 1 . . . T, j = 1 . . . M<label>(2)</label></formula><p>Now attention module j attends to {z j t | t = 1 . . . T }. Since in RNMT+ a projection is applied to the encoder final layer output, we apply a pro- jection to the weighted combination of encoder outputs before the attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Analysis</head><p>Our results, from tables 1 and 2, indicate that adding transparent attention improves the perfor- mance of most of our transformer experiments, but the gains are most pronounced for deeper models. While the baseline transformer fails to train with 12 layers or deeper encoders, transparent atten- tion allows us to train encoders with up to 20 lay- ers, improving by more than 0.7 BLEU points on both datasets. Relative to Transformer Big, deeper models seem to result in better or comparable per- formance with less than half the model capacity. <ref type="bibr">1</ref> Here +1 is for the embedding layer.</p><p>We also observe gains of 0.7 and 1.0 BLEU for RNMT+ models, on En→De and Cs→En re- spectively, as indicated by <ref type="table" target="#tab_3">Tables 3 and 4</ref>. How- ever, experiments comparing wide models against deeper ones are inconclusive. While deeper mod- els perform slightly better than a wide model with double their capacity on Cs-En, they are clearly out-performed by the larger model on En-De.</p><p>The r t plot in <ref type="figure" target="#fig_2">Figure 3</ref>, also indicates that the learning dynamics now resemble what we expect to see with stable training. We also notice that the scale of r t now resembles that of the RNMT+ model, although the lower layers converge more slowly for the Transformer, possibly because it uses a much smaller learning rate.</p><p>A plot of the weights s i,j , in <ref type="figure" target="#fig_3">Figure 4</ref>, also seems to support our findings. The scalar weights for the lowest embeddings layer grow rapidly in the early stages of training, but once these layers converge the weights for layers 16 and 20 become much larger. The weights for the top few layers re- main comparable at convergence, suggesting that the observed gains in performance might also be partially associated with an ensembling effect of the encoder features, similar to the effect observed in ( <ref type="bibr" target="#b17">Peters et al., 2018</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this work we explore deeper encoders for Trans- former and RNMT+ based machine translation models. We observe that Transformer models are extremely difficult to train when encoder depth is increased beyond 12 layers. While RNMT+ mod- els train with deeper encoders, we did not observe any big performance improvements. We associated the difficulty in training deeper encoders with hindered gradient flow, and re- solved it by proposing the transparent attention mechanism. This enabled us to successfully train deeper Transformer and RNMT+ models, result- ing in consistent gains in translation quality on both WMT'14 En→De and WMT'15 Cs→En.</p><p>Our results show that there is potential for im- provement in translation quality by training deeper architectures, even though they pose optimization challenges. While this study explores training deeper encoders for narrow models, we plan to further study extremely deep and wide models to utilize the full strength of these architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Grad-norm ratio (r t ) vs training step (t) comparison for a 6 layer (blue) and 20 layer (red) Transformer trained on WMT 14 En→De.</figDesc><graphic url="image-1.png" coords="2,72.00,62.81,218.27,134.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Grad-norm ratio (r t ) vs training step (t) comparison for a 6 layer (blue) and 20 layer (red) RNMT+ model trained on WMT 14 En→De.</figDesc><graphic url="image-2.png" coords="2,307.28,62.81,218.27,134.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Grad-norm ratio (r t ) vs training step for 20 layer Transformer with transparent attention.</figDesc><graphic url="image-3.png" coords="3,72.00,62.81,218.27,134.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot illustrating the variations in the learned attention weights s i,6 for the 20 layer Transformer encoder over the training process.</figDesc><graphic url="image-4.png" coords="3,307.28,62.81,218.27,134.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>BLEU scores Cs→En newstest 2015 with Transformers. * indicates that a model failed to train.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>En→De WMT 14 
RNMT+ (512) 
(1024) 
Encoder layers 
6 
12 
16 
20 
6 
Num. Parameters 
128M 165M 191M 216M 379M 
Baseline 
26.63 26.32 26.49 26.33 28.49 
Baseline -residuals 26.37 
* 
* 
* 
N/A 
Transparent 
26.61 26.87 27.07 27.33 
N/A 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BLEU scores on En→De newstest 2014 with RNMT+. * indicates that a model failed to train. 

Cs→En WMT 15 
RNMT+ (512) 
(1024) 
Encoder layers 
6 
12 
16 
20 
6 
Num. Parameters 
128M 165M 191M 216M 379M 
Baseline 
25.77 25.86 26.02 25.75 26.66 
Baseline -residuals 25.43 
* 
* 
* 
N/A 
Transparent 
26.69 26.74 26.79 26.72 
N/A 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>BLEU scores Cs→En newstest 2015 with RNMT+. * indicates that a model failed to train.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>We would like to thank the Google Brain and Google Translate teams for their foundational con-tributions to this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07631</idno>
		<title level="m">Deep architectures for neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Mia Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09849</idno>
		<title level="m">The best of both worlds: Combining recent advances in neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01991</idno>
		<title level="m">Sharp models on dull hardware: Fast and accurate neural machine translation decoding on the cpu</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The power of depth for feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<idno>abs/1512.03965</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Identity matters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1603.09382</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrushikesh</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00988</idno>
		<title level="m">Learning functions: when is deep better than shallow</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Emin</forename><surname>Orhan</surname></persName>
		</author>
		<idno>abs/1701.09175</idno>
		<title level="m">Skip connections as effective symmetry-breaking. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
		<title level="m">Deep contextualized word representations. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradients explode-deep networks are shallow-resnet explained</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<idno>abs/1712.05577</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Svcca: Singular vector canonical correlation analysis for deep understanding and improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05806</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Weighted residuals for very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<idno>abs/1605.08831</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Benefits of depth in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno>abs/1602.04485</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
