<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Associative Multichannel Autoencoder for Multimodal Word Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Associative Multichannel Autoencoder for Multimodal Word Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="115" to="124"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>115</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs. Inspired by the re-constructive and associa-tive nature of human memory, we propose a novel associative multichannel autoencoder (AMA). Our model first learns the associations between textual and perceptual modalities , so as to predict the missing perceptual information of concepts. Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings. Using a gating mechanism our model assigns different weights to each modality according to the different concepts. Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing the meaning of a word is a prereq- uisite to solve many linguistic and non-linguistic problems, such as retrieving words with the same meaning, finding the most relevant images or sounds of a word and so on. In recent years we have seen a surge of interest in building computa- tional models that represent word meanings from patterns of word co-occurrence in corpora <ref type="bibr" target="#b38">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b25">Mikolov et al., 2013;</ref><ref type="bibr" target="#b29">Pennington et al., 2014;</ref><ref type="bibr" target="#b9">Clark, 2015;</ref><ref type="bibr" target="#b42">Wang et al., 2018b</ref>). However, word meaning is also tied to the phys- ical world. Many behavioral studies suggest that human semantic representation is grounded in the external environment and sensorimotor experience ( <ref type="bibr" target="#b23">Landau et al., 1998;</ref><ref type="bibr" target="#b4">Barsalou, 2008)</ref>. This has led to the development of multimodal representa- tion models that utilize both textual and perceptual information (e.g., images, sounds).</p><p>As evidenced by a range of evaluations <ref type="bibr" target="#b3">(Andrews et al., 2009;</ref><ref type="bibr" target="#b8">Bruni et al., 2014;</ref><ref type="bibr" target="#b33">Silberer et al., 2016</ref>), multimodal models can learn bet- ter semantic word representations (a.k.a. embed- dings) than text-based models. However, most ex- isting models still have a number of drawbacks. First, they ignore the associations between modal- ities, and thus lack the ability of information trans- ferring between modalities. Consequently they cannot handle words without perceptual informa- tion. Second, they integrate textual and perceptual representations with simple concatenation, which is insufficient to effectively fuse information from various modalities. Third, they typically treat the representations from different modalities equally. This is inconsistent with many psychological find- ings that information from different modalities contributes differently to the meaning of words <ref type="bibr" target="#b27">(Paivio, 1990;</ref><ref type="bibr" target="#b1">Anderson et al., 2017)</ref>.</p><p>In this work, we introduce the associative multi- channel autoencoder (AMA), a novel multimodal word representation model that addresses all the above issues. Our model is built upon the stacked autoencoder ( <ref type="bibr" target="#b6">Bengio et al., 2007</ref>) to learn seman- tic representations by integrating textual and per- ceptual inputs. Inspired by the re-constructive and associative nature of human memory, we pro- pose two associative memory modules as exten- sions. One is to learn associations between modal- ities (e.g., associations between textual and visual features), so as to reconstruct corresponding per- ceptual information of concepts. The other is to learn associations between related concepts, by re- constructing embeddings of both target words and their associated words. Furthermore, we propose a gating mechanism to learn the importance weights of different modalities to each word.</p><p>To summarize, our main contributions in this work are two-fold:</p><p>• We present a novel associative multichannel autoencoder for multimodal word represen- tation, which is capable of utilizing associa- tions between different modalities and related concepts, and assigning different importance weights to each modality according to differ- ent words. Results on six standard bench- marks demonstrate that our methods outper- form strong unimodal baselines and state-of- the-art multimodal models.</p><p>• Our model successfully integrates cognitive insights of the re-constructive and associative nature of semantic memory in humans, sug- gesting that rich information contained in hu- man cognitive processing can be used to en- hance NLP models. Furthermore, our results shed light on the fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing percep- tual information, associating related concepts and grounding word symbols to external en- vironment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cognitive Grounding</head><p>A large body of research evidences that human se- mantic memory is inherently re-constructive and associative <ref type="bibr" target="#b11">(Collins and Loftus, 1975;</ref><ref type="bibr" target="#b2">Anderson and Bower, 2014</ref>). That is, memories are not exact static copies of reality, but are rather reconstructed from their stimuli and associated concepts each time they are retrieved. For example, when we see a dog, not only the concept itself, but also the cor- responding perceptual information and associated words will be jointly activated and reconstructed. Moreover, various theories state that the different sources of information contribute differently to the semantic representation of a concept ( <ref type="bibr" target="#b40">Wang et al., 2010;</ref><ref type="bibr" target="#b30">Ralph et al., 2017)</ref>. For instance, Dual Cod- ing Theory <ref type="bibr" target="#b19">(Hiscock, 1974)</ref> posits that concrete words are represented in the brain in terms of a perceptual and linguistic code, whereas abstract words are encoded only in the linguistic modality. In these respects, our method employs a re- trieval and representation process analogous to that of humans, in which the retrieval of percep- tual information and associated words is triggered and mediated by a linguistic input. The learned cross-modality mapping and reconstruction of as- sociated words are inspired by the human mental model of associations between different modali- ties and related concepts. Moreover, word mean- ing is tied to both linguistic and physical environ- ment, and relies differently on each modality in- puts ( <ref type="bibr" target="#b41">Wang et al., 2018a</ref>). These are also captured by our multimodal representation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Models</head><p>The existing multimodal representation models can be generally classified into two groups: 1) Jointly training models build multimodal repre- sentations with raw inputs of textual and percep- tual resources. 2) Separate training models inde- pendently learn textual and perceptual representa- tions and integrate them afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Jointly training models</head><p>A class of models extends Latent Dirichlet Alloca- tion ( <ref type="bibr" target="#b7">Blei et al., 2003</ref>) to jointly learn topic distri- butions from words and perceptual units <ref type="bibr" target="#b3">(Andrews et al., 2009;</ref><ref type="bibr" target="#b34">Silberer and Lapata, 2012;</ref><ref type="bibr" target="#b31">Roller and Schulte im Walde, 2013)</ref>. Recently introduced work is an extension of the Skip-gram model ( <ref type="bibr" target="#b25">Mikolov et al., 2013</ref>). For instance,  propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. <ref type="bibr" target="#b24">Lazaridou et al. (2015)</ref> propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. <ref type="bibr" target="#b21">Kiela and Clark (2015)</ref> adopt the MMSkip to learn mul- timodal vectors with auditory perceptual inputs.</p><p>These methods can implicitly propagate percep- tual information to word representations and at the same time learn multimodal representations. However, they utilize raw text corpus in which words having perceptual information account for a small portion. This weakens the effect of introduc- ing perceptual information and consequently leads to the slight improvement of textual vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Separate training models</head><p>The simplest approach is concatenation which fuses textual and visual vectors by concatenat- ing them. It has been proven to be effective in learning multimodal representations ( <ref type="bibr" target="#b8">Bruni et al., 2014;</ref><ref type="bibr" target="#b10">Collell et al., 2017)</ref>. Vari- ations of this method employ transformation and dimension reduction on the concatenation result, including application of singular value decom- position (SVD) ( <ref type="bibr" target="#b8">Bruni et al., 2014</ref>) or canoni- cal correlation analysis (CCA) ( ). There is also work using deep learning methods to project different modality inputs into a common space, including restricted Boltzman machines <ref type="bibr" target="#b26">(Ngiam et al., 2011;</ref><ref type="bibr" target="#b37">Srivastava and Salakhutdinov, 2012)</ref>, autoencoders <ref type="bibr" target="#b35">(Silberer and Lapata, 2014;</ref><ref type="bibr" target="#b33">Silberer et al., 2016)</ref>, and recursive neural net- works ( <ref type="bibr" target="#b36">Socher et al., 2013)</ref>. However, the above methods can only generate multimodal vectors of those words that have perceptual information, thus reducing multimodal vocabulary drastically.</p><p>An empirically superior model addresses this problem by predicting missing perceptual infor- mation firstly. This includes  who utilize the ridge regression method to learn a map- ping matrix from textual modality to visual modal- ity, and <ref type="bibr" target="#b10">Collell et al. (2017)</ref> who employ a feed- forward neural network to learn the mapping re- lation between textual vectors and visual vectors. Applying the mapping function on textual repre- sentations, they obtain the predicted visual vectors for all words in textual vocabulary. Then they cal- culate multimodal representations by concatenat- ing textual and predicted visual vectors. However, the above methods learn separate mapping func- tions and fusion models, which are somewhat in- elegant. In this paper we employ a neural-network mapping function to integrate these two processes into a unified multimodal models.</p><p>According to this classification, our method falls into the second group. However, exist- ing models ignore either the associative relations among modalities, associative relations among rel- ative words, or the different contributions of each modality. This paper aims to integrate more per- ceptual information and the human-like associa- tive memory into a unified multimodal model to learn better word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Associative Multichannel Autoencoder</head><p>We first provide a brief description of the basic multichannel autoencoder for learning multimodal word representations <ref type="figure" target="#fig_0">(Figure 1</ref>). Then we extend the model with two associative memory modules and a gating mechanism <ref type="figure" target="#fig_1">(Figure 2</ref>) in the next sec- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Mutichannel Autoencoder</head><p>An autoencoder is an unsupervised neural net- work which is trained to reconstruct a given in- put from its latent representation <ref type="bibr" target="#b5">(Bengio, 2009)</ref>. In this work, we propose a variant of autoen- coder called multichannel autoencoder, which maps multimodal inputs into a common space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>image2vec</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>word2vec sound2vec</head><p>...</p><p>...  Our model extends the unimodal and bimodal au- toencoder <ref type="bibr" target="#b26">(Ngiam et al., 2011;</ref><ref type="bibr" target="#b35">Silberer and Lapata, 2014</ref>) to induce semantic representations in- tegrating textual, visual and auditory information. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model first transforms input textual vector x t , visual vector x v and audi- tory vector x a to hidden representations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal</head><formula xml:id="formula_0">h t = g(W t x t + b t ) h v = g(W v x v + b v ) h a = g(W a x a + b a ).<label>(1)</label></formula><p>Then the hidden representations are concatenated together and mapped to a common space:</p><formula xml:id="formula_1">h m = g(W m [h t ; h v ; h a ] + b m ).<label>(2)</label></formula><p>The model is trained to reconstruct the hidden representations of the three modalities from the multimodal representation h m :</p><formula xml:id="formula_2">[ ˆ h t ; ˆ h v ; ˆ h a ] = g(W m h m + b ˆ m ),<label>(3)</label></formula><p>and finally to reconstruct the original embeddings of textual, visual and auditory inputs:</p><formula xml:id="formula_3">ˆ x t = g(W t ˆ h t + b ˆ t ) ˆ x v = g(W v ˆ h v + b ˆ v ) ˆ x a = g(W a ˆ h a + b ˆ a ),<label>(4)</label></formula><formula xml:id="formula_4">wherê x t , ˆ x v , ˆ x a are the reconstruction of input vectors x t , x v , x a , andˆhandˆ andˆh t , ˆ h v , ˆ h a are the reconstruction of hidden representa- tions h t , h v , h a . The learning parameters {W t , W v , W a , W t , W v , W a , W m , W m } are weight matrices, {b t , b v , b a , b ˆ t , b ˆ v , b ˆ a , b m , b ˆ m } are bias vectors.</formula><p>Here [· ; ·] denotes the vector concatena- tion, and g denotes the non-linear function which we use tanh(·).</p><p>Training a single-layer autoencoder corre- sponds to optimizing the learning parameters to minimize the overall loss between inputs and their reconstructions. Following <ref type="bibr" target="#b39">(Vincent et al., 2010)</ref>, we use squared loss:</p><formula xml:id="formula_5">min θ 1 n i=1 (||x i t − ˆ x i t || 2 + ||x i v − ˆ x i v || 2 + ||x i a − ˆ x i a || 2 ),<label>(5)</label></formula><p>where i denotes the i th word, and the model pa-</p><formula xml:id="formula_6">rameters are θ 1 = {W t , W v , W a , W m , W t , W v , W a , W m , b t , b v , b a , b m , b ˆ t , b ˆ v , b ˆ a , b ˆ m }.</formula><p>Autoencoders can be stacked to create deep net- works. To enhance the quality of semantic repre- sentations, we employ a stacked multichannel au- toencoder, which is composed of multiple hidden layers that are stacked together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integrating Modality Associations</head><p>In reality, the words that have corresponding im- ages or sounds are only a small subset of the tex- tual vocabulary. To obtain the perceptual vec- tors for each word, we need associations between modalities (i.e., text-to-vision and text-to-audition mapping functions), that transform the textual vec- tors into visual and auditory ones. Previous meth- ods learn separate mapping functions and fusion models, which are somewhat inelegant. Here we employ a neural-network mapping function to in- corporate this modality association module into multimodal models.</p><p>Take text-to-vision mapping as an example. Suppose that T ∈ R mt×nt is the textual repre- sentation containing m t words, V ∈ R mv×nv is the visual representation containing m v ( m t ) words, where n t and n v are dimensions of the tex- tual and visual representations respectively. The textual and visual representations of the i th con- cept are denoted as T i and V i respectively. Our goal is to learn a mapping function f : g(W p T + b p ) from textual to visual space such that the pre- diction f (T i ) is similar to the actual visual vec- tor V i . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Association word</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample-specific gate</head><p>In case you need, we've collected the cutest small dog breeds to lift your mood.</p><p>There's nothing that cheers you up quite as fast as a cute dog doing something peculiar. are used to learn the mapping function. To train the model, we employ a square loss:</p><formula xml:id="formula_7">min θ 2 mv i=1 ||f (T i ) − V i || 2 ,<label>(6)</label></formula><p>where the training parameters are θ 2 = {W p , b p }. We adopt the same method to learn the text-to- audition mapping function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integrating Word Associations</head><p>Word associations are a proxy for an aspect of human semantic memory that is not sufficiently captured by the usual training objectives of multi- modal models. Therefore we assume that incorpo- rating the objective of word associations helps to learn better semantic representations. To achieve this, we propose to reconstruct the vector of as- sociated word from the corresponding multimodal semantic representation. Specifically, in the de- coding process we change the equation <ref type="formula" target="#formula_2">(3)</ref> to:</p><formula xml:id="formula_8">[ ˆ h t , ˆ h v , ˆ h a , ˆ h asc ] = g(W m h m + b ˆ m ),<label>(7)</label></formula><p>and equation <ref type="formula" target="#formula_3">(4)</ref> to:</p><formula xml:id="formula_9">ˆ x t = g(W t ˆ h t + b ˆ t ) ˆ x v = g(W v ˆ h v + b ˆ v ) ˆ x a = g(W a ˆ h a + b ˆ a ) ˆ x asc = g(W ascˆhascˆ ascˆh asc + b asc ).<label>(8)</label></formula><p>To train the model, we add an additional ob- jective function, which is the mean square error between the embeddings of the associated word y and their re-constructive embeddingsˆxembeddingsˆ embeddingsˆx asc :</p><formula xml:id="formula_10">min θ 3 n i=1 ||y i − ˆ x i asc || 2 ,<label>(9)</label></formula><p>where y i and x i are the embeddings of a pair of associated words. Here, y is the concatenation of three unimodal vectors [y t ; y v ; y a ]. The pa- rameters of word association module are</p><formula xml:id="formula_11">θ 3 = {W t , W v , W a , W m , ˆ W m , W asc , b t , b v , b a , b m , b ˆ m , b asc }.</formula><p>This additional criterion drives the learn- ing towards a semantic representation capable of reconstructing its associated representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Integrating a Gating Mechanism</head><p>Considering that the meaning of each word has different dependencies on textual and perceptual information, we propose the sample-specific gate to assign different weights to each modality ac- cording to different words. The weight parame- ters are calculated by the following feed-forward neural networks:</p><formula xml:id="formula_12">g t = g(W gt x t + b gt ) g v = g(W gv x v + b gv ) g a = g(W ga x a + b ga ),<label>(10)</label></formula><p>where g t , g v and g a are value or vector gate of tex- tual, visual and auditory representations respec- tively. For the value gate, W gt , W gv and W ga are vectors, and b gt , b gv and b ga are value parameters. For the vector gate, the parameters W gt , W gv and W ga are matrices, b gt , b gv and b ga are vectors. The value gate controls the importance weights of dif- ferent input representations as a whole, whereas the vector gate can adjust the importance weights of each dimension of input representations.</p><p>Finally, we compute element-wise multiplica- tion of the textual, visual and auditory represen- tations with their corresponding gates:</p><formula xml:id="formula_13">x gt = x t g t x gv = x v g v x ga = x a g a .<label>(11)</label></formula><p>The x gt , x gv and x ga can be seen as the weighted textual, visual and auditory representations. The parameters of our gating mechanism is trained to- gether with that of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Training</head><p>To train the AMA model, we use overall objec- tive function of equation (5) + (6) + (9). In the training phase, model inputs are textual vectors, the corresponding visual vectors, auditory vectors, and association words <ref type="figure" target="#fig_1">(Figure 2</ref>). In the testing phase, we only need textual inputs to generate multimodal word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Textual vectors. We use 300-dimensional GloVe vectors 1 which are trained on the Common Crawl corpus consisting of 840B tokens and a vocabulary of 2.2M words 2 .</p><p>Visual vectors. Our source of visual vectors are collected from ImageNet ( <ref type="bibr" target="#b32">Russakovsky et al., 2015</ref>) which covers a total of 21,841 WordNet synsets <ref type="bibr" target="#b13">(Fellbaum, 1998</ref>) that have 14,197,122 im- ages. For our experiments, we delete words with fewer than 50 images or words not in the Glove vectors, and sample at most 100 images for each word. To generate a visual vector for each word, we use the forward pass of a pre-trained VGG- net model <ref type="bibr">3</ref> and extract the hidden representation of the last layer as the feature vector. Then we use averaged feature vectors of the multiple im- ages corresponding to the same word. Finally, we get 8,048 visual vectors of 128 dimensions.</p><p>Auditory vectors. For auditory data, we gather audio files from Freesound 4 , in which we select words with more than 10 audio files and sample at most 50 sounds for one word. To extract auditory features, we use the VGG-net model which is pre- trained on Audioset 5 . The final auditory vectors are averaged feature vectors of multiple audios of the same word, which contains 9,988 words of 128 dimensions <ref type="bibr">6</ref> .</p><p>Word associations. We use the word associ- ation data collected by <ref type="bibr" target="#b12">(De Deyne et al., 2016)</ref>, in which each word pair is generated by at least one subject <ref type="bibr">7</ref> . This dataset includes mostly words with similar meaning (e.g., occasionally &amp; some- times, adored &amp; loved, supervisor &amp; boss) and re- lated words <ref type="bibr">(e.g., eruption &amp; volcano, cortex &amp; brain, umbrella &amp; rain)</ref>. We calculate the associ- ation score for each word pair (cue word + target word) as: the number of person who generated the word pair divided by the total number of people who were presented with the cue word. For train- ing, we select pairs of associated words above a threshold of 0.15 and delete those that are not in the Glove vocabulary, which results in 7,674 word association data sets 8 . For the development set, we randomly sample 5,000 word association col- lections together with their association scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Settings</head><p>Our models are implemented with PyTorch ( <ref type="bibr" target="#b28">Paszke et al., 2017)</ref>, optimized with Adam ( <ref type="bibr" target="#b22">Kingma and Ba, 2014</ref>). We set the initial learn- ing rate to 0.05, and batch size to 64. We tune the number of layers over 1, 2, 3, the size of multi- modal vectors over 100, 200, 300, and the size of each layer in textual channel over <ref type="bibr">300,</ref><ref type="bibr">250,</ref><ref type="bibr">200,</ref><ref type="bibr">150,</ref><ref type="bibr">100</ref> and in visual/auditory channel over 128, 120, 90, 60. We train the model for 500 epochs and select the best parameters on the development set. All models are trained for 3 times and the av- erage results are reported in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>To test the effect of each module, we sep- arately train the following models: multichan- nel autoencoder with modality association (AMA- M), with modality and word associations (AMA- MW), with modality and word associations plus value/vector gate (AMA-MW-Gval/vec).</p><p>For AMA-M model, we initialize the text-to- vision and text-to-audition mapping functions with pre-trained mapping matrices, which are parameters of one-layer feed-forward neural networks. The network uses input of the textual vectors, output of visual or auditory vectors, and is trained with SGD for 100 epochs. We initialize the network biases as zeros and network weights with He-initialisation ( <ref type="bibr" target="#b15">He et al., 2015)</ref>. The best parameters of AMA-M model are 2 hidden layers, with textual channel size of 300, 250 and 150, visual/auditory channel size of 128, 90, 60. For AMA-MW model, we use the best AMA-M model parameters as initialization, and train the model with word association data. The optimal parameter of association channel size is 300, 350, 556 (or 428 for bimodal inputs). For AMA-MW-Gval and AMA-MW-Gvec, we adopt the same training strategy as AMA-MW model. The code for training and evaluation can be found at: https://github.com/wangshaonan/ Associative-multichannel-autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Tasks</head><p>We test the baseline and proposed models on six standard evaluation benchmarks, covering two dif- ferent tasks: (i) Semantic relatedness: Men-3000 ( <ref type="bibr" target="#b8">Bruni et al., 2014</ref>) and Wordrel-252 ( <ref type="bibr" target="#b0">Agirre et al., 2009)</ref>; (ii) Semantic similarity: Simlex-999 ( , Semsim-7576 <ref type="bibr" target="#b35">(Silberer and Lapata, 2014</ref>), <ref type="bibr">Wordsim-203 and Simverb-3500 (Gerz et al., 2016</ref>). All test sets contain a list of word pairs along with their subject ratings.</p><p>We employ Spearman's correlation method to evaluate the performance of our models. This method calculates the correlation coefficients be- tween model predictions and subject ratings, in which the model prediction is the cosine similarity between semantic representations of two words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Multimodal Models</head><p>Most of existing multimodal models only utilize textual and visual modalities. For fair compari- son, we re-implement several representative sys- tems with our own textual and visual vectors. The Concatenation (CONC) model <ref type="bibr" target="#b20">(Kiela and Bottou, 2014</ref>) is simple concatenation of normalized textual and visual vectors. The Mapping ( <ref type="bibr" target="#b10">Collell et al., 2017)</ref> and <ref type="bibr">Ridge (Hill et al., 2014</ref>) mod- els first learn a mapping matrix from textual to vi- sual modality using feed-forward neural network and ridge regression respectively. After applying the mapping function on the textual vectors, they obtain the predicted visual vectors for all words in textual vocabulary. Then they concatenate the normalized textual and predicted visual vectors to get multimodal word representations. The SVD ( <ref type="bibr" target="#b8">Bruni et al., 2014</ref>) and CCA ( ) models first concatenate normalized textual and visual vectors, and then conduct SVD or CCA transformations on the concatenated vectors.</p><p>For multimodal models with textual, visual and </p><note type="other">Spearman's correlations between model predictions and human ratings on six evaluation datasets.</note><p>Here T, V, A denote textual, visual and auditory. TV denotes bimodal inputs of textual and visual. TVA denotes trimodal inputs of textual, visual and auditory. The bold scores are the best results per column in bimodal models and trimodal models respectively. For each test, ALL corresponds to the whole testing set, V/A to those word pairs for which we have textual&amp;visual vectors in bimodal models or textual&amp;visual&amp;auditory in trimodal models, and ZS (zero-shot) denotes word pairs for which we have only textual vectors. The #inst. denotes the number of word pairs.  auditory inputs, we implement CONC and Ridge as baseline models. The trimodal CONC model simply concatenates normalized textual, visual and auditory vectors. The trimodal Ridge model first learns text-to-vision and text-to-audition map- ping matrices with ridge regression method. Then it applies the mapping functions on the textual vec- tors to get the predicted visual and auditory vec- tors. Finally, the normalized textual, predicted- visual and predicted-auditory vectors are concate- nated to get the multimodal representations.</p><formula xml:id="formula_14">MEN SIMLEX SEMSIM SIMVERB WORDSIMM WORDREL ALL V/A ZS ALL V/A ZS ALL V/A ZS ALLL V/A ZS ALL V/A ZS ALL V/A ZS Kiela &amp; Bottou 2014 - 0.72 - - - - - - - - - - - - - - - Silberer &amp; Lapata 2014 - - - - - - 0.70 - - - - - - - - - - - Lazaridou</formula><p>All above baseline models are implemented with Sklearn <ref type="bibr">9</ref> . Same as the proposed AMA model, <ref type="bibr">9</ref> http://scikit-learn.org/ the hyper-parameters of baseline models are tuned on the development set using Spearman's correla- tion method. In Ridge model, the optimal regular- ization parameter is 0.6. The Mapping model is trained with SGD for maximum 100 epochs with early stopping, and the optimal learning rate is 0.001. The output dimension of SVD and CCA models are 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Discussion</head><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, we divide all models into six groups: (1) existing multimodal models (with textual and visual inputs) in which results are reprinted from <ref type="bibr" target="#b10">Collell et al. (2017)</ref>. <ref type="formula" target="#formula_1">(2)</ref> Unimodal models with textual, (predicted) visual or (pre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dicted) auditory inputs. (3) Our re-implementation of baseline bimodal models with textual and vi- sual inputs (TV). (4) Our AMA models with tex- tual and visual inputs. (5) Our implementation of trimodal baseline models with textual, visual and auditory inputs (TVA). (6) Our AMA model with textual, visual and auditory inputs.</head><p>Overall performance Our AMA models (in group 4 and 6) clearly outperform their baseline unimodal and multimodal models (in group 2, 3 and 5). We use Wilcoxon signed-rank test to check if significant difference exists between two mod- els. Results show that our multimodal models per- form significantly better (p &lt; 0.05) than all base- line models.</p><p>As shown clearly, our bimodal and trimodal AMA models achieve better performance than baselines in both V/A (visual or auditory, the test- ing data that have associated visual or auditory vectors) and ZS (zero-shot, the testing data that do not have associated visual or auditory vectors) re- gion. In other words, our models outperform base- line models on words with or without perceptual information. The good results in ZS region also indicate that our models have good generalization capacity. Unimodal baselines As shown in group 2, the Glove vectors are much better than CNN- visual and CNN-auditory vectors, in which CNN- auditory has the worst performance on capturing concept similarities. Comparing with visual and auditory vectors, the predicted visual and auditory vectors achieve much better performance. This in- dicates that the predicted vectors contain richer in- formation than purely perceptual representations and are more useful for building semantic repre- sentations. Multimodal baselines For bimodal models (group 3), the CONC model that combines Glove and visual vectors performs worse than Glove on four out of six datasets, suggesting that simple concatenation might be suboptimal. The Mapping and Ridge models, which combine Glove and pre- dicted visual vectors, improve over Glove on five out of six datasets in ALL regions. This reinforces the conclusion that the predicted visual vectors are more useful in building multimodal models. The SVD model gets similar results as Ridge model. The CCA model maps different modality inputs into a common space, achieving better results on some datasets and worse results on the others.</p><p>The improvement on three benchmark tests shows the potential of mapping multimodal inputs into a common space.</p><p>The above results can also be observed in the tri- modal CONC and Ridge models (group 5). Over- all, the trimodal models, which utilize additional auditory inputs, get slightly worse performance than bimodal models. This is partly caused by the fusion method of concatenation. Note that our proposed AMA models are more effective with tri- modal inputs as shown in group 6. Our multimodal models With either bimodal or trimodal inputs, the proposed AMA-M model out- performs all baseline models by a large margin. Specifically our AMA-M model achieves an rela- tive improvement of 4.1% on average (4.5% with trimodal inputs) over the state-of-the-art Ridge model. This illustrates that our AMA models can productively combine textual and perceptual rep- resentations. Moreover, our AMA-MW model, which employs word associations, achieves an av- erage improvement of 1.5% (2.7% with trimodal inputs) over the AMA-M model. That is to say, the representation ability of multimodal models can be clearly improved by learning associative relations between words. Furthermore, the AMA- MW-Gval model improves the AMA-MW model by 1.3% (0.3% with trimodal inputs) on average, illustrating that the gating mechanism (especially the value gate) helps to learn better semantic rep- resentations.</p><p>In addition, we explore the effect of word asso- ciation data size. We find that the decrease of as- sociation data has no discernible effect on model performance: when using 100%, 80%, 60%, 40%, 20% of the data, the average results are 0.6479, 0.6409, 0.6361, 0.6430, 0.6458 in bimodal model. The same trend is observed in trimodal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have proposed a cognitively-inspired multi- modal model -associative multichannel autoen- coder -which utilizes the associations between modalities and related words to learn multimodal word representations. Performance improvement on six benchmark tests shows that our models can efficiently fuse different modality inputs and build better semantic representations.</p><p>Ultimately, the present paper sheds light on the fundamental questions of how to learn word mean- ings, such as the plausibility of reconstructing per-ceptual information, associating related concepts and grounding word symbols to external environ- ment. We believe that one of the promising fu- ture directions is to learn from how humans learn and store semantic word representations to build a more effective computational model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the multichannel autoencoder with inputs of textual, visual and auditory sources.</figDesc><graphic url="image-19.png" coords="3,426.13,208.91,136.34,86.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the proposed associative multichannel autoencoder.</figDesc><graphic url="image-179.png" coords="4,170.68,216.14,136.34,86.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/projects/ glove 2 We have tried skip-gram vectors and get the same conclusions. 3 http://www.vlfeat.org/matconvnet/ 4 http://www.freesound.org/ 5 https://research.google.com/audioset 6 We build auditory vectors with the released code at: https://github.com/tensorflow/models/ tree/master/research/audioset</note>

			<note place="foot" n="7"> The dataset can be found at: https:// simondedeyne.me/data. 8 We have done experiments with Synonyms (which are extracted from WordNet and PPDB corpora), and the results are not as good as using word associations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The research work descried in this paper has been supported by the National Key Research and De-velopment Program of China under Grant No. 2017YFB1002103 and also supported by the Nat-ural Science Foundation of China under Grant No. 61333018. The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visually grounded and textual semantic models differentially decode brain activity associated with concrete and abstract nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Human associative memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Psychology press</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Integrating experiential and distributional data to learn semantic representations. Psychological review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grounded cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="617" to="645" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Vector space models of lexical meaning. Handbook of Contemporary Semantic Theory, The</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="493" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagined visual representations as multimodal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Collell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teddy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A spreading-activation theory of semantic processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="407" to="428" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Predicting human similarity judgments with distributional models: The value of word associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Simon De Deyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel J</forename><surname>Perfors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navarro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>COLING</publisher>
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simverb-3500: A largescale evaluation set of verb similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00869</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning abstract concept embeddings from multi-modal data: Since you probably can&apos;t see what i mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-modal models for concrete and abstract concept meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="296" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagery and verbal processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merrill</forename><surname>Hiscock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psyccritiques</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">487</biblScope>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning image embeddings using convolutional neural networks for improved multi-modal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-and cross-modal semantics beyond vision: Grounding in auditory perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2461" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object perception and object naming in early development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="24" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mental representations: A dual coding approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Paivio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Soumith Chintala, and Gregory Chanan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Pytorch</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The neural and computational bases of semantic cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Matthew A Lambon Ralph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karalyn</forename><surname>Jefferies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy T</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multimodal lda model integrating textual, cognitive and visual modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1146" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<title level="m">Visually grounded meaning representations. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Grounded models of semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1423" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural representation of abstract and concrete concepts: A meta-analysis of neuroimaging studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Conder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">N</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><forename type="middle">V</forename><surname>Shinkareva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1459" to="1468" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Investigating inner properties of multimodal representation and semantic compositionality with brain-based componential semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5964" to="5972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning multimodal word representation via dynamic fusion methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5973" to="5980" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
