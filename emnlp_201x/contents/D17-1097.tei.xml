<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Promise of Premise: Harnessing Question Premises in Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aroma</forename><surname>Mahendru</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viraj</forename><surname>Prabhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akrit</forename><surname>Mohapatra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<email>dbatra@gatech.edu, steflee@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Tech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Promise of Premise: Harnessing Question Premises in Visual Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="926" to="935"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we make a simple observation that questions about images often contain premises-objects and relationships implied by the question-and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers. We note that a visual question is irrelevant to an image if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of providing natural language answers to free-form questions about an image -i.e. Visual Question Answering (VQA) -has received sub- stantial attention in the past few years ( <ref type="bibr" target="#b16">Malinowski and Fritz, 2014;</ref><ref type="bibr" target="#b3">Antol et al., 2015;</ref><ref type="bibr" target="#b17">Malinowski et al., 2015;</ref><ref type="bibr" target="#b27">Zitnick et al., 2016;</ref><ref type="bibr" target="#b10">Kim et al., 2016;</ref><ref type="bibr" target="#b15">Lu et al., 2016</ref>; Andreas et al., * Denotes equal contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Questions asked about images often contain</head><p>'premises' that imply visual semantics. From the above ques- tion, we can infer that a relevant image must contain a man, a racket, and that the man must be holding the racket. We extract these premises from visually grounded questions and use them to construct a new dataset and models for question relevance prediction. We also find that augmenting standard VQA training with simple premise-based questions results in improvements on tasks requiring compositional reasoning.</p><p>2016; <ref type="bibr" target="#b14">Lu et al., 2017)</ref> and has quickly become a popular problem area. Despite significant progress on VQA benchmarks ( <ref type="bibr" target="#b3">Antol et al., 2015)</ref>, current models still present a number of unintelligent and problematic tendencies.</p><p>When faced with questions that are irrelevant or not applicable for an image, current 'forced choice' models will still produce an answer. For example, given an image of a dog and a query "What color is the bird?", standard VQA models might answer "Red" confidently, based solely on language biases in the training set (i.e. an over- abundance of the word "red"). In these cases, the predicted answers are senseless at best and mis- leading at worst, with either case posing serious problems for real-world applications. Like <ref type="bibr" target="#b20">Ray et al. (2016)</ref>, we argue that practical VQA sys- tems must be able to identify and explain irrelevant questions. For instance, a more intelligent VQA model with this capability might answer "There is no bird in the image" for this example.</p><p>Premises. In this paper, we show that question premises -i.e. objects and relationships implied by a question -can enable VQA models to respond more intelligently to irrelevant or previously un- seen questions. We develop a premise extraction pipeline based on SPICE ( <ref type="bibr" target="#b1">Anderson et al., 2016)</ref> and demonstrate how these premises can be used to improve modern VQA models in the face of ir- relevant or previously unseen questions.</p><p>Concretely, we define premises as facts implied by the language of questions, for example the question "What brand of racket is the man hold- ing?" shown in <ref type="figure">Fig. 1</ref> implies the existence of a man, a racket, and that the man is holding the racket. For visually grounded questions (i.e. those asked about a particular image) these premises im- ply visual qualities, including the presence of ob- jects as well as their attributes and relationships.</p><p>Broadly speaking, we explore the usefulness of premises in two settings -when visual questions are known to be relevant to the images they are asked on (e.g. in the VQA dataset) and in real- life situations where such an assumption cannot be made (e.g. when generated by visually im- paired users). In the former case, we show that knowing that a question is relevant allows us to perform data augmentation by creating additional simple question-answer pairs using the premises of source questions. In the latter case, we show that explicitly reasoning about premises provides an effective and interpretable way of determining whether a question is relevant to an image.</p><p>Irrelevant Question Detection. We consider a question to be relevant to an image if all of the question's premises apply to the corresponding image, that is to say all objects, attributes, and interactions implied by the question are depicted in the image. We refer to premises that apply for a given image as true premises and those that do not apply as false premises. In order to train and evaluate models for this task, we curate a new ir- relevant question detection dataset which we call the Question Relevance Prediction and Explana- tion (QRPE) dataset. QRPE is automatically cu- rated from annotations already present in existing datasets, requiring no additional labeling.</p><p>We collect the QRPE dataset by taking each image-question pair in the VQA dataset <ref type="bibr" target="#b3">(Antol et al., 2015)</ref> and finding the most visually simi- lar other image for which exactly one of the ques- tion premises is false. In this way, we collect tu- ples consisting of two images, a question, and a premise where the question is relevant for one im- age and not for the other due to the premise being false.</p><p>For context, the only other existing irrelevant question detection dataset ( <ref type="bibr" target="#b20">Ray et al., 2016)</ref> col- lected irrelevant question-image pairs by human verification of random pairs. In comparison, QRPE is substantially larger, balanced between irrelevant and relevant examples, and presents a considerably more difficult task due to the close- ness of the image pairs both visually and with re- spect to question premises.</p><p>We train novel models for irrelevant question detection on the QRPE dataset and compare to ex- isting methods. In these experiments, we show that models that explicitly reason about question premises consistently outperform baseline models that do not.</p><p>VQA Data Augmentation. Finally, we also in- troduce an approach to generate simple, templated question-answer pairs about elementary concepts from premises of complex training questions. In initial experiments, we show that adding these simple question-answer pairs to VQA training data can improve performance on tasks requiring compositional reasoning. These simple questions improve training by bringing implicit training con- cepts "to the surface", i.e. introducing direct su- pervision of important implicit concepts by trans- forming them to simple training pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual Question Answering: Starting from simple bag-of-word and CNN+LSTM models ( <ref type="bibr" target="#b3">Antol et al., 2015)</ref>, VQA architectures have seen considerable innovation. Many top-performing models integrate attention mechanisms (over the image, the question, or both) to focus on impor- tant structures ( <ref type="bibr" target="#b5">Fukui et al., 2016;</ref><ref type="bibr" target="#b15">Lu et al., 2016</ref><ref type="bibr" target="#b14">Lu et al., , 2017</ref>, and some have been designed with com- positionality in mind ( <ref type="bibr" target="#b2">Andreas et al., 2016;</ref><ref type="bibr" target="#b19">Hendricks et al., 2016)</ref>. However, improving compo- sitionality or performance through data augmenta- tion remains a largely unstudied area.</p><p>Some other recent work has developed models which produce natural language explanations for their outputs <ref type="bibr" target="#b24">Wang et al., 2016)</ref>, but there has not been work on generating expla- nations for irrelevant questions or false premises.</p><p>Question Relevance: Most related to our work is that of <ref type="bibr" target="#b20">Ray et al. (2016)</ref>, which introduced the task of irrelevant question detection for VQA. To evaluate on this task, they created the Visual True and False Question (VTFQ) dataset by pair- ing VQA questions with random VQA images and having human annotators verify whether or not the question was relevant. As a result, many of the ir- relevant image-question pairs exhibit a complete mismatch of image and question content. Our Question Relevance Prediction and Explanation (QRPE) dataset on the other hand is collected such that irrelevant images for each question closely re- semble the source image both visually and seman- tically. We also provide premise-level annotations which can be used to develop models that not only decide whether a question is relevant, but also pro- vide explanations for why that is the case.</p><p>Semantic Tuple Extraction: Extracting struc- tured facts in the form of semantic tuples from text is a well studied problem ( <ref type="bibr" target="#b22">Schuster et al., 2015;</ref><ref type="bibr" target="#b1">Anderson et al., 2016;</ref><ref type="bibr" target="#b4">Elhoseiny et al., 2016)</ref>; however, recent work has begun extending these techniques to visual domains ( <ref type="bibr" target="#b26">Xu et al., 2017;</ref><ref type="bibr" target="#b8">Johnson et al., 2015)</ref>. Additionally, the Visual Genome ( <ref type="bibr" target="#b11">Krishna et al., 2016</ref>) dataset contains dense image annotations for objects and their at- tributes and relationships. However, we are the first to consider these facts to reason about ques- tion relevancy and compositionality in VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extracting Premises of a Question</head><p>In Section 1, we introduced the concept of premises and how they can be used. We now for- malize this concept and explain how premises can be extracted from questions.</p><p>We define question premises as facts implied about an image from a question asked about it, which we represent as tuples. Returning to our running example question "What brand of racket is the man holding?", we can express these premises as the tuples '&lt;man&gt;', '&lt;racket&gt;', and '&lt;man, holding, racket&gt;' respectively. We cat- egorize these tuples into three groups based on their complexity. First-order premises represent- ing the presence of objects ('&lt;man&gt;', '&lt;cat&gt;', '&lt;sky&gt;'), second-order premises capturing the at- tributes of objects ('&lt;man, tall&gt;', '&lt;car, mov- ing&gt;'), and third-order premises containing in- teractions between objects (e.g. '&lt;man, kicking, ball&gt;', '&lt;cat, above, car&gt;').  <ref type="figure">Figure 2</ref>: Premise Extraction Pipeline. Objects (gray), at- tributes (green), and relations (blue) scene graph nodes are converted into 1st, 2nd, and 3rd order premises respectively.</p><p>Premise Extraction: To extract premises from questions, we use the semantic tuple extraction pipeline used in the SPICE metric <ref type="bibr" target="#b1">(Anderson et al., 2016)</ref>. Originally defined as a metric for image captioning, SPICE transforms a sentence into a scene graph using the Stanford Scene Graph Parser ( <ref type="bibr" target="#b22">Schuster et al., 2015)</ref> and then extracts semantic tuples from this representation. <ref type="figure">Fig. 2</ref> shows this process for a sample question. The question is represented as a graph of objects, at- tributes, and relationships from which first, sec- ond, and third order premises are extracted respec- tively. As this pipeline was originally designed for descriptive captions rather than questions, we found a number of minor modifications helpful in extracting quality question premises, including disabling pronoun resolution, verb lemmatization and METEOR-based Synset matching. We will release our premise extraction code publicly to en- courage reproducibility.</p><p>While this extraction process typically pro- duces high quality premise tuples, there are some sources of noise which must be filtered out. The SPICE process occasionally produces duplicate nodes or object nodes not linked to nouns in the question, which we filter out. We also remove premises containing words like photo, image, etc. that refer to the image rather than its content.</p><p>A more nuanced source of erroneous premises comes from the ambiguity in existential questions, i.e. those about the existence of certain image con- tent. For example, while the question "Is the lit- tle girl moving?" contains the premise '&lt;girl, little&gt;', it is unclear without the answer whether '&lt;girl, moving&gt;' is also a premise. Similarly, for the question "How many giraffes are in the im- age?", '&lt;giraffe, many&gt;' cannot be considered a premise as there may be 0 giraffes in the image. To avoid introducing false premises, we filter out existential and counting questions. For a given question Q and a relevant image I + , we find an irrelevant image I − for which exactly one premise P of the question is false. If there are multiple such candidates, we select the candidate most visually most similar to I + . As can be seen from these examples, the QRPE dataset is very challenging, with only minor visual and semantic differences separating the relevant and irrelevant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Question Relevance Prediction and Explanation (QRPE) Dataset</head><p>As discussed in Section 1, modern VQA models fail to differentiate between relevant and irrele- vant questions, answering either with confidence. This behavior is detrimental to the real world ap- plication of VQA systems. In this section, we cu- rate a new dataset for question relevance in VQA which we call the Question Relevance Prediction and Explanation (QRPE) dataset. We plan to re- lease QRPE publicly to help future efforts. In order to train and evaluate models for irrele- vant question detection, we would like to create a dataset of tuples (I + , Q, P, I − ) comprised of a natural language question Q, an image I + for which Q is relevant, and an image I − for which Q is irrelevant because premise P is false. While it is not required to collect both a relevant and irrele- vant image for each question, we argue that doing so is a simple way to balance the dataset and it ensures that biases against rarer questions (which would be irrelevant for most images) cannot be ex- ploited to inflate performance.</p><p>We base our dataset on the existing VQA corpus ( <ref type="bibr" target="#b3">Antol et al., 2015)</ref>, taking the human-generated (and therefore relevant) image-question pairs from VQA as I + and Q. As previously discussed, we can define the relevancy of a question in terms of the validity of its premises for an image, so we extract premises from each question Q and must find a suitable irrelevant image I − . However, there are certainly many images for which one or more of Q's premises are false and an important design decision is then how to select I − from this set.</p><p>To ensure our dataset is as realistic and chal- lenging as possible, we consider irrelevant images which only have a single false question premise under Q which we denote P . For example, the question "Is the big red dog old?" could be matched with an image containing a big, white dog or a small red dog, but not a small white dog. In this way, we ensure that image content is seman- tically appropriate for the question topic but not quite relevant. Additionally, this provides each irrelevant image with an explanation for why the question does not apply.</p><p>Furthermore, we sort this subset of irrelevant image by their visual distance to the source image I + based on image encodings from a VGGNet <ref type="bibr" target="#b23">(Simonyan and Zisserman, 2014</ref>) pretrained on Ima- geNet ( <ref type="bibr" target="#b21">Russakovsky et al., 2012)</ref>. This ensures that the relevant and irrelevant images are visually similar and act as difficult examples.</p><p>A major difficulty with our proposed data col- lection process is how to verify whether a premise if true or false for any given image in order to iden- tify irrelevant images. We detail dataset construc- tion and our approach for this problem in the fol- lowing section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Construction</head><p>We curate our QRPE dataset automatically from existing annotations in COCO ( <ref type="bibr" target="#b12">Lin et al., 2014</ref>) and Visual Genome ( <ref type="bibr" target="#b11">Krishna et al., 2016)</ref>. COCO is a set of over 300,000 images annotated with ob- ject segmentations and presence information for 80 classes as well as text descriptions of image content. Visual Genome builds on this dataset, providing more detailed object, attribute, and rela- tionship annotations for over 100,000 COCO im- ages. We make use of these data sources to extract first and second order premises from VQA ques- tions which are also based on COCO images. For first order premises (i.e. existential premises), we consider only the 80 classes present in COCO ( <ref type="bibr" target="#b12">Lin et al., 2014</ref>). As VQA and COCO share the same images, we can easily determine if a first order premise is true or false for a candidate irrelevant image simply by checking for the absence of the appropriate class annotation.</p><p>For second order premises (i.e. attributed ob- jects), we rely on Visual Genome ( <ref type="bibr" target="#b11">Krishna et al., 2016)</ref> annotations for object and attribute labels. Unlike in COCO, the lack of a particular object la- bel in an image for Visual Genome does not nec- essarily indicate that the object is not present, both due to annotation noise and the use of multiple synonyms for objects by human labelers. As a consequence, we restrict the set of candidate ir- relevant images to those which contain a match- ing object to the question premise but a differ- ent attribute. Without further restriction, the se- lected irrelevant attributes do not tend to be mutu- ally exclusive with the source attribute (i.e. match- ing '&lt;dog, old&gt;' and '&lt;dog, red&gt;'). To correct this and ensure a false premise, we further re- strict the set to attributes which are antonyms (e.g. '&lt;young&gt;' for source attribute '&lt;old&gt;') or taxo- nomic sister terms (e.g. '&lt;green&gt;' for source at- tribute '&lt;red&gt;') of the original premise attribute. We also experimented with third order premises; however, the lack of a corresponding sense of mu- tual exclusion for verbs and the sparsity of &lt;ob- ject, relationship, object&gt; premises made finding non-trivial irrelevant images difficult.</p><p>To recap, our data collection approach is to take each image-question pair in the VQA dataset and extract its first and second order question premises. For each premise, we find all images which lack only this premise and rank them by their visual distance. The closest of these is kept as the irrelevant image for each image-question pair. <ref type="figure" target="#fig_0">Fig. 3</ref> shows sample (I + , Q, P, I − ) tuples from our dataset. These examples illustrate the diffi- culty of our dataset. For instance, the images in the second column differ only by the presence of the water bottle and images in the fourth column are differentiated by the color of the devices. Both of these are fine details of the image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exploring the Dataset</head><p>The QRPE dataset contains 53,911 (I + , Q, P, I − ) tuples generated from as many premises. In total, it contains 1530 unique premises and 28,853 unique questions. Among the 53,911 premises, 3876 are second-order, attributed object premises while the remaining 50,035 are first-order object/scene premises. We divide our dataset into two parts -a training set with 35,486 tuples that are generated from the VQA training set and a validation set with 18,425 tuples generated from the VQA validation set.</p><p>Manual Validation. We also manually vali- dated 1000 randomly selected (I + , Q, P, I − ) tu- ples from our dataset. We noted that 99.10% of the premises P were valid (i.e. implied by the ques- tion) in I + and 97.3% were false for the negative image I − . This demonstrates the high reliability of our automated annotation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to VTFQ</head><p>We contrast our approach to the VTFQ dataset of <ref type="bibr" target="#b20">Ray et al. (2016)</ref>. As discussed prior, VTFQ was collected by selecting a random question and im- age from the VQA set and asking human anno- tators to report if the question was relevant, pro- ducing a pair. This approach results in irrelevant image-question pairs that are unambiguously un- related, with the visual content of the image hav- ing nothing at all to do with the question or its source image from VQA.</p><p>To quantify this effect and compare to QRPE, we pair each irrelevant image-question pair (I − , Q) from VTFQ with a relevant image from the VQA dataset. Specifically, we find the near- est neighbor question Q nn in the VQA dataset to Q based on an average of the word2vec ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) embedding of each word, and select the image on which Q nn was asked as I + to form (I + , Q, P, I − ) tuples like in our proposed dataset.</p><p>In <ref type="figure" target="#fig_1">Fig. 4</ref>, we present a quantitative and qual- itative comparison of the two datasets based on these tuples. On the left side of the figure, we plot the distributions of Euclidean distance be- tween the fc7 features of each (I + , I − ) pair in both datasets. We find that the mean distance in the VTFQ dataset is nearly twice that of our QRPE dataset, indicating that irrelevant images in VTFQ are less visually related to source images though we do note the distribution of distances in both datasets is long tailed.</p><p>On the right side of <ref type="figure" target="#fig_1">Fig. 4</ref>, we also provide qual- itative examples of questions that occur in both datasets. The example on the last row is perhaps most striking. The source question is asking the color of a fork and the relevant image shows an overhead view of a meal with an orange fork set nearby. The irrelevant image in QRPE is a similar image of food, but with chopsticks! Conversely, the image from VTFQ is a man playing baseball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Question Relevance Detection</head><p>In this section, we introduce a simple baseline for irrelevant question detection on the QRPE dataset and demonstrate that explicitly reasoning about premises improves performance for both our new model and existing methods. More formally, we consider the binary classification task of predict- ing if a question Q i from an image-question pair (I i , Q i ) is relevant to image I i .</p><p>A Simple Premise-Aware Model. Like the standard VQA task, question relevance detection also requires making a prediction based on an en- coded image and question. With this in mind, we begin with a straight-forward approach based on the Deeper LSTM VQA model architecture of <ref type="bibr" target="#b3">Antol et al. (2015)</ref>. This model encodes the image I via a VGGNet and the question Q with an LSTM over one-hot word encodings. The concatenation of these embeddings are input to a multi-layer per- ceptron. We fine-tune this model for the binary question relevance detection task starting from a model pretrained on the VQA task. We denote this model as VQA-Bin.</p><p>We extend the VQA-Bin model to explicitly reason about premises. We extract first and second order premises from the question Q and encode them as two concatenated one-hot vectors. We add an additional LSTM to encode the premises and concatenate this added feature to the image and question feature. We refer to this premise-aware model as VQA-Bin-Premise.</p><p>Attention Models. We also extend the attention based Hierarchical Co-Attention VQA model of <ref type="bibr" target="#b15">Lu et al. (2016)</ref> for the task of question rele- vance in a way similar to Deeper LSTM model. We call this model HieCoAtt-Bin. The cor- responding premise-aware model is referred to as HieCoAtt-Bin-Prem.</p><p>Existing Methods. We compare our approaches with the best performing model of <ref type="bibr" target="#b20">Ray et al. (2016)</ref>. This model (which we denote QC-Sim) uses a pretrained captioning model to automati- cally provide natural language image descriptions and reasons about relevance based on a learned similarity between the question and image caption.</p><p>Specifically, the approach uses NeuralTalk2 ( <ref type="bibr" target="#b9">Karpathy and Li, 2015</ref>) trained on the MS COCO dataset ( <ref type="bibr" target="#b12">Lin et al., 2014</ref>) to generate a caption for each image. Both the caption and question are <ref type="figure">Figure 5</ref>: Question relevance explanation: We provide selected examples of predictions from the False Premise Detection model (FPD) on the QRPE test set. Reasoning about premises presents the opportunity to produce natural language statements indicating why a question is irrelevant to an image, by pointing to the premise that is invalid.  embedded as a fixed length vector through an en- coding LSTM (with words being represented as word2vec ( <ref type="bibr" target="#b18">Mikolov et al., 2013)</ref> vectors). These question and caption embeddings are concatenated and fed to a multilayer perceptron to predict rele- vance. We consider two additional versions of this approach that consider only premise-caption sim- ilarity (PC-Sim) and question-premise-caption similarities (QPC-Sim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Results. We train each model on the QRPE train split and report results on the test set in <ref type="table" target="#tab_2">Table 1</ref>. As the dataset is balanced in the label space, random accuracy stands at 50%. We find that the simple VQA-Bin model achieves 66.5% accuracy while the attention based model HieCoAtt-Bin at- tains 70.74% accuracy. Surprisingly, the caption- similarity based QC-Sim model significantly out- performs these baseline, obtaining an accuracy of 74.35% while only reasoning about relevancy from textual descriptions of images. We note that the caption similarity based approaches use a large amount of outside data during pretraining of the captioning model and the word2vec embeddings, which may have contributed to the effectiveness of these methods. Most interestingly, we find that the addi- tion of extracted premise representations con- sistently improves performance of base mod- els. VQA-Bin-Prem, HieCoAtt-Bin-Prem, PC-Sim, and QPC-Sim outperform their no- premise information counterparts, with QPC-Sim being the overall best performing approach at 75.31% accuracy. This is especially interesting given that the models already have access to the question from which the premises were extracted. This result seems to imply there is value in explic- itly isolating premises from sentence grammar.</p><p>We further divide our test set into two splits consisting of (Q, I) pairs created by either falsi- fying first-order and second-order premises. We find that all our models perform significantly bet- ter on the first-order split. We hypothesize that the significant diversity in visual representations of attributed objects and comparatively fewer ex- amples for each type makes it more difficult to learn subtle differences for second-order premises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Question Relevance Explanation</head><p>In addition to identifying whether a question is ir- relevant to an image, being able to indicate why carries significant real-world utility. From an in- terpretability perspective, reporting which premise is false is more informative than simply answering the question in the negative, as it can help to cor- rect the questioner's misconception regarding the scene. We propose to generate such explanations by identifying the particular question premise(s) that do not apply to an image.</p><p>By construction, irrelevant images in the QRPE dataset are picked on the basis of negating a single premise -we now use our dataset to train mod- els to detect false premises, and use the premises classified as irrelevant to generate templated natu- ral language explanations. <ref type="figure">Fig. 5</ref> illustrates the task setup for false premise detection. Given a question-image pair, say "What color is the cat's tie?", the objective is to iden- tify which (if any) question premises are not grounded in the image, in this case both &lt;cat&gt; and &lt;tie&gt;. Alternatively, for the question "What kind of building is the large white building?", both premises &lt;building, large&gt; and &lt;building, white&gt; are true premises grounded in the image.</p><p>We train a simple false premise detection model for this task. Our model is a multilayer percep- tron that takes one-hot encodings of premises and VGGNet <ref type="bibr" target="#b23">(Simonyan and Zisserman, 2014</ref>) image features as input to predict whether the premise is grounded in the image or not. We trained our false premise detection model (FPD) model on all premises in the QRPE dataset.</p><p>Our FPD model achieves an accuracy of 61.12% on the QRPE dataset. In <ref type="figure">Fig. 5</ref>, we present quali- tative results of our premise classification and ex- planation pipeline. For the question "What color is the cat's tie?", the model correctly recognizes 'cat' and 'tie' as false premises, and we gener- ate statements in natural language indicating the same. Thus, determining question relevance by reasoning about each premise presents the oppor- tunity to generate simple explanations that can provide valuable feedback to the questioner, and help improve model trust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Premise-Based Visual Question Answering Data Augmentation</head><p>In this section, we develop a premise-based data augmentation scheme for VQA that generates simple, templated questions based on premises present in complex visually-grounded questions from the VQA (training) dataset. Using the pipeline presented in Section 3, we extract premises from questions in the VQA dataset and apply a simple templated question gen- eration strategy to transform premises into ques- tion and answer pairs. Note that because the source questions come from sighted humans about an image, we do not need to filter out binary or counting questions in order to avoid false premises as in Section 3. We do however filter based on SPICE similarity between the generated and source questions to avoid generating duplicates.</p><p>We design templates for each type of premise -first-order (e.g. '&lt;man&gt;' -"Is there a man?" Yes), second-order (e.g. '&lt;man, walking&gt;' - "What is the man doing?" Walking, and '&lt;car, red&gt;' -"What is the color of the car?" Red), and third-order ('&lt;man, holding, racket&gt;' -"What is the man holding?" Racket, "Who is holding the racket?" Man). This process transforms im- plicit premise concepts which previously had to Training Data Other Number <ref type="table" target="#tab_2">Yes  No  Total   Source  123,817 29,698 57217 35842 246,574  Premise  137,483 1,850 387,941 0 527,274   Table 2</ref>: Answer type distribution of source and premise questions on the Compositional VQA train set.</p><p>be learned as part of understanding more complex questions into simple, explicit training examples that can be directly supervised. <ref type="figure">Fig. 6</ref> shows sample premise questions pro- duced from source VQA questions using our pipeline. We note that the distribution of premise questions varies drastically from the source VQA distribution (see <ref type="table">Table 2</ref>).</p><p>We evaluate multiple models with and without premise augmentation on two splits of the VQA dataset -the standard split and the compositional split of <ref type="bibr" target="#b0">Agrawal et al. (2017)</ref>. The compositional split is specifically designed to test a model's abil- ity to generalize to unseen/rarely seen combina- tions of concepts at test time.</p><p>Augmentation Strategies. We evaluate the Deeper LSTM model of  on the standard and compositional splits with two aug- mentation strategies -All which includes the entire set of premise questions and Top-1k-A which includes only questions with answers in the top 1000 most common VQA answers. The re- sults are listed in <ref type="table" target="#tab_4">Table 3</ref>. We find minor im- provement of 0.34% on the standard split under Top-1k-A premise question augmentation. On the compositional split, we observe a 1.16% gain with Top-1k-A augmentation over no augmen- tation. In this setting, explicitly reasoning about objects and attributes seen in the questions seems to help the model disentangle objects from their common characteristics.</p><p>Other Models. To check the general effec- tiveness of our approach, we further evaluate Top-1k-A augmentation for three additional VQA models on the compositional split. We find inconsistent improvements for these more ad- vanced models with some improving while others see reductions in accuracy when adding premises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we made the simple observation that questions about images often contain premises im- plied by the question and that reasoning about premises can help VQA models respond more in-What player number is about to swing at the ball? Why is the man looking at the lady? How many people are wearing safety jackets?</p><p>Is there a player number? Yes Who is looking at the lady? <ref type="table">Man  Can you see people in the image? Yes  Is there a ball in the image? Yes  Is there a lady in the image? Yes  What are the people wearing? Jacket  Is there a number in the image? Yes  Is there a man in the</ref>    telligently to irrelevant or novel questions. We develop a system for automatically ex- tracting these question premises. Using these premises, we automatically created a novel dataset for Question Relevance Prediction and Expla- nation (QRPE) which consists of 53,911 ques- tion, relevant image, and irrelevant image triplets. We also train novel question relevance prediction models and show that models that take advantage of premise information outperform models that do not. Furthermore, we demonstrated that questions generated from premises may be an effective data augmentation technique for VQA tasks that re- quire compositional reasoning.</p><p>Integrating Question Relevance Prediction and Explanation (QRPE) models with existing VQA systems would form a natural extension to our ap- proach. In this setting, the relevance prediction model would determine the applicability of a ques- tion to an image, and select an appropriate path of action. If the question is classified as rele- vant, the VQA model would generate a prediction; otherwise, a question relevance explanation model would provide a natural language sentence indicat- ing which premise(s) are not valid for the image. Such systems would be a step in the direction of making VQA systems move beyond academic set- tings to real-world environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Some Examples from QRPE Dataset. For a given question Q and a relevant image I + , we find an irrelevant image I − for which exactly one premise P of the question is false. If there are multiple such candidates, we select the candidate most visually most similar to I +. As can be seen from these examples, the QRPE dataset is very challenging, with only minor visual and semantic differences separating the relevant and irrelevant images.</figDesc><graphic url="image-13.png" coords="4,72.00,62.81,453.54,167.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A comparison of the QRPE and VTFQ Datasets. On the left, we plot the Euclidean distance between VGGNet-fc7 features extracted from each relevant-irrelevant image pair for each dataset. Note that VTFQ has significantly higher visual distances. On the right, we show some qualitative examples of irrelevant images for questions that occur in both datasets. VTFQ images are significantly less related to the source image and question than in our dataset.</figDesc><graphic url="image-14.png" coords="5,72.00,77.75,453.56,179.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Accuracy of Question Relevance models on the 
QRPE test set. We find that premise-aware models consis-
tently outperform alternative models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>image? Yes Who is wearing the jacket? People What is the child sitting on? Where is the pink hat? What is the item called that the cat is looking at?</head><label></label><figDesc></figDesc><table>What is the child doing? Sitting 
What is the color of hat? Pink 
Is there a cat in the image? Yes 
Is there a child in the image? Yes 
Is there a hat in the image? Yes 
Is there an item in the image? Yes 

Figure 6: Sample generated premise questions from source questions. Source questions are in bold. Ground-truth answers are 
extracted using the premise tuples. 

Augmentation Overall Other Number Yes/No 

Standard 
None 
54.23 40.34 33.27 
79.82 
All 
53.74 39.28 33.38 
79.89 
Top-1k-A 
54.47 40.56 33.24 
80.19 

Comp. 
None 
46.69 31.92 29.73 
70.49 
All 
47.63 31.97 30.77 
72.52 
Top-1k-A 
47.85 32.58 30.59 
72.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy on the standard and compositional VQA 
validation sets for different augmentation strategies for Deep-
erLSTM(Antol et al., 2015). 

VQA Model 
Baseline +Premises 

DeeperLSTM(Lu et al., 2015) 46.69 
47.85 
HieCoAtt(Lu et al., 2016) 
50.17 
49.98 
NMN(Andreas et al., 2016) 
49.05 
48.43 
MCB(Fukui et al., 2016) 
50.13 
50.57 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Overall accuracy of different VQA models on the 
Compositional VQA test split using Top-1k-A augmentation. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08243</idno>
		<title level="m">C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic annotation of structured facts in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00466</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<imprint>
			<pubPlace>Rohrbach, Raymond Mooney, Saenko Kate, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minoh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoungtak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deeper lstm and normalized cnn visual question answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/VT-vision-lab/VQA_LSTM_CNN" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multiworld approach to question answering about realworld scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04757</idno>
		<title level="m">Attentive explanations: Justifying decisions and pointing to the evidence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Question relevance in vqa: Identifying non-visual and false-premise questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://www.image-net.org/challenges/LSVRC/2012/" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Vision and Language</title>
		<meeting>the Fourth Workshop on Vision and Language</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anton van den Hengel, and Anthony Dick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05433</idno>
	</analytic>
	<monogr>
		<title level="m">Fvqa: Factbased visual question answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Measuring machine intelligence through visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08716</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
