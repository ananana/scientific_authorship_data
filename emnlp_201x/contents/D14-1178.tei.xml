<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Random Manhattan Integer Indexing: Incremental L 1 Normed Vector Space Construction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrang</forename><forename type="middle">Q</forename><surname>Zadeh</surname></persName>
							<email>behrang.qasemizadeh@insight-centre.org</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Mathematics</orgName>
								<orgName type="institution">Insight Centre National University of Ireland</orgName>
								<address>
									<settlement>Galway Galway</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
							<email>siegfried.handschuh@uni-passau.de</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Passau</orgName>
								<address>
									<settlement>Bavaria</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Random Manhattan Integer Indexing: Incremental L 1 Normed Vector Space Construction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1713" to="1723"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Vector space models (VSMs) are mathematically well-defined frameworks that have been widely used in the distributional approaches to semantics. In VSMs, high-dimensional vectors represent linguistic entities. In an application, the similarity of vectors-and thus the entities that they represent-is computed by a distance formula. The high dimensionality of vectors , however, is a barrier to the performance of methods that employ VSMs. Consequently, a dimensionality reduction technique is employed to alleviate this problem. This paper introduces a novel technique called Random Manhattan Indexing (RMI) for the construction of 1 normed VSMs at reduced dimensionality. RMI combines the construction of a VSM and dimension reduction into an incre-mental and thus scalable two-step procedure. In order to attain its goal, RMI employs the sparse Cauchy random projections. We further introduce Random Man-hattan Integer Indexing (RMII): a compu-tationally enhanced version of RMI. As shown in the reported experiments, RMI and RMII can be used reliably to estimate the 1 distances between vectors in a vector space of low dimensionality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional semantics embraces a set of meth- ods that decipher the meaning of linguistic en- tities using their usages in large corpora <ref type="bibr" target="#b14">(Lenci, 2008)</ref>. In these methods, the distributional proper- ties of linguistic entities in various contexts, which are collected from their observations in corpora, are compared to quantify their meaning. Vector spaces are intuitive, mathematically well-defined frameworks to represent and process such infor- mation. <ref type="bibr">1</ref> In a vector space model (VSM), linguis- tic entities are represented by vectors and a dis- tance formula is employed to measure their distri- butional similarities <ref type="bibr" target="#b20">(Turney and Pantel, 2010)</ref>.</p><p>In a VSM, each element s i of the standard basis of the vector space (informally, each dimension of the VSM) represents a context element. Given n context elements, an entity whose meaning is be- ing analyzed is expressed by a vector v as a linear combination of s i and scalars α i ∈ R such that v = α 1 s 1 + · · · + α n s n . The value of α i is derived from the frequency of the occurrences of the entity that v represents in/with the context element that s i represents. As a result, the values assigned to the coordinates of a vector (i.e. α i ) exhibit the cor- relation of entities and context elements in an n- dimensional real vector space R n . Each vector can be written as a 1×n row matrix, e.g. (α 1 , · · · , α n ). Therefore, a group of m vectors in a vector space is often represented by a matrix M m×n .</p><p>Latent semantic analysis (LSA) is a famil- iar technique that employs a word-by-document VSM ( <ref type="bibr" target="#b6">Deerwester et al., 1990</ref>). <ref type="bibr">2</ref> In this word- by-document model, the meaning of words (i.e. the linguistic entities) is described by their occur- rences in documents (i.e. the context elements). Given m words and n distinct documents, each word is represented by an n-dimensional vector v i = (α i1 , · · · , α in ), where α ij is a numeric value that associates the word v i represents to the doc- ument d j , for 1 &lt; j &lt; n. For instance, the value of α ij may correspond to the frequency of the word in the document. It is hypothesized that the relevance of words can be assessed by count- ing the documents in which they co-occur. There- fore, words with similar vectors are assumed to have the same meaning ( <ref type="figure">Figure 1</ref>). Figure 1: Illustration of a word-by-document model consisting of 2 words and 3 documents. The words are represented in a 3-dimensional vec- tor space, in which each s i (each dimension) rep- resents each of the 3 documents in the model. v 1 = (α 11 , α 12 , α 13 ) and v 2 = (α 21 , α 22 , α 23 ) represent the two words in the model. The dashed line shows the Euclidean distance between the two vectors that represent words, while the sum of dash-dotted lines is the Manhattan distance be- tween them.</p><formula xml:id="formula_0">s 1 ↔ d 1 s 2 ↔ d 2 s 3 ↔ d 3 v1 v2 α12 α11</formula><p>In order to assess the similarity between vectors, a vector space V is endowed with a norm struc- ture. A norm . is a function that maps vectors from V to the set of non-negative real numbers, i.e. V → [0, ∞). The pair of (V, .) is then called a normed space. In a normed space, the similar- ity between vectors is assessed by their distances. The distance between vectors is defined by a func- tion that satisfies certain axioms and assigns a real value to each pair of vectors, i.e.</p><formula xml:id="formula_1">dist : V × V → R, d ( v, t) = v − u. (1)</formula><p>The smaller the distance between two vectors, the more similar they are.</p><p>Euclidean space is the most familiar example of a normed space. It is a vector space that is en- dowed by the 2 norm. In Euclidean space, the 2 norm-which is also called the Euclidean norm- of a vector v = (v 1 , · · · , v n ) is defined as</p><formula xml:id="formula_2">v 2 = n i=1 v 2 i .<label>(2)</label></formula><p>Using the definition of distance given in Equa- tion 1 and the 2 norm, the Euclidean distance is measured as</p><formula xml:id="formula_3">dist 2 ( v, u) = v − u 2 = n i=1 (v i − u i ) 2 . (3)</formula><p>In <ref type="figure">Figure 1</ref>, the dashed line shows the Euclidean distance between the two vectors. In 2 normed vector spaces, various similarity metrics are de- fined using different normalization of the Eu- clidean distance between vectors, e.g. the cosine similarity.</p><p>The similarity between vectors, however, can also be computed in 1 normed spaces. <ref type="bibr">3</ref> The 1 norm for v is given by</p><formula xml:id="formula_4">v 1 = n i=1 |v i |,<label>(4)</label></formula><p>where |.| signifies the modulus. The distance in an 1 normed vector space is often called the Man- hattan or the city block distance. According to the definition given in Equation 1, the Manhattan dis- tance between two vectors v and u is given by</p><formula xml:id="formula_5">dist 1 ( v, u) = v − u 1 = n k=1 |v i − u j |. (5)</formula><p>In <ref type="figure">Figure 1</ref>, the collection of the dash-dotted lines is the 1 distance between the two vectors. Similar to the 2 spaces, various normalizations of the 1 distance 4 define a family of 1 normed similarity metrics.</p><p>As the number of text units that are being mod- elled in a VSM increases, the number of context elements that are required to be utilized to capture their meaning escalates. This phenomenon is ex- plained using power-law distributions of text units in context elements (e.g. the familiar Zipfian dis- tribution of words). As a result, extremely high- dimensional vectors, which are also sparse-i.e. most of the elements of the vectors are zero- represent text units. The high dimensionality of the vectors results in setbacks, which are colloqui- ally known as the curse of dimensionality. For in- stance, in a word-by-document model that consists of a large number of documents, a word appears only in a few documents, and the rest of the doc- uments are irrelevant to the meaning of the word. Few common documents between words results in sparsity of the vectors; and the presence of irrele- vant documents introduces noise.</p><p>Dimension reduction, which usually follows the construction of a VSM, alleviates the problems <ref type="bibr">3</ref> The definition of the norm is generalized to p spaces with vp = i |vi| p 1/p , which is beyond the scope of this paper. <ref type="bibr">4</ref> As long as the axioms in the distance definition hold.</p><p>listed above by reducing the number of context el- ements that are employed for the construction of the VSM. In its simple form, dimensionality re- duction can be performed using a selection pro- cess: choose a subset of contexts and eliminate the rest using a heuristic. Alternatively, transfor- mation methods can be employed. A transforma- tion method maps a vector space V n onto a V m of lowered dimension, i.e. τ : V n → V m , m n.</p><p>The vector space at reduced dimension, i.e. V m , is often the best approximation of the original V n in a sense. LSA employs a dimension reduction technique called truncated singular value decom- position (SVD). In a standard truncated SVD, the transformation guarantees the least distortion in the 2 distances. <ref type="bibr">5</ref> Besides the problem of high computational complexity of SVD computation, 6 which can be addressed by incremental techniques (see e.g. <ref type="bibr" target="#b3">Brand (2006)</ref>), matrix factorization methods such as truncated SVD are data-sensitive: if the struc- ture of the data being analyzed changes, i.e. when either the linguistic entities or context elements are updated, e.g. some are removed or new ones are added, the transformation should be recom- puted and reapplied to the whole VSM to reflect the updates. In addition, a VSM at the original high dimension must be first constructed. Follow- ing the construction of the VSM, the dimension of the VSM is reduced in an independent process. Therefore, the VSM at reduced dimension is avail- able for processing only after the whole sequence of these processes. Construction of the VSM at its original dimension is computationally expen- sive and a delay in access to the VSM at reduced dimension is not desirable. Hence, the application of truncated SVD is not suitable in several appli- cations, particularly when dealing with frequently updated big text-data such as applications in the web context.</p><p>Random indexing (RI) is an alternative method that solves the problems stated above by combin- ing the construction of a vector space and the di- mensionality reduction process. RI, which is in- troduced in <ref type="bibr" target="#b9">Kanerva et al. (2000)</ref>, constructs a VSM directly at reduced dimension. Unlike meth- ods that first construct a VSM at its original high dimension and conduct a dimensionality reduction afterwards, the RI method avoids the construction of the original high-dimensional VSM. Instead, it merges the vector space construction and the di- mensionality reduction process. RI, thus, signifi- cantly enhances the computational complexity of deriving a VSM from text. However, the appli- cation of the RI technique (likewise the standard truncated SVD in LSA) is limited to 2 normed spaces, i.e. when similarities are assessed using a measure based on the 2 distance. It can be verified that using RI causes large distortions in the 1 dis- tances between vectors (Brinkman and <ref type="bibr" target="#b4">Charikar, 2005</ref>). Hence, if the similarities are computed us- ing the 1 distance, then the RI technique is not suitable for the VSM construction.</p><p>Depending on the distribution of vectors in a VSM, the performance of similarity measures based on the 1 and the 2 norms varies from one task to another. For instance, it is known that the 1 distance is more robust to the presence of outliers and non-Gaussian noise than the 2 dis- tance (e.g. see the problem description in <ref type="bibr" target="#b11">Ke and Kanade (2003)</ref>). Hence, the 1 distance can be more reliable than the 2 distance in certain appli- cations. For instance, <ref type="bibr">Weeds et al. (2005)</ref> suggest that the 1 distance outperforms other similarity metrics in a term classification task. In another experiment, <ref type="bibr" target="#b13">Lee (1999)</ref> observed that the 1 dis- tance gives more desirable results than the Cosine and the 2 measures.</p><p>In this paper, we introduce a novel method called Random Manhattan Indexing (RMI). RMI constructs a vector space model directly at re- duced dimension while it preserves the pairwise 1 distances between vectors in the original high- dimensional VSM. We then introduced a compu- tationally enhanced version of RMI called Ran- dom Manhattan Integer Indexing (RMII). RMI and RMII, similar to RI, merge the construction of a VSM and dimension reduction into an incre- mental and thus efficient and scalable process.</p><p>In Section 2, we explain and evaluate the RMI method. In Section 3, the RMII method is ex- plained. We compare the proposed method with RI in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Random Manhattan Indexing</head><p>We propose the RMI method: a novel technique that adapts an incremental procedure for the con- struction of 1 normed vector spaces at a reduced dimension. The RMI method employs a two-step procedure: (a) the creation of index vectors and (b) the construction of context vectors .</p><p>In the first step, each context element is as- signed exactly to one index vector r i . Index vec- tors are high-dimensional and generated randomly such that entries r j of index vectors have the fol- lowing distribution:</p><formula xml:id="formula_6">r i =      −1 U 1</formula><p>with probability s 2 0</p><p>with probability 1 − s</p><formula xml:id="formula_7">1 U 2</formula><p>with probability s 2 ,</p><p>where U 1 and U 2 are independent uniform ran- dom variables in (0, 1). In the second step, each target linguistic entity that is being analyzed in the model is assigned to a context vector v c in which all the elements are initially set to 0. For each encountered occurrence of a linguistic entity and a context element-e.g. through a sequential scan of an input text collection- v c that represents the linguistic entity is accumulated by the index vector r i that represents the context element, i.e.</p><formula xml:id="formula_9">v c = v c + r i .</formula><p>This process results in a VSM of a reduced dimensionality that can be used to estimate the 1 distances between linguistic enti- ties. In the constructed VSM by RMI, the 1 dis- tance between vectors is given by the sample me- dian <ref type="bibr" target="#b7">(Indyk, 2000</ref>). For given vectors v and u, the approximate 1 distance between vectors is esti- mated byˆL</p><formula xml:id="formula_10">byˆ byˆL 1 ( u, v) = median{|v i − u i |, i = 1, · · · , m}, (7)</formula><p>where m is the dimension of the VSM constructed by RMI, and |.| denotes the modulus.</p><p>RMI is based on the random projection (RP) technique for dimensionality reduction. In RP, a high-dimensional vector space is mapped onto a random subspace of lowered dimension expecting that-with a high probability-relative distances between vectors are approximately preserved. Us- ing the matrix notation, this projection is given by</p><formula xml:id="formula_11">M p×m = M p×n R n×m , m p, n,<label>(8)</label></formula><p>where R is often called the random projection ma- trix, and M and M denote p vectors in the orig- inal n-dimensional and reduced m-dimensional vector spaces, respectively.</p><p>In RMI, the stated mapping in Equation 8 is given by Cauchy random projections. Indyk (2000) suggests that vectors in a high-dimensional space R n can be mapped onto a vector space of lowered dimension R m while the relative pairwise 1 distances between vectors are preserved with a high probability. In Indyk (2000, Theorem 3) and Indyk (2006, Theorem 5), it is shown that for an m ≥ m 0 = log(1/δ) O(1//) , where δ &gt; 0 and ≤ 1/2, there exists a mapping from R n onto R m that guarantees the 1 distances between any pair of vectors u and v in R n after the mapping does not increase by a factor more than 1 + with constant probability δ, and it does not decrease by more than 1 − with probability 1 − δ.</p><p>In Indyk (2000), this projection is proved to be obtained using a random projection matrix R that has Cauchy distribution-i.e. for r ij in R, r ij ∼ C(0, 1). Since R has a Cauchy distribu- tion, for every two vectors u and v in the high- dimensional space R n , the projected differences x = ˆ u − ˆ v also have Cauchy distribution, with the scale parameter being the 1 distances, i.e.</p><formula xml:id="formula_12">x ∼ C(0, n i=1 |u i − v i |)</formula><p>. As a result, in Cauchy random projections, estimating the 1 distances boils down to the estimation of the Cauchy scale parameter from independent and identically dis- tributed (i.i.d.) samples x. Because the expectation value of x is infinite, 7 the sample mean cannot be employed to estimate the Cauchy scale parameter. Instead, using the 1-stability of Cauchy distribu- tion, Indyk (2000) proves that the median can be employed to estimate the Cauchy scale parame- ter, and thus the 1 distances at the projected space R m .</p><p>Subsequent studies simplified the method pro- posed by <ref type="bibr" target="#b7">Indyk (2000)</ref>.  shows that R with Cauchy distribution can be substituted by a sparse R that has a mixture of symmetric 1-Pareto distribution. A 1-Pareto distribution can be sam- pled by 1/U , where U is an independent uniform random variable in <ref type="figure">(0, 1)</ref>. This results in a ran- dom matrix R that has the same distribution as described by Equation 6.</p><p>The RMI's two-step procedure is explained us- ing the basic properties of matrix arithmetic and the descriptions given above. Given the projection in Equation 8, the first step of RMI refers to the construction of R: index vectors are the row vec- tors of R. The second step of the process refers to the construction of M : context vectors are the row vectors of M . Using the distributive prop- erty of multiplication over addition in matrices, 8 it can be verified that the explicit construction of M and its multiplication to R can be substituted by a number of summation operations. M can be represented by the sum of unit vectors in which a unit vector corresponds to the co-occurrence of a linguistic entity and a context element. The result of the multiplication of each unit vector and R is the row vector that represents the context element in R-i.e. the index vector. Therefore, M can be computed by the accumulation of the row vectors of R that represent encountered context elements, as stated in the second step of the RMI procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Alternative Distance Estimators</head><p>As stated above, Indyk (2000) suggests using the sample median for the estimation of the 1 dis- tances. However, <ref type="bibr" target="#b17">Li (2008)</ref> argues that sam- ple median estimator can be biased and inaccu- rate, specifically if m-i.e. the targeted (reduced) dimensionality-is small. Hence, <ref type="bibr" target="#b17">Li (2008)</ref> sug- gests using the geometric mean estimator instead of the median sample: 9</p><formula xml:id="formula_13">ˆ L 1 ( u, v) = m i=1 |u i − v i | 1 m .<label>(9)</label></formula><p>We suggest computing thê L 1 ( u, v) in Equation 9 using arithmetic mean of logarithm-transformed values of |u i − v i |. Therefore, using the logarith- mic identities, the multiplications and the power in Equation 9 are, respectively, transformed to a sum and a multiplication:</p><formula xml:id="formula_14">ˆ L 1 ( u, v) = exp 1 m m i=1 ln(|u i − v i |) .<label>(10)</label></formula><p>Equation 10 for computingˆLcomputingˆ computingˆL 1 is more plausible for computational implementation than Equation 9 (e.g. the overflow is less likely to happen dur- ing the process). Moreover, calculating the median involves sorting an array of real numbers. Thus, computation of the geometric mean in logarithmic scales can be faster than computation of the me- dian sample, especially when the value of m is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RMI's Parameters</head><p>In order to employ the RMI method for the con- struction of a VSM at reduced dimension and the estimation of the 1 distance between vectors, two <ref type="bibr">9</ref> See also <ref type="bibr">Li et al. (2007, Lemma 5-9</ref>). model parameters should be decided: (a) the tar- geted (reduced) dimensionality of the VSM, which is indicated by m in Equation 8 and (b) the num- ber of non-zero elements in index vectors, which is determined by s in Equation 6. In contrast to the classic one-dimension-per-context-element meth- ods of VSM construction, 10 the value of m in RPs and thus in RMI is chosen independently of the number of context elements in the model (n in Equation 8). In RMI, m determines the probability and the maximum expected amount of distortions in the pairwise distance between vectors. Based on the proposed refinements of Indyk (2000, Theorem 3) by , it is verified that the pairwise 1 distance between any p vectors is approximated within a factor 1 ± , if m = O(log p// 2 ), with a constant probability. Therefore, the value of in RMI is subject to the number of vectors p in the model. For a fixed p, a larger m yields to lower bounds on the distortion with a higher probabil- ity. Because a small m is desirable from the com- putational complexity outlook, the choice of m is often a trade-off between accuracy and efficiency. According to our experiment, m &gt; 400 is suitable for most applications. The number of non-zero elements in index vec- tors, however, is decided by the number of context elements n and the sparseness of the VSM β at its original dimension. Li (2007) suggests</p><formula xml:id="formula_15">1 O( √ βn)</formula><p>as the value of s in Equation 6. VSMs employed in distributional semantics are highly sparse. The sparsity of a VSM in its original dimension β is often considered to be around 0.0001-0.01. As the original dimension of VSM n is very large- otherwise there would be no need for dimension- ality reduction-the index vectors are often very sparse. Similar to m, larger s produces smaller er- rors; however, it imposes more processes during the construction of a VSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experimental Evaluation of RMI</head><p>We report the performance of the RMI method with respect to its ability to preserve the rela- tive 1 distance between linguistic entities in a VSM. Therefore, instead of a task-specific evalua- tion, we show that the relative 1 distance between a set of words in a high-dimensional word-by- document model remains intact when the model is constructed at reduced dimensionality using the RMI technique. We further explore the effect of the RMI's parameter setting in the observed re- sults. Depending on the structure of the data that is being analyzed and the objective of the task in hand, the performance of the 1 distance for sim- ilarity measurement varies from one application to another. <ref type="bibr">11</ref> The purpose of our reported evalu- ation, thus, is not to show the superiority of the 1 distance (thus RMI) to another similarity measure (e.g. the 2 distance or the cosine similarity) and employed techniques for dimensionality reduction (e.g. RI or truncated SVD) in a specific task. If, in a task, the 1 distance shows higher performance than the 2 distance, then the RMI technique is preferable to the RI technique or truncated SVD. Contrariwise, if the 2 norm shows higher perfor- mance than the 1 , then RI or truncated SVD are more desirable than the RMI method.</p><p>In our experiment, a word-by-document model is first constructed from the UKWaC corpus at its original high dimension. UKWaC is a freely avail- able corpus of 2,692,692 web documents, nearly 2 billion tokens and 4 million types ( <ref type="bibr" target="#b2">Baroni et al., 2009)</ref>. <ref type="bibr">12</ref> Therefore, a word-by-document model constructed from this corpus using the classic one- dimension-per-context-element method has a di- mension of 2.69 million. In order to keep the ex- periments computationally tractable, the reported results are limited to 31 words from this model, which are listed in <ref type="table">Table 1.</ref> In the designed experiment, a word from the list is taken as the reference and its 1 distance to the remaining 30 words is calculated using the vec- tor representations in the high-dimensional VSM. These 30 words are then sorted in ascending or- der by the calculated 1 distance. The procedure is repeated for all the 31 words in the list, one by one. Therefore, the procedure results in 31 sorted lists, each containing 30 words. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of the obtained sorted list, in which the reference is the word 'research'. <ref type="bibr">13</ref> The procedure described above is replicated to obtain the lists of sorted words from VSMs that are constructed by the RMI method at reduced 11 E.g. see the experiments in <ref type="bibr" target="#b5">Bullinaria and Levy (2007)</ref>. <ref type="bibr">12</ref> UkWaC can be obtained from http://goo.gl/ 3isfIE. <ref type="bibr">13</ref> Please note that the number of possible arrangements of 30 words without repetition in a list in which the order is important (i.e. all permutations of 30 words) is 30!. <ref type="table">Noun  website email  support  software  students skills  project  research  nhs  link  services organisations   Adj  online  digital mobile  sustainable  global  unique excellent disabled  new  current fantastic innovative   Verb  use  visit  improve provided  help  ensure develop   Table 1</ref>: Words employed in the experiments. dimensionality, when the method's parameters- i.e. the dimensionality of VSM and the number of non-zero elements in index vectors-are set dif- ferently. We expect the obtained relative 1 dis- tances between each reference word and the 30 other words in an RMI-constructed VSM to be the same as the obtained relative distances in the orig- inal high-dimensional VSM. Therefore, for each VSM that is constructed by the RMI technique, the resulting sorted lists of words are compared by the sorted lists that are obtained from the original high-dimensional VSM. We employ the Spearman's rank correlation co- efficient (ρ) to compare the sorted lists of words and thus the degree of distance preservation in the RMI-constructed VSMs at reduced dimensional- ity. The Spearman's rank correlation measures the strength of association between two ranked vari- ables, i.e. two lists of sorted words in our experi- ments. Given a list of sorted words obtained from the original high-dimensional VSM (list o ) and its corresponding list obtained from a VSM of re- duced dimensionality (list RMI ), the Spearman's rank correlation for the two lists is calculated by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PoS Words</head><formula xml:id="formula_16">ρ = 1 − 6 d 2 i n(n 2 − 1) ,<label>(11)</label></formula><p>where d i is the difference in paired ranks of words in list o and list RMI , and n = 30 is the number of words in each list. We report the average of ρ over the 31 lists of sorted words, denoted by ¯ ρ, to Figure 3: The ¯ ρ axis shows the observed average Spearman' rank correlation between the order of the words in the lists that are sorted by the 1 dis- tance obtained from the original high-dimensional VSM and the VSMs that are constructed by RMI at reduced dimensionality using index vectors of various numbers of non-zero elements.</p><p>indicate the performance of RMI with respect to its ability for distance preservation. The closer ¯ ρ is to 1, the better the performance of RMI. <ref type="figure">Figure 3</ref> shows the observed results at a glance when the distances are estimated using the median (Equation 7). As shown in the figure, when the di- mension of the VSM is above 400 and the number of non-zero elements is more than 12, the obtained relative distances from the VSMs constructed by the RMI technique start to be analogous to the rel- ative distances that are obtained from the origi- nal high-dimensional VSM, i.e. a high correlation (¯ ρ &gt; 0.90). For the baseline, we report the av- erage correlation of ¯ ρ random = −0.004 between the sorted lists of words obtained from the high- dimensional VSM and 31 × 1000 lists of sorted words that are obtained by randomly assigned dis- tances. <ref type="figure">Figure 4</ref> shows the same results as <ref type="figure">Figure 3</ref>, however, in minute detail and only for VSMs of dimension m ∈ {100, 400, 800, 3200}. In these plots, squares ( ) indicate the ¯ ρ while the error bars show the best and the worst observed ρ amongst all the sorted lists of words. The minimum value of ρ-axis is set to 0.611, which is the worst ob- served correlation in the baseline (i.e. randomly generated distances). The dotted line (ρ = .591) shows the best observed correlation in the baseline and the dashed-dotted line shows the average cor- relation in the baseline (ρ = −0.004). As sug- gested in Section 2.2, it can be verified that an increase in the dimension of VSMs (i.e. m) in- creases the stability of the obtained results (i.e. the probability of preserving distances increases). Therefore, for large values of m (i.e. m &gt; 400), the difference between the best and the worst ob- served ρ decreases; average correlation ¯ ρ → 1 and the observed relative distances in RMI-constructed VSMs tend to be identical to those in the original high-dimensional VSM. <ref type="figure">Figure 5</ref> represents the obtained results in the same setting as above, however, when the dis- tances are approximated using the geometric mean (Equation 10). The obtained average correlations ¯ ρ from the geometric mean estimations are al- most identical to the median estimations. How- ever, as expected, the geometric mean estimations are more reliable for small values of m; particu- larly, the worst observed correlations when using the geometric mean are higher than those observed when using the median estimator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Random Manhattan Integer Indexing</head><p>The application of the RMI method is hindered by two obstacles: float arithmetic operations required for the construction and processing of the RMI- constructed VSMs and the calculation of the prod- uct of large numbers when 1 distances are esti- mated using the geometric mean. The proposed method for the generation of in- dex vectors in RMI results in index vectors of non-zero elements that are real numbers. Conse- quently, index vectors and thus context vectors are arrays of floating point numbers. These vectors must be stored and accessed efficiently when using the RMI technique. However, resources that are required for the storage and processing of floating numbers is high. Even if the requirement for the storage of index vectors is alleviated, e.g., using a derandomization technique for their generation, context vectors that are derived from these index vectors are still arrays of float numbers. To tackle this problem, we suggest substituting the value of non-zero elements of RMI's index vectors (given in Equation 6) from 1 U to integer values of 1 U , where 1 U = 0. We argue that the resulting ran- dom projection matrix still has a Cauchy distribu- tion. Therefore, the proposed methodology to esti- mate the 1 distance between vectors is also valid. The 1 distance between context vectors must be estimated using either the median or the geo- metric mean. The use of the median estimator- for the reasons stated in Section 2.1-is not plau- sible. On the other hand, the computation of the geometric mean can be laborious as the overflow is highly likely to happen during its computation. Using the value of 1 U for non-zero elements of index vectors, we know that for any pair of context vectors u = (u 1 , · · · , u m ) and v = (v 1 , · · · , v m ),</p><formula xml:id="formula_17">if u i = v i then |u i − v i | ≥ 1. Therefore, for u i = v i , ln |u i −v i | ≥ 0 and thus m i=1 ln(|u i −v i |) ≥ 0.</formula><p>In this case, the exponent in Equation 10 is a scale factor that can be discarded without a change in the relative distances between vectors. 14 Based on the intuition that the distance between a vector and itself is zero and the explanation given above, in- spired by smoothing techniques and without being able to provide mathematical proofs, we suggest estimating the relative distances between vectors usingˆL</p><formula xml:id="formula_18">usingˆ usingˆL 1 ( u, v) = m i=1 u i =v i ln(|u i − v i |).<label>(12)</label></formula><p>In order to distinguish the above changes in RMI, we name the resulting technique random Manhat- tan integer indexing (RMII). The experiment de- scribed in Section 2.2 is repeated using the RMII method. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, the obtained results are almost identical to the observed results when using the RMI technique. While RMI performs slightly better than RMII in lower dimensions, e.g. m = 400, RMII shows more stable behaviour than RMI at higher dimensions, e.g. m = 800.  ematical theorems. As described above, RMI ap- proximates the 1 distance using a non-linear esti- mator, which has not yet been employed for the construction of VSMs and the calculation of 1 distances in distributional approaches to seman- tics. Moreover, RMI is justified using Cauchy ran- dom projections. In contrast, RI approximates the 2 distance us- ing a linear estimator. RI has initially been justi- fied using the mathematical model of the sparse distributed memory (SDM) <ref type="bibr">15</ref> . Later, <ref type="bibr" target="#b19">Sahlgren (2005)</ref> delineates the RI method using the lemma proposed by <ref type="bibr">Johnson and Lindenstrauss (1984)</ref>- which elucidates random projections in Euclidean spaces-and the reported refinement in <ref type="bibr" target="#b0">Achlioptas (2001)</ref> for the projections employed in the lemma. Although both the RMI and RI methods can be established as α-stable random projections- respectively for α = 1 and α = 2-the meth- ods cannot be compared as they address different goals. If, for a given task, the 1 norm outperforms the 2 norm, then RMI is preferable to RI. Con- trariwise, if the 2 norm outperforms the 1 norm, then RI is preferable to RMI.</p><p>To support the earlier claim that RI-constructed <ref type="bibr">15</ref> See <ref type="bibr" target="#b10">Kanerva (1993)</ref> for an overview of the SDM model. VSMs cannot be used for the 1 distance estima- tion, we evaluate the RI method in the experimen- tal setup that has been used for the evaluation of RMI and RMII. In these experiments, however, we use RI to construct vector spaces at reduced dimensionality and estimate the 1 distance us- ing Equation 5 (the standard 1 distance defini- tion) and Equation 7 (the median estimator) for m ∈ 400, 800. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, the experi- ments support the theoretical claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce a novel technique, named Random Manhattan Indexing (RMI), for the construction of 1 normed VSMs directly at reduced dimensionality. We further suggest the Random Manhattan Integer Indexing (RMII) tech- nique, a computationally enhanced version of the RMI technique. We demonstrated the 1 distance preservation ability of the proposed technique in an experimental setup using a word-by-document model. In these experiments, we showed how the variable parameters of the methods, i.e. the num- ber of non-zero elements in index vectors and the dimensionality of the VSM, influence the obtained results. The proposed incremental (and thus effi- cient and scalable) methods significantly enhance the computation of the 1 distances in VSMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: List of words sorted by their 1 distance to the word 'research'. The distance increases from left to right and top to bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Detailed observation of the obtained correlation between relative distances in RMI-constructed VSMs and the original highdimensional VSM. The 1 distance is estimated using the median. The squares denote ¯ ρ and the error bars show the best and the worst observed correlations. The dashed-dotted line shows the random baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The observed results when using the RMII method for the construction and estimation of the 1 distances between vectors. The method is evaluated in the same setup as the RMI technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Evaluation of RI for the 1 distance estimation for m = 400 and m = 800 when the distances are calculated using the standard definition of distance in 1 normed spaces and the median estimator. The obtained results using RI do not show correlation to the 1 distances in the original highdimensional VSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>4 Comparison of RMI and RI</head><label>4</label><figDesc></figDesc><table>RMI and RI utilize a similar two-step procedure 
consisting of the creation of index vectors and the 
construction of context vectors. Both methods are 
incremental techniques that construct a VSM at 
reduced dimensionality directly, without requiring 
the VSM to be constructed at its original high di-
mension. Despite these similarities, RMI and RI 
are motivated by different applications and math-−0.5 

0 

0.5 

1 
0.8 
0.9 

</table></figure>

			<note place="foot" n="1"> Amongst other representation frameworks. 2 See Martin and Berry (2007) for an overview of the mathematical foundation of LSA.</note>

			<note place="foot" n="5"> Please note that there are matrix factorization techniques that guarantee the least distortion in the 1 distances, see e.g. Kwak (2008). 6 Matrix factorization techniques, in general.</note>

			<note place="foot" n="7"> That is E(x) = ∞, since x has a Cauchy distribution. 8 That is, (A + B)C = AC + BC.</note>

			<note place="foot" n="10"> That is, n context elements are modelled in an ndimensional VSM.</note>

			<note place="foot" n="14"> Please note that according to the axioms in the distance definition, the distance between two numbers is always a nonnegative value. When index vectors consist of non-zero elements of real numbers, the value of |ui − vi| can be between 0 and 1, i.e. 0 &lt; |ui − vi| &lt; 1. Therefore, ln(|ui − vi|) can be a negative number and thus the exponent scale is required to make sure that the result is a non-negative number.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This publication has emanated from research conducted with the financial support of Sci-ence Foundation Ireland under Grant Number SFI/12/RC/2289.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Database-friendly random projections</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth ACM SIGMOD-SIGACTSIGART Symposium on Principles of Database Systems, PODS &apos;01</title>
		<meeting>the Twentieth ACM SIGMOD-SIGACTSIGART Symposium on Principles of Database Systems, PODS &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="274" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast low-rank modifications of the thin singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Special Issue on Large Scale Linear and Nonlinear Eigenvalue Problems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the impossibility of dimension reduction in l1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charikar2005] Bo</forename><surname>Brinkman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><surname>Brinkman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="766" to="788" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Deerwester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stable distributions, pseudorandom generators, embeddings and data stream computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science, 2000. Proceedings. 41st Annual Symposium on</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stable distributions, pseudorandom generators, embeddings, and data stream computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference in modern analysis and probability</title>
		<editor>Johnson and Lindenstrauss1984] William Johnson and Joram Lindenstrauss</editor>
		<meeting><address><addrLine>New Haven, Conn</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1982" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="189" to="206" />
		</imprint>
	</monogr>
	<note>J. ACM</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random indexing of text samples for latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of the Cognitive Science Society</title>
		<meeting>the 22nd Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="2000-01" />
			<biblScope unit="page" from="103" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse distributed memory and related models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pentti</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Associative neural memories: theory and implementation</title>
		<editor>Mohamad H. Hassoun</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press, Inc</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="50" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Robust subspace computation using 1 norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade2003] Qifa Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS-03-172</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Principal component analysis based on l1-norm maximization. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1672" to="1680" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Sept</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measures of distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL &apos;99<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From context to meaning: Distributional models of the lexicon in linguistics and cognitive science, special issue of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Italian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Distributional semantics in linguistic and cognitive research</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear estimators and tail bounds for dimension reduction in L 1 using cauchy random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2497" to="2532" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Very sparse stable random projections for dimension reduction in l α (0 &lt; α &lt; 2) norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07</title>
		<meeting>the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimators and tail bounds for dimension reduction in α (0 &lt; α ≤ 2) using stable random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA &apos;08</title>
		<meeting>the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA &apos;08<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Handbook of latent semantic analysis, chapter Mathematical foundations behind latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">I</forename><surname>Berry2007] Dian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="35" to="55" />
			<pubPlace>Ro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An introduction to random indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using distributional similarity to organise biomedical terminology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<editor>James Dowdall, Gerold Schneider, Bill Keller, and David Weir</editor>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Terminology</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
