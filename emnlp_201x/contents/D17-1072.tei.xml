<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Grained Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
							<email>cgong@stu.suda.edu.cn, {zhli13,minzhang}@suda.edu.cn, xzjiang.hw@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhou</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Grained Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="692" to="703"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditionally, word segmentation (WS) adopts the single-granularity formalism , where a sentence corresponds to a single word sequence. However, Sproat et al. (1996) show that the inter-native-speaker consistency ratio over Chinese word boundaries is only 76%, indicating single-grained WS (SWS) imposes unnecessary challenges on both manual annotation and statistical modeling. Moreover, WS results of different gran-ularities can be complementary and beneficial for high-level applications. This work proposes and addresses multi-grained WS (MWS). First, we build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling. Experiments and analysis lead to many interesting findings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the first processing step of Chinese language processing, word segmentation (WS) has been extensively studied and made great progress during the past decades, thanks to the annotation of large-scale benchmark datasets, among which the most widely- used are Microsoft Research Corpus (MSR) ( <ref type="bibr" target="#b7">Huang et al., 2006</ref>), Peking University * Correspondence author MSR PPD CTB <ref type="table">Table 1</ref>:</p><p>An example of annotation heterogeneity: (all) (country) (every) (place) (medical science) (field) (experts) (walk) (out) (people) (great hall).</p><p>People Daily Corpus (PPD) ( <ref type="bibr">Yu et al., 2003)</ref>, and Penn Chinese Treebank (CTB) ( <ref type="bibr">Xue et al., 2005</ref>). <ref type="table">Table 1</ref> gives an example sentence segmented in different guidelines. Meanwhile, WS approaches gradually evolve from maximum matching based on lexicon dictionaries ( <ref type="bibr" target="#b15">Liu and Liang, 1986)</ref>, to path searching from segmentation graphs based on language modeling scores and other statistics ( <ref type="bibr">Zhang and Liu, 2002</ref>), to character-based sequence labeling <ref type="bibr">(Xue, 2003)</ref>, to shift-reduce incremental parsing ( <ref type="bibr">Zhang and Clark, 2007)</ref>. Recently, neural network models have also achieved success by effectively learning representation of characters and contexts ( <ref type="bibr">Zheng et al., 2013;</ref><ref type="bibr">Pei et al., 2014;</ref><ref type="bibr" target="#b16">Ma and Hinrichs, 2015;</ref><ref type="bibr" target="#b10">Zhang et al., 2016;</ref><ref type="bibr" target="#b0">Cai and Zhao, 2016;</ref><ref type="bibr" target="#b13">Liu et al., 2016</ref>).</p><p>To date, all the labeled datasets adopt the single-granularity formalization, and previous research mainly focuses on single-grained WS (SWS), where one sentence is segmented into a single word sequence. Although different WS guidelines share the same high-level criterion of word boundaries -a character string com- bined closely and used steadily forms a word, people greatly diverge due to individual differ- ences on knowledge and living environments, etc. An anonymous reviewer kindly points out that Vladímir Skalička of the Prague School claimed that unlike the "isolating" languages such as French and English, Chinese belongs to the "polysynthetic" type, in which compound words are normally produced from indigenous morphemes <ref type="bibr" target="#b8">(Jernudd and Shapiro, 1989)</ref>. The vague distinction between mor- phemes and compounds also contribute to the cognition divergence on the concept of words. <ref type="bibr">Sproat et al. (1996)</ref> show that the consensus ratio over word boundaries is only 76% among Chinese native speakers without trained on a common guideline.To fill this gap, WS guidelines need to further group words into many types and provide illustration examples for each type. Nevertheless, it is very challenging even for well-trained annotators to fully grasp the guidelines and to be consistent on uncovered cases. For example, <ref type="bibr">Xiu (2013)</ref> (in <ref type="table" target="#tab_3">Tables 1-3)</ref> shows that about 3% charac- ters are inconsistently segmented in the PPD training data used in <ref type="bibr">SIGHAN Bakeoff 2005</ref><ref type="bibr" target="#b5">(Emerson, 2005</ref>. We have also observed many inconsistency cases in all MSR/PPD/CTB during this work. In a word, SWS imposes great challenge on data annotation, and as a side effect, enforces statistical models to learn subtleness of annotation guidelines rather than the true WS ambiguities.</p><p>From another perspective, WS results of different granularities may be complementary in supporting applications such as information retrieval (IR) ( <ref type="bibr" target="#b14">Liu et al., 2008)</ref> and machine translation (MT) ( <ref type="bibr">Su et al., 2017</ref>). On the one hand, coarse-grained words enable statistical models to perform more exact matching and analyzing. On the other hand, fine-grained words are helpful in both reducing data sparse- ness and supporting deeper understanding of language. <ref type="bibr">1</ref> To solve the above two issues for SWS, this paper proposes and addresses multi-grained WS (MWS). Given an input sentence, the goal is to produce a hierarchy structure of all words of different granularities, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. To tackle the lack of labeled data, we build a large-scale pseudo MWS dataset for model training and tuning by automatically converting annotations of three heterogeneous 1 Words in CTB are generally more fine-grained than those in PPD and MSR, probably due to the requirement of annotating syntactic structures. SWS datasets (i.e. MSR/PPD/CTB) based on the recently proposed coupled sequence labeling approach of <ref type="bibr" target="#b9">Li et al. (2015)</ref>. In order to fully investigate the problem, we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling problems. Experiments and data analysis lead to many interesting findings.</p><p>We will release the newly annotated data and the codes of the benchmark approaches at http://hlt.suda.edu.cn/~zhli. However, due to the license issue, we may not directly release all the pseudo MWS datasets. Instead, we will launch a web service for obtaining MWS annotations given a sentence with one of MSR/PPD/CTB annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pseudo MWS Data Conversion</head><p>This section introduces the process of gather- ing pseudo MWS data by making use of the annotation heterogeneity of the three existing datasets, i.e., MSR/PPD/CTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation Heterogeneity</head><p>MSR is a manually labeled corpus with word boundaries and named entity tags, and is annotated by Microsoft Research Asia for supporting Chinese text processing ( <ref type="bibr" target="#b7">Huang et al., 2006</ref>). The key characteristic of MSR is treating named entities as single words. For example, " (Great Hall of the People)" is a location and forms a word in <ref type="table">Table 1</ref>. In general, MSR is more coarse- grained than PPD and CTB. PPD is a large- scale corpus with word boundaries, POS tag- ging, and phonetic notations to facilitate Chi-nese information processing, and is annotated by Institute of Computational Linguistics at Peking University ( <ref type="bibr">Yu et al., 2003</ref>). Based on the Penn Chinese Treebank Project, CTB is built to create a Mandarin Chinese corpus with syntactic bracketing ( <ref type="bibr">Xue et al., 2005</ref>). We find that CTB is more fine-grained in word boundaries than MSR and PPD, since syntactic annotation tends to require deeper understanding of a sentence. For example, <ref type="table" target="#tab_7">Table 5</ref> reports the averaged number of char- acters per word in each corpus, and confirms our observations.</p><p>For better understanding of annotation het- erogeneity, we summarize high-frequency dif- ferences among the three datasets observed and gathered during this study in Appendix A. However, it is difficult to obtain a complete list of annotation correspondences among the three datasets, since there are too many low- frequency and irregular cases. Moreover, we also observe a lot of inconsistency annota- tions of the same word or words with similar structures in all three datasets, as shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Coupled WS for Conversion: MSR/PPD as Example</head><p>This section introduces how to automatically produce high-quality PPD-side WS labels for a sentence with MSR-side gold-standard WS labels, by leveraging the two non-overlapping SWS data of MSR and PPD with the coupled sequence labeling approach of <ref type="bibr" target="#b9">Li et al. (2015)</ref> and <ref type="bibr" target="#b10">Li et al. (2016)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows the workflow. Given a sentence x = [c 1 , ..., c i , ..., c n ], the coupled model aims to produce a sequence of bundled tags t = [t a 1 t b 1 , ..., t a i t b i , ..., t a n t b n ], where t a i and t b i are two labels corresponding to two heterogeneous guidelines respectively. <ref type="table" target="#tab_1">Table 2</ref> gives an example of coupled WS on MSR/PPD. We employ the standard four-tag label set to mark word boundaries of one gran- ularity, among which B, I, E respectively rep- resent that the concerned character situates at the beginning, inside, end position of a word, and S represents a single-character word. The bottom row shows the gold-standard bundled tag sequence.</p><p>One key advantage of the coupled model is to directly learn from two non-overlapping   Two WS labels are bundled to represent MSR/PPD annotations for a character.</p><formula xml:id="formula_0">MSR-train/dev/test w/ ambiguous labeling MSR-train/dev/test</formula><p>Ambiguous labeling is gained supposing this sentence has MSR-side gold- standard annotations.</p><p>heterogeneous training datasets, where each dataset only contains single-side gold-standard labels. To deal with this partial (or incom- plete) labeling issue, they project each single- side label to a set of bundled labels by consid- ering all labels at the missing side, as shown in the second row in <ref type="table" target="#tab_1">Table 2</ref>. Such ambiguous labelings are used for model supervision.</p><p>Under a traditional CRF, the coupled model defines the score of a bundled tag sequence as</p><formula xml:id="formula_1">Score(x, t; θ) = θ · f(x, t) = n+1 ∑ i=1 θ ·    f joint (x, i, t a i−1 t b i−1 , t a i t b i ) f sep_a (x, i, t a i−1 , t a i ) f sep_b (x, i, t b i−1 , t b i )   </formula><p>where f joint (.) are the joint features whereas f sep_a/sep_b (.) are the separate features. <ref type="bibr" target="#b9">Li et al. (2015)</ref> demonstrate that the joint fea- tures capture the implicit mappings between heterogeneous annotations, while the back-off separate features work as a remedy for the sparseness of the joint features. In their case study of POS tagging, <ref type="bibr" target="#b9">Li et al. (2015)</ref> show the coupled model improves tagging accuracy by 95.0 − 94.1 = 0.9% on CTB5-test over the baseline non-coupled model trained on a single training data.</p><formula xml:id="formula_2">MSR-train/dev/test CTB-train/dev/test PPD-train/dev/test</formula><p>More importantly, they show that the cou- pled model can be naturally used for the task of annotation conversion, where second-side labels are automatically annotated, given one- side gold-standard labels. The given one-side tags are used to obtain ambiguous labelings, as shown in <ref type="table" target="#tab_1">Table 2</ref>, and the coupled model finds the best bundled tag sequence in the constrained search space, instead of in the whole bundled tag space, hence greatly reduc- ing the difficulty. <ref type="bibr" target="#b9">Li et al. (2015)</ref> report that the coupled model can improve conversion accuracy on POS tagging by 93.9 − 90.6 = 3.3% over the non-coupled model. <ref type="bibr">2</ref>  <ref type="figure" target="#fig_2">Figure 3</ref> shows the workflow of producing pseudo MWS data with three separately trained coupled models. Please note that one coupled model is able to perform conversion between one pair of annotation standards, and thus three coupled models are required for three kinds of annotation standards. Another alternative is that we could directly train one coupled model on MSR/PPD/CTB by extending the approach of <ref type="bibr" target="#b9">Li et al. (2015)</ref> from two guidelines into three, which would lead to a much larger bundled tag space. For simplicity, we directly employ their released codes in this work, and leave that for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Producing Pseudo MWS Data</head><p>After conversion, we obtain 9 pseudo MWS datasets (i.e., MSR/PPD/CTB- train/dev/test) and represent each sentence in a hierarchy structure as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Please kindly note that the guideline-specific information are thrown away, since we do not care which word belongs to which guideline.</p><p>In the resulting pseudo MWS data, we find about 0.08% of words overlap with other words, meaning a string "ABC" is segmented into "A/BC" and "AB/C" in two different annotations. We have manually checked these words, and find almost all those cases are caused by conversion errors. This confirms that our treatment of MWS as a hierarchy structure is reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Manual Annotation</head><p>In order to fully investigate the MWS problem, we have manually created a true MWS data of 1,500 sentences for final evaluation. From each test dataset in <ref type="table" target="#tab_7">Table 5</ref>, we randomly sample 500 sentences with converted pseudo MWS annotations for manual correction. First, two coauthors of this work spent about two hours each day on manual correction of the pseudo MWS annotations for two weeks. During this period, we have summarized a list of high- frequency corresponding patterns among the three guidelines (see Appendix A), and have also written a simple program to automati- cally detect inconsistent annotations of given words in different training datasets, so that annotators can use the outputs of the program to decide ambiguous cases, which we find is extremely helpful for annotation.</p><p>Then, we employ 10 postgraduate students as our annotators who are at different fa- miliarity in WS annotation. Before formal annotation, the annotators are trained for two hours on the basic concepts of MWS, high- frequency correspondences among the three guidelines, and the use of the outputs of the program. We also encourage the annotators to access the three training datasets directly for studying concrete cases under real contexts. Moreover, annotators are asked to recheck their annotations before final submission to improve quality.</p><p>To measure the inter-annotator consistency, 150 sentences (10%) are sampled for double annotation, and are grouped into four batches for four pairs of annotators. After annotation, two annotators on the same batch compare   their results and produce a consensus submis- sion through discussion. The annotation process lasts for four days, and each annotator spends about 8 hours in total on completing 160 sentences on average. <ref type="table" target="#tab_3">Table 3</ref> compares data statistics on the 1, 500 sentences before and after manual annotation. The second column reports the number of words, and the last three columns report the distribution of words according to their gran- ularity levels. To illustrate how to gain the distribution, we take <ref type="figure" target="#fig_0">Figure 1</ref> as an example, which contains 1 single-grained words, 9 two- grained words, and 7 three-grained words. 3 <ref type="table" target="#tab_3">Table 3</ref> shows that only 71.6% of all words are single-grained, which is somehow roughly consistent with the inter-native-speaker con- sistency ratio (76%) in <ref type="bibr">Sproat et al. (1996</ref> = 1.5%, and the number of multi- grained words by 74.5 − 71.6 = 2.9%. In fact, during annotation, we also feel that multi- granularity phenomena are under-represented in the pseudo MWS data. The reason may be two-fold. First, the conversion models incline to suppress granularity differences, since most words have the same granularity in different datasets. Second, the exist of many incon- sistencies in the same dataset also makes the conversion models more reluctant to produce multi-grained words.</p><p>The inter-annotator consistency ratio is 3859 3935 = 98.07%, where the denominator is the word number after merging the submission of all annotator pairs, and the numerator is the consensus word number. We argue that the consistency ratio is not high, considering most words do not need correction in the pseudo MWS annotations. In fact, we find that this annotation task is actually very difficult, since the annotators must consider three guidelines simultaneously. The main inconsistency source of all four annotator pairs are due to the situation where one annotator notices a mistake while another annotator overlooks it. To solve this issue, our long-term plan is to compile a unified MWS guideline by integrating existing SWS guidelines, and gradually improve it by more manual MWS annotation. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmark MWS Approaches</head><p>There has recently been a surge of interest in applying neural network models to both parsing and sequence labeling tasks. In this work, we propose three simple benchmark approaches for MWS, inspired by recently neural models for constituent parsing (Cross and Huang, 2016) and SWS ( <ref type="bibr">Pei et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MWS as Constituent Parsing</head><p>Due to its hierarchy structure shown in <ref type="figure" target="#fig_0">Figure  1</ref>, we naturally cast MWS as a constituent parsing problem, where characters are leaf nodes; "C" represent a character, "W" rep- resent a word; "X" means that the spanning word cannot be further merged into a more coarse-grained word. We employ the recently proposed transition- based constituent parser of Cross and Huang (2016) due to its simplicity and competitive performance on different parsing benchmark datasets. In the transition system, a stack S stores processed tokens and partial trees col- lected so far; a queue Q contains unprocessed tokens; structural 5 and labeling 6 decisions are alternatively made to advance the state until a complete tree forms. The network architecture is composed of two parts: 1) two cascaded   bidirectional LSTM layers to encode the input token sequence, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>; 2) two separate multilayer perceptrons (MLPs) to make structural/labeling decisions based on 4/3 simple LSTM span features. A span feature represents a sentence span (i, j) by concatenating the element-wise differences of BiLSTM outputs:</p><note type="other">... f 2 1 ; b 2 1 f 2 2 ; b 2 2 f 2 3 ; b 2 3 ...</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings</head><formula xml:id="formula_3">r (i,j) = [f 1 j −f 1 i−1 ; b 1 i −b 1 j+1 ; f 2 j −f 2 i−1 ; b 2 i −b 2 j+1 ]</formula><p>To adapt the original parsing model to our MWS task, we concatenate bichar embeddings e c i−1 c i with single char embedding e c i as inputs to the first-layer BiLSTM, inspired by <ref type="bibr">Pei et al. (2014)</ref>, who show that bichar embed- dings are very helpful for SWS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MWS as Sequence Labeling</head><p>It is also straightforward to model MWS as a sequence labeling task by replacing SWS labels with MWS labels for each character. <ref type="table" target="#tab_5">Table 4</ref> encodes the MWS structure in <ref type="figure" target="#fig_0">Figure  1</ref> with a sequence of MWS labels. The idea is to concatenate multiple SWS tags simultaneously for one character to denote the positions of the character under words of different granularities. Please note that each MWS label contains at most three SWS labels since we only consider three SWS datasets in this work. Here, we organize the SWS labels in the order of fine-to-coarse granularities.</p><p>For simplicity and fair comparison, we adopt a similar network architecture as the parsing  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MWS as SWS Aggregation</head><p>Instead of directly training a MWS model on the three pseudo MWS training datasets, we can also train three separate SWS models on the three SWS training datasets. Given an input sentence, we apply the three SWS models and then merge their outputs as MWS results.</p><p>The network architecture is the same with the sequence labeling model in Section 4.2, except the MLP outputs correspond to SWS labels instead of MWS labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Data: for MSR, we adopt the training/test datasets of the <ref type="bibr">SIGHAN Bakeoff 2005</ref><ref type="bibr" target="#b5">(Emerson, 2005</ref>), and cut off 10% random training sentences as the dev data following <ref type="bibr" target="#b10">Zhang et al. (2016)</ref>; for PPD and CTB, we follow <ref type="bibr" target="#b9">Li et al. (2015)</ref> and directly adopt their datasets and data split. <ref type="table" target="#tab_7">Table 5</ref> shows the data statistics. 7</p><p>Evaluation Metrics: the goal of MWS is to precisely produce all words of different granularities given the input sentence. There- fore, to reach a balance of both precision (P = #Word gold∩sys #Word sys ) and recall (R = #Word gold∩sys #Word gold ),</p><p>we use the F1 score (= 2P R P +R ) as in SWS. Hyper-parameter: we implement all our approaches based on the codes released by <ref type="bibr" target="#b3">Cross and Huang (2016)</ref>, by making exten- sions such as adding bichar embeddings and   <ref type="bibr" target="#b3">Cross and Huang (2016)</ref>. The dimensions of char and bichar embeddings are both 50 and other hyper-parameters are the same with <ref type="bibr" target="#b3">Cross and Huang (2016)</ref>. In our preliminary experiments, we observe that under their neural network framework, the MWS performance is quite stable when rerun- ing under random initialization or reasonably altering other hyper-parameters. Due to time limitation, we leave the use of pre-trained embeddings and more hyper-parameter tuning for future exploration.</p><p>Training/test settings: when training the parsing and sequence labeling based MWS models (not SWS aggregation) on MSR/PPD/CTB-train, we adopt the simple corpus weighting strategy used in <ref type="bibr" target="#b9">Li et al. (2015)</ref> to balance the contributions of each training dataset. Before each iteration, we randomly sample 10,000 sentences from each training dataset, and merge and shuffle them for one-iteration training. We use merged MSR/PPD/CTB-dev as the MWS dev data for model selection. <ref type="bibr">9</ref> For the SWS aggregation model, three SWS models are separately trained on the three training/dev datasets. For evaluation, three SWS outputs produced independently are merged as one MWS result given a sentence.</p><p>In all experiments, training stops when F- score on the dev data does not improve in 20 consecutive iterations, and we choose the model that performs best on the dev data for final evaluation.</p><p>Main results: <ref type="table" target="#tab_9">Table 6</ref> reports the per- formance of different approaches on both the pseudo MWS dev data and the manually annotated MWS test data. The "#Word" column reports the total number of words returned by the corresponding model; the following three columns show the percentages of words of different granularities; the last "Overlapping" column gives the percent of words that overlap with other words, which only happens in the "SWS aggregation" ap- proach, since no constraint can be applied to the three separate SWS models during testing.</p><p>From the results, we can draw the following findings.</p><p>First, the results suggest that using pseudo training and dev datasets to build a MWS model is feasible, based on two evidences: 1) our simple benchmark model can reach a high F-score of 96.07% on the manually annotated test data, which is 1.77% higher than directly aggregating outputs of three SWS models; 2) the P/R/F scores on the pseudo dev data and on the manually labeled test data are quite consistent in general, indicating that it is reliable to use the pseudo dev data for model selection and tuning.</p><p>Second, the parsing approach and the sequence labeling approach (with or without bichar embeddings) achieve very similar performance (within 0.15% vibration), More importantly, the parsing approach produces more words and more multi-grained words than the sequence labeling approach, indicating that it is potentially more proper to model MWS as a parsing problem in order to better capture and represent multi-granularity structures. Another possible disadvantage of the sequence labeling approach is that the trained model cannot produce more granularity levels (e.g., four-grained) beyond those in the training data. Nevertheless, compared against the manual annotations in <ref type="table" target="#tab_3">Table 3</ref>, both the parsing and sequence labeling approaches retrieve much less multi-grained words, which is caused by the under-representation issue of the pseudo training data, as discussed in Section 3.</p><p>Third, the SWS aggregation approach achieves the best recall at the price of very low precision on both dev/test data. We believe the reason is that training three SWS models separately on one of the three training datasets has two disadvantages: 1) connections among different guidelines are totally ignored, leading to many overlapping words (1.0%); 2) smaller training data also degrades the performance of each SWS model. Finally, using bichar embeddings turns out very helpful for MWS, and leads to 0.97 ∼ 1.18% F-score improvement on dev data and 0.62 ∼ 0.85% on test data, which is consistent with the SWS results in <ref type="bibr">Pei et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>As far as we know, this is the first work that formally proposes and addresses the problem of Chinese MWS under the data-driven ma- chine learning framework. It is true that the industrial community, driven by practical demand, has long been interested in retriev- ing words of different granularities from the engineering perspective, based on lexicon dic- tionaries and heuristic rules ( <ref type="bibr">Zhu and Li, 2008;</ref><ref type="bibr" target="#b6">Hou et al., 2010</ref>). We also discover two pub- licly released toolkits, i.e., IKAnalyzer 10 and PoolWord 11 , which consider all substrings in a sentence and return those above a threshold probability as candidate words. In contrast, this paper defines MWS as a strict hierarchy structure, and propose a supervised learning framework for the problem.</p><p>To alleviate the high OOV-ratio issue of character-based sequence labeling, <ref type="bibr">Zhang et al. (2006)</ref> and <ref type="bibr">Zhao and Kit (2007)</ref> propose subword-based sequence labeling for word segmentation by extracting high- frequency subword and treating them as the basic labeling units. <ref type="bibr" target="#b11">Li (2011)</ref> and <ref type="bibr" target="#b12">Li and Zhou (2012)</ref> propose to jointly parse the internal structures of words and syntactic structure of a sentence. Their definition of internal structures mainly considers prefix or suffix information. They manually annotate the internal structures of words that have high-frequency prefixes or suffixes and left other words with flat structures in CTB. <ref type="bibr">Zhang et al. (2013)</ref> further annotate internal structures of all words in CTB and then perform character-level parsing with WS labels. <ref type="bibr" target="#b2">Cheng et al. (2015)</ref> propose to cope with the multiple WS standard problem based on internal word structures. After close study of the above works, we find that the MWS annotations automatically built in this work actually capture a lot of subwords and word internal structures in previous works. Most importantly, the main focus of previous works is to improve SWS or parsing performance, whereas this work aims to build a hierarchy structure of multi-grained words. We leave the integration of MWS and parsing for future work.</p><p>It has been a long debate whether there exists an optimal WS granularity for MT, which is further complicated by the inevitable mistakes contained in 1-best WS outputs. <ref type="bibr" target="#b4">Dyer et al. (2008)</ref> propose an MT model based on source-language word lattices, obtained by merging the outputs of different segmenters. <ref type="bibr">Xiao et al. (2010)</ref> propose joint SWS and MT based on word lattices. Recently, <ref type="bibr">Su et al. (2017)</ref> propose a word lattice-based neural MT model. They train many segmenters on MSR/PPD/CTB, and merge the outputs to produce word lattices for source-language sentences, which is similar to our SWS ag- gregation approach. All above works show the usefulness of word lattices instead of a single SWS output. In help IR, <ref type="bibr" target="#b14">Liu et al. (2008)</ref> propose a ranking based WS approach for producing words of different granularities. We believe this work can further help both IR and MT by supplying with more accurate MWS results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This work proposes and addresses the prob- lem of MWS, so that all words of different granularities can be captured in a hierarchy structure given a sentence. We can draw the following interesting findings.</p><p>(1) Our annotation conversion approach can gather high-quality pseudo MWS training/dev datasets, and hence it is feasible to use them for model training and tuning.</p><p>(2) Manual MWS data annotation tells us that about 28.4% words are multi-grained, and among them 94.4% are two-grained words.</p><p>(3) The parsing and sequence labeling approaches achieve very similar performance, and outperform the SWS aggregation approach by a large margin.</p><p>We believe there are many exploration di- rections for this new task, among which we are particularly interested in three in the near future: 1) improving our benchmark approaches by considering task-specific fea- tures and neural network architectures, 2) verifying the usefulness of MWS to high- level applications such as MT, 3) integrating MWS with syntactic parsing in some way by exploiting existing treebanks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MWS as a constituent parse tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conversion between MSR/PPD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Producing pseudo MWS data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two-layer BiLSTM architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Coupled WS (MSR/PPD as 
example). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Data statistics of the MWS test data 
before and after manual annotation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>MWS as sequence labeling. SWS 
labels for the same character are organized 
fine-to-coarse. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 5 : Data statistics (in sentence number).</head><label>5</label><figDesc></figDesc><table>The last column reports the averaged charac-
ter number of each word. 

model described in Section 4.1. To decide 
the MWS label of a character c i in the input 
sentence, we feed the outputs of the two-layer 
BiLSTM outputs [f 1 
i ; b 1 
i , f 2 
i ; b 2 
i ] into a single-
hidden-layer MLP. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance of different MWS approaches. 

supporting sequence labeling. 8 For simplicity, 
char and bichar embeddings are randomly 
initialized following </table></figure>

			<note place="foot" n="2"> The accuracy seems quite low. The reason is only the 20% most ambiguous words of each sentence are manually labeled and evaluated in their experiments.</note>

			<note place="foot" n="3"> Formally, we call a word s three-grained if there are two other words s1 and s2 satisfying any one conditions: 1) s2 ∈ s1 ∈ s (like &quot;&quot; in Figure 1); 2) s2 ∈ s ∈ s1 (like &quot;&quot;); 3) s ∈ s1 ∈ s2 (like &quot;&quot;), where ∈ means substring. The definition of twograined words is analogous; otherwise single-grained.</note>

			<note place="foot" n="4"> Although this work has been confined to the three guidelines of MSR/PPD/CTB, we feel that the three guidelines can well capture most multi-granularity phenomena of words. During manual annotation, we have found very few cases where an obvious multigranularity structure is not covered by the three guidelines. 5 Shifting the first token in Q into S, or combining the top two items in S 6 Assigning a non-terminal label or &quot;NULL&quot; to the top item in S</note>

			<note place="foot" n="7"> A DBC-to-SBC (double/single-byte characters) case preprocessing is performed on all datasets to avoid encoding inconsistency.</note>

			<note place="foot" n="8"> https://github.com/jhcross/span-parser. We are very grateful for their helping us solve some code issues at the early stage of this work. 9 For MSR-dev, only the first 3, 000 sentences are used during training due to efficiency concern.</note>

			<note place="foot" n="10"> https://github.com/medcl/elasticsearch-analysis-ik 11 http://pullword.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anony-mous reviewers for the helpful comments. We are greatly grateful to all students in LA group for their hard work as annotators. Espe-cially, we thank Jiawei Sun for her extensive participant and support through this work. We also thank Wenliang Chen and Guodong Zhou for the helpful discussions. This work was supported by National Natural Science Foundation of China <ref type="bibr">(Grant No. 61525205, 61373095, 61502325</ref>).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Collected Annotation Inconsistencies</head><p>In MSR</p><p>(1) " (one day)" is annotated as " (one day)" or " (one)/ (day)".</p><p>(2) " (too much)" is annotated as " (too much)" or " (too)/ (much)".</p><p>(3) " (this one)" and " (this item)" have the same stucture of " (this)" + #, however they are annotated as " (this one)" and " (this)/ (item)" respectively.</p><p>(4) " (the same to)" and " (owe to)" have the same stucture of # + " (to)", however they are annotated as " (the same)/ (to)" and " (owe to)" respectively.</p><p>(5) " (nuclear weapon)" and " (nuclear technology)" have the same stucture of " (nuclear)" + #, however they are annotated as " (nuclear weapon)" and " (nuclear)/ (technology)" respectively.</p><p>(6) " (the next step)" and " (the next game)" have the same stucture of " (the next)" + #, however they are annotated as " (the next step)" and " (next)/ (one)/ (game)" respectively.</p><p>(7) " (deputy director)" and " (vice-president)" have the same stucture of " (vice)" + #, however they are annotated as " (deputy)/ (director)" and " (vice- president)" respectively.</p><p>(8) " (worker)" and " (creator)" have the same stucture of # + " (-er/or)", however they are annotated as " (worker)" and " (create)/ (-or)" respectively.</p><p>(9) " (cross century)" and " (cross border)" have the same stucture of "" + # (cross + #), however they are annotated as " (cross century)" and " (cross)/ (border)"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In PPD</head><p>(1) " (ministerial level)" is annotated as " (ministerial level)" or " (ministerial)/ (level)".</p><p>(2) " (one day)" is annotated as " (one day)" or " (one)/ (day)".</p><p>(3) " (too much)" is annotated as " (too much)" or " (too)/ (much)".</p><p>(4) " (biggest)" is annotated as " (biggest)" or " (most)/ (big)".</p><p>(5) " (and also)" is annotated as " (and also)" or " (also)/ (have)".</p><p>(6) " (reward greatly)" is annotated as " (reward)" or " (reward)/ (greatly)".</p><p>(7) " (by means of)" and " (owe to)" have the same stucture of # + " (to)", however they are annotated as " (by means of)" and " (owe)/ (to)" respectively.  Appendix B: See <ref type="table">Table 7</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural word segmentation learning for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long shortterm memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthetic word parsing improves chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spanbased constituency parsing with a structurelabel system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalizing word lattice translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Method and device for providing multi-granularity word segmentation result</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoling</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rengang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjing</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Chinese Patent</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tokenization Guidelines of Chinese Text (V5.0, in Chinese)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Microsoft Research Asia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Politics of Language Purism (page 214)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Björn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jernudd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Mouton de Gruyter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coupled sequence labeling on heterogeneous annotations: POS tagging as a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1783" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast coupled sequence labeling on heterogeneous annotations via context-aware pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing the internal structure of words: A new paradigm for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1405" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unified dependency parsing of chinese morphological and syntactic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1445" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring segment representations for neural segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2880" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Information retrieval oriented word segmentation based on character association strength ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1061" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fundation of Chinese processing: statistics of modern Chinese word frequencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyuan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
	<note>in Chinese</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate linear-time chinese word segmentation via embedding matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1733" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">have the same stucture of # + &quot; (Union)&quot;, however they are annotated as &quot; / (Yugoslavia Union)&quot; and &quot; (Yugoslavia Union)&quot; respectively</title>
	</analytic>
	<monogr>
		<title level="m">The CPC Central Committee)&quot; and &quot; (Vietnamese Communist Party)</title>
		<imprint/>
	</monogr>
	<note>The CPC Central Committee. and &quot; (Vietnamese Communist Party)/ (central)&quot; respectively</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">and &quot; (the next game)&quot; have the same stucture of</title>
		<imprint/>
	</monogr>
	<note>the next step. the next)&quot; + #, however they are annotated as &quot; (the next step)&quot; and &quot; (next)/ (one)/ (game)&quot; respectively</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">+ # (cross + #), however they are annotated as &quot; (go beyond)/ (year)&quot; and &quot; (cross border)</title>
	</analytic>
	<monogr>
		<title level="j">respectively. In CTB</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
		</imprint>
	</monogr>
	<note>and &quot; (cross border)&quot; have the same stucture of. heavyweight)&quot; is annotated as &quot; (heavyweight)&quot; or &quot; (heavy)/ (weight</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">(one day)&quot; is annotated as</title>
		<imprint/>
	</monogr>
	<note>one day)&quot; or &quot; (one)/ (day</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">(re-employment)&quot; is annotated as &quot; (reemployment)&quot; or &quot; (once again)/ (employment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">(and also)&quot; is annotated as</title>
		<imprint/>
	</monogr>
	<note>and also)&quot; or &quot; (also)/ (have</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">(vice-president)&quot; is annotated as &quot; (vicepresident)&quot; or &quot; (vice)/ (president</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">change into)&quot; is annotated as &quot; (change into)&quot; or &quot; (change) (into</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">have the same stucture of # + &quot; (to)&quot;, however they are annotated as &quot; (beneficial to)&quot; and &quot; (owe)/ (to)&quot; respectively. (11) &quot; (go beyond the year)&quot; and &quot; (cross border)&quot; have the same stucture of</title>
		<imprint/>
	</monogr>
	<note>+ # (cross + #), however they are annotated as. go beyond)/ (year)&quot; and &quot; (cross border)&quot; respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
