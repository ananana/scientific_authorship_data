<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extractive Summarization Using Multi-Task Learning with Document Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Isonuma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Fujino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichiro</forename><surname>Mori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Sakata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extractive Summarization Using Multi-Task Learning with Document Classification</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2101" to="2110"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The need for automatic document sum-marization that can be used for practical applications is increasing rapidly. In this paper, we propose a general framework for summarization that extracts sentences from a document using externally related information. Our work is aimed at single document summarization using small amounts of reference summaries. In particular, we address document sum-marization in the framework of multi-task learning using curriculum learning for sentence extraction and document classification. The proposed framework enables us to obtain better feature representations to extract sentences from documents. We evaluate our proposed summarization method on two datasets: financial report and news corpus. Experimental results demonstrate that our summarizers achieve performance that is comparable to state-of-the-art systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With rapid increase in the volume of textual data that are available both online and offline, the need for automatic document summarization that can be implement in practical scenarios is in- creasing ( <ref type="bibr" target="#b4">Chopra et al., 2016;</ref><ref type="bibr" target="#b25">Takase et al., 2016</ref>). Among the several sum- marization systems, extractive summarization ap- proaches ( <ref type="bibr" target="#b7">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b17">McDonald, 2007;</ref><ref type="bibr" target="#b27">Wong et al., 2008)</ref> are widely used. These techniques identify and subsequently concatenate relevant sentences automatically from a docu- ment to create its summary while preserving its original information content. Such approaches are popular and widely used for practical appli- cations because they are computationally cost- effective and less complex. Extractive summariza- tion approaches based on neural network-based approaches <ref type="bibr" target="#b10">(Kågebäck et al., 2014;</ref><ref type="bibr" target="#b2">Cao et al., 2015;</ref><ref type="bibr" target="#b30">Yin and Pei, 2015;</ref><ref type="bibr" target="#b1">Cao et al., 2016</ref>) have ad- vanced rapidly. Recently, an attentional encoder- decoder for extractive single-document summa- rization was proposed and its application to the news corpus was demonstrated <ref type="bibr" target="#b3">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b19">Nallapati et al., 2017)</ref>.</p><p>The neural network-based approaches rely heavily on large amounts of reference sum- maries for training neural models, and conse- quently, for tuning a large number of parameters. The reference summaries are manually or semi- automatically created in advance. Some existing studies employ parallel corpora as artificial ref- erence summaries <ref type="bibr" target="#b28">(Woodsend and Lapata, 2010;</ref><ref type="bibr" target="#b3">Cheng and Lapata, 2016)</ref>. However, preparing such large volumes of reference summaries man- ually is sometimes costly. Particularly, it is infea- sible for humans to create hundreds of thousands of reference summaries in cases where summa- rization requires domain-specific or expert knowl- edge. Such cases include financial reports, finan- cial and economic news ( <ref type="bibr" target="#b8">Filippova et al., 2009)</ref>, and scientific articles ( <ref type="bibr" target="#b21">Parveen et al., 2016)</ref>.</p><p>A fundamental requirement in extractive sum- marization is the identification of salient sentences from a document, i.e., sentences that represent key subjects mentioned in the document. Such sub- jects are often described in the form of topics, cat- egories, sentiments, and other meta-information about a document. Sometimes they are extracted from external information related to document contents. Once one knows the subjects of a docu- ment beforehand, a straightforward strategy in ex- tractive summarization is to select sentences that are relevant to the subjects. Importantly, subjects should be inferred from sentences identified from the document. For example, assume that we are about to summarize a financial report of a com- pany with knowledge from external information sources that the company has strong earnings. In this case, we might select sentences that explain factors affecting increase of earnings so that a reader of the summary can intuitively understand the company's financial situation.</p><p>The key idea is that we regard the subjects of a document as pseudo-rough reference sum- maries. Then, if we are able to estimate the sub- jects with small amounts of documents and the ex- ternal information in them, the identification of salient sentences from a document can be sup- ported by sentence features that have been learned from document subject estimation. As a result, smaller amounts of actual reference summaries are only needed as mutually learning feature represen- tations for both subject estimation and sentence identification from pseudo-rough reference sum- maries.</p><p>As described earlier, we focus on single docu- ment summarization with small amounts of refer- ence summaries, and propose a general framework for summarization that is useful for extracting sen- tences from a document along with its external re- lated information. Particularly, we formalize esti- mation of the above-described document subjects as a document classification task and solve doc- ument summarization in the framework of multi- task learning for sentence extraction and docu- ment classification.</p><p>Our proposed summarization framework com- prises two components: one designed for sentence extraction, which selects sentences relevant to the subjects of an input document, and one for docu- ment classification, which predicts the subject of the input document. In the multi-task learning framework, document classification supports sen- tence extraction by learning common feature rep- resentations of salient sentences for summariza- tion. We use recurrent neural network encoder- decoder as sentence extractor and document clas- sifier.</p><p>We evaluate our proposed summarization method on two datasets: the NIKKEI, the leading financial news publisher in Japan and a financial report corpus; and the New York Times Annotated Corpus <ref type="bibr" target="#b22">(Sandhaus, 2008)</ref>. The results of experi- mental evaluations demonstrate that our summa- rizers achieve a performance that is comparable to those of state-of-the-art systems.</p><p>The contribution of this paper is two-fold. First, we propose a general framework for single doc- ument summarization with small amounts of ref- erence summaries, which is important for practi- cal implementation of summarization techniques. Second, we propose a multi-task learning method with curriculum learning that supports sentence extraction from a document while solving docu- ment classification. Here, we assume that a doc- ument is classifiable into certain subjects, which comprise the meta-information of the document. Furthermore, sentences for a summary are ex- tracted in relation to the subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement and Data Preparation</head><p>In this section, we define the task of sentence extraction for document summarization as ad- dressed in this paper. We specifically exam- ine documents that satisfy the following require- ments. (1) Reference summaries are few. <ref type="formula" target="#formula_2">(2)</ref> The document is associated with a list of sub- jects, {a 1 , a 2 , · · · , a m } (a i ∈ {0, 1}), that in- cludes topics, categories, sentiments, and other meta-information. a j = 1 denotes that the doc- ument is classified into the subject j. Given a doc- ument D consisting of a sequence of sentences {s 1 , s 2 , · · · , s n }, we aim to extract k sentences in relation to a document subject a j , which is ex- pected to be included in the summary (k &lt; n) of the document. We predict both a subject a j for the document and a label y i ∈ {0, 1} for each sen- tence within the document, which indicates that the i-th sentence should be extracted.</p><p>In this study, we use two datasets for our sen- tence extraction task: the NIKKEI financial report corpus and New York Times news corpus.</p><p>For the financial report corpus, we used finan- cial reports published every quarter during 2013- 2016 by Japanese exchange listed companies. The reports explained the economic activity and the factors affecting revenue or profits for the quarter. For the reference summary, which is the gold stan- dard summary used for training a classifier that predicts a sentence label, we use financial news articles published by the NIKKEI <ref type="bibr">1</ref>   <ref type="figure">Figure 1</ref>: Proposed multi-task learning framework for sentence extraction with document classification language of the reports and the articles is Japanese. For the document subject of a financial report, we used profit and revenue information as a subject a j (j = 1, 2), which indicates its profit and rev- enue increase compared to an earlier term.</p><p>Our second dataset, the New York Times An- notated Corpus (NYTAC), is a collection of arti- cles from the New York Times. The gold standard summaries are attached to some of the articles. As the subject of a document article, we use already annotated category of the news from its metadata such as Business and Arts.</p><p>For the task of sentence extraction, the gold standard labels indicating sentences that should be extracted are needed. To attach the labels on sen- tences that maximize the Rouge score with respect to gold summaries, we introduce a greedy ap- proach ( <ref type="bibr" target="#b3">Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b19">Nallapati et al., 2017)</ref>. We first select one sentence that has a max- imum Rouge score with respect to the entire gold standard summary. We add it to the reference sum- mary set and select sentences incrementally until no candidate sentence improves the score when added to the current summary set. The labels of sentences in the summary set are set as y = 1. The greedy approach is efficient because the com- putational costs associated with the identification of a global optimal summary set are too large.</p><p>The labels are attached by computing ROUGE- 1 <ref type="bibr" target="#b14">(Lin, 2004</ref>). ROUGE-1 and ROUGE-2 are re- ported as best for emulating evaluation by hu- mans ( <ref type="bibr" target="#b20">Owczarzak et al., 2012</ref>). For financial re- ports, words apart from nouns, verbs, adjectives, and adverbs are removed for computing appropri- ate ROUGE scores. The accuracy between the la- bels attached by ROUGE-1 and humans is 81%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summarization Model</head><p>This section introduces our novel summarization framework. <ref type="figure">Figure 1</ref> presents the proposed multi- task learning framework for sentence extraction with document classification. The left half of the figure shows the common sentence extraction part. The right half depicts a novel sentence extraction by document classification. We assume that a doc- ument is classifiable into certain subjects that rep- resent meta-information of the document, and as- sume that sentences for a summary are extracted in relation to the subjects. Therefore, solving doc- ument classification supports sentence extraction from a document with multi-task learning of both tasks.</p><p>In <ref type="figure">Fig. 1</ref>, s i denotes the embedding of sentence i. Furthermore, y i denotes whether the sentence should be extracted and a j is a subject of a doc- ument, which includes topics, categories, senti- ments, or other meta-information. The predictor component computes p i ≡ p(y i = 1 | D), the probability of sentence i extraction. Our proposed method estimates p i by learning both sentence ex- traction and document classification.</p><p>We now explain how learning document clas- sification supports sentence extraction. In <ref type="figure">Fig. 1</ref>, s avg is the weighted average of s i in terms of the probability of sentence extraction. It means that s avg includes much more information about sentences with higher extraction probability. The probability that the document is related to a sub- ject</p><formula xml:id="formula_0">q j ≡ p(a j = 1 | D) is estimated by s avg .</formula><p>The error is larger if the contents of extracted sen- tences do not correspond with the document sub- ject. By feeding back this error to the predictor, the model learns to extract sentences related to  <ref type="figure">Figure 2</ref>: Sentence extraction model using LSTM-RNN with multi-task learning the document subject. For example, in the case of a financial report, profit information indicating profit and revenue increase compared with an ear- lier term is used as a document subject. Positive sentences are expected to be extracted so that the extracted sentences reflect good financial results if the profit and revenue increase. <ref type="figure">Figure.</ref> 2 shows the entire model. With respect to the predictor component in the proposed model, we use an encoder-decoder architecture modeled by recurrent neural networks ( <ref type="bibr" target="#b12">Kim et al., 2016</ref>) based on recent neural extractive summarization approaches <ref type="bibr" target="#b3">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b19">Nallapati et al., 2017)</ref>. However, our summarization frame- work is applicable to all models of sentence ex- traction using distributed representation as inputs. We explain four sub-modules of the summariza- tion model: sentence encoder, document encoder, sentence extraction, and document classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Encoder</head><p>We use Convolutional Neural Network (CNN) to obtain a sentence embedding from word embed- dings. The training speed of single-layer CNN is high. It is effective for sentence-level classifica- tion such as sentiment analysis <ref type="bibr" target="#b11">(Kim, 2014)</ref>. Ac- tually, CNN is suitable for use because our model requires a high computational cost. Sentence em- beddings are used for both sentence extraction and document classification. Let x i ∈ R d denote the embedding of the i- th word in the sentence, and x i:i+q−1 ∈ R dq rep- resent a concatenated vector that represents a se- quence of q words. Convolutional filter w ∈ R dq is applied as</p><formula xml:id="formula_1">s i w = f (w · x i:i+q + b),<label>(1)</label></formula><p>where f is a nonlinear function such as the hy- perbolic tangent and b is the bias. Max pooling over time is applied to obtain a single feature s w representing the sentence under filter w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Encoder and Extractor</head><p>The LSTM-RNN Encoder-Decoder model is used for sentence extraction. First, on the encoder part, all sentences of a document are input into the hid- den layers of RNN. LSTM assigns the input gate, forget gate, output gate, and memory cells as acti- vation functions of RNN.</p><p>In <ref type="figure">Fig. 2</ref>, h n ∈ R k is the output of the encoder part, for which information of all sentences is in- put. The extraction probability is estimated based on the encoder part output. The hidden layer of the decoder part ¯ h t ∈ R k is updated by LSTM equal to the encoder part. The initial value ¯ h 0 is h n . The input is the prior sentence s t−1 multiplied by the extraction probability p t−1 . Therefore, more in- formation about sentences that are likely to be ex- tracted is input to the hidden layer. Based on each hidden layer, the extraction probability of sentence t is computed as shown below.</p><formula xml:id="formula_2">h t = LSTM(s t , h t−1 )<label>(2)</label></formula><formula xml:id="formula_3">¯ h t = LSTM(p t−1 · s t−1 , ¯ h t−1 )<label>(3)</label></formula><formula xml:id="formula_4">p t = σ(w y · [h t : ¯ h t ] + b y )<label>(4)</label></formula><p>Here, w y ∈ R 2k represents the weight vector, b y stands for the bias, and : is the concatenation operator of vectors. By concatenating the hidden layers of the decoder part with the encoder part, the extraction probability is computed by refer- encing the information of input sentences more di- rectly. It seems reasonable to pay attention to the input sentence directly when deciding whether the sentence should be extracted or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document Classification</head><p>Using the embeddings and estimated extraction probability of sentences, the probability distribu- tion of input document subjects is estimated. The probability that a document is classified into the subject j, q j , is computed as shown below.</p><formula xml:id="formula_5">q j = σ(w a · s avg + b a )<label>(5)</label></formula><formula xml:id="formula_6">s avg = t p t · s t i p i<label>(6)</label></formula><p>Here, w a signifies the weight vector and b a is the bias. Additionally, s avg represents the weighted average of sentence embeddings. Each sentence is weighted by the estimated extraction probability. The predictor computes the probabil- ity distribution of a document subject from s avg , which means that the predictor pays more atten- tion to sentences that are likely to be extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-Task Learning with Curriculum Learning</head><p>This section presents an explanation of the proce- dure followed to train the summarization model. The model parameters are updated to maximize the likelihood of all sentence labels and document subject labels. This is equivalent to minimization of the following error terms.</p><formula xml:id="formula_7">E y (θ) =− n t=1 (y t log p t +(1−y t ) log(1−p t )) +λ θ θ 2<label>(7)</label></formula><formula xml:id="formula_8">E a (θ) =− m j=1 (a j log q j +(1−a j ) log(1−q j )) +λ θ θ 2<label>(8)</label></formula><p>In these equations, θ 2 is the L2 norm, and λ θ is the regularization term. L2 regularization is in- troduced to avoid overfitting.</p><p>Multi-task learning is generally complicated be- cause the parameter is optimized simultaneously for sentence extraction and document classifica- tion. We introduce curriculum learning ( <ref type="bibr" target="#b0">Bengio et al., 2009</ref>) to overcome this difficulty. Curricu- lum learning is a learning method that aims to im- prove the performance of a complicated model or data. The model starts by learning a simple model or data, and gradually adapts to more complicated ones.</p><p>We introduce two kinds of curriculum learn- ing for multi-task learning. We apply baby step curriculum learning <ref type="bibr" target="#b5">(Cirik et al., 2016)</ref>, which demonstrates the effectiveness of the LSTM-RNN architecture. In this, the dataset is categorized based on the difficulty and added to the order of ease.</p><p>We divide the dataset into three subsets based on the combination of document type and objec- tive function. The first subset has documents with an attached reference summary. The model is trained for optimizing sentence extraction. The second uses the same documents as the first. How- ever, the objective function is document classifica- tion. The last one has documents with no refer- ence summary. Only the likelihood of document subjects as pseudo-rough reference summaries is maximized in the last dataset. For sentence extrac- tion task, it is more difficult to train from the last dataset than the first dataset because information related to document subjects are more truncated than the reference summary. The second dataset is the bridge between the first and the last.</p><p>For document classification, sentences are weight-averaged by the estimated sentence extrac- tion probability p t . In the second dataset, sen- tences are weighted not only by p t , but also the true label y t . p t in Eq.(6) is replaced by ¯ p t as fol- lows.</p><formula xml:id="formula_9">¯ p t = κp t + (1 − κ)y t<label>(9)</label></formula><p>Here, κ is the mixing rate of extraction probabil- ity and the true label. At the beginning of training, p t is not predicted accurately. This will affect doc- ument classification adversely, therefore κ is set to nearly zero so that the true label y t is used for doc- ument classification. By using the true label, train- ing for sentence extraction and document classi- fication does not mutually interfere. As training progress, κ gets larger and document classification supports sentence extraction.</p><p>We believe that our basic idea of curriculum learning, with some modifications depending on the task applied, can be applied for other kinds of multi-task learning in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The NIKKEI Financial Report Corpus</head><p>For training and evaluation, we used financial re- ports published from April 2013 through Septem- ber 2016. The reports used for evaluation and validation were published in the last and the sec- ond to last quarter. All other reports were used for training. The numbers of reports used for training, validation, and evaluation <ref type="bibr">were 12, 262, 191, and 183.</ref> In the training dataset, 8, 725 re- ports with no reference summary were included and used only for training of document classifi- cation, which predicts document subjects. As for document subjects, we used subjects of two kinds as a j ∈ {0, 1} (j = 1, 2), indicating that the profit and revenue increases compared with the prior term. a j = 1 denotes that the value in- creases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New York Times Annotated Corpus</head><p>For the experiment using NYTAC, we evaluated our model using different amounts of reference summary. The numbers of articles used for both validation and evaluation were 200. For training, we prepared 125, 250, and 500 articles. For train- ing of document classification, we used 3000 ar- ticles for which the reference summary was not attached. As document subjects, we used the category of a news article a j ∈ {0, 1} (j = 1, 2, · · · , C) as a subject. Each subject corre- sponded to a news article category, such as "Busi- ness" and "Arts." a j = 1 denotes that a document is classified into the category j. C is the number of categories, which is 26 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The word embeddings were pre-trained using Skip-gram ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) on all 1, 043, 064 articles in the Japanese version of Wikipedia. The dimensions of word embedding were 200. Those of the hidden layer in LSTM-RNN were 400. For CNN, the list of kernel sizes was {1, 2, 3, 4, 5, 6}. The number of feature maps was 50. Adadelta <ref type="bibr" target="#b31">(Zeiler, 2012)</ref> was used for updat- ing parameters. The initial learning rate was 10 −6 . The hyper parameters were optimized using grid search. We extracted three sentences with the highest scores in the manner described in an ear- lier report <ref type="bibr" target="#b3">(Cheng and Lapata, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>For the NIKKEI financial report dataset, we used LEAD, which extracts the leading three sentences of a document as a baseline. We also built a base- line classifier LREG using logistic regression and human engineered features. The features were sentence length, position in the document, number of entities, nouns, verbs, adverbs, and adjectives in the sentence. We also added the sentiment of sen- tence to the features. For the financial report sum- marization, sentiment information is important be- cause positive/negative sentences are frequently included in the summary when the revenue in- creases/decreases. The sentiment is computed by the frequency of words that appear in the articles when the revenue increases/decreases. For both datasets, we assigned NN-SE( <ref type="bibr" target="#b3">Cheng and Lapata, 2016)</ref> as the baseline. The difference between NN-SE and our model is the introduction of multi- task learning and curriculum learning. The hyper- parameters are the same as those of our model. Through comparison with NN-SE, we can validate the effectiveness of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results obtained from the NIKKEI Financial Report Corpus</head><p>Table1 presents the results for financial reports using F-measure. The precision and recall are calculated based on binary classification setup. LEAD, LREG, and NN-SE are used as the base- lines. The proposed neural multi-task learning model, NN-ML, is significantly inferior to NN-SE and LREG. However, NN-ML-CL, the proposed model with curriculum learning, is superior to all  other models. This result shows that merely in- troducing multi-task learning does not positively influence on sentence extraction. However, cur- riculum learning overcomes the difficulty of multi- task learning; thus, document classification has positive effects on sentence extraction. We confirmed the relation between the effec- tiveness of our model and the amount of refer- ence summaries. We compared NN-ML-CL (our model) and NN-SE in several cases for which the amounts of reference summaries were 125, 250, 500, 1000, and 2000; the results are shown in Ta- ble 2. As observed, NN-ML-CL is superior to NN- SE in all cases. The margin between NN-ML-CL and NN-SE grows as the amount decreases, which means that document classification is more effec- tive in cases with fewer reference summaries.</p><p>We also reported the results of human evalu- ation for summaries generated by the respective systems. Referring to the gold summary, partic- ipants ranked the generated summaries generated by four systems: NN-ML-CL(our system), NN- SE, LEAD, and LREG. The judging criteria was informativeness, which indicates how a generated summary covers information in the gold summary. From the test documents, we remove summaries for which the same sentences were extracted by different systems and randomly sampled 20 docu- ments. 6 persons participated in the evaluation. <ref type="table" target="#tab_4">Table 3</ref> presents the distribution of ranking and the average. Our NN-ML-CL model is ranked first in more than half the tests and markedly surpasses other models. Comparison with NN-SE verifies the effectiveness of multi-task learning for human evaluation.  18.3 12.7 500</p><p>18.5 12.9 <ref type="table" target="#tab_5">Table 4</ref> presents our results for NYTAC using ROUGE-1 and ROUGE-2. In all cases, NN-ML- CL outperforms NN-SE on both metrics. When the amount of reference summary is 250, the mar- gin between NN-ML-CL and NN-SE is the largest on each metric. For cases with 125 and 500 ref- erence summaries, improvement is observed, but the margin is smaller than in the case for financial reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on NYTAC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>In this section, we discuss how document clas- sification contributes to the improvement of sen- tence extraction performance on the financial re- port dataset. As mentioned above, NN-ML, the model that uses multi-task learning, exhibits a performance that is worse than that of NN-SE, the model that does not use multi-task learning. One possible ex- planation would be that it is difficult to optimize the parameters to maximize the likelihood of both sentence extraction and document classification si- multaneously. If the learning task of sentence ex- traction does not proceed well enough, the task of document classification may also not work well. The reason is that the classification task relies on the estimated probability for sentence extraction in our proposed summarization framework.</p><p>However, curriculum learning improves the per- formance of the model with multi-task learn- ing. By introducing curriculum learning into the framework, we are able to start training the model  The rapid progress of the strong yen adversely influenced financial results, but the growth of revenue in areas such as Europe, Asia, and Oceania increased the revenue to 535 billion yen. The demand for air conditioners increased because of the intense July heat. Sentences extracted by NN-SE As for fluorine resin, the demand for semiconductors rose steadily. However, competing Chinese companies gained power, and revenue for electrical wire use declined. The revenue of parts for guided missiles increase year-on- year, but the revenue of medical equipment decrease. News Article (Gold summary) In Southeast Asia and Europe, high-end models of air conditioners sold well. In China and US, revenue rose and overcame the adverse influence of strong yen. The cor- porate tax ratio reduction also supported business perfor- mance. The revenue of air-conditioners, a leading prod- uct of Daikin, rose 9% in Southeast Asia. The revenue network in Vietnam and Indonesia expanded. revenue of air-conditioners rose at a higher pace than market scale.</p><p>In China, the revenue of air conditioners for business use recovered. High-end models were also selling well.</p><p>only for sentence extraction. Then, the training for document classification is started gradually. Even- tually, it contributes to the improvement of the per- formance of sentence extraction through the multi- task learning approach. From the results for the financial report corpus, we confirmed that the contents of sentences ex- tracted by our model corresponded with revenue and profit changes. Before validation, the sen- tences were categorized as corresponding to or not corresponding to others. We compared the results of sentence extraction with NN-ML-CL and NN- SE and checked the category distribution of sen- tences extracted using NN-ML-CL or NN-SE. <ref type="table" target="#tab_6">Table 5</ref> shows that 85.7% of sentences extracted using NN-ML-CL correspond to changes of rev- enues and profits. However, only 40.0% of sen- tences extracted by NN-SE correspond to these pa- rameters, which indicates that document classifi- cation supports extraction of sentences related to the revenue and profit change, and contributes to the improvement. <ref type="table" target="#tab_7">Table 6</ref> shows sentences extracted from finan- cial reports published by Daikin, Ltd., the lead- ing air-conditioner manufacturer in Japan. During this term, the air conditioner revenue increased; moreover, revenues and profits increased consid- erably year-on-year. NN-ML-CL extracted sen- tences that mention the good revenue performance of air-conditioners in Asia and Europe, which is the same as that in the gold summary. In contrast, NN-SE extracts sentences mentioning the bad rev- enue performance of fluorine resin and medical equipment, which are not described in the gold summary. NN-SE is badly affected owing to train- ing on past reports and articles. Our model ex- tracts sentences with words that appear frequently in a positive context. Therefore, sentences related to good revenue performance are extracted.</p><p>There are two main ways of applications for our summarization approach with document classifi- cation. In the first case, the text collection has explicitly annotated document labels, which in- cludes the collection of news articles with their category information, product reviews with their rating, scholarly paper abstracts with their disci- pline information, etc. In the second case, a docu- ment label can be acquired from external informa- tion sources about the text collection. For financial reports, the information about financial situation of a target company is extracted from the financial statement, which in turn can be used for a label of document classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Based on the recent advances of neural network- based approaches <ref type="bibr" target="#b10">(Kågebäck et al., 2014;</ref><ref type="bibr" target="#b2">Cao et al., 2015;</ref><ref type="bibr" target="#b30">Yin and Pei, 2015;</ref><ref type="bibr" target="#b1">Cao et al., 2016)</ref>, an attentional encoder-decoder for extrac- tive single-document summarization and its appli- cation to the news corpus was proposed <ref type="bibr" target="#b3">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b19">Nallapati et al., 2017)</ref>. Al- though we employ an encoder-decoder architec- ture in the predictor component of our summariza- tion framework, the framework can be applied to all models of sentence extraction using distributed representation as inputs, including recently ad- vanced other attention-based encoder-decoder net- works ( <ref type="bibr" target="#b26">Wang et al., 2016;</ref><ref type="bibr" target="#b29">Yang et al., 2016)</ref> ( <ref type="bibr" target="#b3">Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b19">Nallapati et al., 2017)</ref> argue that a stumbling block to applying neural network models to extractive summariza- tion is the lack of training data and documents with sentences labeled as summary-worthy. To over- come this, several studies have used artificial ref- erence summaries <ref type="bibr" target="#b23">(Sun et al., 2005;</ref><ref type="bibr" target="#b24">Svore et al., 2007;</ref><ref type="bibr" target="#b28">Woodsend and Lapata, 2010;</ref><ref type="bibr" target="#b3">Cheng and Lapata, 2016)</ref> compiled by collecting documents and corresponding highlights from other sources. However, preparing such a parallel corpus often requires domain-specific or expert knowledge de- pending on the domain ( <ref type="bibr" target="#b8">Filippova et al., 2009;</ref><ref type="bibr" target="#b21">Parveen et al., 2016)</ref>. Our summarization uses document-associated information as pseudo rough reference summaries, which enables us to learn feature representations for both document classi- fication and sentence identification with smaller amounts of actual reference summaries.</p><p>Neural networks based multi-task learning has recently proven effective in many NLP problems ( <ref type="bibr" target="#b16">Liu et al., 2015</ref><ref type="bibr" target="#b15">Liu et al., , 2016</ref><ref type="bibr" target="#b9">Firat et al., 2016;</ref>). Aiming at single document summa- rization with relatively small amounts of reference summaries, we demonstrated document summa- rization in the framework of multi-task learning with curriculum learning for sentence extraction and document classification. This enabled us to obtain better feature representations to extract sen- tences from documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a general framework for extractive summarization using document sub- jects. Our key idea is to use a multi-task learn- ing method that supports sentence extraction while enabling document classification, assuming that a document can be classified into certain subjects, and sentences for a summary are extracted in rela- tion to the subjects. This framework enables single document sum- marization with relatively small amounts of refer- ence summaries since document subjects can be used as pseudo-rough reference summaries. Our proposed method can be widely applied for actual documents attached with meta-information such as product reviews, sports news and so on.</p><p>Experimental results showed that our model is less effective on the news corpus. For higher per- formance, more information such as the embed- dings of news descriptors for document classifica- tion must be used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>The NIKKEI 
publishes articles summarizing financial reports of 
each company. It covers approximately 10 % of all 
the reports: 3911 reports from 2013 to 2016. The true 
doc. subject 

weighted 
sentences 

predicted 
doc. subject 

input 
sentences 

predicted 
extraction 

s 1 
s 2 
s n 

… 

p 1 

p 2 

p n 

Predictor 

a 1 
a j 

s avg 

q 1 

q j 

… 

_ 
_ 
_ 

feedback error 

s 1 
s 2 
s n 

weighted average 
of sentences 

… 

… 

… 

true 
extraction 
y 1 
y 2 

y n 

Sentence Extraction 


Document Classification 

… 

… 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : F-measure evaluation (%) on financial re- ports</head><label>1</label><figDesc></figDesc><table>Models 
F-measure Precision Recall 
LEAD 
42.1 
39.1 
50.4 
LREG 
60.5 
67.6 
66.5 
NN-SE 
59.9 
58.0 
68.8 
NN-ML 
55.2 
52.1 
64.6 
NN-ML-CL 60.6 
54.6 
74.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F-measure evaluation (%) depending on 
the amount of reference summary 

Models 
125 
250 
500 
1000 2000 
NN-SE 
55.2 56.1 58.0 58.0 
58.1 
NN-ML-CL 58.1 58.4 59.4 59.3 
59.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : Ranking distributions (%) and the aver- age evaluated by humans</head><label>3</label><figDesc></figDesc><table>Models 
1st 
2nd 
3rd 
4th Ave. 
LEAD 
21.7 20.0 28.3 30.0 2.67 
LREG 
20.0 28.3 26.7 25.0 2.45 
NN-SE 
31.7 21.7 16.7 30.0 2.57 
NN-ML-CL 51.7 20.0 21.7 
6.7 1.83 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>ROUGE scores (%) for various amounts 
of reference summaries in NYTAC 

Model 
Ref. ROUGE-1 ROUGE-2 
NN-SE 
125 
17.2 
12.1 
250 
16.8 
11.3 
500 
18.0 
12.5 
NN-ML-CL 
125 
18.1 
12.7 
250 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 : Rates (%) of extracted sentences corre- sponding with a document subject</head><label>5</label><figDesc></figDesc><table>Correspond Not 
Others 
NN-SE 
40.0 
40.0 20.0 
NN-ML-CL 85.7 
14.3 0.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Example of gold summary and sentences 
extracted using NN-ML-CL and NN-SE 

Sentences extracted by NN-ML-CL 
</table></figure>

			<note place="foot" n="1"> http://www.nikkei.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by JST CREST Grant Number JPMJCR1513, Japan and the New Energy and Industrial Technology Development Organi-zation (NEDO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<title level="m">Curriculum learning. In ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attsum: Joint learning of focusing and summarization with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ranking with recursive neural networks and its application to multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2153" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visualizing and understanding curriculum learning for long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06204</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lexpagerank: Prestige in multi-document text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Companyoriented extractive summarization of financial news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01073</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extractive summarization using continuous vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olof</forename><surname>Mogren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devdatt</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the second Workshop on Continuous Vector Space Models and their Compositionality in EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abstractive news summarization based on event semantic link network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhuge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out Workshop in ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJICAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2873" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="912" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop in ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An assessment of the accuracy of automatic evaluation in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Evaluation Metrics and System Comparison for Automatic Summarization in ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating coherent summaries of scientific articles using coherence patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daraksha</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="772" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The new york times annotated corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Web-page summarization using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua-Jun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancing single-document summarization by combining ranknet and thirdparty sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krysta</forename><forename type="middle">Marie</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="448" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural headline generation on abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extractive summarization using supervised and semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic generation of story highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimizing sentence modeling and selection for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1383" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
