<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-risk learning: acquiring new word vectors from tiny data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
							<email>aurelie.herbelot@cantab.net</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Translation and Language Sciences</orgName>
								<orgName type="department" key="dep2">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution" key="instit1">Universitat Pompeu Fabra</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<email>marco.baroni@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Translation and Language Sciences</orgName>
								<orgName type="department" key="dep2">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution" key="instit1">Universitat Pompeu Fabra</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">High-risk learning: acquiring new word vectors from tiny data</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="304" to="309"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn &apos;a good vector&apos; for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences&apos; worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional models (DS: <ref type="bibr" target="#b18">Turney and Pantel (2010)</ref>; <ref type="bibr" target="#b4">Clark (2012)</ref>; <ref type="bibr" target="#b6">Erk (2012)</ref>), and in par- ticular neural network approaches ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b5">Collobert et al., 2011;</ref><ref type="bibr" target="#b7">Huang et al., 2012;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>, do not fare well in the ab- sence of large corpora. That is, for a DS model to learn a word vector, it must have seen that word a sufficient number of times. This is in sharp con- trast with the human ability to perform fast map- ping, i.e. the acquisition of a new concept from a single exposure to information <ref type="bibr" target="#b9">(Lake et al., 2011;</ref><ref type="bibr" target="#b17">Trueswell et al., 2013;</ref><ref type="bibr" target="#b10">Lake et al., 2016)</ref>.</p><p>There are at least two reasons for wanting to ac- quire vectors from very small data. First, some words are simply rare in corpora, but potentially crucial to some applications (consider, for in- stance, the processing of text containing technical terminology). Second, it seems that fast-mapping should be a prerequisite for any system pretending to cognitive plausibility: an intelligent agent with learning capabilities should be able to make edu- cated guesses about new concepts it encounters.</p><p>One way to deal with data sparsity issues when learning word vectors is to use morphological structure as a way to overcome the lack of pri- mary data ( <ref type="bibr" target="#b12">Lazaridou et al., 2013;</ref><ref type="bibr" target="#b13">Luong et al., 2013;</ref><ref type="bibr" target="#b8">Kisselew et al., 2015;</ref><ref type="bibr" target="#b15">Padó et al., 2016</ref>). Whilst such work has shown promising result, it is only applicable when there is transparent mor- phology to fall back on. Another strand of re- search has been started by <ref type="bibr" target="#b11">Lazaridou et al. (2017)</ref>, who recently showed that by using simple sum- mation over the (previously learnt) contexts of a nonce word, it is possible to obtain good correla- tion with human judgements in a similarity task. It is important to note that both these strategies as- sume that rare words are special cases of the dis- tributional semantics apparatus, and thus require separate approaches to model them.</p><p>Having different algorithms for modelling the same phenomenon means however that we need some meta-theory to know when to apply one or the other: it is for instance unclear at which fre- quency a rare word is not rare anymore. Fur- ther, methods like summation are naturally self- limiting: they create frustratingly strong baselines but are too simplistic to be extended and improved in any meaningful way. In this paper, our un- derlying assumption is thus that it would be de- sirable to build a single, all-purpose architecture to learn word representations from any amount of data. The work we present views fast-mapping as a component of an incremental architecture: the rare word case is simply the first part of the concept learning process, regardless of how many times it will eventually be encountered.</p><p>With the aim of producing such an incremen-tal system, we demonstrate that the general archi- tecture of neural language models like Word2Vec ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>) is actually suited to mod- elling words from a few occurrences only, provid- ing minor adjustments are made to the model itself and its parameters. Our main conclusion is that the combination of a heightened learning rate and greedy processing results in very reasonable one- shot learning, but that some safeguards must be in place to mitigate the high risks associated with this strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>We want to simulate the process by which a com- petent speaker encounters a new word in known contexts. That is, we assume an existing vocab- ulary (i.e. a previously trained semantic space) which can help the speaker 'guess' the meaning of the new word. To evaluate this process, we use two datasets, described below.</p><p>The definitional nonce dataset We build a novel dataset based on encyclopedic data, simu- lating the case where the context of the unknown word is supposedly maximally informative. <ref type="bibr">1</ref> We first record all Wikipedia titles containing one word only (e.g. Albedo, Insulin). We then ex- tract the first sentence of the Wikipedia page corre- sponding to each target title (e.g. Insulin is a pep- tide hormone produced by beta cells in the pan- creas.), and tokenise that sentence using the Spacy toolkit. <ref type="bibr">2</ref> Each occurrence of the target in the sen- tence is replaced with a slot ( ). From this original dataset, we only retain sen- tences with enough information (i.e. a length over 10 words), corresponding to targets which are fre- quent enough in the UkWaC corpus ( <ref type="bibr" target="#b0">Baroni et al. (2009)</ref>, minimum frequency of 200). The fre- quency threshold allows us to make sure that we have a high-quality gold vector to compare our learnt representation to. We then randomly sam- ple 1000 sentences, manually checking the data to remove instances that are, in fact, not definitional. We split the data into 700 training and 300 test in- stances.</p><p>On this dataset, we simulate first-time exposure to the nonce word by changing the label of the gold standard vector in the background semantic space, and producing a new, randomly initialised vector for the nonce. So for instance, insulin becomes in- sulin gold, and a new random embedding is added to the input matrix for insulin. This setup allows us to easily measure the similarity of the newly learnt vector, obtained from one definition, to the vec- tor produced by exposure to the whole Wikipedia. To measure the relative performance of various se- tups, we calculate the Reciprocal Rank (RR) of the gold vector in the list of all nearest neighbours to the learnt representation. We average RRs over the number of instances in the dataset, thus obtaining a single MRR figure (Mean Reciprocal Rank).</p><p>The Chimera dataset Our second dataset is the 'Chimera' dataset of ( <ref type="bibr" target="#b11">Lazaridou et al., 2017)</ref>. <ref type="bibr">3</ref> This dataset was specifically constructed to sim- ulate a nonce situation where a speaker encoun- ters a word for the first time in naturally-occurring (and not necessarily informative) sentences. Each instance in the data is a nonce, associated with 2-6 sentences showing the word in context. The novel concept is created as a 'chimera', i.e. a mixture of two existing and somewhat related concepts (e.g., a buffalo crossed with an elephant). The sentences associated with the nonce are utterances contain- ing one of the components of the chimera, ran- domly extracted from a large corpus.</p><p>The dataset was annotated by humans in terms of the similarity of the nonce to other, randomly selected concepts. <ref type="figure">Fig. 1</ref> gives an example of a data point with 2 sentences of context, with the nonce capitalised (VALTUOR, a combination of cucumber and celery). The sentences are followed by the 'probes' of the trial, i.e. the concepts that the nonce must be compared to. Finally, human similarity responses are given for each probe with respect to the nonce. Each chimera was rated by an average of 143 subjects. In our experiments, we simply replace all occurrences of the original nonce with a slot ( ) and learn a representation for that slot. For each setting (2, 4 and 6 sen- tences), we randomly split the 330 instances in the data into 220 for training and 110 for testing.</p><p>Following the authors of the dataset, we evalu- ate by calculating the correlation between system and human judgements. For each trial, we calcu- late Spearman correlation (ρ) between the similar- ities given by the system to each nonce-probe pair, and the human responses. The overall result is the average Spearman across all trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentences:</head><p>Canned sardines and VALTUOR between two slices of wholemeal bread and thinly spread Flora Original. @@ Erm, VALTUOR, low fat dairy products, incidents of heart disease for those who have an olive oil rich diet.</p><p>Probes: rhubarb, onion, pear, strawberry, limousine, cushion Human responses: 3, 2.86, 1.43, 2.14, 1.29, 1.71</p><p>Figure 1: An example chimera (VALTUOR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline models</head><p>We test two state-of-the art systems: a) Word2Vec (W2V) in its Gensim 4 implementation, allowing for update of a prior semantic space; b) the ad- ditive model of <ref type="bibr" target="#b11">Lazaridou et al. (2017)</ref>, using a background space from W2V.</p><p>We note that both models allow for some sort of incrementality. W2V processes input one context at a time (or several, if mini-batches are imple- mented), performing gradient descent after each new input. The network's weights in the input, which correspond to the created word vectors, can be inspected at any time. <ref type="bibr">5</ref> As for addition, it also affords the ability to stop and restart training at any time: a typical implementation of this behaviour can be found in distributional semantics models based on random indexing (see e.g. <ref type="bibr" target="#b16">QasemiZadeh et al., 2017)</ref>. This is in contrast with so-called 'count-based' models calculated by computing a frequency matrix over a fixed corpus, which is then globally modified through a transformation such as Pointwise Mutual Information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word2Vec</head><p>We consider W2V's 'skip-gram' model, which learns word vectors by predicting the context words of a particular target. The W2V architecture includes several important parame- ters, which we briefly describe below.</p><p>In W2V, predicting a word implies the ability to distinguish it from so-called negative samples, i.e. other words which are not the observed item. The number of negative samples to be considered can be tuned. What counts as a context for a particular target depends on the window size around that tar- get. W2V features random resizing of the window, which has been shown to increase the model's per- formance. Further, each sentence passed to the model undergoes subsampling, a random process by which some words are dropped out of the input <ref type="bibr">4</ref> Available at https://github.com/ RaRe-Technologies/gensim. <ref type="bibr">5</ref> Technically speaking, standard W2V is not fully incre- mental, as it requires a first pass through the corpus to com- pute a vocabulary, with associated frequencies. As we show in §5, it however allows for an incremental interpretation, given minor modifications.</p><p>as a function of their overall frequency. Finally, the learning rate α measures how quickly the sys- tem learns at each training iteration. Traditionally, α is set low (0.025 for Gensim) in order not to overshoot the system's error minimum.</p><p>Gensim has an update function which allows us to save a W2V model and continue learning from new data: this lets us simulate prior acquisition of a background vocabulary and new learning from a nonce's context. As background vocabulary, we use a semantic space trained on a Wikipedia snap- shot of 1.6B words with Gensim's standard pa- rameters (initial learning rate of 0.025, 5 nega- tive samples, a window of ±5 words, subsampling 1e −3 , 5 epochs). We use the skip-gram model with a minimum word count of 50 and vector dimen- sionality 400. This results in a space with 259, 376 word vectors. We verify the quality of this space by calculating correlation with the similarity rat- ings in the MEN dataset ( <ref type="bibr" target="#b2">Bruni et al., 2014</ref>). We obtain ρ = 0.75, indicating an excellent fit with human judgements.</p><p>Additive model <ref type="bibr" target="#b11">Lazaridou et al. (2017)</ref> use a simple additive model, which sums the vectors of the context words of the nonce, taking as context the entire sentence where the target occurs. Their model operates on multimodal vectors, built over both text and images. In the present work, how- ever, we use the semantic space described above, built on Wikipedia text only. We do not normalise vectors before summing, as we found that the sys- tem's performance was better than with normali- sation. We also discard function words when sum- ming, using a stopword list. We found that this step affects results very positively.</p><p>The results for our state-of-the-art models are shown in the top sections of <ref type="table" target="#tab_0">Tables 1 and 2.</ref> W2V is run with the standard Gensim parame- ters, under the skip-gram model. It is clear from the results that W2V is unable to learn nonces from definitions (M RR = 0.00007). The ad- ditive model, on the other hand, performs well: an M RR of 0.03686 means that the median rank of the true vector is 861, out of a challenging 259, 376 neighbours (the size of the vocabulary). On the Chimeras dataset, W2V still performs well under the sum model -although the difference is not as marked and possibly indicates that this dataset is more difficult (which we would expect, as the sentences are not as informative as in the encyclopedia case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Nonce2Vec</head><p>Our system, Nonce2Vec (N2V), <ref type="bibr">6</ref> modifies W2V in the following ways.</p><p>Initialisation: since addition gives a good ap- proximation of the nonce word, we initialise our vectors to the sum of all known words in the con- text sentences (see §3). Note that this is not strictly equivalent to the pure sum model, as subsampling takes care of frequent word deletion in this setup (as opposed to a stopword list). In practice, this means that the initialised vectors are of slightly lesser quality than the ones from the sum model.</p><p>Parameter choice: we experiment with higher learning rates coupled with larger window sizes.</p><p>That is, the model should take the risk of a) over- shooting a minimum error; b) greedily considering irrelevant contexts in order to increase its chance to learn anything. We mitigate these risks through selective training and appropriate parameter de- cay (see below).</p><p>Window resizing: we suppress the random window resizing step when learning the nonce. This is because we need as much data as possi- ble and accordingly need a large window around the target. Resizing would make us run the risk of ending up with a small window of a few words only, which would be uninformative.</p><p>Subsampling: With the goal of keeping most of our tiny data, we adopt a subsampling rate that only discards extremely frequent words.</p><p>Selective training: we only train the nonce. That is, we only update the weights of the net- work for the target. This ensures that, despite the high selected learning rate, the previously learnt vectors, associated with the other words in the sentence, will not be radically shifted towards the meaning expressed in that particular sentence.</p><p>Whilst the above modifications are appropriate to deal with the first mention of a word, we must ask in what measure they still are applicable when the term is encountered again (see §1). With a <ref type="bibr">6</ref> Code available at https://github.com/ minimalparts/nonce2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRR</head><p>Median rank W2V 0.00007 111012 Sum 0.03686 861 N2V 0.04907 623  <ref type="table">Table 2</ref>: Results on chimera dataset view to cater for incrementality, we introduce a notion of parameter decay in the system. We hy- pothesise that the initial high-risk strategy, com- bining high learning rate and greedy processing of the data, should only be used in the very first train- ing steps. Indeed, this strategy drastically moves the initialised vector to what the system assumes is the right neighbourhood of the semantic space. Once this positioning has taken place, the system should refine its guess rather than wildly moving in the space. We thus suggest that the learning rate itself, but also the subsampling value and window size should be returned to more conventional stan- dards as soon as it is desirable. To achieve this, we apply some exponential decay to the learning rate of the nonce, proportional to the number of times the term has been seen: every time t that we train a pair containing the target word, we set α to α 0 e −λt , where α 0 is our initial learning rate. We also decrease the window size and increase sub- sampling rate on a per-sentence basis (see §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first tune N2V's initial parameters on the training part of the definitional dataset. We ex- periment with a range of values for the learn- ing rate <ref type="bibr">([0.5, 0.8, 1, 2, 5, 10, 20]</ref>), window size ( <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20]</ref>), the number of negative samples ( <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">10]</ref>), the number of epochs ( <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>) and the subsampling rate ( <ref type="bibr">[500,</ref><ref type="bibr">1000,</ref><ref type="bibr">10000]</ref>). Here, given the size of the data, the minimum frequency for a word to be considered is 1. The best per- formance is obtained for a window of 15 words, 3 negative samples, a learning rate of 1, a subsam- pling rate of 10000, an exponential decay where λ = 1 70 , and one single epoch (that is, the system truly implements fast-mapping). When applied to the test set, N2V shows a dramatic improvement in performance over the simple sum model, reaching M M R = 0.04907 (median rank 623).</p><p>On the training set of the Chimeras, we fur- ther tune the per-sentence decrease in window size and increase in subsampling. For the window size, we experiment with a reduction of [1...6] words on either side of the target, not going un- der a window of ±3 words. Further, we adjust each word's subsampling rate by a factor in the range [1.1, 1.2...1.9, 2.0]. Our results confirm that indeed, an appropriate change in those parame- ters is required: keeping them constant results in decreasing performance as more sentences are introduced. On the training set, we obtain our best performance (averaged over the 2-, 4-and 6- sentences datasets) for a per-sentence window size decrease of 5 words on either side of the target, and adjusting subsampling by a factor of 1.9. <ref type="table">Ta- ble 2</ref> shows results on the three corresponding test sets using those parameters. Unfortunately, on this dataset, N2V does not improve on addition.</p><p>The difference in performance between the def- initional and the Chimeras datasets may be ex- plained in two ways. First, the chimera sentences were randomly selected and thus, are not neces- sarily hugely informative about the nature of the nonce. Second, the most informative sentences are not necessarily at the beginning of the fragment, so the system heightens its learning rate on the wrong data: the risk does not pay off. This suggests that a truly intelligent system should adjust its param- eters in a non-monotonic way, to take into account the quality of the information it is processing. This point seems to be an important general require- ment for any architecture that claims incremental- ity: our results indicate very strongly that a notion of informativeness must play a role in the learn- ing decisions of the system. This conclusion is in line with work in other domains, e.g. interactive word learning using dialogue, where performance is linked to the ability of the system to measure its own confidence in particular pieces of knowledge and ask questions with a high information gain ( <ref type="bibr" target="#b19">Yu et al., 2016)</ref>. It also meets with general considera- tions on language acquisition, which accounts for the ability of young children to learn from limited 'primary linguistic data' by restricting explanatory models to those that provide such efficiency <ref type="bibr" target="#b3">(Clark and Lappin, 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed Nonce2Vec, a Word2Vec- inspired architecture to learn new words from tiny data. It requires a high-risk strategy combining heightened learning rate and greedy processing of the context. The particularly good performance of the system on definitions makes us confident that it is possible to build a unique, unified algorithm for learning word meaning from any amount of data. However, the less impressive performance on naturally-occurring sentences indicates that an ideal system should modulate its learning as a function of the informativeness of a context sen- tence, that is, take risks 'at the right time'.</p><p>As pointed out in the introduction, Nonce2Vec is designed with a view to be an essential com- ponent of an incremental concept learning archi- tecture. In order to validate our system as a suit- able, generic solution for word learning, we will have to test it on various data sizes, from the type of low-to middle-frequency terms found in e.g. the Rare Words dataset ( <ref type="bibr" target="#b13">Luong et al., 2013)</ref>, to highly frequent words. We would like to system- atically evaluate, in particular, how fast the sys- tem can gain an understanding of a concept which is fully equivalent to a vector built from big data. We believe that both quality and speed of learning will be strongly influenced by the ability of the al- gorithm to detect what we called informative sen- tences. Our future work will thus investigate how to capture and measure informativeness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Results on definitional dataset</head><label>1</label><figDesc></figDesc><table>L2 ρ 
L4 ρ 
L6 ρ 
W2V 0.1459 0.2457 0.2498 
Sum 0.3376 0.3624 0.4080 
N2V 0.3320 0.3668 0.3890 

</table></figure>

			<note place="foot" n="1"> Data available at http://aurelieherbelot. net/resources/. 2 https://spacy.io/</note>

			<note place="foot" n="3"> Available at http://clic.cimec.unitn.it/ Files/PublicData/chimeras.zip.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Katrin Erk for inspiring con-versations about tiny data and fast-mapping, and to Raffaella Bernardi and Sandro Pezzelle for comments on an early draft of this paper. We also thank the anonymous reviewers for their time and valuable comments. We acknowledge ERC 2011 Starting Independent Research Grant No 283554 (COMPOSES). This project has also re-ceived funding from the European Union's Hori-zon 2020 research and innovation programme un-der the Marie Skłodowska-Curie grant agreement No 751250.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The WaCky wide web: a collection of very large linguistically processed 308 web-crawled corpora. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computational learning theory and language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophy of linguistics</title>
		<editor>Ruth M Kempson, Tim Fernando, and Nicholas Asher</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="445" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vector space models of lexical meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Contemporary Semantics-second edition</title>
		<editor>Shalom Lappin and Chris Fox</editor>
		<imprint>
			<publisher>Wiley-Blackwell</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vector space models of word meaning and phrase meaning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="635" to="653" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL2012)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Obtaining a Better Understanding of Distributional Models of German Derivational Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kisselew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Snajder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics (IWCS2015)</title>
		<meeting>the 11th International Conference on Computational Semantics (IWCS2015)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Meeting of the Cognitive Science Society (CogSci2012)</title>
		<meeting>the 33rd Annual Meeting of the Cognitive Science Society (CogSci2012)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tomer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
		<idno>arxiv, abs/1604.00289</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal word meaning induction from minimal exposure to natural text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">S4</biblScope>
			<biblScope unit="page" from="677" to="705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1517" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Better Word Representations with Recursive Neural Networks for Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Computational Natural Language Learning (CoNLL2013)</title>
		<meeting>the 17th Conference on Computational Natural Language Learning (CoNLL2013)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predictability of distributional semantics in derivational word formation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kisselew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Snajder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING2016)</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING2016)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-Negative Randomized Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrang</forename><surname>Qasemizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Kallmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Traitement automatique des langues naturelles (TALN2017)</title>
		<meeting>Traitement automatique des langues naturelles (TALN2017)<address><addrLine>Orléans, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Propose but verify: Fast mapping meets cross-situational word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">Nicol</forename><surname>John C Trueswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lila</forename><forename type="middle">R</forename><surname>Hafri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gleitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="126" to="156" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training an adaptive dialogue policy for interactive learning of visually grounded word meanings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL2016)</title>
		<meeting>the 17th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL2016)<address><addrLine>Los Angeles,CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
